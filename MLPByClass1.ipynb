{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs4KizaLSJyjOvRxoUfTFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vajihe-ameri/predict-software-bugs-in-java-classes/blob/main/MLPByClass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7jHTbeotRQLV",
        "outputId": "2bd6929e-0fc7-4f5c-f246-138cb6161671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2952 sha256=5398fead4558a6fab9ac791300fd50560d98dad213dba509b43a30203487a139\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn pandas\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_1.csv\")\n",
        "train_features = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_1.csv\")\n",
        "test_target = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_NB_1.csv\")\n",
        "train_target = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_NB_1.csv\")"
      ],
      "metadata": {
        "id": "k-NJh-s4RVqK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons\n",
        "f_measure = tensorflow_addons.metrics.F1Score(num_classes=2, average='macro', threshold=0.5)"
      ],
      "metadata": {
        "id": "-MTCXh0lRWCF",
        "outputId": "6918dbca-e8de-45e4-898b-6ee2655a8768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/612.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***import requirements***#\n",
        "#-------------------------#\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "file_path = \"/content/drive/MyDrive/new_df/best_model_by_class1.hdf5\""
      ],
      "metadata": {
        "id": "ttn14_jcRWZh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#***Build Model***#\n",
        "#-----------------#\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu', input_dim = train_features.shape[1]))\n",
        "model.add(Dense(80, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(60, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(40, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(20, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "feauoHLTRWcM",
        "outputId": "61c2cb5e-fd40-4ae4-e2aa-101fbc97fba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               8500      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 80)                8080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 60)                4860      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 40)                2440      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                820       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,721\n",
            "Trainable params: 24,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 8224, epochs = 10000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_2', 'class 1']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFZoAhH6RWfE",
        "outputId": "f15a8734-8c92-4b20-8ee7-03fb3c70c478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3630 - accuracy: 0.8208\n",
            "Epoch 8752: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3644 - accuracy: 0.8183 - val_loss: 0.3874 - val_accuracy: 0.7696\n",
            "Epoch 8753/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4209 - accuracy: 0.7858\n",
            "Epoch 8753: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4166 - accuracy: 0.7888 - val_loss: 0.4859 - val_accuracy: 0.7145\n",
            "Epoch 8754/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3725 - accuracy: 0.8130\n",
            "Epoch 8754: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3752 - accuracy: 0.8118 - val_loss: 0.6709 - val_accuracy: 0.5719\n",
            "Epoch 8755/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3651 - accuracy: 0.8135\n",
            "Epoch 8755: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3646 - accuracy: 0.8142 - val_loss: 0.6672 - val_accuracy: 0.5738\n",
            "Epoch 8756/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8158\n",
            "Epoch 8756: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3639 - accuracy: 0.8161 - val_loss: 0.6177 - val_accuracy: 0.6118\n",
            "Epoch 8757/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3617 - accuracy: 0.8180\n",
            "Epoch 8757: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3616 - accuracy: 0.8185 - val_loss: 0.7331 - val_accuracy: 0.5449\n",
            "Epoch 8758/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3780 - accuracy: 0.8064\n",
            "Epoch 8758: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3776 - accuracy: 0.8061 - val_loss: 0.6194 - val_accuracy: 0.6144\n",
            "Epoch 8759/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3674 - accuracy: 0.8112\n",
            "Epoch 8759: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3673 - accuracy: 0.8111 - val_loss: 0.5444 - val_accuracy: 0.6765\n",
            "Epoch 8760/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8203\n",
            "Epoch 8760: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3628 - accuracy: 0.8184 - val_loss: 0.3878 - val_accuracy: 0.7836\n",
            "Epoch 8761/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4072 - accuracy: 0.7886\n",
            "Epoch 8761: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.4032 - accuracy: 0.7905 - val_loss: 0.4189 - val_accuracy: 0.7560\n",
            "Epoch 8762/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4217 - accuracy: 0.7847\n",
            "Epoch 8762: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4199 - accuracy: 0.7852 - val_loss: 0.5438 - val_accuracy: 0.6693\n",
            "Epoch 8763/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3805 - accuracy: 0.8016\n",
            "Epoch 8763: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3847 - accuracy: 0.8010 - val_loss: 0.7439 - val_accuracy: 0.5475\n",
            "Epoch 8764/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8076\n",
            "Epoch 8764: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3767 - accuracy: 0.8070 - val_loss: 0.9140 - val_accuracy: 0.4610\n",
            "Epoch 8765/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4039 - accuracy: 0.7947\n",
            "Epoch 8765: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3996 - accuracy: 0.7968 - val_loss: 0.8850 - val_accuracy: 0.4847\n",
            "Epoch 8766/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4528 - accuracy: 0.7657\n",
            "Epoch 8766: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4456 - accuracy: 0.7690 - val_loss: 0.7112 - val_accuracy: 0.5504\n",
            "Epoch 8767/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4226 - accuracy: 0.7806\n",
            "Epoch 8767: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.4259 - accuracy: 0.7796 - val_loss: 0.4778 - val_accuracy: 0.7222\n",
            "Epoch 8768/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3776 - accuracy: 0.8095\n",
            "Epoch 8768: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3830 - accuracy: 0.8063 - val_loss: 0.5044 - val_accuracy: 0.6928\n",
            "Epoch 8769/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8129\n",
            "Epoch 8769: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3806 - accuracy: 0.8113 - val_loss: 0.4343 - val_accuracy: 0.7507\n",
            "Epoch 8770/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3771 - accuracy: 0.8068\n",
            "Epoch 8770: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3819 - accuracy: 0.8038 - val_loss: 0.4785 - val_accuracy: 0.7121\n",
            "Epoch 8771/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3856 - accuracy: 0.8086\n",
            "Epoch 8771: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3854 - accuracy: 0.8073 - val_loss: 0.4617 - val_accuracy: 0.7358\n",
            "Epoch 8772/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3734 - accuracy: 0.8119\n",
            "Epoch 8772: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3742 - accuracy: 0.8097 - val_loss: 0.4999 - val_accuracy: 0.7022\n",
            "Epoch 8773/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3692 - accuracy: 0.8130\n",
            "Epoch 8773: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3727 - accuracy: 0.8106 - val_loss: 0.4153 - val_accuracy: 0.7621\n",
            "Epoch 8774/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3759 - accuracy: 0.8089\n",
            "Epoch 8774: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3743 - accuracy: 0.8095 - val_loss: 0.4765 - val_accuracy: 0.7123\n",
            "Epoch 8775/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3703 - accuracy: 0.8129\n",
            "Epoch 8775: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3680 - accuracy: 0.8153 - val_loss: 0.4978 - val_accuracy: 0.7038\n",
            "Epoch 8776/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3637 - accuracy: 0.8193\n",
            "Epoch 8776: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3623 - accuracy: 0.8188 - val_loss: 0.5159 - val_accuracy: 0.6827\n",
            "Epoch 8777/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8145\n",
            "Epoch 8777: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3650 - accuracy: 0.8147 - val_loss: 0.5112 - val_accuracy: 0.6906\n",
            "Epoch 8778/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3736 - accuracy: 0.8111\n",
            "Epoch 8778: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3734 - accuracy: 0.8114 - val_loss: 0.5125 - val_accuracy: 0.6945\n",
            "Epoch 8779/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3735 - accuracy: 0.8078\n",
            "Epoch 8779: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3741 - accuracy: 0.8075 - val_loss: 0.6905 - val_accuracy: 0.5659\n",
            "Epoch 8780/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8135\n",
            "Epoch 8780: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3692 - accuracy: 0.8132 - val_loss: 0.6340 - val_accuracy: 0.6015\n",
            "Epoch 8781/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3667 - accuracy: 0.8113\n",
            "Epoch 8781: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3651 - accuracy: 0.8117 - val_loss: 0.5917 - val_accuracy: 0.6327\n",
            "Epoch 8782/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3620 - accuracy: 0.8176\n",
            "Epoch 8782: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3606 - accuracy: 0.8192 - val_loss: 0.4907 - val_accuracy: 0.7077\n",
            "Epoch 8783/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3558 - accuracy: 0.8239\n",
            "Epoch 8783: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3550 - accuracy: 0.8238 - val_loss: 0.4522 - val_accuracy: 0.7415\n",
            "Epoch 8784/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3673 - accuracy: 0.8151\n",
            "Epoch 8784: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3649 - accuracy: 0.8161 - val_loss: 0.4339 - val_accuracy: 0.7551\n",
            "Epoch 8785/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8090\n",
            "Epoch 8785: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3711 - accuracy: 0.8108 - val_loss: 0.5061 - val_accuracy: 0.7016\n",
            "Epoch 8786/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3684 - accuracy: 0.8109\n",
            "Epoch 8786: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3701 - accuracy: 0.8104 - val_loss: 0.6290 - val_accuracy: 0.6000\n",
            "Epoch 8787/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8199\n",
            "Epoch 8787: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3583 - accuracy: 0.8190 - val_loss: 0.7064 - val_accuracy: 0.5598\n",
            "Epoch 8788/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3700 - accuracy: 0.8112\n",
            "Epoch 8788: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3701 - accuracy: 0.8113 - val_loss: 0.6176 - val_accuracy: 0.6076\n",
            "Epoch 8789/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3645 - accuracy: 0.8175\n",
            "Epoch 8789: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3640 - accuracy: 0.8174 - val_loss: 0.4796 - val_accuracy: 0.7165\n",
            "Epoch 8790/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8197\n",
            "Epoch 8790: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3583 - accuracy: 0.8196 - val_loss: 0.5005 - val_accuracy: 0.7084\n",
            "Epoch 8791/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3613 - accuracy: 0.8192\n",
            "Epoch 8791: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3613 - accuracy: 0.8192 - val_loss: 0.6207 - val_accuracy: 0.6111\n",
            "Epoch 8792/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8185\n",
            "Epoch 8792: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3601 - accuracy: 0.8191 - val_loss: 0.5472 - val_accuracy: 0.6684\n",
            "Epoch 8793/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3600 - accuracy: 0.8209\n",
            "Epoch 8793: loss did not improve from 0.35460\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3604 - accuracy: 0.8195 - val_loss: 0.5018 - val_accuracy: 0.7002\n",
            "Epoch 8794/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3540 - accuracy: 0.8223\n",
            "Epoch 8794: loss improved from 0.35460 to 0.35460, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 1s 253ms/step - loss: 0.3546 - accuracy: 0.8210 - val_loss: 0.5256 - val_accuracy: 0.6798\n",
            "Epoch 8795/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8231\n",
            "Epoch 8795: loss improved from 0.35460 to 0.35449, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.3545 - accuracy: 0.8236 - val_loss: 0.5209 - val_accuracy: 0.6829\n",
            "Epoch 8796/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3535 - accuracy: 0.8222\n",
            "Epoch 8796: loss improved from 0.35449 to 0.35394, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.3539 - accuracy: 0.8225 - val_loss: 0.5328 - val_accuracy: 0.6794\n",
            "Epoch 8797/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8211\n",
            "Epoch 8797: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3546 - accuracy: 0.8199 - val_loss: 0.5842 - val_accuracy: 0.6331\n",
            "Epoch 8798/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3549 - accuracy: 0.8213\n",
            "Epoch 8798: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3567 - accuracy: 0.8191 - val_loss: 0.6002 - val_accuracy: 0.6230\n",
            "Epoch 8799/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3541 - accuracy: 0.8222\n",
            "Epoch 8799: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3546 - accuracy: 0.8215 - val_loss: 0.5734 - val_accuracy: 0.6397\n",
            "Epoch 8800/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8188\n",
            "Epoch 8800: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3557 - accuracy: 0.8200 - val_loss: 0.6117 - val_accuracy: 0.6103\n",
            "Epoch 8801/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3579 - accuracy: 0.8199\n",
            "Epoch 8801: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3587 - accuracy: 0.8194 - val_loss: 0.4656 - val_accuracy: 0.7259\n",
            "Epoch 8802/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8169\n",
            "Epoch 8802: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3616 - accuracy: 0.8173 - val_loss: 0.5312 - val_accuracy: 0.6792\n",
            "Epoch 8803/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8212\n",
            "Epoch 8803: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3576 - accuracy: 0.8210 - val_loss: 0.6286 - val_accuracy: 0.5951\n",
            "Epoch 8804/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8151\n",
            "Epoch 8804: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3598 - accuracy: 0.8162 - val_loss: 0.4999 - val_accuracy: 0.7046\n",
            "Epoch 8805/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8242\n",
            "Epoch 8805: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3560 - accuracy: 0.8237 - val_loss: 0.5062 - val_accuracy: 0.6996\n",
            "Epoch 8806/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3582 - accuracy: 0.8204\n",
            "Epoch 8806: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3586 - accuracy: 0.8201 - val_loss: 0.6256 - val_accuracy: 0.5995\n",
            "Epoch 8807/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3629 - accuracy: 0.8157\n",
            "Epoch 8807: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3649 - accuracy: 0.8146 - val_loss: 0.4382 - val_accuracy: 0.7400\n",
            "Epoch 8808/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3841 - accuracy: 0.8023\n",
            "Epoch 8808: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3842 - accuracy: 0.8029 - val_loss: 0.5819 - val_accuracy: 0.6443\n",
            "Epoch 8809/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8199\n",
            "Epoch 8809: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3604 - accuracy: 0.8184 - val_loss: 0.8161 - val_accuracy: 0.5045\n",
            "Epoch 8810/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4150 - accuracy: 0.7836\n",
            "Epoch 8810: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.4136 - accuracy: 0.7843 - val_loss: 0.5147 - val_accuracy: 0.6901\n",
            "Epoch 8811/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8103\n",
            "Epoch 8811: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3762 - accuracy: 0.8075 - val_loss: 0.3978 - val_accuracy: 0.7654\n",
            "Epoch 8812/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4055 - accuracy: 0.7890\n",
            "Epoch 8812: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4002 - accuracy: 0.7924 - val_loss: 0.4281 - val_accuracy: 0.7540\n",
            "Epoch 8813/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4163 - accuracy: 0.7863\n",
            "Epoch 8813: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4143 - accuracy: 0.7874 - val_loss: 0.5705 - val_accuracy: 0.6467\n",
            "Epoch 8814/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3723 - accuracy: 0.8085\n",
            "Epoch 8814: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3782 - accuracy: 0.8048 - val_loss: 0.8203 - val_accuracy: 0.5106\n",
            "Epoch 8815/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3872 - accuracy: 0.8002\n",
            "Epoch 8815: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3862 - accuracy: 0.8014 - val_loss: 0.7753 - val_accuracy: 0.5383\n",
            "Epoch 8816/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4147 - accuracy: 0.7828\n",
            "Epoch 8816: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.4103 - accuracy: 0.7861 - val_loss: 0.7077 - val_accuracy: 0.5620\n",
            "Epoch 8817/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4113 - accuracy: 0.7876\n",
            "Epoch 8817: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4128 - accuracy: 0.7867 - val_loss: 0.4930 - val_accuracy: 0.7081\n",
            "Epoch 8818/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3834 - accuracy: 0.8042\n",
            "Epoch 8818: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3899 - accuracy: 0.7996 - val_loss: 0.4537 - val_accuracy: 0.7327\n",
            "Epoch 8819/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3821 - accuracy: 0.8098\n",
            "Epoch 8819: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3838 - accuracy: 0.8074 - val_loss: 0.4341 - val_accuracy: 0.7450\n",
            "Epoch 8820/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3807 - accuracy: 0.8093\n",
            "Epoch 8820: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3826 - accuracy: 0.8069 - val_loss: 0.4071 - val_accuracy: 0.7615\n",
            "Epoch 8821/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3874 - accuracy: 0.8016\n",
            "Epoch 8821: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3874 - accuracy: 0.8022 - val_loss: 0.4079 - val_accuracy: 0.7663\n",
            "Epoch 8822/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3870 - accuracy: 0.8011\n",
            "Epoch 8822: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3859 - accuracy: 0.8016 - val_loss: 0.5790 - val_accuracy: 0.6364\n",
            "Epoch 8823/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3594 - accuracy: 0.8174\n",
            "Epoch 8823: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3603 - accuracy: 0.8170 - val_loss: 0.4825 - val_accuracy: 0.7130\n",
            "Epoch 8824/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8174\n",
            "Epoch 8824: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3585 - accuracy: 0.8173 - val_loss: 0.4973 - val_accuracy: 0.7018\n",
            "Epoch 8825/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3698 - accuracy: 0.8118\n",
            "Epoch 8825: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3679 - accuracy: 0.8139 - val_loss: 0.5235 - val_accuracy: 0.6811\n",
            "Epoch 8826/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3633 - accuracy: 0.8176\n",
            "Epoch 8826: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3610 - accuracy: 0.8186 - val_loss: 0.5202 - val_accuracy: 0.6814\n",
            "Epoch 8827/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3691 - accuracy: 0.8120\n",
            "Epoch 8827: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3717 - accuracy: 0.8102 - val_loss: 0.6772 - val_accuracy: 0.5668\n",
            "Epoch 8828/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3667 - accuracy: 0.8130\n",
            "Epoch 8828: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3676 - accuracy: 0.8119 - val_loss: 0.7040 - val_accuracy: 0.5591\n",
            "Epoch 8829/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3762 - accuracy: 0.8087\n",
            "Epoch 8829: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3746 - accuracy: 0.8091 - val_loss: 0.6530 - val_accuracy: 0.5831\n",
            "Epoch 8830/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3765 - accuracy: 0.8060\n",
            "Epoch 8830: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3771 - accuracy: 0.8057 - val_loss: 0.5615 - val_accuracy: 0.6566\n",
            "Epoch 8831/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3689 - accuracy: 0.8146\n",
            "Epoch 8831: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3701 - accuracy: 0.8137 - val_loss: 0.5330 - val_accuracy: 0.6706\n",
            "Epoch 8832/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3681 - accuracy: 0.8151\n",
            "Epoch 8832: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3709 - accuracy: 0.8125 - val_loss: 0.4420 - val_accuracy: 0.7505\n",
            "Epoch 8833/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3636 - accuracy: 0.8177\n",
            "Epoch 8833: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3647 - accuracy: 0.8168 - val_loss: 0.4391 - val_accuracy: 0.7501\n",
            "Epoch 8834/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3749 - accuracy: 0.8089\n",
            "Epoch 8834: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3747 - accuracy: 0.8084 - val_loss: 0.5946 - val_accuracy: 0.6267\n",
            "Epoch 8835/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3602 - accuracy: 0.8192\n",
            "Epoch 8835: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3592 - accuracy: 0.8204 - val_loss: 0.5744 - val_accuracy: 0.6425\n",
            "Epoch 8836/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8219\n",
            "Epoch 8836: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3583 - accuracy: 0.8215 - val_loss: 0.5368 - val_accuracy: 0.6730\n",
            "Epoch 8837/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8217\n",
            "Epoch 8837: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3574 - accuracy: 0.8206 - val_loss: 0.5104 - val_accuracy: 0.7022\n",
            "Epoch 8838/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8195\n",
            "Epoch 8838: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3597 - accuracy: 0.8203 - val_loss: 0.5966 - val_accuracy: 0.6333\n",
            "Epoch 8839/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3598 - accuracy: 0.8172\n",
            "Epoch 8839: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3590 - accuracy: 0.8187 - val_loss: 0.5894 - val_accuracy: 0.6338\n",
            "Epoch 8840/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8226\n",
            "Epoch 8840: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3549 - accuracy: 0.8226 - val_loss: 0.4701 - val_accuracy: 0.7200\n",
            "Epoch 8841/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3587 - accuracy: 0.8195\n",
            "Epoch 8841: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3579 - accuracy: 0.8197 - val_loss: 0.5777 - val_accuracy: 0.6381\n",
            "Epoch 8842/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8216\n",
            "Epoch 8842: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3551 - accuracy: 0.8213 - val_loss: 0.5934 - val_accuracy: 0.6239\n",
            "Epoch 8843/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3649 - accuracy: 0.8149\n",
            "Epoch 8843: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3691 - accuracy: 0.8125 - val_loss: 0.3918 - val_accuracy: 0.7705\n",
            "Epoch 8844/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4044 - accuracy: 0.7904\n",
            "Epoch 8844: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3992 - accuracy: 0.7947 - val_loss: 0.4105 - val_accuracy: 0.7645\n",
            "Epoch 8845/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4132 - accuracy: 0.7850\n",
            "Epoch 8845: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4159 - accuracy: 0.7836 - val_loss: 0.7956 - val_accuracy: 0.5082\n",
            "Epoch 8846/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8054\n",
            "Epoch 8846: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3784 - accuracy: 0.8054 - val_loss: 0.7597 - val_accuracy: 0.5427\n",
            "Epoch 8847/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4026 - accuracy: 0.7875\n",
            "Epoch 8847: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3980 - accuracy: 0.7909 - val_loss: 0.6302 - val_accuracy: 0.5962\n",
            "Epoch 8848/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8087\n",
            "Epoch 8848: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3766 - accuracy: 0.8076 - val_loss: 0.4975 - val_accuracy: 0.7031\n",
            "Epoch 8849/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8151\n",
            "Epoch 8849: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3676 - accuracy: 0.8139 - val_loss: 0.5216 - val_accuracy: 0.6811\n",
            "Epoch 8850/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3638 - accuracy: 0.8197\n",
            "Epoch 8850: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3658 - accuracy: 0.8173 - val_loss: 0.4292 - val_accuracy: 0.7509\n",
            "Epoch 8851/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3803 - accuracy: 0.8075\n",
            "Epoch 8851: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3773 - accuracy: 0.8092 - val_loss: 0.5451 - val_accuracy: 0.6693\n",
            "Epoch 8852/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3599 - accuracy: 0.8216\n",
            "Epoch 8852: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3592 - accuracy: 0.8219 - val_loss: 0.5761 - val_accuracy: 0.6309\n",
            "Epoch 8853/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3598 - accuracy: 0.8205\n",
            "Epoch 8853: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3617 - accuracy: 0.8189 - val_loss: 0.4542 - val_accuracy: 0.7334\n",
            "Epoch 8854/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3655 - accuracy: 0.8149\n",
            "Epoch 8854: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3652 - accuracy: 0.8154 - val_loss: 0.4556 - val_accuracy: 0.7373\n",
            "Epoch 8855/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3813 - accuracy: 0.8036\n",
            "Epoch 8855: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3811 - accuracy: 0.8037 - val_loss: 0.6197 - val_accuracy: 0.6101\n",
            "Epoch 8856/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3597 - accuracy: 0.8174\n",
            "Epoch 8856: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3604 - accuracy: 0.8164 - val_loss: 0.5527 - val_accuracy: 0.6579\n",
            "Epoch 8857/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8227\n",
            "Epoch 8857: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3571 - accuracy: 0.8210 - val_loss: 0.4928 - val_accuracy: 0.7064\n",
            "Epoch 8858/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8210\n",
            "Epoch 8858: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3585 - accuracy: 0.8190 - val_loss: 0.5043 - val_accuracy: 0.6965\n",
            "Epoch 8859/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8182\n",
            "Epoch 8859: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3581 - accuracy: 0.8188 - val_loss: 0.5621 - val_accuracy: 0.6456\n",
            "Epoch 8860/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3551 - accuracy: 0.8214\n",
            "Epoch 8860: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3557 - accuracy: 0.8210 - val_loss: 0.5388 - val_accuracy: 0.6713\n",
            "Epoch 8861/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3568 - accuracy: 0.8203\n",
            "Epoch 8861: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3564 - accuracy: 0.8207 - val_loss: 0.4784 - val_accuracy: 0.7174\n",
            "Epoch 8862/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8175\n",
            "Epoch 8862: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3594 - accuracy: 0.8167 - val_loss: 0.5558 - val_accuracy: 0.6557\n",
            "Epoch 8863/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3566 - accuracy: 0.8200\n",
            "Epoch 8863: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3588 - accuracy: 0.8188 - val_loss: 0.6325 - val_accuracy: 0.6054\n",
            "Epoch 8864/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3680 - accuracy: 0.8134\n",
            "Epoch 8864: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3676 - accuracy: 0.8138 - val_loss: 0.6488 - val_accuracy: 0.5872\n",
            "Epoch 8865/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3930 - accuracy: 0.8022\n",
            "Epoch 8865: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3976 - accuracy: 0.7988 - val_loss: 0.4296 - val_accuracy: 0.7470\n",
            "Epoch 8866/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3810 - accuracy: 0.8091\n",
            "Epoch 8866: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3800 - accuracy: 0.8084 - val_loss: 0.4517 - val_accuracy: 0.7389\n",
            "Epoch 8867/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3781 - accuracy: 0.8067\n",
            "Epoch 8867: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3768 - accuracy: 0.8087 - val_loss: 0.5797 - val_accuracy: 0.6381\n",
            "Epoch 8868/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3635 - accuracy: 0.8138\n",
            "Epoch 8868: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3660 - accuracy: 0.8126 - val_loss: 0.7448 - val_accuracy: 0.5350\n",
            "Epoch 8869/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3841 - accuracy: 0.7994\n",
            "Epoch 8869: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3824 - accuracy: 0.8009 - val_loss: 0.5075 - val_accuracy: 0.6954\n",
            "Epoch 8870/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8187\n",
            "Epoch 8870: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3578 - accuracy: 0.8185 - val_loss: 0.5123 - val_accuracy: 0.6895\n",
            "Epoch 8871/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8201\n",
            "Epoch 8871: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3585 - accuracy: 0.8202 - val_loss: 0.6323 - val_accuracy: 0.5980\n",
            "Epoch 8872/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8203\n",
            "Epoch 8872: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3584 - accuracy: 0.8198 - val_loss: 0.6563 - val_accuracy: 0.5839\n",
            "Epoch 8873/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3785 - accuracy: 0.8088\n",
            "Epoch 8873: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3776 - accuracy: 0.8092 - val_loss: 0.5467 - val_accuracy: 0.6616\n",
            "Epoch 8874/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8206\n",
            "Epoch 8874: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3609 - accuracy: 0.8193 - val_loss: 0.4868 - val_accuracy: 0.7070\n",
            "Epoch 8875/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8213\n",
            "Epoch 8875: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 0.3586 - accuracy: 0.8202 - val_loss: 0.4984 - val_accuracy: 0.7073\n",
            "Epoch 8876/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3599 - accuracy: 0.8198\n",
            "Epoch 8876: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3587 - accuracy: 0.8198 - val_loss: 0.4848 - val_accuracy: 0.7125\n",
            "Epoch 8877/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8207\n",
            "Epoch 8877: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3560 - accuracy: 0.8207 - val_loss: 0.4734 - val_accuracy: 0.7193\n",
            "Epoch 8878/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3830 - accuracy: 0.8048\n",
            "Epoch 8878: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3828 - accuracy: 0.8052 - val_loss: 0.6564 - val_accuracy: 0.5901\n",
            "Epoch 8879/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3634 - accuracy: 0.8169\n",
            "Epoch 8879: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3656 - accuracy: 0.8152 - val_loss: 0.9745 - val_accuracy: 0.4376\n",
            "Epoch 8880/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4549 - accuracy: 0.7665\n",
            "Epoch 8880: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4506 - accuracy: 0.7689 - val_loss: 0.4890 - val_accuracy: 0.7053\n",
            "Epoch 8881/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3688 - accuracy: 0.8113\n",
            "Epoch 8881: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3721 - accuracy: 0.8088 - val_loss: 0.4293 - val_accuracy: 0.7522\n",
            "Epoch 8882/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3860 - accuracy: 0.8034\n",
            "Epoch 8882: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3834 - accuracy: 0.8044 - val_loss: 0.4641 - val_accuracy: 0.7255\n",
            "Epoch 8883/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3672 - accuracy: 0.8135\n",
            "Epoch 8883: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3674 - accuracy: 0.8129 - val_loss: 0.5023 - val_accuracy: 0.6948\n",
            "Epoch 8884/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3651 - accuracy: 0.8151\n",
            "Epoch 8884: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3669 - accuracy: 0.8144 - val_loss: 0.6651 - val_accuracy: 0.5899\n",
            "Epoch 8885/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3694 - accuracy: 0.8093\n",
            "Epoch 8885: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3681 - accuracy: 0.8110 - val_loss: 0.5548 - val_accuracy: 0.6496\n",
            "Epoch 8886/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8216\n",
            "Epoch 8886: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3564 - accuracy: 0.8223 - val_loss: 0.4944 - val_accuracy: 0.7066\n",
            "Epoch 8887/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3582 - accuracy: 0.8194\n",
            "Epoch 8887: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3579 - accuracy: 0.8208 - val_loss: 0.4684 - val_accuracy: 0.7257\n",
            "Epoch 8888/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3622 - accuracy: 0.8192\n",
            "Epoch 8888: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3618 - accuracy: 0.8190 - val_loss: 0.5645 - val_accuracy: 0.6485\n",
            "Epoch 8889/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8205\n",
            "Epoch 8889: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3576 - accuracy: 0.8211 - val_loss: 0.5898 - val_accuracy: 0.6298\n",
            "Epoch 8890/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8224\n",
            "Epoch 8890: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3568 - accuracy: 0.8210 - val_loss: 0.5272 - val_accuracy: 0.6774\n",
            "Epoch 8891/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8161\n",
            "Epoch 8891: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3630 - accuracy: 0.8154 - val_loss: 0.7086 - val_accuracy: 0.5545\n",
            "Epoch 8892/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3889 - accuracy: 0.7978\n",
            "Epoch 8892: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3857 - accuracy: 0.8004 - val_loss: 0.5466 - val_accuracy: 0.6614\n",
            "Epoch 8893/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3627 - accuracy: 0.8183\n",
            "Epoch 8893: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3650 - accuracy: 0.8171 - val_loss: 0.4071 - val_accuracy: 0.7724\n",
            "Epoch 8894/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3820 - accuracy: 0.8037\n",
            "Epoch 8894: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3803 - accuracy: 0.8048 - val_loss: 0.4991 - val_accuracy: 0.7018\n",
            "Epoch 8895/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8186\n",
            "Epoch 8895: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3597 - accuracy: 0.8188 - val_loss: 0.6127 - val_accuracy: 0.6101\n",
            "Epoch 8896/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8203\n",
            "Epoch 8896: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3568 - accuracy: 0.8203 - val_loss: 0.6290 - val_accuracy: 0.6043\n",
            "Epoch 8897/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3674 - accuracy: 0.8099\n",
            "Epoch 8897: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3662 - accuracy: 0.8104 - val_loss: 0.5334 - val_accuracy: 0.6724\n",
            "Epoch 8898/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3544 - accuracy: 0.8226\n",
            "Epoch 8898: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3566 - accuracy: 0.8205 - val_loss: 0.4686 - val_accuracy: 0.7233\n",
            "Epoch 8899/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3804 - accuracy: 0.8008\n",
            "Epoch 8899: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3837 - accuracy: 0.7999 - val_loss: 0.7718 - val_accuracy: 0.5179\n",
            "Epoch 8900/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3709 - accuracy: 0.8085\n",
            "Epoch 8900: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3723 - accuracy: 0.8078 - val_loss: 0.9316 - val_accuracy: 0.4624\n",
            "Epoch 8901/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4359 - accuracy: 0.7751\n",
            "Epoch 8901: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4289 - accuracy: 0.7798 - val_loss: 0.6986 - val_accuracy: 0.5622\n",
            "Epoch 8902/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4053 - accuracy: 0.7906\n",
            "Epoch 8902: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4094 - accuracy: 0.7874 - val_loss: 0.4340 - val_accuracy: 0.7512\n",
            "Epoch 8903/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3738 - accuracy: 0.8071\n",
            "Epoch 8903: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3752 - accuracy: 0.8072 - val_loss: 0.4825 - val_accuracy: 0.7132\n",
            "Epoch 8904/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8127\n",
            "Epoch 8904: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3710 - accuracy: 0.8117 - val_loss: 0.4325 - val_accuracy: 0.7527\n",
            "Epoch 8905/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3894 - accuracy: 0.7999\n",
            "Epoch 8905: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3880 - accuracy: 0.8013 - val_loss: 0.5518 - val_accuracy: 0.6638\n",
            "Epoch 8906/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8143\n",
            "Epoch 8906: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3645 - accuracy: 0.8150 - val_loss: 0.5821 - val_accuracy: 0.6333\n",
            "Epoch 8907/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8189\n",
            "Epoch 8907: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3616 - accuracy: 0.8161 - val_loss: 0.7255 - val_accuracy: 0.5523\n",
            "Epoch 8908/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3850 - accuracy: 0.8014\n",
            "Epoch 8908: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3828 - accuracy: 0.8022 - val_loss: 0.4892 - val_accuracy: 0.7123\n",
            "Epoch 8909/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8199\n",
            "Epoch 8909: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3593 - accuracy: 0.8191 - val_loss: 0.5119 - val_accuracy: 0.6934\n",
            "Epoch 8910/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8222\n",
            "Epoch 8910: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3576 - accuracy: 0.8213 - val_loss: 0.4761 - val_accuracy: 0.7167\n",
            "Epoch 8911/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8199\n",
            "Epoch 8911: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3598 - accuracy: 0.8190 - val_loss: 0.4708 - val_accuracy: 0.7255\n",
            "Epoch 8912/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3812 - accuracy: 0.8025\n",
            "Epoch 8912: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3850 - accuracy: 0.7999 - val_loss: 0.7689 - val_accuracy: 0.5203\n",
            "Epoch 8913/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3805 - accuracy: 0.8033\n",
            "Epoch 8913: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3780 - accuracy: 0.8052 - val_loss: 0.8088 - val_accuracy: 0.5058\n",
            "Epoch 8914/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4144 - accuracy: 0.7862\n",
            "Epoch 8914: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4115 - accuracy: 0.7885 - val_loss: 0.5189 - val_accuracy: 0.6849\n",
            "Epoch 8915/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8213\n",
            "Epoch 8915: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3639 - accuracy: 0.8176 - val_loss: 0.4078 - val_accuracy: 0.7604\n",
            "Epoch 8916/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3884 - accuracy: 0.7968\n",
            "Epoch 8916: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3834 - accuracy: 0.8010 - val_loss: 0.4559 - val_accuracy: 0.7343\n",
            "Epoch 8917/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3886 - accuracy: 0.7974\n",
            "Epoch 8917: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3897 - accuracy: 0.7970 - val_loss: 0.7166 - val_accuracy: 0.5475\n",
            "Epoch 8918/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3735 - accuracy: 0.8079\n",
            "Epoch 8918: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3723 - accuracy: 0.8083 - val_loss: 0.6141 - val_accuracy: 0.6068\n",
            "Epoch 8919/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3692 - accuracy: 0.8106\n",
            "Epoch 8919: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3733 - accuracy: 0.8074 - val_loss: 0.3859 - val_accuracy: 0.7830\n",
            "Epoch 8920/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3859 - accuracy: 0.8008\n",
            "Epoch 8920: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3813 - accuracy: 0.8036 - val_loss: 0.4338 - val_accuracy: 0.7441\n",
            "Epoch 8921/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4009 - accuracy: 0.7930\n",
            "Epoch 8921: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3997 - accuracy: 0.7936 - val_loss: 0.4903 - val_accuracy: 0.7106\n",
            "Epoch 8922/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3878 - accuracy: 0.8034\n",
            "Epoch 8922: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3915 - accuracy: 0.8019 - val_loss: 0.6509 - val_accuracy: 0.5826\n",
            "Epoch 8923/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3662 - accuracy: 0.8111\n",
            "Epoch 8923: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3710 - accuracy: 0.8090 - val_loss: 0.8100 - val_accuracy: 0.5074\n",
            "Epoch 8924/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3903 - accuracy: 0.7944\n",
            "Epoch 8924: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3868 - accuracy: 0.7980 - val_loss: 0.6633 - val_accuracy: 0.5831\n",
            "Epoch 8925/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3746 - accuracy: 0.8098\n",
            "Epoch 8925: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3753 - accuracy: 0.8094 - val_loss: 0.4493 - val_accuracy: 0.7391\n",
            "Epoch 8926/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3668 - accuracy: 0.8137\n",
            "Epoch 8926: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3650 - accuracy: 0.8147 - val_loss: 0.4469 - val_accuracy: 0.7465\n",
            "Epoch 8927/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3658 - accuracy: 0.8160\n",
            "Epoch 8927: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.3671 - accuracy: 0.8143 - val_loss: 0.3939 - val_accuracy: 0.7707\n",
            "Epoch 8928/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4234 - accuracy: 0.7786\n",
            "Epoch 8928: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4187 - accuracy: 0.7827 - val_loss: 0.5052 - val_accuracy: 0.6961\n",
            "Epoch 8929/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8081\n",
            "Epoch 8929: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3830 - accuracy: 0.8056 - val_loss: 0.7792 - val_accuracy: 0.5284\n",
            "Epoch 8930/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3796 - accuracy: 0.8066\n",
            "Epoch 8930: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3790 - accuracy: 0.8070 - val_loss: 0.7574 - val_accuracy: 0.5357\n",
            "Epoch 8931/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3774 - accuracy: 0.8061\n",
            "Epoch 8931: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3755 - accuracy: 0.8075 - val_loss: 0.7084 - val_accuracy: 0.5578\n",
            "Epoch 8932/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3798 - accuracy: 0.8051\n",
            "Epoch 8932: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3788 - accuracy: 0.8061 - val_loss: 0.5196 - val_accuracy: 0.6783\n",
            "Epoch 8933/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3623 - accuracy: 0.8185\n",
            "Epoch 8933: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3609 - accuracy: 0.8192 - val_loss: 0.4545 - val_accuracy: 0.7354\n",
            "Epoch 8934/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3688 - accuracy: 0.8110\n",
            "Epoch 8934: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3689 - accuracy: 0.8110 - val_loss: 0.5133 - val_accuracy: 0.6945\n",
            "Epoch 8935/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3673 - accuracy: 0.8154\n",
            "Epoch 8935: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3683 - accuracy: 0.8145 - val_loss: 0.5947 - val_accuracy: 0.6300\n",
            "Epoch 8936/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3597 - accuracy: 0.8186\n",
            "Epoch 8936: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3582 - accuracy: 0.8200 - val_loss: 0.5829 - val_accuracy: 0.6373\n",
            "Epoch 8937/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8199\n",
            "Epoch 8937: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3564 - accuracy: 0.8202 - val_loss: 0.6776 - val_accuracy: 0.5677\n",
            "Epoch 8938/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3708 - accuracy: 0.8121\n",
            "Epoch 8938: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3679 - accuracy: 0.8149 - val_loss: 0.7473 - val_accuracy: 0.5341\n",
            "Epoch 8939/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4013 - accuracy: 0.7927\n",
            "Epoch 8939: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4012 - accuracy: 0.7926 - val_loss: 0.4695 - val_accuracy: 0.7231\n",
            "Epoch 8940/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3640 - accuracy: 0.8167\n",
            "Epoch 8940: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3652 - accuracy: 0.8159 - val_loss: 0.4428 - val_accuracy: 0.7391\n",
            "Epoch 8941/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3693 - accuracy: 0.8118\n",
            "Epoch 8941: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3684 - accuracy: 0.8112 - val_loss: 0.4120 - val_accuracy: 0.7645\n",
            "Epoch 8942/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4134 - accuracy: 0.7898\n",
            "Epoch 8942: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4094 - accuracy: 0.7925 - val_loss: 0.4413 - val_accuracy: 0.7452\n",
            "Epoch 8943/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4242 - accuracy: 0.7862\n",
            "Epoch 8943: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4248 - accuracy: 0.7858 - val_loss: 0.5935 - val_accuracy: 0.6230\n",
            "Epoch 8944/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3696 - accuracy: 0.8080\n",
            "Epoch 8944: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3747 - accuracy: 0.8064 - val_loss: 0.8143 - val_accuracy: 0.5128\n",
            "Epoch 8945/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3833 - accuracy: 0.8035\n",
            "Epoch 8945: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3831 - accuracy: 0.8039 - val_loss: 0.6529 - val_accuracy: 0.5953\n",
            "Epoch 8946/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3622 - accuracy: 0.8185\n",
            "Epoch 8946: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3630 - accuracy: 0.8176 - val_loss: 0.7483 - val_accuracy: 0.5471\n",
            "Epoch 8947/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3934 - accuracy: 0.7967\n",
            "Epoch 8947: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3923 - accuracy: 0.7968 - val_loss: 0.5611 - val_accuracy: 0.6498\n",
            "Epoch 8948/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3630 - accuracy: 0.8176\n",
            "Epoch 8948: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3621 - accuracy: 0.8180 - val_loss: 0.4737 - val_accuracy: 0.7228\n",
            "Epoch 8949/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8152\n",
            "Epoch 8949: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3602 - accuracy: 0.8176 - val_loss: 0.5904 - val_accuracy: 0.6313\n",
            "Epoch 8950/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8158\n",
            "Epoch 8950: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3616 - accuracy: 0.8168 - val_loss: 0.5371 - val_accuracy: 0.6649\n",
            "Epoch 8951/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3578 - accuracy: 0.8216\n",
            "Epoch 8951: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3598 - accuracy: 0.8191 - val_loss: 0.3912 - val_accuracy: 0.7773\n",
            "Epoch 8952/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3980 - accuracy: 0.7938\n",
            "Epoch 8952: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3949 - accuracy: 0.7954 - val_loss: 0.4494 - val_accuracy: 0.7386\n",
            "Epoch 8953/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3835 - accuracy: 0.8042\n",
            "Epoch 8953: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3834 - accuracy: 0.8048 - val_loss: 0.6870 - val_accuracy: 0.5690\n",
            "Epoch 8954/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3641 - accuracy: 0.8134\n",
            "Epoch 8954: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3658 - accuracy: 0.8122 - val_loss: 0.7427 - val_accuracy: 0.5359\n",
            "Epoch 8955/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3700 - accuracy: 0.8119\n",
            "Epoch 8955: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3698 - accuracy: 0.8126 - val_loss: 0.6458 - val_accuracy: 0.5918\n",
            "Epoch 8956/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3613 - accuracy: 0.8154\n",
            "Epoch 8956: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3619 - accuracy: 0.8151 - val_loss: 0.6778 - val_accuracy: 0.5679\n",
            "Epoch 8957/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3779 - accuracy: 0.8074\n",
            "Epoch 8957: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3777 - accuracy: 0.8076 - val_loss: 0.5268 - val_accuracy: 0.6796\n",
            "Epoch 8958/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3551 - accuracy: 0.8249\n",
            "Epoch 8958: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3583 - accuracy: 0.8221 - val_loss: 0.4058 - val_accuracy: 0.7709\n",
            "Epoch 8959/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3808 - accuracy: 0.8019\n",
            "Epoch 8959: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3799 - accuracy: 0.8032 - val_loss: 0.4788 - val_accuracy: 0.7108\n",
            "Epoch 8960/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3896 - accuracy: 0.7999\n",
            "Epoch 8960: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3896 - accuracy: 0.8003 - val_loss: 0.5597 - val_accuracy: 0.6476\n",
            "Epoch 8961/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3609 - accuracy: 0.8158\n",
            "Epoch 8961: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3634 - accuracy: 0.8151 - val_loss: 0.7222 - val_accuracy: 0.5451\n",
            "Epoch 8962/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3690 - accuracy: 0.8090\n",
            "Epoch 8962: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3676 - accuracy: 0.8120 - val_loss: 0.6781 - val_accuracy: 0.5690\n",
            "Epoch 8963/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3727 - accuracy: 0.8090\n",
            "Epoch 8963: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3699 - accuracy: 0.8104 - val_loss: 0.5167 - val_accuracy: 0.6869\n",
            "Epoch 8964/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8225\n",
            "Epoch 8964: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3575 - accuracy: 0.8211 - val_loss: 0.4897 - val_accuracy: 0.7088\n",
            "Epoch 8965/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8194\n",
            "Epoch 8965: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3594 - accuracy: 0.8189 - val_loss: 0.4612 - val_accuracy: 0.7281\n",
            "Epoch 8966/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3641 - accuracy: 0.8133\n",
            "Epoch 8966: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3640 - accuracy: 0.8137 - val_loss: 0.5352 - val_accuracy: 0.6781\n",
            "Epoch 8967/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8138\n",
            "Epoch 8967: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3619 - accuracy: 0.8139 - val_loss: 0.5980 - val_accuracy: 0.6252\n",
            "Epoch 8968/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3607 - accuracy: 0.8144\n",
            "Epoch 8968: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3612 - accuracy: 0.8147 - val_loss: 0.5953 - val_accuracy: 0.6265\n",
            "Epoch 8969/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8216\n",
            "Epoch 8969: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3567 - accuracy: 0.8204 - val_loss: 0.5781 - val_accuracy: 0.6386\n",
            "Epoch 8970/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8211\n",
            "Epoch 8970: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3558 - accuracy: 0.8214 - val_loss: 0.5881 - val_accuracy: 0.6234\n",
            "Epoch 8971/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8180\n",
            "Epoch 8971: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3606 - accuracy: 0.8171 - val_loss: 0.4100 - val_accuracy: 0.7705\n",
            "Epoch 8972/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3724 - accuracy: 0.8106\n",
            "Epoch 8972: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3714 - accuracy: 0.8116 - val_loss: 0.5317 - val_accuracy: 0.6770\n",
            "Epoch 8973/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3583 - accuracy: 0.8182\n",
            "Epoch 8973: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3608 - accuracy: 0.8167 - val_loss: 0.7137 - val_accuracy: 0.5519\n",
            "Epoch 8974/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8071\n",
            "Epoch 8974: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3759 - accuracy: 0.8083 - val_loss: 0.6092 - val_accuracy: 0.6160\n",
            "Epoch 8975/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3652 - accuracy: 0.8192\n",
            "Epoch 8975: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3663 - accuracy: 0.8175 - val_loss: 0.4198 - val_accuracy: 0.7597\n",
            "Epoch 8976/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3800 - accuracy: 0.8056\n",
            "Epoch 8976: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3772 - accuracy: 0.8079 - val_loss: 0.5192 - val_accuracy: 0.6818\n",
            "Epoch 8977/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3622 - accuracy: 0.8155\n",
            "Epoch 8977: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3634 - accuracy: 0.8150 - val_loss: 0.6493 - val_accuracy: 0.6000\n",
            "Epoch 8978/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8093\n",
            "Epoch 8978: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3699 - accuracy: 0.8097 - val_loss: 0.4907 - val_accuracy: 0.7081\n",
            "Epoch 8979/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3541 - accuracy: 0.8214\n",
            "Epoch 8979: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3566 - accuracy: 0.8198 - val_loss: 0.4394 - val_accuracy: 0.7509\n",
            "Epoch 8980/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3908 - accuracy: 0.7949\n",
            "Epoch 8980: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3911 - accuracy: 0.7964 - val_loss: 0.6417 - val_accuracy: 0.5901\n",
            "Epoch 8981/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8191\n",
            "Epoch 8981: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3599 - accuracy: 0.8178 - val_loss: 0.7693 - val_accuracy: 0.5247\n",
            "Epoch 8982/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3844 - accuracy: 0.8035\n",
            "Epoch 8982: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3840 - accuracy: 0.8039 - val_loss: 0.6056 - val_accuracy: 0.6219\n",
            "Epoch 8983/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3876 - accuracy: 0.8003\n",
            "Epoch 8983: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3906 - accuracy: 0.7984 - val_loss: 0.5101 - val_accuracy: 0.6915\n",
            "Epoch 8984/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3708 - accuracy: 0.8140\n",
            "Epoch 8984: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3728 - accuracy: 0.8127 - val_loss: 0.4053 - val_accuracy: 0.7628\n",
            "Epoch 8985/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3779 - accuracy: 0.8073\n",
            "Epoch 8985: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3776 - accuracy: 0.8072 - val_loss: 0.4451 - val_accuracy: 0.7415\n",
            "Epoch 8986/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3680 - accuracy: 0.8125\n",
            "Epoch 8986: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3668 - accuracy: 0.8128 - val_loss: 0.5419 - val_accuracy: 0.6715\n",
            "Epoch 8987/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8168\n",
            "Epoch 8987: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3622 - accuracy: 0.8166 - val_loss: 0.5358 - val_accuracy: 0.6726\n",
            "Epoch 8988/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8222\n",
            "Epoch 8988: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3566 - accuracy: 0.8210 - val_loss: 0.6202 - val_accuracy: 0.6024\n",
            "Epoch 8989/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8169\n",
            "Epoch 8989: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3639 - accuracy: 0.8165 - val_loss: 0.4463 - val_accuracy: 0.7435\n",
            "Epoch 8990/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3721 - accuracy: 0.8111\n",
            "Epoch 8990: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 0.3735 - accuracy: 0.8106 - val_loss: 0.6711 - val_accuracy: 0.5727\n",
            "Epoch 8991/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3636 - accuracy: 0.8137\n",
            "Epoch 8991: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3630 - accuracy: 0.8143 - val_loss: 0.5832 - val_accuracy: 0.6291\n",
            "Epoch 8992/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8200\n",
            "Epoch 8992: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3587 - accuracy: 0.8194 - val_loss: 0.5018 - val_accuracy: 0.6972\n",
            "Epoch 8993/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8217\n",
            "Epoch 8993: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3560 - accuracy: 0.8191 - val_loss: 0.5389 - val_accuracy: 0.6647\n",
            "Epoch 8994/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8239\n",
            "Epoch 8994: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3547 - accuracy: 0.8238 - val_loss: 0.5809 - val_accuracy: 0.6327\n",
            "Epoch 8995/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3616 - accuracy: 0.8177\n",
            "Epoch 8995: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3627 - accuracy: 0.8173 - val_loss: 0.4381 - val_accuracy: 0.7450\n",
            "Epoch 8996/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3855 - accuracy: 0.8006\n",
            "Epoch 8996: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3848 - accuracy: 0.8024 - val_loss: 0.5514 - val_accuracy: 0.6643\n",
            "Epoch 8997/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8205\n",
            "Epoch 8997: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3601 - accuracy: 0.8189 - val_loss: 0.7441 - val_accuracy: 0.5348\n",
            "Epoch 8998/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3845 - accuracy: 0.7980\n",
            "Epoch 8998: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3807 - accuracy: 0.8011 - val_loss: 0.7431 - val_accuracy: 0.5418\n",
            "Epoch 8999/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4293 - accuracy: 0.7760\n",
            "Epoch 8999: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.4308 - accuracy: 0.7751 - val_loss: 0.4819 - val_accuracy: 0.7209\n",
            "Epoch 9000/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3675 - accuracy: 0.8167\n",
            "Epoch 9000: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3732 - accuracy: 0.8126 - val_loss: 0.3608 - val_accuracy: 0.7856\n",
            "Epoch 9001/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4375 - accuracy: 0.7701\n",
            "Epoch 9001: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.4292 - accuracy: 0.7759 - val_loss: 0.4169 - val_accuracy: 0.7555\n",
            "Epoch 9002/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4437 - accuracy: 0.7721\n",
            "Epoch 9002: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4388 - accuracy: 0.7744 - val_loss: 0.4858 - val_accuracy: 0.7090\n",
            "Epoch 9003/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3881 - accuracy: 0.7999\n",
            "Epoch 9003: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3937 - accuracy: 0.7971 - val_loss: 0.7638 - val_accuracy: 0.5282\n",
            "Epoch 9004/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3778 - accuracy: 0.8060\n",
            "Epoch 9004: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3768 - accuracy: 0.8066 - val_loss: 0.9195 - val_accuracy: 0.4694\n",
            "Epoch 9005/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4168 - accuracy: 0.7842\n",
            "Epoch 9005: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4103 - accuracy: 0.7892 - val_loss: 0.6980 - val_accuracy: 0.5648\n",
            "Epoch 9006/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3796 - accuracy: 0.8071\n",
            "Epoch 9006: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3788 - accuracy: 0.8072 - val_loss: 0.5684 - val_accuracy: 0.6533\n",
            "Epoch 9007/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3704 - accuracy: 0.8116\n",
            "Epoch 9007: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3713 - accuracy: 0.8108 - val_loss: 0.5266 - val_accuracy: 0.6801\n",
            "Epoch 9008/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3601 - accuracy: 0.8202\n",
            "Epoch 9008: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3614 - accuracy: 0.8195 - val_loss: 0.5009 - val_accuracy: 0.7009\n",
            "Epoch 9009/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3576 - accuracy: 0.8218\n",
            "Epoch 9009: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3591 - accuracy: 0.8205 - val_loss: 0.6048 - val_accuracy: 0.6162\n",
            "Epoch 9010/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8185\n",
            "Epoch 9010: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3572 - accuracy: 0.8164 - val_loss: 0.5109 - val_accuracy: 0.6869\n",
            "Epoch 9011/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3563 - accuracy: 0.8203\n",
            "Epoch 9011: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3576 - accuracy: 0.8194 - val_loss: 0.5972 - val_accuracy: 0.6197\n",
            "Epoch 9012/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8204\n",
            "Epoch 9012: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3565 - accuracy: 0.8199 - val_loss: 0.5077 - val_accuracy: 0.6908\n",
            "Epoch 9013/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3554 - accuracy: 0.8222\n",
            "Epoch 9013: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3556 - accuracy: 0.8211 - val_loss: 0.5053 - val_accuracy: 0.6989\n",
            "Epoch 9014/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8239\n",
            "Epoch 9014: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3552 - accuracy: 0.8230 - val_loss: 0.5853 - val_accuracy: 0.6287\n",
            "Epoch 9015/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3539 - accuracy: 0.8227\n",
            "Epoch 9015: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3549 - accuracy: 0.8232 - val_loss: 0.6857 - val_accuracy: 0.5613\n",
            "Epoch 9016/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3767 - accuracy: 0.8039\n",
            "Epoch 9016: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3763 - accuracy: 0.8043 - val_loss: 0.5436 - val_accuracy: 0.6759\n",
            "Epoch 9017/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8230\n",
            "Epoch 9017: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3581 - accuracy: 0.8224 - val_loss: 0.5287 - val_accuracy: 0.6715\n",
            "Epoch 9018/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8125\n",
            "Epoch 9018: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3671 - accuracy: 0.8109 - val_loss: 0.4587 - val_accuracy: 0.7270\n",
            "Epoch 9019/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3885 - accuracy: 0.8008\n",
            "Epoch 9019: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3836 - accuracy: 0.8038 - val_loss: 0.4003 - val_accuracy: 0.7847\n",
            "Epoch 9020/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4030 - accuracy: 0.7907\n",
            "Epoch 9020: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4015 - accuracy: 0.7920 - val_loss: 0.5232 - val_accuracy: 0.6917\n",
            "Epoch 9021/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3766 - accuracy: 0.8072\n",
            "Epoch 9021: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3794 - accuracy: 0.8054 - val_loss: 0.7888 - val_accuracy: 0.5091\n",
            "Epoch 9022/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3802 - accuracy: 0.8090\n",
            "Epoch 9022: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3791 - accuracy: 0.8091 - val_loss: 0.7307 - val_accuracy: 0.5311\n",
            "Epoch 9023/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3783 - accuracy: 0.8054\n",
            "Epoch 9023: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3762 - accuracy: 0.8067 - val_loss: 0.5828 - val_accuracy: 0.6338\n",
            "Epoch 9024/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8177\n",
            "Epoch 9024: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3610 - accuracy: 0.8178 - val_loss: 0.5213 - val_accuracy: 0.6869\n",
            "Epoch 9025/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3566 - accuracy: 0.8220\n",
            "Epoch 9025: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3565 - accuracy: 0.8218 - val_loss: 0.5178 - val_accuracy: 0.6926\n",
            "Epoch 9026/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8219\n",
            "Epoch 9026: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3561 - accuracy: 0.8221 - val_loss: 0.4962 - val_accuracy: 0.7000\n",
            "Epoch 9027/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3601 - accuracy: 0.8182\n",
            "Epoch 9027: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3593 - accuracy: 0.8193 - val_loss: 0.5272 - val_accuracy: 0.6724\n",
            "Epoch 9028/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3594 - accuracy: 0.8180\n",
            "Epoch 9028: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3593 - accuracy: 0.8184 - val_loss: 0.6571 - val_accuracy: 0.5837\n",
            "Epoch 9029/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3668 - accuracy: 0.8154\n",
            "Epoch 9029: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3644 - accuracy: 0.8163 - val_loss: 0.6267 - val_accuracy: 0.6006\n",
            "Epoch 9030/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3724 - accuracy: 0.8112\n",
            "Epoch 9030: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3741 - accuracy: 0.8107 - val_loss: 0.4632 - val_accuracy: 0.7246\n",
            "Epoch 9031/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3682 - accuracy: 0.8157\n",
            "Epoch 9031: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3663 - accuracy: 0.8172 - val_loss: 0.5027 - val_accuracy: 0.7000\n",
            "Epoch 9032/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3594 - accuracy: 0.8205\n",
            "Epoch 9032: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3599 - accuracy: 0.8199 - val_loss: 0.4843 - val_accuracy: 0.7110\n",
            "Epoch 9033/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3791 - accuracy: 0.8042\n",
            "Epoch 9033: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3806 - accuracy: 0.8038 - val_loss: 0.7756 - val_accuracy: 0.5269\n",
            "Epoch 9034/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3879 - accuracy: 0.8016\n",
            "Epoch 9034: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3856 - accuracy: 0.8022 - val_loss: 0.6501 - val_accuracy: 0.5842\n",
            "Epoch 9035/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8092\n",
            "Epoch 9035: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3794 - accuracy: 0.8070 - val_loss: 0.3987 - val_accuracy: 0.7779\n",
            "Epoch 9036/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3774 - accuracy: 0.8058\n",
            "Epoch 9036: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3751 - accuracy: 0.8078 - val_loss: 0.4684 - val_accuracy: 0.7257\n",
            "Epoch 9037/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3708 - accuracy: 0.8118\n",
            "Epoch 9037: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3726 - accuracy: 0.8097 - val_loss: 0.5590 - val_accuracy: 0.6568\n",
            "Epoch 9038/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3536 - accuracy: 0.8234\n",
            "Epoch 9038: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3551 - accuracy: 0.8222 - val_loss: 0.5538 - val_accuracy: 0.6616\n",
            "Epoch 9039/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3531 - accuracy: 0.8206\n",
            "Epoch 9039: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3555 - accuracy: 0.8204 - val_loss: 0.6463 - val_accuracy: 0.5846\n",
            "Epoch 9040/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3667 - accuracy: 0.8130\n",
            "Epoch 9040: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3649 - accuracy: 0.8142 - val_loss: 0.4930 - val_accuracy: 0.7084\n",
            "Epoch 9041/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3593 - accuracy: 0.8192\n",
            "Epoch 9041: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3564 - accuracy: 0.8211 - val_loss: 0.5351 - val_accuracy: 0.6728\n",
            "Epoch 9042/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3541 - accuracy: 0.8215\n",
            "Epoch 9042: loss did not improve from 0.35394\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3548 - accuracy: 0.8212 - val_loss: 0.5604 - val_accuracy: 0.6570\n",
            "Epoch 9043/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8231\n",
            "Epoch 9043: loss improved from 0.35394 to 0.35344, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 205ms/step - loss: 0.3534 - accuracy: 0.8247 - val_loss: 0.6011 - val_accuracy: 0.6219\n",
            "Epoch 9044/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3600 - accuracy: 0.8196\n",
            "Epoch 9044: loss did not improve from 0.35344\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3597 - accuracy: 0.8190 - val_loss: 0.4622 - val_accuracy: 0.7310\n",
            "Epoch 9045/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3724 - accuracy: 0.8094\n",
            "Epoch 9045: loss did not improve from 0.35344\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3711 - accuracy: 0.8107 - val_loss: 0.5674 - val_accuracy: 0.6463\n",
            "Epoch 9046/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3516 - accuracy: 0.8232\n",
            "Epoch 9046: loss improved from 0.35344 to 0.35328, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 156ms/step - loss: 0.3533 - accuracy: 0.8214 - val_loss: 0.7507 - val_accuracy: 0.5348\n",
            "Epoch 9047/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3889 - accuracy: 0.8005\n",
            "Epoch 9047: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3892 - accuracy: 0.8005 - val_loss: 0.4486 - val_accuracy: 0.7433\n",
            "Epoch 9048/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3573 - accuracy: 0.8205\n",
            "Epoch 9048: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3585 - accuracy: 0.8182 - val_loss: 0.4443 - val_accuracy: 0.7461\n",
            "Epoch 9049/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3657 - accuracy: 0.8128\n",
            "Epoch 9049: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3658 - accuracy: 0.8130 - val_loss: 0.6392 - val_accuracy: 0.5997\n",
            "Epoch 9050/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8149\n",
            "Epoch 9050: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3665 - accuracy: 0.8145 - val_loss: 0.4624 - val_accuracy: 0.7316\n",
            "Epoch 9051/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8211\n",
            "Epoch 9051: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3576 - accuracy: 0.8212 - val_loss: 0.4198 - val_accuracy: 0.7645\n",
            "Epoch 9052/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3818 - accuracy: 0.8039\n",
            "Epoch 9052: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3856 - accuracy: 0.8006 - val_loss: 0.7422 - val_accuracy: 0.5396\n",
            "Epoch 9053/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3738 - accuracy: 0.8079\n",
            "Epoch 9053: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3731 - accuracy: 0.8088 - val_loss: 0.7693 - val_accuracy: 0.5328\n",
            "Epoch 9054/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3953 - accuracy: 0.7963\n",
            "Epoch 9054: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3936 - accuracy: 0.7977 - val_loss: 0.5382 - val_accuracy: 0.6768\n",
            "Epoch 9055/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8151\n",
            "Epoch 9055: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3713 - accuracy: 0.8119 - val_loss: 0.3904 - val_accuracy: 0.7652\n",
            "Epoch 9056/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4247 - accuracy: 0.7784\n",
            "Epoch 9056: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.4183 - accuracy: 0.7825 - val_loss: 0.4547 - val_accuracy: 0.7340\n",
            "Epoch 9057/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3957 - accuracy: 0.7964\n",
            "Epoch 9057: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.4001 - accuracy: 0.7937 - val_loss: 0.7999 - val_accuracy: 0.5157\n",
            "Epoch 9058/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3767 - accuracy: 0.8069\n",
            "Epoch 9058: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3790 - accuracy: 0.8055 - val_loss: 0.9132 - val_accuracy: 0.4646\n",
            "Epoch 9059/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4121 - accuracy: 0.7839\n",
            "Epoch 9059: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4060 - accuracy: 0.7890 - val_loss: 0.8695 - val_accuracy: 0.4872\n",
            "Epoch 9060/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4427 - accuracy: 0.7696\n",
            "Epoch 9060: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.4362 - accuracy: 0.7737 - val_loss: 0.6435 - val_accuracy: 0.5962\n",
            "Epoch 9061/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3871 - accuracy: 0.8054\n",
            "Epoch 9061: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3893 - accuracy: 0.8026 - val_loss: 0.4475 - val_accuracy: 0.7417\n",
            "Epoch 9062/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3720 - accuracy: 0.8106\n",
            "Epoch 9062: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3743 - accuracy: 0.8080 - val_loss: 0.4642 - val_accuracy: 0.7272\n",
            "Epoch 9063/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3763 - accuracy: 0.8106\n",
            "Epoch 9063: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3740 - accuracy: 0.8117 - val_loss: 0.4874 - val_accuracy: 0.7075\n",
            "Epoch 9064/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3612 - accuracy: 0.8152\n",
            "Epoch 9064: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3607 - accuracy: 0.8159 - val_loss: 0.5048 - val_accuracy: 0.6985\n",
            "Epoch 9065/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3647 - accuracy: 0.8150\n",
            "Epoch 9065: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3626 - accuracy: 0.8166 - val_loss: 0.5052 - val_accuracy: 0.6961\n",
            "Epoch 9066/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3647 - accuracy: 0.8150\n",
            "Epoch 9066: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 125ms/step - loss: 0.3653 - accuracy: 0.8157 - val_loss: 0.6824 - val_accuracy: 0.5703\n",
            "Epoch 9067/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3690 - accuracy: 0.8095\n",
            "Epoch 9067: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3685 - accuracy: 0.8092 - val_loss: 0.5450 - val_accuracy: 0.6568\n",
            "Epoch 9068/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8233\n",
            "Epoch 9068: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3561 - accuracy: 0.8226 - val_loss: 0.5271 - val_accuracy: 0.6713\n",
            "Epoch 9069/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8187\n",
            "Epoch 9069: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3572 - accuracy: 0.8198 - val_loss: 0.5018 - val_accuracy: 0.6978\n",
            "Epoch 9070/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3581 - accuracy: 0.8180\n",
            "Epoch 9070: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3573 - accuracy: 0.8195 - val_loss: 0.5685 - val_accuracy: 0.6491\n",
            "Epoch 9071/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8203\n",
            "Epoch 9071: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3579 - accuracy: 0.8198 - val_loss: 0.6284 - val_accuracy: 0.5947\n",
            "Epoch 9072/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3609 - accuracy: 0.8186\n",
            "Epoch 9072: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3602 - accuracy: 0.8196 - val_loss: 0.5436 - val_accuracy: 0.6603\n",
            "Epoch 9073/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8217\n",
            "Epoch 9073: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3540 - accuracy: 0.8205 - val_loss: 0.4559 - val_accuracy: 0.7358\n",
            "Epoch 9074/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3603 - accuracy: 0.8183\n",
            "Epoch 9074: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3601 - accuracy: 0.8190 - val_loss: 0.4976 - val_accuracy: 0.7007\n",
            "Epoch 9075/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3554 - accuracy: 0.8217\n",
            "Epoch 9075: loss did not improve from 0.35328\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3548 - accuracy: 0.8222 - val_loss: 0.5292 - val_accuracy: 0.6711\n",
            "Epoch 9076/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8249\n",
            "Epoch 9076: loss improved from 0.35328 to 0.35213, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3521 - accuracy: 0.8232 - val_loss: 0.5565 - val_accuracy: 0.6460\n",
            "Epoch 9077/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3515 - accuracy: 0.8238\n",
            "Epoch 9077: loss improved from 0.35213 to 0.35124, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3512 - accuracy: 0.8238 - val_loss: 0.5582 - val_accuracy: 0.6548\n",
            "Epoch 9078/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3483 - accuracy: 0.8253\n",
            "Epoch 9078: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3513 - accuracy: 0.8233 - val_loss: 0.5987 - val_accuracy: 0.6241\n",
            "Epoch 9079/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3540 - accuracy: 0.8231\n",
            "Epoch 9079: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3538 - accuracy: 0.8231 - val_loss: 0.6081 - val_accuracy: 0.6138\n",
            "Epoch 9080/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8220\n",
            "Epoch 9080: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3555 - accuracy: 0.8220 - val_loss: 0.4569 - val_accuracy: 0.7338\n",
            "Epoch 9081/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3643 - accuracy: 0.8125\n",
            "Epoch 9081: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3657 - accuracy: 0.8117 - val_loss: 0.6375 - val_accuracy: 0.5956\n",
            "Epoch 9082/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3634 - accuracy: 0.8136\n",
            "Epoch 9082: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3614 - accuracy: 0.8154 - val_loss: 0.6218 - val_accuracy: 0.6037\n",
            "Epoch 9083/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3852 - accuracy: 0.8005\n",
            "Epoch 9083: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3889 - accuracy: 0.7976 - val_loss: 0.4458 - val_accuracy: 0.7454\n",
            "Epoch 9084/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3714 - accuracy: 0.8136\n",
            "Epoch 9084: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3781 - accuracy: 0.8099 - val_loss: 0.3730 - val_accuracy: 0.7792\n",
            "Epoch 9085/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4203 - accuracy: 0.7792\n",
            "Epoch 9085: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4156 - accuracy: 0.7822 - val_loss: 0.3919 - val_accuracy: 0.7698\n",
            "Epoch 9086/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4500 - accuracy: 0.7687\n",
            "Epoch 9086: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4451 - accuracy: 0.7714 - val_loss: 0.6551 - val_accuracy: 0.5907\n",
            "Epoch 9087/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3674 - accuracy: 0.8118\n",
            "Epoch 9087: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3677 - accuracy: 0.8119 - val_loss: 0.7338 - val_accuracy: 0.5433\n",
            "Epoch 9088/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3704 - accuracy: 0.8116\n",
            "Epoch 9088: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3715 - accuracy: 0.8111 - val_loss: 0.7658 - val_accuracy: 0.5278\n",
            "Epoch 9089/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3945 - accuracy: 0.7974\n",
            "Epoch 9089: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3921 - accuracy: 0.7991 - val_loss: 0.5900 - val_accuracy: 0.6342\n",
            "Epoch 9090/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3686 - accuracy: 0.8138\n",
            "Epoch 9090: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3691 - accuracy: 0.8138 - val_loss: 0.5325 - val_accuracy: 0.6724\n",
            "Epoch 9091/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8169\n",
            "Epoch 9091: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3626 - accuracy: 0.8157 - val_loss: 0.4661 - val_accuracy: 0.7242\n",
            "Epoch 9092/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3635 - accuracy: 0.8196\n",
            "Epoch 9092: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3633 - accuracy: 0.8187 - val_loss: 0.4117 - val_accuracy: 0.7555\n",
            "Epoch 9093/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4308 - accuracy: 0.7757\n",
            "Epoch 9093: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4258 - accuracy: 0.7788 - val_loss: 0.4956 - val_accuracy: 0.7112\n",
            "Epoch 9094/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3839 - accuracy: 0.8022\n",
            "Epoch 9094: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3870 - accuracy: 0.8006 - val_loss: 0.7558 - val_accuracy: 0.5346\n",
            "Epoch 9095/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3701 - accuracy: 0.8105\n",
            "Epoch 9095: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3728 - accuracy: 0.8082 - val_loss: 0.8459 - val_accuracy: 0.5008\n",
            "Epoch 9096/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3960 - accuracy: 0.7956\n",
            "Epoch 9096: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3932 - accuracy: 0.7973 - val_loss: 0.9264 - val_accuracy: 0.4569\n",
            "Epoch 9097/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4367 - accuracy: 0.7741\n",
            "Epoch 9097: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4300 - accuracy: 0.7776 - val_loss: 0.5785 - val_accuracy: 0.6401\n",
            "Epoch 9098/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3793 - accuracy: 0.8033\n",
            "Epoch 9098: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3818 - accuracy: 0.8015 - val_loss: 0.5100 - val_accuracy: 0.6926\n",
            "Epoch 9099/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3641 - accuracy: 0.8184\n",
            "Epoch 9099: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3669 - accuracy: 0.8170 - val_loss: 0.3998 - val_accuracy: 0.7770\n",
            "Epoch 9100/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3736 - accuracy: 0.8080\n",
            "Epoch 9100: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3715 - accuracy: 0.8100 - val_loss: 0.5315 - val_accuracy: 0.6829\n",
            "Epoch 9101/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8231\n",
            "Epoch 9101: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3598 - accuracy: 0.8206 - val_loss: 0.4390 - val_accuracy: 0.7465\n",
            "Epoch 9102/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3756 - accuracy: 0.8067\n",
            "Epoch 9102: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3732 - accuracy: 0.8097 - val_loss: 0.6579 - val_accuracy: 0.5837\n",
            "Epoch 9103/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3614 - accuracy: 0.8155\n",
            "Epoch 9103: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3628 - accuracy: 0.8155 - val_loss: 0.5591 - val_accuracy: 0.6570\n",
            "Epoch 9104/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8219\n",
            "Epoch 9104: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3582 - accuracy: 0.8222 - val_loss: 0.4559 - val_accuracy: 0.7332\n",
            "Epoch 9105/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3690 - accuracy: 0.8138\n",
            "Epoch 9105: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3696 - accuracy: 0.8134 - val_loss: 0.4521 - val_accuracy: 0.7338\n",
            "Epoch 9106/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8029\n",
            "Epoch 9106: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3846 - accuracy: 0.8031 - val_loss: 0.5340 - val_accuracy: 0.6759\n",
            "Epoch 9107/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3651 - accuracy: 0.8137\n",
            "Epoch 9107: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3664 - accuracy: 0.8142 - val_loss: 0.6972 - val_accuracy: 0.5591\n",
            "Epoch 9108/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8121\n",
            "Epoch 9108: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3647 - accuracy: 0.8134 - val_loss: 0.6507 - val_accuracy: 0.5883\n",
            "Epoch 9109/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3635 - accuracy: 0.8167\n",
            "Epoch 9109: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3620 - accuracy: 0.8170 - val_loss: 0.6639 - val_accuracy: 0.5727\n",
            "Epoch 9110/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3772 - accuracy: 0.8081\n",
            "Epoch 9110: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3786 - accuracy: 0.8072 - val_loss: 0.5146 - val_accuracy: 0.6829\n",
            "Epoch 9111/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3681 - accuracy: 0.8135\n",
            "Epoch 9111: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3751 - accuracy: 0.8095 - val_loss: 0.4212 - val_accuracy: 0.7536\n",
            "Epoch 9112/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4057 - accuracy: 0.7912\n",
            "Epoch 9112: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.4014 - accuracy: 0.7927 - val_loss: 0.4207 - val_accuracy: 0.7615\n",
            "Epoch 9113/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3818 - accuracy: 0.8014\n",
            "Epoch 9113: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3811 - accuracy: 0.8019 - val_loss: 0.5022 - val_accuracy: 0.6985\n",
            "Epoch 9114/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3724 - accuracy: 0.8115\n",
            "Epoch 9114: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3733 - accuracy: 0.8117 - val_loss: 0.6099 - val_accuracy: 0.6276\n",
            "Epoch 9115/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8183\n",
            "Epoch 9115: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3661 - accuracy: 0.8174 - val_loss: 0.7339 - val_accuracy: 0.5394\n",
            "Epoch 9116/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3715 - accuracy: 0.8096\n",
            "Epoch 9116: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3714 - accuracy: 0.8100 - val_loss: 0.6970 - val_accuracy: 0.5706\n",
            "Epoch 9117/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3843 - accuracy: 0.8039\n",
            "Epoch 9117: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3831 - accuracy: 0.8053 - val_loss: 0.5303 - val_accuracy: 0.6706\n",
            "Epoch 9118/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3631 - accuracy: 0.8186\n",
            "Epoch 9118: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3644 - accuracy: 0.8171 - val_loss: 0.4428 - val_accuracy: 0.7454\n",
            "Epoch 9119/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3652 - accuracy: 0.8158\n",
            "Epoch 9119: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3639 - accuracy: 0.8161 - val_loss: 0.4394 - val_accuracy: 0.7435\n",
            "Epoch 9120/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3787 - accuracy: 0.8057\n",
            "Epoch 9120: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3768 - accuracy: 0.8076 - val_loss: 0.4575 - val_accuracy: 0.7305\n",
            "Epoch 9121/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3990 - accuracy: 0.7954\n",
            "Epoch 9121: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4008 - accuracy: 0.7953 - val_loss: 0.6183 - val_accuracy: 0.6193\n",
            "Epoch 9122/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3671 - accuracy: 0.8119\n",
            "Epoch 9122: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3693 - accuracy: 0.8105 - val_loss: 0.8891 - val_accuracy: 0.4768\n",
            "Epoch 9123/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4012 - accuracy: 0.7929\n",
            "Epoch 9123: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 0.3962 - accuracy: 0.7962 - val_loss: 0.6983 - val_accuracy: 0.5723\n",
            "Epoch 9124/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3791 - accuracy: 0.8071\n",
            "Epoch 9124: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3802 - accuracy: 0.8061 - val_loss: 0.5299 - val_accuracy: 0.6864\n",
            "Epoch 9125/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3583 - accuracy: 0.8227\n",
            "Epoch 9125: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3589 - accuracy: 0.8215 - val_loss: 0.4290 - val_accuracy: 0.7492\n",
            "Epoch 9126/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8101\n",
            "Epoch 9126: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3709 - accuracy: 0.8100 - val_loss: 0.4764 - val_accuracy: 0.7187\n",
            "Epoch 9127/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3733 - accuracy: 0.8089\n",
            "Epoch 9127: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3725 - accuracy: 0.8102 - val_loss: 0.4824 - val_accuracy: 0.7112\n",
            "Epoch 9128/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8155\n",
            "Epoch 9128: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3609 - accuracy: 0.8162 - val_loss: 0.5632 - val_accuracy: 0.6474\n",
            "Epoch 9129/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3558 - accuracy: 0.8210\n",
            "Epoch 9129: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3555 - accuracy: 0.8211 - val_loss: 0.7338 - val_accuracy: 0.5348\n",
            "Epoch 9130/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3879 - accuracy: 0.8003\n",
            "Epoch 9130: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3854 - accuracy: 0.8024 - val_loss: 0.6240 - val_accuracy: 0.6000\n",
            "Epoch 9131/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3825 - accuracy: 0.8044\n",
            "Epoch 9131: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3853 - accuracy: 0.8020 - val_loss: 0.4556 - val_accuracy: 0.7325\n",
            "Epoch 9132/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3639 - accuracy: 0.8200\n",
            "Epoch 9132: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3691 - accuracy: 0.8157 - val_loss: 0.4107 - val_accuracy: 0.7520\n",
            "Epoch 9133/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3922 - accuracy: 0.7996\n",
            "Epoch 9133: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3885 - accuracy: 0.8007 - val_loss: 0.3776 - val_accuracy: 0.7917\n",
            "Epoch 9134/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4165 - accuracy: 0.7825\n",
            "Epoch 9134: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.4140 - accuracy: 0.7837 - val_loss: 0.4962 - val_accuracy: 0.7044\n",
            "Epoch 9135/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3945 - accuracy: 0.7990\n",
            "Epoch 9135: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3958 - accuracy: 0.7988 - val_loss: 0.5988 - val_accuracy: 0.6342\n",
            "Epoch 9136/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3751 - accuracy: 0.8100\n",
            "Epoch 9136: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3811 - accuracy: 0.8073 - val_loss: 0.7731 - val_accuracy: 0.5269\n",
            "Epoch 9137/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3803 - accuracy: 0.8047\n",
            "Epoch 9137: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3861 - accuracy: 0.8015 - val_loss: 0.9734 - val_accuracy: 0.4575\n",
            "Epoch 9138/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4227 - accuracy: 0.7864\n",
            "Epoch 9138: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4197 - accuracy: 0.7880 - val_loss: 0.9779 - val_accuracy: 0.4540\n",
            "Epoch 9139/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4313 - accuracy: 0.7761\n",
            "Epoch 9139: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4235 - accuracy: 0.7816 - val_loss: 0.9003 - val_accuracy: 0.4718\n",
            "Epoch 9140/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4236 - accuracy: 0.7788\n",
            "Epoch 9140: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4187 - accuracy: 0.7816 - val_loss: 0.7264 - val_accuracy: 0.5508\n",
            "Epoch 9141/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4160 - accuracy: 0.7825\n",
            "Epoch 9141: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4147 - accuracy: 0.7828 - val_loss: 0.4996 - val_accuracy: 0.6983\n",
            "Epoch 9142/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3641 - accuracy: 0.8196\n",
            "Epoch 9142: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3656 - accuracy: 0.8183 - val_loss: 0.4842 - val_accuracy: 0.7202\n",
            "Epoch 9143/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3609 - accuracy: 0.8200\n",
            "Epoch 9143: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3611 - accuracy: 0.8188 - val_loss: 0.5474 - val_accuracy: 0.6634\n",
            "Epoch 9144/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3549 - accuracy: 0.8239\n",
            "Epoch 9144: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3549 - accuracy: 0.8234 - val_loss: 0.5372 - val_accuracy: 0.6708\n",
            "Epoch 9145/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8224\n",
            "Epoch 9145: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3540 - accuracy: 0.8235 - val_loss: 0.5603 - val_accuracy: 0.6528\n",
            "Epoch 9146/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3553 - accuracy: 0.8218\n",
            "Epoch 9146: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3553 - accuracy: 0.8218 - val_loss: 0.5930 - val_accuracy: 0.6212\n",
            "Epoch 9147/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3544 - accuracy: 0.8214\n",
            "Epoch 9147: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3553 - accuracy: 0.8203 - val_loss: 0.7197 - val_accuracy: 0.5418\n",
            "Epoch 9148/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3650 - accuracy: 0.8130\n",
            "Epoch 9148: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3661 - accuracy: 0.8121 - val_loss: 0.5880 - val_accuracy: 0.6307\n",
            "Epoch 9149/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8214\n",
            "Epoch 9149: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3557 - accuracy: 0.8222 - val_loss: 0.6425 - val_accuracy: 0.5912\n",
            "Epoch 9150/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8123\n",
            "Epoch 9150: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3646 - accuracy: 0.8140 - val_loss: 0.6290 - val_accuracy: 0.6105\n",
            "Epoch 9151/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3882 - accuracy: 0.8032\n",
            "Epoch 9151: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3891 - accuracy: 0.8036 - val_loss: 0.5556 - val_accuracy: 0.6528\n",
            "Epoch 9152/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3769 - accuracy: 0.8103\n",
            "Epoch 9152: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3816 - accuracy: 0.8066 - val_loss: 0.4224 - val_accuracy: 0.7527\n",
            "Epoch 9153/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3679 - accuracy: 0.8146\n",
            "Epoch 9153: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3711 - accuracy: 0.8123 - val_loss: 0.4043 - val_accuracy: 0.7586\n",
            "Epoch 9154/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3761 - accuracy: 0.8103\n",
            "Epoch 9154: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3767 - accuracy: 0.8093 - val_loss: 0.4027 - val_accuracy: 0.7645\n",
            "Epoch 9155/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3934 - accuracy: 0.7980\n",
            "Epoch 9155: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3904 - accuracy: 0.8002 - val_loss: 0.4047 - val_accuracy: 0.7593\n",
            "Epoch 9156/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4079 - accuracy: 0.7892\n",
            "Epoch 9156: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4041 - accuracy: 0.7917 - val_loss: 0.4032 - val_accuracy: 0.7659\n",
            "Epoch 9157/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4525 - accuracy: 0.7657\n",
            "Epoch 9157: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4496 - accuracy: 0.7667 - val_loss: 0.4496 - val_accuracy: 0.7389\n",
            "Epoch 9158/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3996 - accuracy: 0.7947\n",
            "Epoch 9158: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4002 - accuracy: 0.7940 - val_loss: 0.5569 - val_accuracy: 0.6638\n",
            "Epoch 9159/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3756 - accuracy: 0.8097\n",
            "Epoch 9159: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3792 - accuracy: 0.8075 - val_loss: 0.6970 - val_accuracy: 0.5719\n",
            "Epoch 9160/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3711 - accuracy: 0.8099\n",
            "Epoch 9160: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3728 - accuracy: 0.8088 - val_loss: 0.6498 - val_accuracy: 0.5853\n",
            "Epoch 9161/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3607 - accuracy: 0.8161\n",
            "Epoch 9161: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3622 - accuracy: 0.8160 - val_loss: 0.8603 - val_accuracy: 0.4933\n",
            "Epoch 9162/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4043 - accuracy: 0.7919\n",
            "Epoch 9162: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3994 - accuracy: 0.7955 - val_loss: 0.8163 - val_accuracy: 0.5016\n",
            "Epoch 9163/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4253 - accuracy: 0.7799\n",
            "Epoch 9163: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4205 - accuracy: 0.7829 - val_loss: 0.6390 - val_accuracy: 0.6017\n",
            "Epoch 9164/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3934 - accuracy: 0.7990\n",
            "Epoch 9164: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3937 - accuracy: 0.7975 - val_loss: 0.5085 - val_accuracy: 0.6890\n",
            "Epoch 9165/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3633 - accuracy: 0.8187\n",
            "Epoch 9165: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3672 - accuracy: 0.8149 - val_loss: 0.4408 - val_accuracy: 0.7417\n",
            "Epoch 9166/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3635 - accuracy: 0.8168\n",
            "Epoch 9166: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3648 - accuracy: 0.8165 - val_loss: 0.4560 - val_accuracy: 0.7402\n",
            "Epoch 9167/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3654 - accuracy: 0.8154\n",
            "Epoch 9167: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3646 - accuracy: 0.8150 - val_loss: 0.4228 - val_accuracy: 0.7474\n",
            "Epoch 9168/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3862 - accuracy: 0.8009\n",
            "Epoch 9168: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3834 - accuracy: 0.8030 - val_loss: 0.4851 - val_accuracy: 0.7108\n",
            "Epoch 9169/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3645 - accuracy: 0.8165\n",
            "Epoch 9169: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3650 - accuracy: 0.8160 - val_loss: 0.5934 - val_accuracy: 0.6243\n",
            "Epoch 9170/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3550 - accuracy: 0.8234\n",
            "Epoch 9170: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3551 - accuracy: 0.8232 - val_loss: 0.5848 - val_accuracy: 0.6305\n",
            "Epoch 9171/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8199\n",
            "Epoch 9171: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3577 - accuracy: 0.8196 - val_loss: 0.6192 - val_accuracy: 0.6061\n",
            "Epoch 9172/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3586 - accuracy: 0.8186\n",
            "Epoch 9172: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3588 - accuracy: 0.8193 - val_loss: 0.5628 - val_accuracy: 0.6498\n",
            "Epoch 9173/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3581 - accuracy: 0.8188\n",
            "Epoch 9173: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3586 - accuracy: 0.8181 - val_loss: 0.5667 - val_accuracy: 0.6482\n",
            "Epoch 9174/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8205\n",
            "Epoch 9174: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3555 - accuracy: 0.8202 - val_loss: 0.4784 - val_accuracy: 0.7156\n",
            "Epoch 9175/10000\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8235\n",
            "Epoch 9175: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 1s 223ms/step - loss: 0.3544 - accuracy: 0.8235 - val_loss: 0.3915 - val_accuracy: 0.7786\n",
            "Epoch 9176/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3953 - accuracy: 0.7942\n",
            "Epoch 9176: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3916 - accuracy: 0.7971 - val_loss: 0.5128 - val_accuracy: 0.6906\n",
            "Epoch 9177/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3627 - accuracy: 0.8149\n",
            "Epoch 9177: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 130ms/step - loss: 0.3639 - accuracy: 0.8152 - val_loss: 0.6481 - val_accuracy: 0.5844\n",
            "Epoch 9178/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3582 - accuracy: 0.8174\n",
            "Epoch 9178: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 0.3623 - accuracy: 0.8152 - val_loss: 0.7346 - val_accuracy: 0.5449\n",
            "Epoch 9179/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3706 - accuracy: 0.8109\n",
            "Epoch 9179: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3713 - accuracy: 0.8093 - val_loss: 0.6174 - val_accuracy: 0.6024\n",
            "Epoch 9180/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8197\n",
            "Epoch 9180: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3589 - accuracy: 0.8177 - val_loss: 0.6860 - val_accuracy: 0.5655\n",
            "Epoch 9181/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3726 - accuracy: 0.8093\n",
            "Epoch 9181: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3717 - accuracy: 0.8097 - val_loss: 0.6454 - val_accuracy: 0.5890\n",
            "Epoch 9182/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3690 - accuracy: 0.8129\n",
            "Epoch 9182: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 0.3661 - accuracy: 0.8142 - val_loss: 0.5717 - val_accuracy: 0.6408\n",
            "Epoch 9183/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3656 - accuracy: 0.8137\n",
            "Epoch 9183: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 0.3680 - accuracy: 0.8123 - val_loss: 0.4866 - val_accuracy: 0.7127\n",
            "Epoch 9184/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8202\n",
            "Epoch 9184: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.3619 - accuracy: 0.8191 - val_loss: 0.4594 - val_accuracy: 0.7310\n",
            "Epoch 9185/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8203\n",
            "Epoch 9185: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3664 - accuracy: 0.8182 - val_loss: 0.4010 - val_accuracy: 0.7687\n",
            "Epoch 9186/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3966 - accuracy: 0.7933\n",
            "Epoch 9186: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3930 - accuracy: 0.7965 - val_loss: 0.4830 - val_accuracy: 0.7152\n",
            "Epoch 9187/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3737 - accuracy: 0.8117\n",
            "Epoch 9187: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 0.3719 - accuracy: 0.8128 - val_loss: 0.4936 - val_accuracy: 0.7046\n",
            "Epoch 9188/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3654 - accuracy: 0.8100\n",
            "Epoch 9188: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.3660 - accuracy: 0.8111 - val_loss: 0.7811 - val_accuracy: 0.5100\n",
            "Epoch 9189/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3762 - accuracy: 0.8082\n",
            "Epoch 9189: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 130ms/step - loss: 0.3753 - accuracy: 0.8086 - val_loss: 0.6807 - val_accuracy: 0.5767\n",
            "Epoch 9190/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3740 - accuracy: 0.8112\n",
            "Epoch 9190: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3708 - accuracy: 0.8136 - val_loss: 0.6215 - val_accuracy: 0.6019\n",
            "Epoch 9191/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8180\n",
            "Epoch 9191: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3590 - accuracy: 0.8178 - val_loss: 0.6310 - val_accuracy: 0.6046\n",
            "Epoch 9192/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8131\n",
            "Epoch 9192: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 0.3647 - accuracy: 0.8153 - val_loss: 0.6384 - val_accuracy: 0.5864\n",
            "Epoch 9193/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3696 - accuracy: 0.8102\n",
            "Epoch 9193: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 158ms/step - loss: 0.3690 - accuracy: 0.8103 - val_loss: 0.5290 - val_accuracy: 0.6715\n",
            "Epoch 9194/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8138\n",
            "Epoch 9194: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 0.3688 - accuracy: 0.8110 - val_loss: 0.4464 - val_accuracy: 0.7364\n",
            "Epoch 9195/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3659 - accuracy: 0.8146\n",
            "Epoch 9195: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3675 - accuracy: 0.8139 - val_loss: 0.3649 - val_accuracy: 0.7845\n",
            "Epoch 9196/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4393 - accuracy: 0.7713\n",
            "Epoch 9196: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.4294 - accuracy: 0.7775 - val_loss: 0.4063 - val_accuracy: 0.7593\n",
            "Epoch 9197/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4239 - accuracy: 0.7788\n",
            "Epoch 9197: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.4204 - accuracy: 0.7802 - val_loss: 0.4943 - val_accuracy: 0.7066\n",
            "Epoch 9198/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3760 - accuracy: 0.8070\n",
            "Epoch 9198: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3763 - accuracy: 0.8072 - val_loss: 0.6294 - val_accuracy: 0.6004\n",
            "Epoch 9199/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8139\n",
            "Epoch 9199: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 0.3612 - accuracy: 0.8145 - val_loss: 0.6610 - val_accuracy: 0.5835\n",
            "Epoch 9200/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3677 - accuracy: 0.8104\n",
            "Epoch 9200: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3662 - accuracy: 0.8112 - val_loss: 0.5184 - val_accuracy: 0.6770\n",
            "Epoch 9201/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3584 - accuracy: 0.8202\n",
            "Epoch 9201: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3573 - accuracy: 0.8207 - val_loss: 0.5421 - val_accuracy: 0.6645\n",
            "Epoch 9202/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8223\n",
            "Epoch 9202: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3568 - accuracy: 0.8216 - val_loss: 0.4739 - val_accuracy: 0.7224\n",
            "Epoch 9203/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8151\n",
            "Epoch 9203: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3636 - accuracy: 0.8151 - val_loss: 0.5594 - val_accuracy: 0.6513\n",
            "Epoch 9204/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3541 - accuracy: 0.8239\n",
            "Epoch 9204: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3553 - accuracy: 0.8226 - val_loss: 0.5272 - val_accuracy: 0.6722\n",
            "Epoch 9205/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8246\n",
            "Epoch 9205: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3523 - accuracy: 0.8245 - val_loss: 0.4708 - val_accuracy: 0.7228\n",
            "Epoch 9206/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3558 - accuracy: 0.8203\n",
            "Epoch 9206: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3551 - accuracy: 0.8217 - val_loss: 0.5165 - val_accuracy: 0.6895\n",
            "Epoch 9207/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3560 - accuracy: 0.8213\n",
            "Epoch 9207: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3558 - accuracy: 0.8212 - val_loss: 0.5596 - val_accuracy: 0.6491\n",
            "Epoch 9208/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8223\n",
            "Epoch 9208: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3532 - accuracy: 0.8223 - val_loss: 0.5585 - val_accuracy: 0.6502\n",
            "Epoch 9209/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3525 - accuracy: 0.8237\n",
            "Epoch 9209: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3524 - accuracy: 0.8232 - val_loss: 0.6388 - val_accuracy: 0.5879\n",
            "Epoch 9210/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8169\n",
            "Epoch 9210: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3584 - accuracy: 0.8181 - val_loss: 0.6169 - val_accuracy: 0.6096\n",
            "Epoch 9211/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3637 - accuracy: 0.8148\n",
            "Epoch 9211: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3618 - accuracy: 0.8172 - val_loss: 0.5556 - val_accuracy: 0.6561\n",
            "Epoch 9212/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8225\n",
            "Epoch 9212: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3582 - accuracy: 0.8204 - val_loss: 0.4751 - val_accuracy: 0.7206\n",
            "Epoch 9213/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8231\n",
            "Epoch 9213: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3551 - accuracy: 0.8224 - val_loss: 0.4273 - val_accuracy: 0.7549\n",
            "Epoch 9214/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3723 - accuracy: 0.8072\n",
            "Epoch 9214: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3712 - accuracy: 0.8083 - val_loss: 0.5373 - val_accuracy: 0.6715\n",
            "Epoch 9215/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8242\n",
            "Epoch 9215: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3550 - accuracy: 0.8233 - val_loss: 0.6636 - val_accuracy: 0.5809\n",
            "Epoch 9216/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3598 - accuracy: 0.8183\n",
            "Epoch 9216: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3613 - accuracy: 0.8178 - val_loss: 0.5451 - val_accuracy: 0.6632\n",
            "Epoch 9217/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3594 - accuracy: 0.8168\n",
            "Epoch 9217: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3569 - accuracy: 0.8187 - val_loss: 0.5888 - val_accuracy: 0.6322\n",
            "Epoch 9218/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3625 - accuracy: 0.8191\n",
            "Epoch 9218: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3613 - accuracy: 0.8188 - val_loss: 0.4500 - val_accuracy: 0.7406\n",
            "Epoch 9219/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3594 - accuracy: 0.8182\n",
            "Epoch 9219: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3573 - accuracy: 0.8193 - val_loss: 0.5491 - val_accuracy: 0.6564\n",
            "Epoch 9220/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3518 - accuracy: 0.8236\n",
            "Epoch 9220: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3536 - accuracy: 0.8223 - val_loss: 0.6623 - val_accuracy: 0.5824\n",
            "Epoch 9221/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3747 - accuracy: 0.8113\n",
            "Epoch 9221: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3746 - accuracy: 0.8103 - val_loss: 0.4722 - val_accuracy: 0.7250\n",
            "Epoch 9222/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8202\n",
            "Epoch 9222: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3596 - accuracy: 0.8196 - val_loss: 0.4690 - val_accuracy: 0.7275\n",
            "Epoch 9223/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3598 - accuracy: 0.8188\n",
            "Epoch 9223: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3602 - accuracy: 0.8185 - val_loss: 0.4743 - val_accuracy: 0.7220\n",
            "Epoch 9224/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3673 - accuracy: 0.8095\n",
            "Epoch 9224: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3675 - accuracy: 0.8095 - val_loss: 0.6878 - val_accuracy: 0.5624\n",
            "Epoch 9225/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3626 - accuracy: 0.8153\n",
            "Epoch 9225: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3622 - accuracy: 0.8155 - val_loss: 0.7374 - val_accuracy: 0.5348\n",
            "Epoch 9226/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3981 - accuracy: 0.7927\n",
            "Epoch 9226: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3977 - accuracy: 0.7925 - val_loss: 0.4360 - val_accuracy: 0.7426\n",
            "Epoch 9227/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8120\n",
            "Epoch 9227: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3737 - accuracy: 0.8133 - val_loss: 0.3886 - val_accuracy: 0.7777\n",
            "Epoch 9228/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4185 - accuracy: 0.7817\n",
            "Epoch 9228: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4158 - accuracy: 0.7829 - val_loss: 0.5513 - val_accuracy: 0.6614\n",
            "Epoch 9229/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3749 - accuracy: 0.8066\n",
            "Epoch 9229: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3779 - accuracy: 0.8052 - val_loss: 0.7097 - val_accuracy: 0.5710\n",
            "Epoch 9230/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3720 - accuracy: 0.8121\n",
            "Epoch 9230: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3709 - accuracy: 0.8131 - val_loss: 0.7119 - val_accuracy: 0.5552\n",
            "Epoch 9231/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3685 - accuracy: 0.8124\n",
            "Epoch 9231: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3675 - accuracy: 0.8130 - val_loss: 0.5994 - val_accuracy: 0.6188\n",
            "Epoch 9232/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8200\n",
            "Epoch 9232: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3584 - accuracy: 0.8208 - val_loss: 0.5323 - val_accuracy: 0.6684\n",
            "Epoch 9233/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3543 - accuracy: 0.8193\n",
            "Epoch 9233: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3541 - accuracy: 0.8201 - val_loss: 0.4985 - val_accuracy: 0.7048\n",
            "Epoch 9234/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3587 - accuracy: 0.8188\n",
            "Epoch 9234: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3595 - accuracy: 0.8181 - val_loss: 0.5160 - val_accuracy: 0.6893\n",
            "Epoch 9235/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8177\n",
            "Epoch 9235: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3578 - accuracy: 0.8172 - val_loss: 0.6078 - val_accuracy: 0.6186\n",
            "Epoch 9236/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8168\n",
            "Epoch 9236: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3586 - accuracy: 0.8177 - val_loss: 0.5428 - val_accuracy: 0.6658\n",
            "Epoch 9237/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8184\n",
            "Epoch 9237: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3574 - accuracy: 0.8183 - val_loss: 0.5366 - val_accuracy: 0.6768\n",
            "Epoch 9238/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8254\n",
            "Epoch 9238: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3562 - accuracy: 0.8212 - val_loss: 0.4227 - val_accuracy: 0.7597\n",
            "Epoch 9239/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3764 - accuracy: 0.8088\n",
            "Epoch 9239: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3739 - accuracy: 0.8107 - val_loss: 0.5369 - val_accuracy: 0.6761\n",
            "Epoch 9240/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8172\n",
            "Epoch 9240: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3634 - accuracy: 0.8153 - val_loss: 0.7596 - val_accuracy: 0.5234\n",
            "Epoch 9241/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3772 - accuracy: 0.8070\n",
            "Epoch 9241: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3761 - accuracy: 0.8082 - val_loss: 0.6748 - val_accuracy: 0.5947\n",
            "Epoch 9242/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8083\n",
            "Epoch 9242: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3763 - accuracy: 0.8095 - val_loss: 0.6039 - val_accuracy: 0.6133\n",
            "Epoch 9243/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3656 - accuracy: 0.8162\n",
            "Epoch 9243: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3678 - accuracy: 0.8141 - val_loss: 0.4056 - val_accuracy: 0.7661\n",
            "Epoch 9244/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4027 - accuracy: 0.7923\n",
            "Epoch 9244: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4007 - accuracy: 0.7939 - val_loss: 0.5110 - val_accuracy: 0.6897\n",
            "Epoch 9245/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8143\n",
            "Epoch 9245: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3701 - accuracy: 0.8108 - val_loss: 0.7574 - val_accuracy: 0.5306\n",
            "Epoch 9246/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3799 - accuracy: 0.8039\n",
            "Epoch 9246: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3776 - accuracy: 0.8058 - val_loss: 0.7230 - val_accuracy: 0.5635\n",
            "Epoch 9247/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3902 - accuracy: 0.7996\n",
            "Epoch 9247: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3894 - accuracy: 0.8001 - val_loss: 0.4858 - val_accuracy: 0.7046\n",
            "Epoch 9248/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3610 - accuracy: 0.8182\n",
            "Epoch 9248: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3601 - accuracy: 0.8185 - val_loss: 0.4969 - val_accuracy: 0.6976\n",
            "Epoch 9249/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8219\n",
            "Epoch 9249: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3540 - accuracy: 0.8210 - val_loss: 0.4398 - val_accuracy: 0.7494\n",
            "Epoch 9250/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3675 - accuracy: 0.8118\n",
            "Epoch 9250: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3663 - accuracy: 0.8133 - val_loss: 0.4471 - val_accuracy: 0.7400\n",
            "Epoch 9251/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8033\n",
            "Epoch 9251: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3800 - accuracy: 0.8021 - val_loss: 0.6908 - val_accuracy: 0.5565\n",
            "Epoch 9252/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3609 - accuracy: 0.8174\n",
            "Epoch 9252: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3625 - accuracy: 0.8160 - val_loss: 0.7542 - val_accuracy: 0.5302\n",
            "Epoch 9253/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3745 - accuracy: 0.8076\n",
            "Epoch 9253: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3744 - accuracy: 0.8076 - val_loss: 0.6017 - val_accuracy: 0.6219\n",
            "Epoch 9254/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3630 - accuracy: 0.8135\n",
            "Epoch 9254: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3636 - accuracy: 0.8137 - val_loss: 0.4778 - val_accuracy: 0.7196\n",
            "Epoch 9255/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8225\n",
            "Epoch 9255: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3572 - accuracy: 0.8207 - val_loss: 0.4482 - val_accuracy: 0.7303\n",
            "Epoch 9256/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3584 - accuracy: 0.8177\n",
            "Epoch 9256: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3594 - accuracy: 0.8176 - val_loss: 0.5232 - val_accuracy: 0.6801\n",
            "Epoch 9257/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8240\n",
            "Epoch 9257: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3527 - accuracy: 0.8243 - val_loss: 0.5536 - val_accuracy: 0.6531\n",
            "Epoch 9258/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3513 - accuracy: 0.8228\n",
            "Epoch 9258: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3542 - accuracy: 0.8213 - val_loss: 0.7861 - val_accuracy: 0.5117\n",
            "Epoch 9259/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3934 - accuracy: 0.7943\n",
            "Epoch 9259: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3892 - accuracy: 0.7971 - val_loss: 0.6613 - val_accuracy: 0.5817\n",
            "Epoch 9260/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3978 - accuracy: 0.7949\n",
            "Epoch 9260: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4040 - accuracy: 0.7905 - val_loss: 0.4334 - val_accuracy: 0.7522\n",
            "Epoch 9261/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8149\n",
            "Epoch 9261: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3719 - accuracy: 0.8136 - val_loss: 0.3585 - val_accuracy: 0.7933\n",
            "Epoch 9262/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4319 - accuracy: 0.7759\n",
            "Epoch 9262: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 0.4241 - accuracy: 0.7812 - val_loss: 0.4540 - val_accuracy: 0.7406\n",
            "Epoch 9263/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3974 - accuracy: 0.7937\n",
            "Epoch 9263: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3997 - accuracy: 0.7920 - val_loss: 0.6540 - val_accuracy: 0.5980\n",
            "Epoch 9264/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3642 - accuracy: 0.8149\n",
            "Epoch 9264: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3668 - accuracy: 0.8136 - val_loss: 0.9810 - val_accuracy: 0.4512\n",
            "Epoch 9265/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4187 - accuracy: 0.7874\n",
            "Epoch 9265: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.4130 - accuracy: 0.7897 - val_loss: 0.7537 - val_accuracy: 0.5341\n",
            "Epoch 9266/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3944 - accuracy: 0.7968\n",
            "Epoch 9266: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3918 - accuracy: 0.7985 - val_loss: 0.5833 - val_accuracy: 0.6357\n",
            "Epoch 9267/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3699 - accuracy: 0.8128\n",
            "Epoch 9267: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3674 - accuracy: 0.8150 - val_loss: 0.6270 - val_accuracy: 0.5997\n",
            "Epoch 9268/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8087\n",
            "Epoch 9268: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3814 - accuracy: 0.8064 - val_loss: 0.4670 - val_accuracy: 0.7244\n",
            "Epoch 9269/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3656 - accuracy: 0.8170\n",
            "Epoch 9269: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3713 - accuracy: 0.8140 - val_loss: 0.3904 - val_accuracy: 0.7768\n",
            "Epoch 9270/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3916 - accuracy: 0.7986\n",
            "Epoch 9270: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3893 - accuracy: 0.7998 - val_loss: 0.4253 - val_accuracy: 0.7531\n",
            "Epoch 9271/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3966 - accuracy: 0.7977\n",
            "Epoch 9271: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3927 - accuracy: 0.8005 - val_loss: 0.5938 - val_accuracy: 0.6296\n",
            "Epoch 9272/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3610 - accuracy: 0.8156\n",
            "Epoch 9272: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3638 - accuracy: 0.8148 - val_loss: 0.6934 - val_accuracy: 0.5673\n",
            "Epoch 9273/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3698 - accuracy: 0.8122\n",
            "Epoch 9273: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3684 - accuracy: 0.8132 - val_loss: 0.6576 - val_accuracy: 0.5774\n",
            "Epoch 9274/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8118\n",
            "Epoch 9274: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3656 - accuracy: 0.8116 - val_loss: 0.5000 - val_accuracy: 0.7040\n",
            "Epoch 9275/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8231\n",
            "Epoch 9275: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3570 - accuracy: 0.8218 - val_loss: 0.5094 - val_accuracy: 0.6994\n",
            "Epoch 9276/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8182\n",
            "Epoch 9276: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3618 - accuracy: 0.8177 - val_loss: 0.5174 - val_accuracy: 0.6849\n",
            "Epoch 9277/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3651 - accuracy: 0.8158\n",
            "Epoch 9277: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3662 - accuracy: 0.8150 - val_loss: 0.6871 - val_accuracy: 0.5600\n",
            "Epoch 9278/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3633 - accuracy: 0.8142\n",
            "Epoch 9278: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3629 - accuracy: 0.8144 - val_loss: 0.5999 - val_accuracy: 0.6294\n",
            "Epoch 9279/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8213\n",
            "Epoch 9279: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3574 - accuracy: 0.8206 - val_loss: 0.5769 - val_accuracy: 0.6362\n",
            "Epoch 9280/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3554 - accuracy: 0.8230\n",
            "Epoch 9280: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3542 - accuracy: 0.8235 - val_loss: 0.4766 - val_accuracy: 0.7171\n",
            "Epoch 9281/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3527 - accuracy: 0.8202\n",
            "Epoch 9281: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3544 - accuracy: 0.8196 - val_loss: 0.5032 - val_accuracy: 0.7018\n",
            "Epoch 9282/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3640 - accuracy: 0.8147\n",
            "Epoch 9282: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3632 - accuracy: 0.8153 - val_loss: 0.4640 - val_accuracy: 0.7329\n",
            "Epoch 9283/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3763 - accuracy: 0.8039\n",
            "Epoch 9283: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3776 - accuracy: 0.8041 - val_loss: 0.5636 - val_accuracy: 0.6430\n",
            "Epoch 9284/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3695 - accuracy: 0.8113\n",
            "Epoch 9284: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3768 - accuracy: 0.8069 - val_loss: 0.7448 - val_accuracy: 0.5365\n",
            "Epoch 9285/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3692 - accuracy: 0.8107\n",
            "Epoch 9285: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3721 - accuracy: 0.8095 - val_loss: 0.8254 - val_accuracy: 0.5069\n",
            "Epoch 9286/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3855 - accuracy: 0.8015\n",
            "Epoch 9286: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3842 - accuracy: 0.8024 - val_loss: 0.8135 - val_accuracy: 0.5111\n",
            "Epoch 9287/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4018 - accuracy: 0.7938\n",
            "Epoch 9287: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3979 - accuracy: 0.7964 - val_loss: 0.5415 - val_accuracy: 0.6684\n",
            "Epoch 9288/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3601 - accuracy: 0.8202\n",
            "Epoch 9288: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3619 - accuracy: 0.8186 - val_loss: 0.4363 - val_accuracy: 0.7479\n",
            "Epoch 9289/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3729 - accuracy: 0.8108\n",
            "Epoch 9289: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3699 - accuracy: 0.8132 - val_loss: 0.3964 - val_accuracy: 0.7733\n",
            "Epoch 9290/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4040 - accuracy: 0.7902\n",
            "Epoch 9290: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4018 - accuracy: 0.7919 - val_loss: 0.4991 - val_accuracy: 0.6976\n",
            "Epoch 9291/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3879 - accuracy: 0.8001\n",
            "Epoch 9291: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3911 - accuracy: 0.7982 - val_loss: 0.6260 - val_accuracy: 0.6133\n",
            "Epoch 9292/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8126\n",
            "Epoch 9292: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3703 - accuracy: 0.8112 - val_loss: 0.8540 - val_accuracy: 0.4920\n",
            "Epoch 9293/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3852 - accuracy: 0.8017\n",
            "Epoch 9293: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3836 - accuracy: 0.8035 - val_loss: 0.6759 - val_accuracy: 0.5767\n",
            "Epoch 9294/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3664 - accuracy: 0.8150\n",
            "Epoch 9294: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3653 - accuracy: 0.8155 - val_loss: 0.7155 - val_accuracy: 0.5477\n",
            "Epoch 9295/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3718 - accuracy: 0.8104\n",
            "Epoch 9295: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3695 - accuracy: 0.8119 - val_loss: 0.6923 - val_accuracy: 0.5620\n",
            "Epoch 9296/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3826 - accuracy: 0.8014\n",
            "Epoch 9296: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3815 - accuracy: 0.8021 - val_loss: 0.4881 - val_accuracy: 0.7064\n",
            "Epoch 9297/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3573 - accuracy: 0.8188\n",
            "Epoch 9297: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3609 - accuracy: 0.8166 - val_loss: 0.4539 - val_accuracy: 0.7380\n",
            "Epoch 9298/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8202\n",
            "Epoch 9298: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3638 - accuracy: 0.8186 - val_loss: 0.3953 - val_accuracy: 0.7731\n",
            "Epoch 9299/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3973 - accuracy: 0.7935\n",
            "Epoch 9299: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3921 - accuracy: 0.7958 - val_loss: 0.4504 - val_accuracy: 0.7415\n",
            "Epoch 9300/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3723 - accuracy: 0.8088\n",
            "Epoch 9300: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3709 - accuracy: 0.8091 - val_loss: 0.4769 - val_accuracy: 0.7272\n",
            "Epoch 9301/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3621 - accuracy: 0.8158\n",
            "Epoch 9301: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3635 - accuracy: 0.8150 - val_loss: 0.5852 - val_accuracy: 0.6346\n",
            "Epoch 9302/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3597 - accuracy: 0.8198\n",
            "Epoch 9302: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3588 - accuracy: 0.8201 - val_loss: 0.5924 - val_accuracy: 0.6274\n",
            "Epoch 9303/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8220\n",
            "Epoch 9303: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3561 - accuracy: 0.8216 - val_loss: 0.4825 - val_accuracy: 0.7174\n",
            "Epoch 9304/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8129\n",
            "Epoch 9304: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3637 - accuracy: 0.8147 - val_loss: 0.4779 - val_accuracy: 0.7187\n",
            "Epoch 9305/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3726 - accuracy: 0.8082\n",
            "Epoch 9305: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3731 - accuracy: 0.8083 - val_loss: 0.5731 - val_accuracy: 0.6482\n",
            "Epoch 9306/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3610 - accuracy: 0.8141\n",
            "Epoch 9306: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3646 - accuracy: 0.8120 - val_loss: 0.7155 - val_accuracy: 0.5600\n",
            "Epoch 9307/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3740 - accuracy: 0.8108\n",
            "Epoch 9307: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3724 - accuracy: 0.8110 - val_loss: 0.6152 - val_accuracy: 0.6068\n",
            "Epoch 9308/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3599 - accuracy: 0.8190\n",
            "Epoch 9308: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3595 - accuracy: 0.8185 - val_loss: 0.4861 - val_accuracy: 0.7123\n",
            "Epoch 9309/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3573 - accuracy: 0.8215\n",
            "Epoch 9309: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3557 - accuracy: 0.8218 - val_loss: 0.5892 - val_accuracy: 0.6193\n",
            "Epoch 9310/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8218\n",
            "Epoch 9310: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3580 - accuracy: 0.8190 - val_loss: 0.4022 - val_accuracy: 0.7652\n",
            "Epoch 9311/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3936 - accuracy: 0.7958\n",
            "Epoch 9311: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3913 - accuracy: 0.7979 - val_loss: 0.6581 - val_accuracy: 0.5813\n",
            "Epoch 9312/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8188\n",
            "Epoch 9312: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3580 - accuracy: 0.8198 - val_loss: 0.6554 - val_accuracy: 0.6011\n",
            "Epoch 9313/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3783 - accuracy: 0.8070\n",
            "Epoch 9313: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3775 - accuracy: 0.8065 - val_loss: 0.5256 - val_accuracy: 0.6783\n",
            "Epoch 9314/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3632 - accuracy: 0.8180\n",
            "Epoch 9314: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3625 - accuracy: 0.8176 - val_loss: 0.4251 - val_accuracy: 0.7527\n",
            "Epoch 9315/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3723 - accuracy: 0.8090\n",
            "Epoch 9315: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3720 - accuracy: 0.8098 - val_loss: 0.4993 - val_accuracy: 0.7018\n",
            "Epoch 9316/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3779 - accuracy: 0.8075\n",
            "Epoch 9316: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3786 - accuracy: 0.8065 - val_loss: 0.6352 - val_accuracy: 0.6052\n",
            "Epoch 9317/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8165\n",
            "Epoch 9317: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3623 - accuracy: 0.8161 - val_loss: 0.6104 - val_accuracy: 0.6074\n",
            "Epoch 9318/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3554 - accuracy: 0.8208\n",
            "Epoch 9318: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3563 - accuracy: 0.8197 - val_loss: 0.5122 - val_accuracy: 0.6950\n",
            "Epoch 9319/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3518 - accuracy: 0.8230\n",
            "Epoch 9319: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3531 - accuracy: 0.8227 - val_loss: 0.5857 - val_accuracy: 0.6276\n",
            "Epoch 9320/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3536 - accuracy: 0.8234\n",
            "Epoch 9320: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3529 - accuracy: 0.8247 - val_loss: 0.7569 - val_accuracy: 0.5354\n",
            "Epoch 9321/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3980 - accuracy: 0.7931\n",
            "Epoch 9321: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3976 - accuracy: 0.7941 - val_loss: 0.4794 - val_accuracy: 0.7180\n",
            "Epoch 9322/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3568 - accuracy: 0.8223\n",
            "Epoch 9322: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3587 - accuracy: 0.8210 - val_loss: 0.4060 - val_accuracy: 0.7654\n",
            "Epoch 9323/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3879 - accuracy: 0.8018\n",
            "Epoch 9323: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 0.3848 - accuracy: 0.8045 - val_loss: 0.5237 - val_accuracy: 0.6772\n",
            "Epoch 9324/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8220\n",
            "Epoch 9324: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3588 - accuracy: 0.8191 - val_loss: 0.7689 - val_accuracy: 0.5240\n",
            "Epoch 9325/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3844 - accuracy: 0.8022\n",
            "Epoch 9325: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3832 - accuracy: 0.8031 - val_loss: 0.6023 - val_accuracy: 0.6237\n",
            "Epoch 9326/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3681 - accuracy: 0.8109\n",
            "Epoch 9326: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3681 - accuracy: 0.8109 - val_loss: 0.5233 - val_accuracy: 0.6805\n",
            "Epoch 9327/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8218\n",
            "Epoch 9327: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3595 - accuracy: 0.8206 - val_loss: 0.4792 - val_accuracy: 0.7202\n",
            "Epoch 9328/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3622 - accuracy: 0.8192\n",
            "Epoch 9328: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3610 - accuracy: 0.8198 - val_loss: 0.4139 - val_accuracy: 0.7604\n",
            "Epoch 9329/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3823 - accuracy: 0.8048\n",
            "Epoch 9329: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3835 - accuracy: 0.8031 - val_loss: 0.5974 - val_accuracy: 0.6276\n",
            "Epoch 9330/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8150\n",
            "Epoch 9330: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 0.3664 - accuracy: 0.8134 - val_loss: 0.7961 - val_accuracy: 0.5192\n",
            "Epoch 9331/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3828 - accuracy: 0.8058\n",
            "Epoch 9331: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3794 - accuracy: 0.8081 - val_loss: 0.7287 - val_accuracy: 0.5387\n",
            "Epoch 9332/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8124\n",
            "Epoch 9332: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3667 - accuracy: 0.8131 - val_loss: 0.6114 - val_accuracy: 0.6111\n",
            "Epoch 9333/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3577 - accuracy: 0.8185\n",
            "Epoch 9333: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3563 - accuracy: 0.8205 - val_loss: 0.6023 - val_accuracy: 0.6140\n",
            "Epoch 9334/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3603 - accuracy: 0.8161\n",
            "Epoch 9334: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3607 - accuracy: 0.8157 - val_loss: 0.5180 - val_accuracy: 0.6836\n",
            "Epoch 9335/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8146\n",
            "Epoch 9335: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3674 - accuracy: 0.8124 - val_loss: 0.4516 - val_accuracy: 0.7351\n",
            "Epoch 9336/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3625 - accuracy: 0.8165\n",
            "Epoch 9336: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3621 - accuracy: 0.8167 - val_loss: 0.4354 - val_accuracy: 0.7452\n",
            "Epoch 9337/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3732 - accuracy: 0.8093\n",
            "Epoch 9337: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3749 - accuracy: 0.8088 - val_loss: 0.4981 - val_accuracy: 0.6965\n",
            "Epoch 9338/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3729 - accuracy: 0.8100\n",
            "Epoch 9338: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.3735 - accuracy: 0.8098 - val_loss: 0.6455 - val_accuracy: 0.5953\n",
            "Epoch 9339/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3631 - accuracy: 0.8135\n",
            "Epoch 9339: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3666 - accuracy: 0.8119 - val_loss: 0.7694 - val_accuracy: 0.5335\n",
            "Epoch 9340/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3787 - accuracy: 0.8055\n",
            "Epoch 9340: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3780 - accuracy: 0.8063 - val_loss: 0.7410 - val_accuracy: 0.5385\n",
            "Epoch 9341/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3853 - accuracy: 0.8026\n",
            "Epoch 9341: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3833 - accuracy: 0.8033 - val_loss: 0.5465 - val_accuracy: 0.6627\n",
            "Epoch 9342/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8193\n",
            "Epoch 9342: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3590 - accuracy: 0.8183 - val_loss: 0.5134 - val_accuracy: 0.6827\n",
            "Epoch 9343/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8227\n",
            "Epoch 9343: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3564 - accuracy: 0.8222 - val_loss: 0.5170 - val_accuracy: 0.6816\n",
            "Epoch 9344/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8235\n",
            "Epoch 9344: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.3523 - accuracy: 0.8218 - val_loss: 0.4540 - val_accuracy: 0.7369\n",
            "Epoch 9345/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3635 - accuracy: 0.8147\n",
            "Epoch 9345: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3638 - accuracy: 0.8138 - val_loss: 0.5501 - val_accuracy: 0.6568\n",
            "Epoch 9346/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3508 - accuracy: 0.8241\n",
            "Epoch 9346: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3525 - accuracy: 0.8225 - val_loss: 0.5240 - val_accuracy: 0.6807\n",
            "Epoch 9347/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8241\n",
            "Epoch 9347: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3513 - accuracy: 0.8230 - val_loss: 0.5719 - val_accuracy: 0.6438\n",
            "Epoch 9348/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3548 - accuracy: 0.8225\n",
            "Epoch 9348: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3544 - accuracy: 0.8224 - val_loss: 0.4020 - val_accuracy: 0.7729\n",
            "Epoch 9349/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3971 - accuracy: 0.7957\n",
            "Epoch 9349: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3950 - accuracy: 0.7973 - val_loss: 0.4792 - val_accuracy: 0.7226\n",
            "Epoch 9350/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3791 - accuracy: 0.8049\n",
            "Epoch 9350: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3832 - accuracy: 0.8034 - val_loss: 0.7781 - val_accuracy: 0.5227\n",
            "Epoch 9351/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3719 - accuracy: 0.8085\n",
            "Epoch 9351: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3739 - accuracy: 0.8066 - val_loss: 0.8052 - val_accuracy: 0.5133\n",
            "Epoch 9352/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3873 - accuracy: 0.8011\n",
            "Epoch 9352: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3833 - accuracy: 0.8045 - val_loss: 0.6289 - val_accuracy: 0.5973\n",
            "Epoch 9353/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8196\n",
            "Epoch 9353: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3586 - accuracy: 0.8187 - val_loss: 0.4658 - val_accuracy: 0.7259\n",
            "Epoch 9354/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3607 - accuracy: 0.8161\n",
            "Epoch 9354: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3613 - accuracy: 0.8164 - val_loss: 0.5545 - val_accuracy: 0.6581\n",
            "Epoch 9355/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8216\n",
            "Epoch 9355: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3570 - accuracy: 0.8194 - val_loss: 0.7379 - val_accuracy: 0.5293\n",
            "Epoch 9356/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3775 - accuracy: 0.8044\n",
            "Epoch 9356: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3761 - accuracy: 0.8055 - val_loss: 0.5376 - val_accuracy: 0.6722\n",
            "Epoch 9357/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3597 - accuracy: 0.8196\n",
            "Epoch 9357: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3615 - accuracy: 0.8181 - val_loss: 0.4484 - val_accuracy: 0.7310\n",
            "Epoch 9358/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3979 - accuracy: 0.7934\n",
            "Epoch 9358: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3944 - accuracy: 0.7959 - val_loss: 0.5168 - val_accuracy: 0.6807\n",
            "Epoch 9359/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3630 - accuracy: 0.8137\n",
            "Epoch 9359: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3653 - accuracy: 0.8127 - val_loss: 0.7404 - val_accuracy: 0.5383\n",
            "Epoch 9360/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3652 - accuracy: 0.8113\n",
            "Epoch 9360: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3668 - accuracy: 0.8105 - val_loss: 0.8248 - val_accuracy: 0.5063\n",
            "Epoch 9361/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4122 - accuracy: 0.7859\n",
            "Epoch 9361: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4088 - accuracy: 0.7872 - val_loss: 0.5354 - val_accuracy: 0.6704\n",
            "Epoch 9362/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3634 - accuracy: 0.8173\n",
            "Epoch 9362: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3655 - accuracy: 0.8154 - val_loss: 0.5253 - val_accuracy: 0.6838\n",
            "Epoch 9363/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8206\n",
            "Epoch 9363: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3637 - accuracy: 0.8187 - val_loss: 0.4316 - val_accuracy: 0.7549\n",
            "Epoch 9364/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8151\n",
            "Epoch 9364: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3640 - accuracy: 0.8156 - val_loss: 0.4431 - val_accuracy: 0.7457\n",
            "Epoch 9365/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3696 - accuracy: 0.8104\n",
            "Epoch 9365: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3676 - accuracy: 0.8123 - val_loss: 0.4902 - val_accuracy: 0.7066\n",
            "Epoch 9366/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8085\n",
            "Epoch 9366: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3754 - accuracy: 0.8063 - val_loss: 0.7403 - val_accuracy: 0.5458\n",
            "Epoch 9367/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3666 - accuracy: 0.8122\n",
            "Epoch 9367: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3680 - accuracy: 0.8120 - val_loss: 0.6666 - val_accuracy: 0.5817\n",
            "Epoch 9368/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8165\n",
            "Epoch 9368: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3633 - accuracy: 0.8156 - val_loss: 0.6367 - val_accuracy: 0.6022\n",
            "Epoch 9369/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3631 - accuracy: 0.8155\n",
            "Epoch 9369: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3617 - accuracy: 0.8153 - val_loss: 0.6673 - val_accuracy: 0.5782\n",
            "Epoch 9370/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3831 - accuracy: 0.8023\n",
            "Epoch 9370: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3846 - accuracy: 0.8010 - val_loss: 0.4669 - val_accuracy: 0.7239\n",
            "Epoch 9371/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3548 - accuracy: 0.8235\n",
            "Epoch 9371: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3573 - accuracy: 0.8202 - val_loss: 0.4183 - val_accuracy: 0.7604\n",
            "Epoch 9372/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3759 - accuracy: 0.8076\n",
            "Epoch 9372: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3730 - accuracy: 0.8091 - val_loss: 0.4781 - val_accuracy: 0.7202\n",
            "Epoch 9373/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3685 - accuracy: 0.8116\n",
            "Epoch 9373: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3693 - accuracy: 0.8116 - val_loss: 0.6797 - val_accuracy: 0.5716\n",
            "Epoch 9374/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8143\n",
            "Epoch 9374: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3621 - accuracy: 0.8157 - val_loss: 0.5809 - val_accuracy: 0.6344\n",
            "Epoch 9375/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3531 - accuracy: 0.8232\n",
            "Epoch 9375: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3551 - accuracy: 0.8212 - val_loss: 0.5255 - val_accuracy: 0.6794\n",
            "Epoch 9376/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3503 - accuracy: 0.8245\n",
            "Epoch 9376: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3513 - accuracy: 0.8239 - val_loss: 0.5206 - val_accuracy: 0.6816\n",
            "Epoch 9377/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8187\n",
            "Epoch 9377: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3573 - accuracy: 0.8188 - val_loss: 0.6593 - val_accuracy: 0.5809\n",
            "Epoch 9378/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8164\n",
            "Epoch 9378: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3603 - accuracy: 0.8176 - val_loss: 0.6545 - val_accuracy: 0.5872\n",
            "Epoch 9379/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3687 - accuracy: 0.8128\n",
            "Epoch 9379: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3684 - accuracy: 0.8119 - val_loss: 0.5289 - val_accuracy: 0.6717\n",
            "Epoch 9380/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8171\n",
            "Epoch 9380: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3654 - accuracy: 0.8126 - val_loss: 0.4105 - val_accuracy: 0.7709\n",
            "Epoch 9381/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8152\n",
            "Epoch 9381: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3664 - accuracy: 0.8142 - val_loss: 0.4303 - val_accuracy: 0.7529\n",
            "Epoch 9382/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3843 - accuracy: 0.8011\n",
            "Epoch 9382: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3850 - accuracy: 0.8010 - val_loss: 0.5963 - val_accuracy: 0.6169\n",
            "Epoch 9383/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8199\n",
            "Epoch 9383: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3604 - accuracy: 0.8179 - val_loss: 0.8327 - val_accuracy: 0.4953\n",
            "Epoch 9384/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3882 - accuracy: 0.7995\n",
            "Epoch 9384: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3841 - accuracy: 0.8022 - val_loss: 0.6903 - val_accuracy: 0.5578\n",
            "Epoch 9385/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3725 - accuracy: 0.8098\n",
            "Epoch 9385: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3726 - accuracy: 0.8093 - val_loss: 0.4613 - val_accuracy: 0.7327\n",
            "Epoch 9386/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3566 - accuracy: 0.8208\n",
            "Epoch 9386: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3558 - accuracy: 0.8218 - val_loss: 0.5307 - val_accuracy: 0.6730\n",
            "Epoch 9387/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3502 - accuracy: 0.8264\n",
            "Epoch 9387: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3521 - accuracy: 0.8247 - val_loss: 0.4939 - val_accuracy: 0.7079\n",
            "Epoch 9388/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3543 - accuracy: 0.8221\n",
            "Epoch 9388: loss did not improve from 0.35124\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3555 - accuracy: 0.8208 - val_loss: 0.5808 - val_accuracy: 0.6313\n",
            "Epoch 9389/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8248\n",
            "Epoch 9389: loss improved from 0.35124 to 0.34995, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 1s 274ms/step - loss: 0.3500 - accuracy: 0.8253 - val_loss: 0.6107 - val_accuracy: 0.6142\n",
            "Epoch 9390/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3536 - accuracy: 0.8230\n",
            "Epoch 9390: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3547 - accuracy: 0.8218 - val_loss: 0.4723 - val_accuracy: 0.7299\n",
            "Epoch 9391/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3549 - accuracy: 0.8216\n",
            "Epoch 9391: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3534 - accuracy: 0.8231 - val_loss: 0.4910 - val_accuracy: 0.7112\n",
            "Epoch 9392/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3578 - accuracy: 0.8189\n",
            "Epoch 9392: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3583 - accuracy: 0.8177 - val_loss: 0.6932 - val_accuracy: 0.5659\n",
            "Epoch 9393/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3751 - accuracy: 0.8064\n",
            "Epoch 9393: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3754 - accuracy: 0.8066 - val_loss: 0.4707 - val_accuracy: 0.7270\n",
            "Epoch 9394/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8206\n",
            "Epoch 9394: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3541 - accuracy: 0.8209 - val_loss: 0.5136 - val_accuracy: 0.6862\n",
            "Epoch 9395/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8229\n",
            "Epoch 9395: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3535 - accuracy: 0.8221 - val_loss: 0.5674 - val_accuracy: 0.6432\n",
            "Epoch 9396/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3534 - accuracy: 0.8222\n",
            "Epoch 9396: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3551 - accuracy: 0.8206 - val_loss: 0.4134 - val_accuracy: 0.7597\n",
            "Epoch 9397/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3905 - accuracy: 0.7964\n",
            "Epoch 9397: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3892 - accuracy: 0.7973 - val_loss: 0.6013 - val_accuracy: 0.6291\n",
            "Epoch 9398/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8183\n",
            "Epoch 9398: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3586 - accuracy: 0.8184 - val_loss: 0.7831 - val_accuracy: 0.5212\n",
            "Epoch 9399/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3906 - accuracy: 0.7989\n",
            "Epoch 9399: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3881 - accuracy: 0.8009 - val_loss: 0.5255 - val_accuracy: 0.6772\n",
            "Epoch 9400/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3620 - accuracy: 0.8160\n",
            "Epoch 9400: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3662 - accuracy: 0.8133 - val_loss: 0.4189 - val_accuracy: 0.7540\n",
            "Epoch 9401/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4041 - accuracy: 0.7943\n",
            "Epoch 9401: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4008 - accuracy: 0.7957 - val_loss: 0.5706 - val_accuracy: 0.6478\n",
            "Epoch 9402/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8203\n",
            "Epoch 9402: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3604 - accuracy: 0.8176 - val_loss: 0.6521 - val_accuracy: 0.5912\n",
            "Epoch 9403/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8169\n",
            "Epoch 9403: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3590 - accuracy: 0.8173 - val_loss: 0.6020 - val_accuracy: 0.6188\n",
            "Epoch 9404/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8220\n",
            "Epoch 9404: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3585 - accuracy: 0.8213 - val_loss: 0.5083 - val_accuracy: 0.7048\n",
            "Epoch 9405/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3558 - accuracy: 0.8203\n",
            "Epoch 9405: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3570 - accuracy: 0.8193 - val_loss: 0.4443 - val_accuracy: 0.7443\n",
            "Epoch 9406/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8047\n",
            "Epoch 9406: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3841 - accuracy: 0.8064 - val_loss: 0.6126 - val_accuracy: 0.6136\n",
            "Epoch 9407/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8213\n",
            "Epoch 9407: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3574 - accuracy: 0.8204 - val_loss: 0.7643 - val_accuracy: 0.5240\n",
            "Epoch 9408/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3890 - accuracy: 0.7948\n",
            "Epoch 9408: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3865 - accuracy: 0.7959 - val_loss: 0.5351 - val_accuracy: 0.6697\n",
            "Epoch 9409/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3573 - accuracy: 0.8231\n",
            "Epoch 9409: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3594 - accuracy: 0.8216 - val_loss: 0.4546 - val_accuracy: 0.7347\n",
            "Epoch 9410/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8197\n",
            "Epoch 9410: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3579 - accuracy: 0.8195 - val_loss: 0.4906 - val_accuracy: 0.7027\n",
            "Epoch 9411/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8200\n",
            "Epoch 9411: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3561 - accuracy: 0.8215 - val_loss: 0.5490 - val_accuracy: 0.6627\n",
            "Epoch 9412/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3518 - accuracy: 0.8233\n",
            "Epoch 9412: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3521 - accuracy: 0.8240 - val_loss: 0.4949 - val_accuracy: 0.7031\n",
            "Epoch 9413/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8193\n",
            "Epoch 9413: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3523 - accuracy: 0.8201 - val_loss: 0.5628 - val_accuracy: 0.6496\n",
            "Epoch 9414/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8221\n",
            "Epoch 9414: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3524 - accuracy: 0.8230 - val_loss: 0.5983 - val_accuracy: 0.6243\n",
            "Epoch 9415/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8172\n",
            "Epoch 9415: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3624 - accuracy: 0.8151 - val_loss: 0.4433 - val_accuracy: 0.7428\n",
            "Epoch 9416/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3643 - accuracy: 0.8161\n",
            "Epoch 9416: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3645 - accuracy: 0.8152 - val_loss: 0.6010 - val_accuracy: 0.6263\n",
            "Epoch 9417/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3530 - accuracy: 0.8236\n",
            "Epoch 9417: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3536 - accuracy: 0.8223 - val_loss: 0.5518 - val_accuracy: 0.6585\n",
            "Epoch 9418/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8198\n",
            "Epoch 9418: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3532 - accuracy: 0.8209 - val_loss: 0.4488 - val_accuracy: 0.7446\n",
            "Epoch 9419/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8154\n",
            "Epoch 9419: loss did not improve from 0.34995\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3620 - accuracy: 0.8162 - val_loss: 0.5206 - val_accuracy: 0.6827\n",
            "Epoch 9420/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3494 - accuracy: 0.8236\n",
            "Epoch 9420: loss improved from 0.34995 to 0.34965, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 190ms/step - loss: 0.3496 - accuracy: 0.8238 - val_loss: 0.5411 - val_accuracy: 0.6660\n",
            "Epoch 9421/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3530 - accuracy: 0.8226\n",
            "Epoch 9421: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3530 - accuracy: 0.8234 - val_loss: 0.6046 - val_accuracy: 0.6232\n",
            "Epoch 9422/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8171\n",
            "Epoch 9422: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3561 - accuracy: 0.8182 - val_loss: 0.7505 - val_accuracy: 0.5357\n",
            "Epoch 9423/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3950 - accuracy: 0.7946\n",
            "Epoch 9423: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3987 - accuracy: 0.7926 - val_loss: 0.3759 - val_accuracy: 0.7786\n",
            "Epoch 9424/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4053 - accuracy: 0.7885\n",
            "Epoch 9424: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4001 - accuracy: 0.7925 - val_loss: 0.5249 - val_accuracy: 0.6906\n",
            "Epoch 9425/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3578 - accuracy: 0.8175\n",
            "Epoch 9425: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3589 - accuracy: 0.8174 - val_loss: 0.6116 - val_accuracy: 0.6171\n",
            "Epoch 9426/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3581 - accuracy: 0.8199\n",
            "Epoch 9426: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3578 - accuracy: 0.8185 - val_loss: 0.4658 - val_accuracy: 0.7277\n",
            "Epoch 9427/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3703 - accuracy: 0.8109\n",
            "Epoch 9427: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3715 - accuracy: 0.8106 - val_loss: 0.6592 - val_accuracy: 0.5850\n",
            "Epoch 9428/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3614 - accuracy: 0.8158\n",
            "Epoch 9428: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3606 - accuracy: 0.8161 - val_loss: 0.7610 - val_accuracy: 0.5249\n",
            "Epoch 9429/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4055 - accuracy: 0.7868\n",
            "Epoch 9429: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4071 - accuracy: 0.7861 - val_loss: 0.4677 - val_accuracy: 0.7217\n",
            "Epoch 9430/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8181\n",
            "Epoch 9430: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3612 - accuracy: 0.8173 - val_loss: 0.4481 - val_accuracy: 0.7435\n",
            "Epoch 9431/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8180\n",
            "Epoch 9431: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3607 - accuracy: 0.8185 - val_loss: 0.4023 - val_accuracy: 0.7672\n",
            "Epoch 9432/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4122 - accuracy: 0.7888\n",
            "Epoch 9432: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4100 - accuracy: 0.7902 - val_loss: 0.5519 - val_accuracy: 0.6614\n",
            "Epoch 9433/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3701 - accuracy: 0.8116\n",
            "Epoch 9433: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3738 - accuracy: 0.8097 - val_loss: 0.7641 - val_accuracy: 0.5302\n",
            "Epoch 9434/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8089\n",
            "Epoch 9434: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3749 - accuracy: 0.8069 - val_loss: 1.0897 - val_accuracy: 0.4145\n",
            "Epoch 9435/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4849 - accuracy: 0.7535\n",
            "Epoch 9435: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4771 - accuracy: 0.7568 - val_loss: 0.5698 - val_accuracy: 0.6491\n",
            "Epoch 9436/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3946 - accuracy: 0.8005\n",
            "Epoch 9436: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3995 - accuracy: 0.7975 - val_loss: 0.5110 - val_accuracy: 0.6847\n",
            "Epoch 9437/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3869 - accuracy: 0.8051\n",
            "Epoch 9437: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3975 - accuracy: 0.7995 - val_loss: 0.3938 - val_accuracy: 0.7705\n",
            "Epoch 9438/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3858 - accuracy: 0.8040\n",
            "Epoch 9438: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3856 - accuracy: 0.8043 - val_loss: 0.3999 - val_accuracy: 0.7762\n",
            "Epoch 9439/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3716 - accuracy: 0.8103\n",
            "Epoch 9439: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3738 - accuracy: 0.8081 - val_loss: 0.3689 - val_accuracy: 0.7823\n",
            "Epoch 9440/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4322 - accuracy: 0.7788\n",
            "Epoch 9440: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4242 - accuracy: 0.7832 - val_loss: 0.3788 - val_accuracy: 0.7799\n",
            "Epoch 9441/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4653 - accuracy: 0.7637\n",
            "Epoch 9441: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4577 - accuracy: 0.7672 - val_loss: 0.4438 - val_accuracy: 0.7426\n",
            "Epoch 9442/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4038 - accuracy: 0.7906\n",
            "Epoch 9442: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4019 - accuracy: 0.7923 - val_loss: 0.5994 - val_accuracy: 0.6300\n",
            "Epoch 9443/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3703 - accuracy: 0.8143\n",
            "Epoch 9443: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3729 - accuracy: 0.8116 - val_loss: 0.6094 - val_accuracy: 0.6226\n",
            "Epoch 9444/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3660 - accuracy: 0.8130\n",
            "Epoch 9444: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3690 - accuracy: 0.8121 - val_loss: 0.7680 - val_accuracy: 0.5306\n",
            "Epoch 9445/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3826 - accuracy: 0.8013\n",
            "Epoch 9445: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3807 - accuracy: 0.8022 - val_loss: 0.6543 - val_accuracy: 0.5802\n",
            "Epoch 9446/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8146\n",
            "Epoch 9446: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3651 - accuracy: 0.8147 - val_loss: 0.5679 - val_accuracy: 0.6443\n",
            "Epoch 9447/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8170\n",
            "Epoch 9447: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3607 - accuracy: 0.8163 - val_loss: 0.5785 - val_accuracy: 0.6379\n",
            "Epoch 9448/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3633 - accuracy: 0.8165\n",
            "Epoch 9448: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3668 - accuracy: 0.8141 - val_loss: 0.4042 - val_accuracy: 0.7694\n",
            "Epoch 9449/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3735 - accuracy: 0.8109\n",
            "Epoch 9449: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3722 - accuracy: 0.8118 - val_loss: 0.4512 - val_accuracy: 0.7312\n",
            "Epoch 9450/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3699 - accuracy: 0.8100\n",
            "Epoch 9450: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3682 - accuracy: 0.8115 - val_loss: 0.4404 - val_accuracy: 0.7441\n",
            "Epoch 9451/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3707 - accuracy: 0.8108\n",
            "Epoch 9451: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3712 - accuracy: 0.8095 - val_loss: 0.6375 - val_accuracy: 0.5857\n",
            "Epoch 9452/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8158\n",
            "Epoch 9452: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3605 - accuracy: 0.8161 - val_loss: 0.8484 - val_accuracy: 0.4979\n",
            "Epoch 9453/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3938 - accuracy: 0.7954\n",
            "Epoch 9453: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3905 - accuracy: 0.7981 - val_loss: 0.6729 - val_accuracy: 0.5723\n",
            "Epoch 9454/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3657 - accuracy: 0.8130\n",
            "Epoch 9454: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3668 - accuracy: 0.8119 - val_loss: 0.4798 - val_accuracy: 0.7154\n",
            "Epoch 9455/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3587 - accuracy: 0.8202\n",
            "Epoch 9455: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3586 - accuracy: 0.8206 - val_loss: 0.4692 - val_accuracy: 0.7246\n",
            "Epoch 9456/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3560 - accuracy: 0.8218\n",
            "Epoch 9456: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3576 - accuracy: 0.8207 - val_loss: 0.4846 - val_accuracy: 0.7178\n",
            "Epoch 9457/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3577 - accuracy: 0.8190\n",
            "Epoch 9457: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3589 - accuracy: 0.8184 - val_loss: 0.4964 - val_accuracy: 0.6998\n",
            "Epoch 9458/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8159\n",
            "Epoch 9458: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3597 - accuracy: 0.8178 - val_loss: 0.6818 - val_accuracy: 0.5657\n",
            "Epoch 9459/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3593 - accuracy: 0.8196\n",
            "Epoch 9459: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3596 - accuracy: 0.8200 - val_loss: 0.7028 - val_accuracy: 0.5567\n",
            "Epoch 9460/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3681 - accuracy: 0.8107\n",
            "Epoch 9460: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3679 - accuracy: 0.8101 - val_loss: 0.5426 - val_accuracy: 0.6728\n",
            "Epoch 9461/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8248\n",
            "Epoch 9461: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3531 - accuracy: 0.8224 - val_loss: 0.4652 - val_accuracy: 0.7248\n",
            "Epoch 9462/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3685 - accuracy: 0.8115\n",
            "Epoch 9462: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3671 - accuracy: 0.8123 - val_loss: 0.5341 - val_accuracy: 0.6728\n",
            "Epoch 9463/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8196\n",
            "Epoch 9463: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3574 - accuracy: 0.8200 - val_loss: 0.6165 - val_accuracy: 0.6063\n",
            "Epoch 9464/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3576 - accuracy: 0.8187\n",
            "Epoch 9464: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3580 - accuracy: 0.8188 - val_loss: 0.5604 - val_accuracy: 0.6504\n",
            "Epoch 9465/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3626 - accuracy: 0.8151\n",
            "Epoch 9465: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3627 - accuracy: 0.8150 - val_loss: 0.4862 - val_accuracy: 0.7114\n",
            "Epoch 9466/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8196\n",
            "Epoch 9466: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3615 - accuracy: 0.8184 - val_loss: 0.4803 - val_accuracy: 0.7160\n",
            "Epoch 9467/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3676 - accuracy: 0.8121\n",
            "Epoch 9467: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3666 - accuracy: 0.8128 - val_loss: 0.5790 - val_accuracy: 0.6344\n",
            "Epoch 9468/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8183\n",
            "Epoch 9468: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3573 - accuracy: 0.8191 - val_loss: 0.5376 - val_accuracy: 0.6682\n",
            "Epoch 9469/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3592 - accuracy: 0.8175\n",
            "Epoch 9469: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3589 - accuracy: 0.8177 - val_loss: 0.6252 - val_accuracy: 0.6032\n",
            "Epoch 9470/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3603 - accuracy: 0.8194\n",
            "Epoch 9470: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3606 - accuracy: 0.8181 - val_loss: 0.4511 - val_accuracy: 0.7378\n",
            "Epoch 9471/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3666 - accuracy: 0.8109\n",
            "Epoch 9471: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3653 - accuracy: 0.8116 - val_loss: 0.5026 - val_accuracy: 0.6939\n",
            "Epoch 9472/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3666 - accuracy: 0.8135\n",
            "Epoch 9472: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3686 - accuracy: 0.8123 - val_loss: 0.7199 - val_accuracy: 0.5563\n",
            "Epoch 9473/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8101\n",
            "Epoch 9473: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3675 - accuracy: 0.8123 - val_loss: 0.7116 - val_accuracy: 0.5666\n",
            "Epoch 9474/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3930 - accuracy: 0.7983\n",
            "Epoch 9474: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3930 - accuracy: 0.7988 - val_loss: 0.5393 - val_accuracy: 0.6770\n",
            "Epoch 9475/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3631 - accuracy: 0.8190\n",
            "Epoch 9475: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3644 - accuracy: 0.8169 - val_loss: 0.4947 - val_accuracy: 0.7051\n",
            "Epoch 9476/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3674 - accuracy: 0.8160\n",
            "Epoch 9476: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3687 - accuracy: 0.8144 - val_loss: 0.4687 - val_accuracy: 0.7259\n",
            "Epoch 9477/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8182\n",
            "Epoch 9477: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3639 - accuracy: 0.8182 - val_loss: 0.4069 - val_accuracy: 0.7621\n",
            "Epoch 9478/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3980 - accuracy: 0.7945\n",
            "Epoch 9478: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3949 - accuracy: 0.7975 - val_loss: 0.5242 - val_accuracy: 0.6759\n",
            "Epoch 9479/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8191\n",
            "Epoch 9479: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3606 - accuracy: 0.8191 - val_loss: 0.5390 - val_accuracy: 0.6704\n",
            "Epoch 9480/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3645 - accuracy: 0.8148\n",
            "Epoch 9480: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3671 - accuracy: 0.8137 - val_loss: 0.6621 - val_accuracy: 0.5804\n",
            "Epoch 9481/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3645 - accuracy: 0.8109\n",
            "Epoch 9481: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3637 - accuracy: 0.8130 - val_loss: 0.7086 - val_accuracy: 0.5574\n",
            "Epoch 9482/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8106\n",
            "Epoch 9482: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3712 - accuracy: 0.8105 - val_loss: 0.4987 - val_accuracy: 0.7022\n",
            "Epoch 9483/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8197\n",
            "Epoch 9483: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3580 - accuracy: 0.8206 - val_loss: 0.5187 - val_accuracy: 0.6908\n",
            "Epoch 9484/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3553 - accuracy: 0.8230\n",
            "Epoch 9484: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3551 - accuracy: 0.8228 - val_loss: 0.5191 - val_accuracy: 0.6880\n",
            "Epoch 9485/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3534 - accuracy: 0.8207\n",
            "Epoch 9485: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3531 - accuracy: 0.8216 - val_loss: 0.5137 - val_accuracy: 0.6851\n",
            "Epoch 9486/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3675 - accuracy: 0.8143\n",
            "Epoch 9486: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3675 - accuracy: 0.8142 - val_loss: 0.6509 - val_accuracy: 0.5958\n",
            "Epoch 9487/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3688 - accuracy: 0.8119\n",
            "Epoch 9487: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3666 - accuracy: 0.8143 - val_loss: 0.7006 - val_accuracy: 0.5532\n",
            "Epoch 9488/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3947 - accuracy: 0.7962\n",
            "Epoch 9488: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3954 - accuracy: 0.7948 - val_loss: 0.4377 - val_accuracy: 0.7465\n",
            "Epoch 9489/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3657 - accuracy: 0.8130\n",
            "Epoch 9489: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3688 - accuracy: 0.8109 - val_loss: 0.4398 - val_accuracy: 0.7413\n",
            "Epoch 9490/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4002 - accuracy: 0.7967\n",
            "Epoch 9490: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3961 - accuracy: 0.7985 - val_loss: 0.4133 - val_accuracy: 0.7648\n",
            "Epoch 9491/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3936 - accuracy: 0.7957\n",
            "Epoch 9491: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3922 - accuracy: 0.7966 - val_loss: 0.5018 - val_accuracy: 0.6974\n",
            "Epoch 9492/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3930 - accuracy: 0.7992\n",
            "Epoch 9492: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3997 - accuracy: 0.7957 - val_loss: 0.7101 - val_accuracy: 0.5644\n",
            "Epoch 9493/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8137\n",
            "Epoch 9493: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3689 - accuracy: 0.8114 - val_loss: 0.6462 - val_accuracy: 0.5872\n",
            "Epoch 9494/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3586 - accuracy: 0.8163\n",
            "Epoch 9494: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3626 - accuracy: 0.8129 - val_loss: 0.7085 - val_accuracy: 0.5558\n",
            "Epoch 9495/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8134\n",
            "Epoch 9495: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3657 - accuracy: 0.8136 - val_loss: 0.6457 - val_accuracy: 0.5890\n",
            "Epoch 9496/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3673 - accuracy: 0.8123\n",
            "Epoch 9496: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3664 - accuracy: 0.8127 - val_loss: 0.5553 - val_accuracy: 0.6542\n",
            "Epoch 9497/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3614 - accuracy: 0.8211\n",
            "Epoch 9497: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3601 - accuracy: 0.8202 - val_loss: 0.5245 - val_accuracy: 0.6860\n",
            "Epoch 9498/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3612 - accuracy: 0.8146\n",
            "Epoch 9498: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3629 - accuracy: 0.8134 - val_loss: 0.4731 - val_accuracy: 0.7209\n",
            "Epoch 9499/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3664 - accuracy: 0.8154\n",
            "Epoch 9499: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3670 - accuracy: 0.8148 - val_loss: 0.5284 - val_accuracy: 0.6790\n",
            "Epoch 9500/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8213\n",
            "Epoch 9500: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3550 - accuracy: 0.8209 - val_loss: 0.6384 - val_accuracy: 0.5903\n",
            "Epoch 9501/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8140\n",
            "Epoch 9501: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3642 - accuracy: 0.8144 - val_loss: 0.4827 - val_accuracy: 0.7156\n",
            "Epoch 9502/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8220\n",
            "Epoch 9502: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3591 - accuracy: 0.8195 - val_loss: 0.4294 - val_accuracy: 0.7527\n",
            "Epoch 9503/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3803 - accuracy: 0.8051\n",
            "Epoch 9503: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3753 - accuracy: 0.8088 - val_loss: 0.4762 - val_accuracy: 0.7200\n",
            "Epoch 9504/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3652 - accuracy: 0.8131\n",
            "Epoch 9504: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3683 - accuracy: 0.8115 - val_loss: 0.7662 - val_accuracy: 0.5269\n",
            "Epoch 9505/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3857 - accuracy: 0.7982\n",
            "Epoch 9505: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3833 - accuracy: 0.7988 - val_loss: 0.5151 - val_accuracy: 0.6858\n",
            "Epoch 9506/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8250\n",
            "Epoch 9506: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3552 - accuracy: 0.8219 - val_loss: 0.4116 - val_accuracy: 0.7650\n",
            "Epoch 9507/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3717 - accuracy: 0.8084\n",
            "Epoch 9507: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3710 - accuracy: 0.8091 - val_loss: 0.5390 - val_accuracy: 0.6686\n",
            "Epoch 9508/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3535 - accuracy: 0.8230\n",
            "Epoch 9508: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3544 - accuracy: 0.8224 - val_loss: 0.7663 - val_accuracy: 0.5304\n",
            "Epoch 9509/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4074 - accuracy: 0.7898\n",
            "Epoch 9509: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.4076 - accuracy: 0.7897 - val_loss: 0.4502 - val_accuracy: 0.7408\n",
            "Epoch 9510/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3597 - accuracy: 0.8205\n",
            "Epoch 9510: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3635 - accuracy: 0.8178 - val_loss: 0.4114 - val_accuracy: 0.7584\n",
            "Epoch 9511/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3747 - accuracy: 0.8106\n",
            "Epoch 9511: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3743 - accuracy: 0.8105 - val_loss: 0.3706 - val_accuracy: 0.7922\n",
            "Epoch 9512/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4205 - accuracy: 0.7797\n",
            "Epoch 9512: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4150 - accuracy: 0.7828 - val_loss: 0.4517 - val_accuracy: 0.7321\n",
            "Epoch 9513/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4069 - accuracy: 0.7897\n",
            "Epoch 9513: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4064 - accuracy: 0.7903 - val_loss: 0.6505 - val_accuracy: 0.5984\n",
            "Epoch 9514/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3623 - accuracy: 0.8156\n",
            "Epoch 9514: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3644 - accuracy: 0.8151 - val_loss: 0.7648 - val_accuracy: 0.5315\n",
            "Epoch 9515/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3769 - accuracy: 0.8049\n",
            "Epoch 9515: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3749 - accuracy: 0.8074 - val_loss: 0.5528 - val_accuracy: 0.6623\n",
            "Epoch 9516/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3551 - accuracy: 0.8188\n",
            "Epoch 9516: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3559 - accuracy: 0.8184 - val_loss: 0.4440 - val_accuracy: 0.7411\n",
            "Epoch 9517/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3729 - accuracy: 0.8095\n",
            "Epoch 9517: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3716 - accuracy: 0.8103 - val_loss: 0.5343 - val_accuracy: 0.6748\n",
            "Epoch 9518/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3516 - accuracy: 0.8233\n",
            "Epoch 9518: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3515 - accuracy: 0.8242 - val_loss: 0.5509 - val_accuracy: 0.6566\n",
            "Epoch 9519/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8220\n",
            "Epoch 9519: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3544 - accuracy: 0.8212 - val_loss: 0.4241 - val_accuracy: 0.7580\n",
            "Epoch 9520/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3754 - accuracy: 0.8067\n",
            "Epoch 9520: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3741 - accuracy: 0.8079 - val_loss: 0.5338 - val_accuracy: 0.6774\n",
            "Epoch 9521/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3526 - accuracy: 0.8225\n",
            "Epoch 9521: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3532 - accuracy: 0.8213 - val_loss: 0.6621 - val_accuracy: 0.5789\n",
            "Epoch 9522/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8157\n",
            "Epoch 9522: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3614 - accuracy: 0.8170 - val_loss: 0.5443 - val_accuracy: 0.6673\n",
            "Epoch 9523/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3542 - accuracy: 0.8220\n",
            "Epoch 9523: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3541 - accuracy: 0.8219 - val_loss: 0.5006 - val_accuracy: 0.7024\n",
            "Epoch 9524/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8243\n",
            "Epoch 9524: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3535 - accuracy: 0.8241 - val_loss: 0.4406 - val_accuracy: 0.7446\n",
            "Epoch 9525/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3654 - accuracy: 0.8144\n",
            "Epoch 9525: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3659 - accuracy: 0.8145 - val_loss: 0.5306 - val_accuracy: 0.6752\n",
            "Epoch 9526/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8164\n",
            "Epoch 9526: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3629 - accuracy: 0.8149 - val_loss: 0.7258 - val_accuracy: 0.5508\n",
            "Epoch 9527/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3706 - accuracy: 0.8112\n",
            "Epoch 9527: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.3689 - accuracy: 0.8128 - val_loss: 0.5584 - val_accuracy: 0.6550\n",
            "Epoch 9528/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8200\n",
            "Epoch 9528: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3588 - accuracy: 0.8190 - val_loss: 0.4621 - val_accuracy: 0.7301\n",
            "Epoch 9529/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3664 - accuracy: 0.8143\n",
            "Epoch 9529: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3647 - accuracy: 0.8158 - val_loss: 0.5823 - val_accuracy: 0.6351\n",
            "Epoch 9530/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8194\n",
            "Epoch 9530: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3571 - accuracy: 0.8183 - val_loss: 0.4824 - val_accuracy: 0.7185\n",
            "Epoch 9531/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8220\n",
            "Epoch 9531: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3572 - accuracy: 0.8210 - val_loss: 0.5887 - val_accuracy: 0.6269\n",
            "Epoch 9532/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3568 - accuracy: 0.8174\n",
            "Epoch 9532: loss did not improve from 0.34965\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3559 - accuracy: 0.8179 - val_loss: 0.5225 - val_accuracy: 0.6796\n",
            "Epoch 9533/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3485 - accuracy: 0.8261\n",
            "Epoch 9533: loss improved from 0.34965 to 0.34791, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 194ms/step - loss: 0.3479 - accuracy: 0.8263 - val_loss: 0.4608 - val_accuracy: 0.7364\n",
            "Epoch 9534/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3626 - accuracy: 0.8156\n",
            "Epoch 9534: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3643 - accuracy: 0.8140 - val_loss: 0.5549 - val_accuracy: 0.6520\n",
            "Epoch 9535/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8207\n",
            "Epoch 9535: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3535 - accuracy: 0.8213 - val_loss: 0.7109 - val_accuracy: 0.5499\n",
            "Epoch 9536/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3824 - accuracy: 0.8013\n",
            "Epoch 9536: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3803 - accuracy: 0.8018 - val_loss: 0.5331 - val_accuracy: 0.6787\n",
            "Epoch 9537/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8211\n",
            "Epoch 9537: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3634 - accuracy: 0.8186 - val_loss: 0.4619 - val_accuracy: 0.7296\n",
            "Epoch 9538/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3584 - accuracy: 0.8193\n",
            "Epoch 9538: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3585 - accuracy: 0.8191 - val_loss: 0.4972 - val_accuracy: 0.7046\n",
            "Epoch 9539/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8219\n",
            "Epoch 9539: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3534 - accuracy: 0.8232 - val_loss: 0.4984 - val_accuracy: 0.7046\n",
            "Epoch 9540/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8223\n",
            "Epoch 9540: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3532 - accuracy: 0.8219 - val_loss: 0.5651 - val_accuracy: 0.6430\n",
            "Epoch 9541/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3527 - accuracy: 0.8242\n",
            "Epoch 9541: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3515 - accuracy: 0.8246 - val_loss: 0.6284 - val_accuracy: 0.5986\n",
            "Epoch 9542/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3583 - accuracy: 0.8195\n",
            "Epoch 9542: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3606 - accuracy: 0.8175 - val_loss: 0.4202 - val_accuracy: 0.7643\n",
            "Epoch 9543/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3730 - accuracy: 0.8099\n",
            "Epoch 9543: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3737 - accuracy: 0.8102 - val_loss: 0.5988 - val_accuracy: 0.6296\n",
            "Epoch 9544/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3534 - accuracy: 0.8208\n",
            "Epoch 9544: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3550 - accuracy: 0.8202 - val_loss: 0.6087 - val_accuracy: 0.6098\n",
            "Epoch 9545/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8145\n",
            "Epoch 9545: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3574 - accuracy: 0.8163 - val_loss: 0.6078 - val_accuracy: 0.6072\n",
            "Epoch 9546/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3592 - accuracy: 0.8192\n",
            "Epoch 9546: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3594 - accuracy: 0.8191 - val_loss: 0.3982 - val_accuracy: 0.7740\n",
            "Epoch 9547/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3716 - accuracy: 0.8104\n",
            "Epoch 9547: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3687 - accuracy: 0.8127 - val_loss: 0.5955 - val_accuracy: 0.6215\n",
            "Epoch 9548/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8210\n",
            "Epoch 9548: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3570 - accuracy: 0.8199 - val_loss: 0.4375 - val_accuracy: 0.7468\n",
            "Epoch 9549/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3616 - accuracy: 0.8143\n",
            "Epoch 9549: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3613 - accuracy: 0.8148 - val_loss: 0.5818 - val_accuracy: 0.6324\n",
            "Epoch 9550/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8210\n",
            "Epoch 9550: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3519 - accuracy: 0.8222 - val_loss: 0.5188 - val_accuracy: 0.6860\n",
            "Epoch 9551/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3491 - accuracy: 0.8263\n",
            "Epoch 9551: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3506 - accuracy: 0.8249 - val_loss: 0.4900 - val_accuracy: 0.7020\n",
            "Epoch 9552/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3614 - accuracy: 0.8138\n",
            "Epoch 9552: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3647 - accuracy: 0.8125 - val_loss: 0.7762 - val_accuracy: 0.5214\n",
            "Epoch 9553/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3757 - accuracy: 0.8063\n",
            "Epoch 9553: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3732 - accuracy: 0.8072 - val_loss: 0.5717 - val_accuracy: 0.6474\n",
            "Epoch 9554/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8147\n",
            "Epoch 9554: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3684 - accuracy: 0.8125 - val_loss: 0.4308 - val_accuracy: 0.7441\n",
            "Epoch 9555/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3888 - accuracy: 0.8017\n",
            "Epoch 9555: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3857 - accuracy: 0.8033 - val_loss: 0.5570 - val_accuracy: 0.6575\n",
            "Epoch 9556/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8184\n",
            "Epoch 9556: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3549 - accuracy: 0.8200 - val_loss: 0.5324 - val_accuracy: 0.6704\n",
            "Epoch 9557/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8260\n",
            "Epoch 9557: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3515 - accuracy: 0.8258 - val_loss: 0.5027 - val_accuracy: 0.6952\n",
            "Epoch 9558/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3515 - accuracy: 0.8244\n",
            "Epoch 9558: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3527 - accuracy: 0.8238 - val_loss: 0.4630 - val_accuracy: 0.7329\n",
            "Epoch 9559/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3629 - accuracy: 0.8162\n",
            "Epoch 9559: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3635 - accuracy: 0.8156 - val_loss: 0.6569 - val_accuracy: 0.5877\n",
            "Epoch 9560/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8194\n",
            "Epoch 9560: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3555 - accuracy: 0.8189 - val_loss: 0.6948 - val_accuracy: 0.5569\n",
            "Epoch 9561/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3687 - accuracy: 0.8127\n",
            "Epoch 9561: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3703 - accuracy: 0.8124 - val_loss: 0.4082 - val_accuracy: 0.7685\n",
            "Epoch 9562/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3706 - accuracy: 0.8098\n",
            "Epoch 9562: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3689 - accuracy: 0.8111 - val_loss: 0.4591 - val_accuracy: 0.7303\n",
            "Epoch 9563/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3655 - accuracy: 0.8144\n",
            "Epoch 9563: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3681 - accuracy: 0.8124 - val_loss: 0.8092 - val_accuracy: 0.5109\n",
            "Epoch 9564/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3915 - accuracy: 0.7997\n",
            "Epoch 9564: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3894 - accuracy: 0.8001 - val_loss: 0.5316 - val_accuracy: 0.6717\n",
            "Epoch 9565/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8190\n",
            "Epoch 9565: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3596 - accuracy: 0.8166 - val_loss: 0.4562 - val_accuracy: 0.7369\n",
            "Epoch 9566/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8178\n",
            "Epoch 9566: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3614 - accuracy: 0.8182 - val_loss: 0.4534 - val_accuracy: 0.7347\n",
            "Epoch 9567/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3708 - accuracy: 0.8089\n",
            "Epoch 9567: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3737 - accuracy: 0.8076 - val_loss: 0.7223 - val_accuracy: 0.5460\n",
            "Epoch 9568/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3679 - accuracy: 0.8123\n",
            "Epoch 9568: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3652 - accuracy: 0.8147 - val_loss: 0.7155 - val_accuracy: 0.5552\n",
            "Epoch 9569/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3824 - accuracy: 0.8030\n",
            "Epoch 9569: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3820 - accuracy: 0.8034 - val_loss: 0.4435 - val_accuracy: 0.7371\n",
            "Epoch 9570/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3676 - accuracy: 0.8123\n",
            "Epoch 9570: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3676 - accuracy: 0.8122 - val_loss: 0.5595 - val_accuracy: 0.6498\n",
            "Epoch 9571/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8233\n",
            "Epoch 9571: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3530 - accuracy: 0.8217 - val_loss: 0.5558 - val_accuracy: 0.6583\n",
            "Epoch 9572/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3508 - accuracy: 0.8241\n",
            "Epoch 9572: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3511 - accuracy: 0.8239 - val_loss: 0.6096 - val_accuracy: 0.6136\n",
            "Epoch 9573/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8184\n",
            "Epoch 9573: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3569 - accuracy: 0.8188 - val_loss: 0.4367 - val_accuracy: 0.7584\n",
            "Epoch 9574/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3634 - accuracy: 0.8142\n",
            "Epoch 9574: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3616 - accuracy: 0.8161 - val_loss: 0.5277 - val_accuracy: 0.6739\n",
            "Epoch 9575/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3516 - accuracy: 0.8234\n",
            "Epoch 9575: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3549 - accuracy: 0.8211 - val_loss: 0.8089 - val_accuracy: 0.5139\n",
            "Epoch 9576/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3960 - accuracy: 0.7944\n",
            "Epoch 9576: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3950 - accuracy: 0.7953 - val_loss: 0.4583 - val_accuracy: 0.7343\n",
            "Epoch 9577/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3642 - accuracy: 0.8127\n",
            "Epoch 9577: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3659 - accuracy: 0.8122 - val_loss: 0.7875 - val_accuracy: 0.5210\n",
            "Epoch 9578/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3986 - accuracy: 0.7907\n",
            "Epoch 9578: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3964 - accuracy: 0.7923 - val_loss: 0.5183 - val_accuracy: 0.6816\n",
            "Epoch 9579/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3526 - accuracy: 0.8234\n",
            "Epoch 9579: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3561 - accuracy: 0.8205 - val_loss: 0.4155 - val_accuracy: 0.7628\n",
            "Epoch 9580/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3748 - accuracy: 0.8080\n",
            "Epoch 9580: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3731 - accuracy: 0.8097 - val_loss: 0.5205 - val_accuracy: 0.6897\n",
            "Epoch 9581/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8216\n",
            "Epoch 9581: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3534 - accuracy: 0.8224 - val_loss: 0.6876 - val_accuracy: 0.5635\n",
            "Epoch 9582/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3742 - accuracy: 0.8101\n",
            "Epoch 9582: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3729 - accuracy: 0.8110 - val_loss: 0.4959 - val_accuracy: 0.7016\n",
            "Epoch 9583/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3553 - accuracy: 0.8240\n",
            "Epoch 9583: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3544 - accuracy: 0.8243 - val_loss: 0.4542 - val_accuracy: 0.7373\n",
            "Epoch 9584/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3563 - accuracy: 0.8181\n",
            "Epoch 9584: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3560 - accuracy: 0.8187 - val_loss: 0.5968 - val_accuracy: 0.6190\n",
            "Epoch 9585/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3517 - accuracy: 0.8233\n",
            "Epoch 9585: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3530 - accuracy: 0.8225 - val_loss: 0.6042 - val_accuracy: 0.6234\n",
            "Epoch 9586/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8163\n",
            "Epoch 9586: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3589 - accuracy: 0.8172 - val_loss: 0.4809 - val_accuracy: 0.7174\n",
            "Epoch 9587/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3520 - accuracy: 0.8255\n",
            "Epoch 9587: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3529 - accuracy: 0.8239 - val_loss: 0.4354 - val_accuracy: 0.7476\n",
            "Epoch 9588/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3744 - accuracy: 0.8076\n",
            "Epoch 9588: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3748 - accuracy: 0.8074 - val_loss: 0.6327 - val_accuracy: 0.6035\n",
            "Epoch 9589/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3536 - accuracy: 0.8206\n",
            "Epoch 9589: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3556 - accuracy: 0.8195 - val_loss: 0.6836 - val_accuracy: 0.5727\n",
            "Epoch 9590/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3807 - accuracy: 0.8050\n",
            "Epoch 9590: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3787 - accuracy: 0.8058 - val_loss: 0.6056 - val_accuracy: 0.6138\n",
            "Epoch 9591/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3962 - accuracy: 0.7966\n",
            "Epoch 9591: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.4068 - accuracy: 0.7911 - val_loss: 0.3695 - val_accuracy: 0.7882\n",
            "Epoch 9592/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3994 - accuracy: 0.7945\n",
            "Epoch 9592: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3968 - accuracy: 0.7950 - val_loss: 0.3890 - val_accuracy: 0.7738\n",
            "Epoch 9593/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4633 - accuracy: 0.7626\n",
            "Epoch 9593: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.4599 - accuracy: 0.7654 - val_loss: 0.6212 - val_accuracy: 0.6118\n",
            "Epoch 9594/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3625 - accuracy: 0.8133\n",
            "Epoch 9594: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3673 - accuracy: 0.8107 - val_loss: 0.8265 - val_accuracy: 0.5041\n",
            "Epoch 9595/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3900 - accuracy: 0.8002\n",
            "Epoch 9595: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3871 - accuracy: 0.8017 - val_loss: 0.7413 - val_accuracy: 0.5484\n",
            "Epoch 9596/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3985 - accuracy: 0.7969\n",
            "Epoch 9596: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3991 - accuracy: 0.7965 - val_loss: 0.5214 - val_accuracy: 0.6801\n",
            "Epoch 9597/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3696 - accuracy: 0.8147\n",
            "Epoch 9597: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3776 - accuracy: 0.8083 - val_loss: 0.3870 - val_accuracy: 0.7773\n",
            "Epoch 9598/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3889 - accuracy: 0.8044\n",
            "Epoch 9598: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3853 - accuracy: 0.8071 - val_loss: 0.4052 - val_accuracy: 0.7676\n",
            "Epoch 9599/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3956 - accuracy: 0.7964\n",
            "Epoch 9599: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3935 - accuracy: 0.7987 - val_loss: 0.6002 - val_accuracy: 0.6259\n",
            "Epoch 9600/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3585 - accuracy: 0.8186\n",
            "Epoch 9600: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3595 - accuracy: 0.8187 - val_loss: 0.7055 - val_accuracy: 0.5556\n",
            "Epoch 9601/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3718 - accuracy: 0.8112\n",
            "Epoch 9601: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3701 - accuracy: 0.8121 - val_loss: 0.5893 - val_accuracy: 0.6355\n",
            "Epoch 9602/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8165\n",
            "Epoch 9602: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3657 - accuracy: 0.8155 - val_loss: 0.5615 - val_accuracy: 0.6520\n",
            "Epoch 9603/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8211\n",
            "Epoch 9603: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3582 - accuracy: 0.8214 - val_loss: 0.4402 - val_accuracy: 0.7408\n",
            "Epoch 9604/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3617 - accuracy: 0.8166\n",
            "Epoch 9604: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3617 - accuracy: 0.8160 - val_loss: 0.5111 - val_accuracy: 0.6814\n",
            "Epoch 9605/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3600 - accuracy: 0.8166\n",
            "Epoch 9605: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3615 - accuracy: 0.8159 - val_loss: 0.4945 - val_accuracy: 0.7106\n",
            "Epoch 9606/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3522 - accuracy: 0.8239\n",
            "Epoch 9606: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3537 - accuracy: 0.8223 - val_loss: 0.4907 - val_accuracy: 0.7024\n",
            "Epoch 9607/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8206\n",
            "Epoch 9607: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3566 - accuracy: 0.8205 - val_loss: 0.6060 - val_accuracy: 0.6182\n",
            "Epoch 9608/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3536 - accuracy: 0.8251\n",
            "Epoch 9608: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3545 - accuracy: 0.8230 - val_loss: 0.5931 - val_accuracy: 0.6280\n",
            "Epoch 9609/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3536 - accuracy: 0.8205\n",
            "Epoch 9609: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3537 - accuracy: 0.8216 - val_loss: 0.6610 - val_accuracy: 0.5842\n",
            "Epoch 9610/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3750 - accuracy: 0.8053\n",
            "Epoch 9610: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3778 - accuracy: 0.8043 - val_loss: 0.4659 - val_accuracy: 0.7209\n",
            "Epoch 9611/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3609 - accuracy: 0.8175\n",
            "Epoch 9611: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3624 - accuracy: 0.8165 - val_loss: 0.4459 - val_accuracy: 0.7391\n",
            "Epoch 9612/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3734 - accuracy: 0.8083\n",
            "Epoch 9612: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3725 - accuracy: 0.8091 - val_loss: 0.7056 - val_accuracy: 0.5662\n",
            "Epoch 9613/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3870 - accuracy: 0.7978\n",
            "Epoch 9613: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3826 - accuracy: 0.8014 - val_loss: 0.8205 - val_accuracy: 0.5126\n",
            "Epoch 9614/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4393 - accuracy: 0.7775\n",
            "Epoch 9614: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4387 - accuracy: 0.7765 - val_loss: 0.4483 - val_accuracy: 0.7367\n",
            "Epoch 9615/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3715 - accuracy: 0.8124\n",
            "Epoch 9615: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3722 - accuracy: 0.8119 - val_loss: 0.4446 - val_accuracy: 0.7430\n",
            "Epoch 9616/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8123\n",
            "Epoch 9616: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3721 - accuracy: 0.8130 - val_loss: 0.5659 - val_accuracy: 0.6550\n",
            "Epoch 9617/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8219\n",
            "Epoch 9617: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3562 - accuracy: 0.8219 - val_loss: 0.5603 - val_accuracy: 0.6502\n",
            "Epoch 9618/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3539 - accuracy: 0.8208\n",
            "Epoch 9618: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3547 - accuracy: 0.8204 - val_loss: 0.5928 - val_accuracy: 0.6232\n",
            "Epoch 9619/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8217\n",
            "Epoch 9619: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3529 - accuracy: 0.8219 - val_loss: 0.5832 - val_accuracy: 0.6254\n",
            "Epoch 9620/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3543 - accuracy: 0.8207\n",
            "Epoch 9620: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3547 - accuracy: 0.8214 - val_loss: 0.4578 - val_accuracy: 0.7358\n",
            "Epoch 9621/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3632 - accuracy: 0.8145\n",
            "Epoch 9621: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3628 - accuracy: 0.8154 - val_loss: 0.4733 - val_accuracy: 0.7255\n",
            "Epoch 9622/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3833 - accuracy: 0.8003\n",
            "Epoch 9622: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3867 - accuracy: 0.7992 - val_loss: 0.6306 - val_accuracy: 0.6046\n",
            "Epoch 9623/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3600 - accuracy: 0.8163\n",
            "Epoch 9623: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3625 - accuracy: 0.8151 - val_loss: 0.8370 - val_accuracy: 0.5008\n",
            "Epoch 9624/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4107 - accuracy: 0.7896\n",
            "Epoch 9624: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4070 - accuracy: 0.7917 - val_loss: 0.6508 - val_accuracy: 0.5938\n",
            "Epoch 9625/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4012 - accuracy: 0.7975\n",
            "Epoch 9625: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4046 - accuracy: 0.7957 - val_loss: 0.4462 - val_accuracy: 0.7437\n",
            "Epoch 9626/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3706 - accuracy: 0.8140\n",
            "Epoch 9626: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3787 - accuracy: 0.8090 - val_loss: 0.4276 - val_accuracy: 0.7461\n",
            "Epoch 9627/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3766 - accuracy: 0.8094\n",
            "Epoch 9627: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3834 - accuracy: 0.8054 - val_loss: 0.3609 - val_accuracy: 0.7924\n",
            "Epoch 9628/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4121 - accuracy: 0.7888\n",
            "Epoch 9628: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4081 - accuracy: 0.7902 - val_loss: 0.3858 - val_accuracy: 0.7781\n",
            "Epoch 9629/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4270 - accuracy: 0.7830\n",
            "Epoch 9629: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4210 - accuracy: 0.7861 - val_loss: 0.4307 - val_accuracy: 0.7512\n",
            "Epoch 9630/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4297 - accuracy: 0.7805\n",
            "Epoch 9630: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4274 - accuracy: 0.7816 - val_loss: 0.4882 - val_accuracy: 0.7127\n",
            "Epoch 9631/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3907 - accuracy: 0.8023\n",
            "Epoch 9631: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3943 - accuracy: 0.8002 - val_loss: 0.6461 - val_accuracy: 0.5995\n",
            "Epoch 9632/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3664 - accuracy: 0.8128\n",
            "Epoch 9632: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3679 - accuracy: 0.8116 - val_loss: 0.6229 - val_accuracy: 0.6118\n",
            "Epoch 9633/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3631 - accuracy: 0.8146\n",
            "Epoch 9633: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3624 - accuracy: 0.8149 - val_loss: 0.5606 - val_accuracy: 0.6585\n",
            "Epoch 9634/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3568 - accuracy: 0.8198\n",
            "Epoch 9634: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3583 - accuracy: 0.8194 - val_loss: 0.5759 - val_accuracy: 0.6370\n",
            "Epoch 9635/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8199\n",
            "Epoch 9635: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3553 - accuracy: 0.8198 - val_loss: 0.5905 - val_accuracy: 0.6241\n",
            "Epoch 9636/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8239\n",
            "Epoch 9636: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3555 - accuracy: 0.8224 - val_loss: 0.6489 - val_accuracy: 0.5846\n",
            "Epoch 9637/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8127\n",
            "Epoch 9637: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3653 - accuracy: 0.8142 - val_loss: 0.5921 - val_accuracy: 0.6226\n",
            "Epoch 9638/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3586 - accuracy: 0.8196\n",
            "Epoch 9638: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3610 - accuracy: 0.8174 - val_loss: 0.4790 - val_accuracy: 0.7196\n",
            "Epoch 9639/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8194\n",
            "Epoch 9639: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3611 - accuracy: 0.8178 - val_loss: 0.4328 - val_accuracy: 0.7507\n",
            "Epoch 9640/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8198\n",
            "Epoch 9640: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3640 - accuracy: 0.8176 - val_loss: 0.3907 - val_accuracy: 0.7733\n",
            "Epoch 9641/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3808 - accuracy: 0.8051\n",
            "Epoch 9641: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3779 - accuracy: 0.8069 - val_loss: 0.4589 - val_accuracy: 0.7360\n",
            "Epoch 9642/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3699 - accuracy: 0.8130\n",
            "Epoch 9642: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3698 - accuracy: 0.8124 - val_loss: 0.5963 - val_accuracy: 0.6188\n",
            "Epoch 9643/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3535 - accuracy: 0.8190\n",
            "Epoch 9643: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3534 - accuracy: 0.8198 - val_loss: 0.6670 - val_accuracy: 0.5778\n",
            "Epoch 9644/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3595 - accuracy: 0.8188\n",
            "Epoch 9644: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3588 - accuracy: 0.8200 - val_loss: 0.5969 - val_accuracy: 0.6226\n",
            "Epoch 9645/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8168\n",
            "Epoch 9645: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3584 - accuracy: 0.8172 - val_loss: 0.5824 - val_accuracy: 0.6340\n",
            "Epoch 9646/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8224\n",
            "Epoch 9646: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3558 - accuracy: 0.8218 - val_loss: 0.5578 - val_accuracy: 0.6460\n",
            "Epoch 9647/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3645 - accuracy: 0.8160\n",
            "Epoch 9647: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.3679 - accuracy: 0.8132 - val_loss: 0.3983 - val_accuracy: 0.7727\n",
            "Epoch 9648/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3747 - accuracy: 0.8068\n",
            "Epoch 9648: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3725 - accuracy: 0.8081 - val_loss: 0.4533 - val_accuracy: 0.7332\n",
            "Epoch 9649/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3684 - accuracy: 0.8130\n",
            "Epoch 9649: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3667 - accuracy: 0.8138 - val_loss: 0.5578 - val_accuracy: 0.6526\n",
            "Epoch 9650/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8217\n",
            "Epoch 9650: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.3551 - accuracy: 0.8220 - val_loss: 0.6317 - val_accuracy: 0.5984\n",
            "Epoch 9651/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8182\n",
            "Epoch 9651: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3612 - accuracy: 0.8172 - val_loss: 0.5784 - val_accuracy: 0.6390\n",
            "Epoch 9652/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8205\n",
            "Epoch 9652: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3569 - accuracy: 0.8204 - val_loss: 0.4891 - val_accuracy: 0.7099\n",
            "Epoch 9653/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8231\n",
            "Epoch 9653: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3547 - accuracy: 0.8239 - val_loss: 0.5170 - val_accuracy: 0.6825\n",
            "Epoch 9654/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3498 - accuracy: 0.8235\n",
            "Epoch 9654: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3508 - accuracy: 0.8227 - val_loss: 0.5866 - val_accuracy: 0.6353\n",
            "Epoch 9655/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8203\n",
            "Epoch 9655: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3548 - accuracy: 0.8206 - val_loss: 0.5777 - val_accuracy: 0.6348\n",
            "Epoch 9656/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8210\n",
            "Epoch 9656: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3562 - accuracy: 0.8199 - val_loss: 0.5303 - val_accuracy: 0.6759\n",
            "Epoch 9657/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3503 - accuracy: 0.8250\n",
            "Epoch 9657: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3509 - accuracy: 0.8248 - val_loss: 0.5610 - val_accuracy: 0.6564\n",
            "Epoch 9658/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3506 - accuracy: 0.8247\n",
            "Epoch 9658: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3509 - accuracy: 0.8248 - val_loss: 0.5136 - val_accuracy: 0.6842\n",
            "Epoch 9659/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8215\n",
            "Epoch 9659: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3506 - accuracy: 0.8217 - val_loss: 0.4394 - val_accuracy: 0.7472\n",
            "Epoch 9660/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3811 - accuracy: 0.8068\n",
            "Epoch 9660: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3813 - accuracy: 0.8066 - val_loss: 0.6579 - val_accuracy: 0.5938\n",
            "Epoch 9661/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3617 - accuracy: 0.8151\n",
            "Epoch 9661: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3610 - accuracy: 0.8160 - val_loss: 0.8751 - val_accuracy: 0.4764\n",
            "Epoch 9662/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4263 - accuracy: 0.7787\n",
            "Epoch 9662: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4199 - accuracy: 0.7824 - val_loss: 0.6139 - val_accuracy: 0.6133\n",
            "Epoch 9663/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4059 - accuracy: 0.7937\n",
            "Epoch 9663: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.4118 - accuracy: 0.7902 - val_loss: 0.4950 - val_accuracy: 0.7024\n",
            "Epoch 9664/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3738 - accuracy: 0.8132\n",
            "Epoch 9664: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3831 - accuracy: 0.8076 - val_loss: 0.3616 - val_accuracy: 0.7992\n",
            "Epoch 9665/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3970 - accuracy: 0.7972\n",
            "Epoch 9665: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3927 - accuracy: 0.7996 - val_loss: 0.4687 - val_accuracy: 0.7193\n",
            "Epoch 9666/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8118\n",
            "Epoch 9666: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3670 - accuracy: 0.8141 - val_loss: 0.5152 - val_accuracy: 0.6853\n",
            "Epoch 9667/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8188\n",
            "Epoch 9667: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3589 - accuracy: 0.8199 - val_loss: 0.6417 - val_accuracy: 0.5989\n",
            "Epoch 9668/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3592 - accuracy: 0.8207\n",
            "Epoch 9668: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3598 - accuracy: 0.8191 - val_loss: 0.6458 - val_accuracy: 0.5833\n",
            "Epoch 9669/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3692 - accuracy: 0.8097\n",
            "Epoch 9669: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3680 - accuracy: 0.8116 - val_loss: 0.6117 - val_accuracy: 0.6155\n",
            "Epoch 9670/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3732 - accuracy: 0.8115\n",
            "Epoch 9670: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3749 - accuracy: 0.8112 - val_loss: 0.4973 - val_accuracy: 0.7055\n",
            "Epoch 9671/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3576 - accuracy: 0.8214\n",
            "Epoch 9671: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3617 - accuracy: 0.8180 - val_loss: 0.3671 - val_accuracy: 0.7878\n",
            "Epoch 9672/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4390 - accuracy: 0.7710\n",
            "Epoch 9672: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4315 - accuracy: 0.7758 - val_loss: 0.4085 - val_accuracy: 0.7643\n",
            "Epoch 9673/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4301 - accuracy: 0.7819\n",
            "Epoch 9673: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4294 - accuracy: 0.7814 - val_loss: 0.5332 - val_accuracy: 0.6816\n",
            "Epoch 9674/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3810 - accuracy: 0.8029\n",
            "Epoch 9674: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3857 - accuracy: 0.8018 - val_loss: 0.7016 - val_accuracy: 0.5697\n",
            "Epoch 9675/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8079\n",
            "Epoch 9675: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3749 - accuracy: 0.8066 - val_loss: 0.7828 - val_accuracy: 0.5350\n",
            "Epoch 9676/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3819 - accuracy: 0.8036\n",
            "Epoch 9676: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3808 - accuracy: 0.8044 - val_loss: 0.8847 - val_accuracy: 0.4725\n",
            "Epoch 9677/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4086 - accuracy: 0.7879\n",
            "Epoch 9677: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4028 - accuracy: 0.7917 - val_loss: 0.6918 - val_accuracy: 0.5767\n",
            "Epoch 9678/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3905 - accuracy: 0.7996\n",
            "Epoch 9678: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3890 - accuracy: 0.7998 - val_loss: 0.6352 - val_accuracy: 0.6070\n",
            "Epoch 9679/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3774 - accuracy: 0.8089\n",
            "Epoch 9679: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3825 - accuracy: 0.8058 - val_loss: 0.4299 - val_accuracy: 0.7492\n",
            "Epoch 9680/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8140\n",
            "Epoch 9680: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3674 - accuracy: 0.8125 - val_loss: 0.4269 - val_accuracy: 0.7481\n",
            "Epoch 9681/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3731 - accuracy: 0.8096\n",
            "Epoch 9681: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3709 - accuracy: 0.8116 - val_loss: 0.4723 - val_accuracy: 0.7233\n",
            "Epoch 9682/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3670 - accuracy: 0.8154\n",
            "Epoch 9682: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3669 - accuracy: 0.8154 - val_loss: 0.6068 - val_accuracy: 0.6241\n",
            "Epoch 9683/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3576 - accuracy: 0.8203\n",
            "Epoch 9683: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3574 - accuracy: 0.8210 - val_loss: 0.6622 - val_accuracy: 0.5864\n",
            "Epoch 9684/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3601 - accuracy: 0.8176\n",
            "Epoch 9684: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3613 - accuracy: 0.8164 - val_loss: 0.5416 - val_accuracy: 0.6743\n",
            "Epoch 9685/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8226\n",
            "Epoch 9685: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3547 - accuracy: 0.8230 - val_loss: 0.6499 - val_accuracy: 0.5953\n",
            "Epoch 9686/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3771 - accuracy: 0.8061\n",
            "Epoch 9686: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3770 - accuracy: 0.8061 - val_loss: 0.5371 - val_accuracy: 0.6772\n",
            "Epoch 9687/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8220\n",
            "Epoch 9687: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3594 - accuracy: 0.8191 - val_loss: 0.4095 - val_accuracy: 0.7645\n",
            "Epoch 9688/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3704 - accuracy: 0.8129\n",
            "Epoch 9688: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3686 - accuracy: 0.8131 - val_loss: 0.4301 - val_accuracy: 0.7461\n",
            "Epoch 9689/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3917 - accuracy: 0.7982\n",
            "Epoch 9689: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3884 - accuracy: 0.7993 - val_loss: 0.4917 - val_accuracy: 0.7038\n",
            "Epoch 9690/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3681 - accuracy: 0.8114\n",
            "Epoch 9690: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3695 - accuracy: 0.8111 - val_loss: 0.5447 - val_accuracy: 0.6610\n",
            "Epoch 9691/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8191\n",
            "Epoch 9691: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3566 - accuracy: 0.8192 - val_loss: 0.5743 - val_accuracy: 0.6362\n",
            "Epoch 9692/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8177\n",
            "Epoch 9692: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3571 - accuracy: 0.8181 - val_loss: 0.6407 - val_accuracy: 0.5971\n",
            "Epoch 9693/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3625 - accuracy: 0.8147\n",
            "Epoch 9693: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3633 - accuracy: 0.8139 - val_loss: 0.5794 - val_accuracy: 0.6346\n",
            "Epoch 9694/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3549 - accuracy: 0.8241\n",
            "Epoch 9694: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3555 - accuracy: 0.8244 - val_loss: 0.5440 - val_accuracy: 0.6662\n",
            "Epoch 9695/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8232\n",
            "Epoch 9695: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3541 - accuracy: 0.8228 - val_loss: 0.5517 - val_accuracy: 0.6522\n",
            "Epoch 9696/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8248\n",
            "Epoch 9696: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3549 - accuracy: 0.8228 - val_loss: 0.4913 - val_accuracy: 0.7068\n",
            "Epoch 9697/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3544 - accuracy: 0.8220\n",
            "Epoch 9697: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3550 - accuracy: 0.8209 - val_loss: 0.5312 - val_accuracy: 0.6750\n",
            "Epoch 9698/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8231\n",
            "Epoch 9698: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3589 - accuracy: 0.8211 - val_loss: 0.4009 - val_accuracy: 0.7718\n",
            "Epoch 9699/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3826 - accuracy: 0.8015\n",
            "Epoch 9699: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3801 - accuracy: 0.8033 - val_loss: 0.6166 - val_accuracy: 0.6079\n",
            "Epoch 9700/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3610 - accuracy: 0.8177\n",
            "Epoch 9700: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3600 - accuracy: 0.8182 - val_loss: 0.5062 - val_accuracy: 0.7027\n",
            "Epoch 9701/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8182\n",
            "Epoch 9701: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3557 - accuracy: 0.8190 - val_loss: 0.5454 - val_accuracy: 0.6592\n",
            "Epoch 9702/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3582 - accuracy: 0.8186\n",
            "Epoch 9702: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3578 - accuracy: 0.8192 - val_loss: 0.4617 - val_accuracy: 0.7277\n",
            "Epoch 9703/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3806 - accuracy: 0.8014\n",
            "Epoch 9703: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3829 - accuracy: 0.8011 - val_loss: 0.7169 - val_accuracy: 0.5473\n",
            "Epoch 9704/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8145\n",
            "Epoch 9704: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3666 - accuracy: 0.8141 - val_loss: 0.7486 - val_accuracy: 0.5488\n",
            "Epoch 9705/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3946 - accuracy: 0.7969\n",
            "Epoch 9705: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3924 - accuracy: 0.7975 - val_loss: 0.6127 - val_accuracy: 0.6076\n",
            "Epoch 9706/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3678 - accuracy: 0.8102\n",
            "Epoch 9706: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3649 - accuracy: 0.8135 - val_loss: 0.5942 - val_accuracy: 0.6197\n",
            "Epoch 9707/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8128\n",
            "Epoch 9707: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3717 - accuracy: 0.8120 - val_loss: 0.5690 - val_accuracy: 0.6375\n",
            "Epoch 9708/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8011\n",
            "Epoch 9708: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3901 - accuracy: 0.7982 - val_loss: 0.4460 - val_accuracy: 0.7296\n",
            "Epoch 9709/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3722 - accuracy: 0.8115\n",
            "Epoch 9709: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3725 - accuracy: 0.8114 - val_loss: 0.3693 - val_accuracy: 0.7939\n",
            "Epoch 9710/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3921 - accuracy: 0.7980\n",
            "Epoch 9710: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3879 - accuracy: 0.8012 - val_loss: 0.4671 - val_accuracy: 0.7209\n",
            "Epoch 9711/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3922 - accuracy: 0.8001\n",
            "Epoch 9711: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3893 - accuracy: 0.8017 - val_loss: 0.4956 - val_accuracy: 0.7018\n",
            "Epoch 9712/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3700 - accuracy: 0.8118\n",
            "Epoch 9712: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3743 - accuracy: 0.8090 - val_loss: 0.6717 - val_accuracy: 0.5745\n",
            "Epoch 9713/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3623 - accuracy: 0.8129\n",
            "Epoch 9713: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3637 - accuracy: 0.8121 - val_loss: 0.6460 - val_accuracy: 0.5947\n",
            "Epoch 9714/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3601 - accuracy: 0.8172\n",
            "Epoch 9714: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3597 - accuracy: 0.8179 - val_loss: 0.8600 - val_accuracy: 0.4847\n",
            "Epoch 9715/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4171 - accuracy: 0.7806\n",
            "Epoch 9715: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.4122 - accuracy: 0.7835 - val_loss: 0.6505 - val_accuracy: 0.5956\n",
            "Epoch 9716/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3926 - accuracy: 0.7994\n",
            "Epoch 9716: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3909 - accuracy: 0.8009 - val_loss: 0.5201 - val_accuracy: 0.6847\n",
            "Epoch 9717/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3670 - accuracy: 0.8152\n",
            "Epoch 9717: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3678 - accuracy: 0.8129 - val_loss: 0.4328 - val_accuracy: 0.7529\n",
            "Epoch 9718/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3629 - accuracy: 0.8161\n",
            "Epoch 9718: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3646 - accuracy: 0.8153 - val_loss: 0.4608 - val_accuracy: 0.7316\n",
            "Epoch 9719/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3642 - accuracy: 0.8200\n",
            "Epoch 9719: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3657 - accuracy: 0.8179 - val_loss: 0.3910 - val_accuracy: 0.7744\n",
            "Epoch 9720/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3836 - accuracy: 0.8017\n",
            "Epoch 9720: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3808 - accuracy: 0.8039 - val_loss: 0.4198 - val_accuracy: 0.7663\n",
            "Epoch 9721/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3852 - accuracy: 0.8014\n",
            "Epoch 9721: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 0.3847 - accuracy: 0.8009 - val_loss: 0.5224 - val_accuracy: 0.6862\n",
            "Epoch 9722/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8193\n",
            "Epoch 9722: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3574 - accuracy: 0.8181 - val_loss: 0.5952 - val_accuracy: 0.6241\n",
            "Epoch 9723/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3527 - accuracy: 0.8227\n",
            "Epoch 9723: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3525 - accuracy: 0.8228 - val_loss: 0.5300 - val_accuracy: 0.6794\n",
            "Epoch 9724/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3478 - accuracy: 0.8275\n",
            "Epoch 9724: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3498 - accuracy: 0.8259 - val_loss: 0.5466 - val_accuracy: 0.6544\n",
            "Epoch 9725/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3499 - accuracy: 0.8246\n",
            "Epoch 9725: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3491 - accuracy: 0.8260 - val_loss: 0.5582 - val_accuracy: 0.6458\n",
            "Epoch 9726/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3506 - accuracy: 0.8246\n",
            "Epoch 9726: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3507 - accuracy: 0.8241 - val_loss: 0.4122 - val_accuracy: 0.7724\n",
            "Epoch 9727/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3686 - accuracy: 0.8095\n",
            "Epoch 9727: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3682 - accuracy: 0.8102 - val_loss: 0.5817 - val_accuracy: 0.6302\n",
            "Epoch 9728/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3529 - accuracy: 0.8227\n",
            "Epoch 9728: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3548 - accuracy: 0.8218 - val_loss: 0.7531 - val_accuracy: 0.5324\n",
            "Epoch 9729/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3828 - accuracy: 0.8020\n",
            "Epoch 9729: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3805 - accuracy: 0.8033 - val_loss: 0.6039 - val_accuracy: 0.6195\n",
            "Epoch 9730/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3691 - accuracy: 0.8129\n",
            "Epoch 9730: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3704 - accuracy: 0.8116 - val_loss: 0.4774 - val_accuracy: 0.7174\n",
            "Epoch 9731/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8202\n",
            "Epoch 9731: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3601 - accuracy: 0.8178 - val_loss: 0.3701 - val_accuracy: 0.7896\n",
            "Epoch 9732/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3963 - accuracy: 0.7951\n",
            "Epoch 9732: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3924 - accuracy: 0.7973 - val_loss: 0.4850 - val_accuracy: 0.7070\n",
            "Epoch 9733/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3650 - accuracy: 0.8166\n",
            "Epoch 9733: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3636 - accuracy: 0.8172 - val_loss: 0.5482 - val_accuracy: 0.6684\n",
            "Epoch 9734/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3542 - accuracy: 0.8202\n",
            "Epoch 9734: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3561 - accuracy: 0.8193 - val_loss: 0.6325 - val_accuracy: 0.6015\n",
            "Epoch 9735/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8182\n",
            "Epoch 9735: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3582 - accuracy: 0.8187 - val_loss: 0.5820 - val_accuracy: 0.6392\n",
            "Epoch 9736/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3548 - accuracy: 0.8242\n",
            "Epoch 9736: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3552 - accuracy: 0.8235 - val_loss: 0.4757 - val_accuracy: 0.7224\n",
            "Epoch 9737/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8177\n",
            "Epoch 9737: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3591 - accuracy: 0.8178 - val_loss: 0.5217 - val_accuracy: 0.6735\n",
            "Epoch 9738/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3535 - accuracy: 0.8213\n",
            "Epoch 9738: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3549 - accuracy: 0.8209 - val_loss: 0.7084 - val_accuracy: 0.5569\n",
            "Epoch 9739/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3694 - accuracy: 0.8100\n",
            "Epoch 9739: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3683 - accuracy: 0.8105 - val_loss: 0.6260 - val_accuracy: 0.6030\n",
            "Epoch 9740/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8132\n",
            "Epoch 9740: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3692 - accuracy: 0.8100 - val_loss: 0.4003 - val_accuracy: 0.7702\n",
            "Epoch 9741/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3847 - accuracy: 0.8009\n",
            "Epoch 9741: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3799 - accuracy: 0.8046 - val_loss: 0.3874 - val_accuracy: 0.7819\n",
            "Epoch 9742/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4094 - accuracy: 0.7871\n",
            "Epoch 9742: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4073 - accuracy: 0.7894 - val_loss: 0.6005 - val_accuracy: 0.6166\n",
            "Epoch 9743/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8140\n",
            "Epoch 9743: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3703 - accuracy: 0.8108 - val_loss: 0.8453 - val_accuracy: 0.5076\n",
            "Epoch 9744/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3866 - accuracy: 0.7999\n",
            "Epoch 9744: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3880 - accuracy: 0.7999 - val_loss: 0.9647 - val_accuracy: 0.4442\n",
            "Epoch 9745/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4107 - accuracy: 0.7864\n",
            "Epoch 9745: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4062 - accuracy: 0.7896 - val_loss: 1.0977 - val_accuracy: 0.4139\n",
            "Epoch 9746/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4801 - accuracy: 0.7543\n",
            "Epoch 9746: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4696 - accuracy: 0.7596 - val_loss: 0.9346 - val_accuracy: 0.4610\n",
            "Epoch 9747/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4496 - accuracy: 0.7701\n",
            "Epoch 9747: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.4427 - accuracy: 0.7733 - val_loss: 0.6570 - val_accuracy: 0.5868\n",
            "Epoch 9748/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3963 - accuracy: 0.7949\n",
            "Epoch 9748: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3970 - accuracy: 0.7953 - val_loss: 0.5503 - val_accuracy: 0.6660\n",
            "Epoch 9749/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3802 - accuracy: 0.8074\n",
            "Epoch 9749: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3813 - accuracy: 0.8063 - val_loss: 0.5742 - val_accuracy: 0.6390\n",
            "Epoch 9750/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3766 - accuracy: 0.8113\n",
            "Epoch 9750: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3784 - accuracy: 0.8097 - val_loss: 0.4428 - val_accuracy: 0.7400\n",
            "Epoch 9751/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3651 - accuracy: 0.8143\n",
            "Epoch 9751: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3671 - accuracy: 0.8131 - val_loss: 0.4573 - val_accuracy: 0.7277\n",
            "Epoch 9752/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3685 - accuracy: 0.8158\n",
            "Epoch 9752: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3690 - accuracy: 0.8147 - val_loss: 0.4263 - val_accuracy: 0.7544\n",
            "Epoch 9753/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3654 - accuracy: 0.8114\n",
            "Epoch 9753: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3657 - accuracy: 0.8121 - val_loss: 0.4912 - val_accuracy: 0.7044\n",
            "Epoch 9754/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8202\n",
            "Epoch 9754: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3643 - accuracy: 0.8178 - val_loss: 0.3954 - val_accuracy: 0.7792\n",
            "Epoch 9755/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8096\n",
            "Epoch 9755: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3712 - accuracy: 0.8093 - val_loss: 0.4831 - val_accuracy: 0.7112\n",
            "Epoch 9756/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3599 - accuracy: 0.8195\n",
            "Epoch 9756: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3590 - accuracy: 0.8209 - val_loss: 0.4355 - val_accuracy: 0.7457\n",
            "Epoch 9757/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8070\n",
            "Epoch 9757: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3712 - accuracy: 0.8083 - val_loss: 0.5268 - val_accuracy: 0.6752\n",
            "Epoch 9758/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8185\n",
            "Epoch 9758: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3559 - accuracy: 0.8197 - val_loss: 0.5611 - val_accuracy: 0.6616\n",
            "Epoch 9759/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3514 - accuracy: 0.8205\n",
            "Epoch 9759: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3523 - accuracy: 0.8211 - val_loss: 0.5681 - val_accuracy: 0.6331\n",
            "Epoch 9760/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3538 - accuracy: 0.8207\n",
            "Epoch 9760: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3530 - accuracy: 0.8209 - val_loss: 0.5303 - val_accuracy: 0.6754\n",
            "Epoch 9761/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3493 - accuracy: 0.8266\n",
            "Epoch 9761: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3498 - accuracy: 0.8250 - val_loss: 0.5051 - val_accuracy: 0.6899\n",
            "Epoch 9762/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3473 - accuracy: 0.8262\n",
            "Epoch 9762: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3499 - accuracy: 0.8247 - val_loss: 0.6110 - val_accuracy: 0.6050\n",
            "Epoch 9763/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3475 - accuracy: 0.8268\n",
            "Epoch 9763: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3492 - accuracy: 0.8252 - val_loss: 0.6724 - val_accuracy: 0.5758\n",
            "Epoch 9764/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3659 - accuracy: 0.8134\n",
            "Epoch 9764: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3629 - accuracy: 0.8151 - val_loss: 0.5617 - val_accuracy: 0.6476\n",
            "Epoch 9765/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8222\n",
            "Epoch 9765: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3542 - accuracy: 0.8203 - val_loss: 0.4431 - val_accuracy: 0.7490\n",
            "Epoch 9766/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3609 - accuracy: 0.8177\n",
            "Epoch 9766: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3596 - accuracy: 0.8175 - val_loss: 0.4969 - val_accuracy: 0.6978\n",
            "Epoch 9767/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3477 - accuracy: 0.8250\n",
            "Epoch 9767: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3492 - accuracy: 0.8238 - val_loss: 0.5639 - val_accuracy: 0.6528\n",
            "Epoch 9768/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8246\n",
            "Epoch 9768: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3526 - accuracy: 0.8239 - val_loss: 0.5001 - val_accuracy: 0.7005\n",
            "Epoch 9769/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8219\n",
            "Epoch 9769: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3513 - accuracy: 0.8229 - val_loss: 0.5310 - val_accuracy: 0.6768\n",
            "Epoch 9770/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8281\n",
            "Epoch 9770: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3491 - accuracy: 0.8289 - val_loss: 0.5121 - val_accuracy: 0.6838\n",
            "Epoch 9771/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3506 - accuracy: 0.8251\n",
            "Epoch 9771: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3498 - accuracy: 0.8246 - val_loss: 0.5289 - val_accuracy: 0.6763\n",
            "Epoch 9772/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8236\n",
            "Epoch 9772: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3543 - accuracy: 0.8225 - val_loss: 0.4465 - val_accuracy: 0.7443\n",
            "Epoch 9773/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3645 - accuracy: 0.8165\n",
            "Epoch 9773: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3632 - accuracy: 0.8172 - val_loss: 0.3570 - val_accuracy: 0.7937\n",
            "Epoch 9774/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4360 - accuracy: 0.7743\n",
            "Epoch 9774: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.4331 - accuracy: 0.7757 - val_loss: 0.4946 - val_accuracy: 0.6978\n",
            "Epoch 9775/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4012 - accuracy: 0.7952\n",
            "Epoch 9775: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4007 - accuracy: 0.7957 - val_loss: 0.4787 - val_accuracy: 0.7121\n",
            "Epoch 9776/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4113 - accuracy: 0.7902\n",
            "Epoch 9776: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.4181 - accuracy: 0.7859 - val_loss: 0.6733 - val_accuracy: 0.5732\n",
            "Epoch 9777/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3731 - accuracy: 0.8056\n",
            "Epoch 9777: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3787 - accuracy: 0.8033 - val_loss: 0.6958 - val_accuracy: 0.5767\n",
            "Epoch 9778/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3731 - accuracy: 0.8109\n",
            "Epoch 9778: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3734 - accuracy: 0.8114 - val_loss: 0.7879 - val_accuracy: 0.5133\n",
            "Epoch 9779/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3768 - accuracy: 0.8044\n",
            "Epoch 9779: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3755 - accuracy: 0.8060 - val_loss: 0.7536 - val_accuracy: 0.5453\n",
            "Epoch 9780/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3855 - accuracy: 0.8041\n",
            "Epoch 9780: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.3814 - accuracy: 0.8072 - val_loss: 0.7219 - val_accuracy: 0.5464\n",
            "Epoch 9781/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3829 - accuracy: 0.8031\n",
            "Epoch 9781: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3823 - accuracy: 0.8031 - val_loss: 0.5354 - val_accuracy: 0.6803\n",
            "Epoch 9782/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3651 - accuracy: 0.8133\n",
            "Epoch 9782: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3670 - accuracy: 0.8129 - val_loss: 0.5283 - val_accuracy: 0.6796\n",
            "Epoch 9783/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8237\n",
            "Epoch 9783: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3557 - accuracy: 0.8234 - val_loss: 0.5530 - val_accuracy: 0.6605\n",
            "Epoch 9784/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3580 - accuracy: 0.8177\n",
            "Epoch 9784: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3580 - accuracy: 0.8172 - val_loss: 0.4989 - val_accuracy: 0.6965\n",
            "Epoch 9785/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3593 - accuracy: 0.8225\n",
            "Epoch 9785: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3596 - accuracy: 0.8214 - val_loss: 0.4838 - val_accuracy: 0.7185\n",
            "Epoch 9786/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3577 - accuracy: 0.8192\n",
            "Epoch 9786: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3590 - accuracy: 0.8181 - val_loss: 0.4384 - val_accuracy: 0.7505\n",
            "Epoch 9787/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3627 - accuracy: 0.8158\n",
            "Epoch 9787: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3617 - accuracy: 0.8164 - val_loss: 0.4762 - val_accuracy: 0.7185\n",
            "Epoch 9788/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8174\n",
            "Epoch 9788: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3575 - accuracy: 0.8175 - val_loss: 0.5569 - val_accuracy: 0.6526\n",
            "Epoch 9789/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3541 - accuracy: 0.8216\n",
            "Epoch 9789: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.3547 - accuracy: 0.8213 - val_loss: 0.6978 - val_accuracy: 0.5539\n",
            "Epoch 9790/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3672 - accuracy: 0.8124\n",
            "Epoch 9790: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3664 - accuracy: 0.8127 - val_loss: 0.6362 - val_accuracy: 0.5951\n",
            "Epoch 9791/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3716 - accuracy: 0.8088\n",
            "Epoch 9791: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3726 - accuracy: 0.8078 - val_loss: 0.4497 - val_accuracy: 0.7338\n",
            "Epoch 9792/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8242\n",
            "Epoch 9792: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3545 - accuracy: 0.8234 - val_loss: 0.4231 - val_accuracy: 0.7612\n",
            "Epoch 9793/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3674 - accuracy: 0.8113\n",
            "Epoch 9793: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3655 - accuracy: 0.8138 - val_loss: 0.4788 - val_accuracy: 0.7213\n",
            "Epoch 9794/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8206\n",
            "Epoch 9794: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3560 - accuracy: 0.8217 - val_loss: 0.5120 - val_accuracy: 0.6908\n",
            "Epoch 9795/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8229\n",
            "Epoch 9795: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3493 - accuracy: 0.8235 - val_loss: 0.5116 - val_accuracy: 0.6888\n",
            "Epoch 9796/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3488 - accuracy: 0.8261\n",
            "Epoch 9796: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3495 - accuracy: 0.8250 - val_loss: 0.4796 - val_accuracy: 0.7196\n",
            "Epoch 9797/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8195\n",
            "Epoch 9797: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3540 - accuracy: 0.8204 - val_loss: 0.6898 - val_accuracy: 0.5618\n",
            "Epoch 9798/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8134\n",
            "Epoch 9798: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3640 - accuracy: 0.8139 - val_loss: 0.5858 - val_accuracy: 0.6243\n",
            "Epoch 9799/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8223\n",
            "Epoch 9799: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3580 - accuracy: 0.8205 - val_loss: 0.4804 - val_accuracy: 0.7180\n",
            "Epoch 9800/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8188\n",
            "Epoch 9800: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3592 - accuracy: 0.8167 - val_loss: 0.3860 - val_accuracy: 0.7716\n",
            "Epoch 9801/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4077 - accuracy: 0.7892\n",
            "Epoch 9801: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4029 - accuracy: 0.7914 - val_loss: 0.3469 - val_accuracy: 0.7992\n",
            "Epoch 9802/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4993 - accuracy: 0.7467\n",
            "Epoch 9802: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4847 - accuracy: 0.7548 - val_loss: 0.3976 - val_accuracy: 0.7621\n",
            "Epoch 9803/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4913 - accuracy: 0.7580\n",
            "Epoch 9803: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4835 - accuracy: 0.7611 - val_loss: 0.4356 - val_accuracy: 0.7450\n",
            "Epoch 9804/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4457 - accuracy: 0.7737\n",
            "Epoch 9804: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4412 - accuracy: 0.7754 - val_loss: 0.4281 - val_accuracy: 0.7575\n",
            "Epoch 9805/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4375 - accuracy: 0.7747\n",
            "Epoch 9805: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4347 - accuracy: 0.7764 - val_loss: 0.4944 - val_accuracy: 0.7027\n",
            "Epoch 9806/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4036 - accuracy: 0.7972\n",
            "Epoch 9806: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4046 - accuracy: 0.7957 - val_loss: 0.5848 - val_accuracy: 0.6395\n",
            "Epoch 9807/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3834 - accuracy: 0.8031\n",
            "Epoch 9807: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3886 - accuracy: 0.7997 - val_loss: 0.6035 - val_accuracy: 0.6230\n",
            "Epoch 9808/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8056\n",
            "Epoch 9808: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3802 - accuracy: 0.8041 - val_loss: 0.7258 - val_accuracy: 0.5543\n",
            "Epoch 9809/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8110\n",
            "Epoch 9809: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3832 - accuracy: 0.8065 - val_loss: 0.8894 - val_accuracy: 0.4786\n",
            "Epoch 9810/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3968 - accuracy: 0.7933\n",
            "Epoch 9810: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3978 - accuracy: 0.7934 - val_loss: 0.8374 - val_accuracy: 0.5027\n",
            "Epoch 9811/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3879 - accuracy: 0.8002\n",
            "Epoch 9811: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3873 - accuracy: 0.8010 - val_loss: 0.7265 - val_accuracy: 0.5517\n",
            "Epoch 9812/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3748 - accuracy: 0.8075\n",
            "Epoch 9812: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3740 - accuracy: 0.8081 - val_loss: 0.7001 - val_accuracy: 0.5587\n",
            "Epoch 9813/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3632 - accuracy: 0.8142\n",
            "Epoch 9813: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3649 - accuracy: 0.8127 - val_loss: 0.6358 - val_accuracy: 0.5967\n",
            "Epoch 9814/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8179\n",
            "Epoch 9814: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3618 - accuracy: 0.8172 - val_loss: 0.6865 - val_accuracy: 0.5642\n",
            "Epoch 9815/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3636 - accuracy: 0.8160\n",
            "Epoch 9815: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3639 - accuracy: 0.8157 - val_loss: 0.7967 - val_accuracy: 0.5080\n",
            "Epoch 9816/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3838 - accuracy: 0.8005\n",
            "Epoch 9816: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3799 - accuracy: 0.8038 - val_loss: 0.8168 - val_accuracy: 0.5135\n",
            "Epoch 9817/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4011 - accuracy: 0.7910\n",
            "Epoch 9817: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3972 - accuracy: 0.7940 - val_loss: 0.5661 - val_accuracy: 0.6449\n",
            "Epoch 9818/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8213\n",
            "Epoch 9818: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3567 - accuracy: 0.8210 - val_loss: 0.4796 - val_accuracy: 0.7154\n",
            "Epoch 9819/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3522 - accuracy: 0.8227\n",
            "Epoch 9819: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3541 - accuracy: 0.8218 - val_loss: 0.4547 - val_accuracy: 0.7266\n",
            "Epoch 9820/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3660 - accuracy: 0.8167\n",
            "Epoch 9820: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3651 - accuracy: 0.8171 - val_loss: 0.5062 - val_accuracy: 0.6921\n",
            "Epoch 9821/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8189\n",
            "Epoch 9821: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3549 - accuracy: 0.8204 - val_loss: 0.5971 - val_accuracy: 0.6175\n",
            "Epoch 9822/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8247\n",
            "Epoch 9822: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3517 - accuracy: 0.8231 - val_loss: 0.6395 - val_accuracy: 0.5969\n",
            "Epoch 9823/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8172\n",
            "Epoch 9823: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3570 - accuracy: 0.8190 - val_loss: 0.6658 - val_accuracy: 0.5844\n",
            "Epoch 9824/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3751 - accuracy: 0.8098\n",
            "Epoch 9824: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3736 - accuracy: 0.8104 - val_loss: 0.5036 - val_accuracy: 0.6961\n",
            "Epoch 9825/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3539 - accuracy: 0.8244\n",
            "Epoch 9825: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3529 - accuracy: 0.8239 - val_loss: 0.4132 - val_accuracy: 0.7680\n",
            "Epoch 9826/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3644 - accuracy: 0.8112\n",
            "Epoch 9826: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3635 - accuracy: 0.8115 - val_loss: 0.4728 - val_accuracy: 0.7250\n",
            "Epoch 9827/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3579 - accuracy: 0.8186\n",
            "Epoch 9827: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3579 - accuracy: 0.8183 - val_loss: 0.5111 - val_accuracy: 0.6917\n",
            "Epoch 9828/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8235\n",
            "Epoch 9828: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3500 - accuracy: 0.8236 - val_loss: 0.5428 - val_accuracy: 0.6572\n",
            "Epoch 9829/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8228\n",
            "Epoch 9829: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3495 - accuracy: 0.8234 - val_loss: 0.5581 - val_accuracy: 0.6515\n",
            "Epoch 9830/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3470 - accuracy: 0.8268\n",
            "Epoch 9830: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3488 - accuracy: 0.8255 - val_loss: 0.5081 - val_accuracy: 0.6869\n",
            "Epoch 9831/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3493 - accuracy: 0.8236\n",
            "Epoch 9831: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3491 - accuracy: 0.8239 - val_loss: 0.5180 - val_accuracy: 0.6768\n",
            "Epoch 9832/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3491 - accuracy: 0.8247\n",
            "Epoch 9832: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3496 - accuracy: 0.8236 - val_loss: 0.5087 - val_accuracy: 0.6917\n",
            "Epoch 9833/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8247\n",
            "Epoch 9833: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3512 - accuracy: 0.8234 - val_loss: 0.5406 - val_accuracy: 0.6664\n",
            "Epoch 9834/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3458 - accuracy: 0.8256\n",
            "Epoch 9834: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3484 - accuracy: 0.8245 - val_loss: 0.6025 - val_accuracy: 0.6118\n",
            "Epoch 9835/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3482 - accuracy: 0.8240\n",
            "Epoch 9835: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3490 - accuracy: 0.8241 - val_loss: 0.6924 - val_accuracy: 0.5640\n",
            "Epoch 9836/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3670 - accuracy: 0.8138\n",
            "Epoch 9836: loss did not improve from 0.34791\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3659 - accuracy: 0.8145 - val_loss: 0.5437 - val_accuracy: 0.6632\n",
            "Epoch 9837/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3471 - accuracy: 0.8271\n",
            "Epoch 9837: loss improved from 0.34791 to 0.34713, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.3471 - accuracy: 0.8269 - val_loss: 0.5324 - val_accuracy: 0.6768\n",
            "Epoch 9838/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3465 - accuracy: 0.8285\n",
            "Epoch 9838: loss improved from 0.34713 to 0.34705, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 0.3471 - accuracy: 0.8275 - val_loss: 0.4695 - val_accuracy: 0.7270\n",
            "Epoch 9839/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3515 - accuracy: 0.8220\n",
            "Epoch 9839: loss did not improve from 0.34705\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3524 - accuracy: 0.8222 - val_loss: 0.6003 - val_accuracy: 0.6138\n",
            "Epoch 9840/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8214\n",
            "Epoch 9840: loss did not improve from 0.34705\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3524 - accuracy: 0.8211 - val_loss: 0.4988 - val_accuracy: 0.7007\n",
            "Epoch 9841/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3465 - accuracy: 0.8284\n",
            "Epoch 9841: loss improved from 0.34705 to 0.34623, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 0.3462 - accuracy: 0.8280 - val_loss: 0.5565 - val_accuracy: 0.6509\n",
            "Epoch 9842/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3489 - accuracy: 0.8267\n",
            "Epoch 9842: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3507 - accuracy: 0.8251 - val_loss: 0.4367 - val_accuracy: 0.7538\n",
            "Epoch 9843/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8199\n",
            "Epoch 9843: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3546 - accuracy: 0.8202 - val_loss: 0.5563 - val_accuracy: 0.6522\n",
            "Epoch 9844/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3491 - accuracy: 0.8262\n",
            "Epoch 9844: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3493 - accuracy: 0.8257 - val_loss: 0.5148 - val_accuracy: 0.6915\n",
            "Epoch 9845/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3468 - accuracy: 0.8270\n",
            "Epoch 9845: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3478 - accuracy: 0.8266 - val_loss: 0.5671 - val_accuracy: 0.6465\n",
            "Epoch 9846/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3540 - accuracy: 0.8224\n",
            "Epoch 9846: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3535 - accuracy: 0.8220 - val_loss: 0.4255 - val_accuracy: 0.7516\n",
            "Epoch 9847/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3649 - accuracy: 0.8147\n",
            "Epoch 9847: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3627 - accuracy: 0.8158 - val_loss: 0.4219 - val_accuracy: 0.7580\n",
            "Epoch 9848/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3945 - accuracy: 0.7968\n",
            "Epoch 9848: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 0.3929 - accuracy: 0.7984 - val_loss: 0.5156 - val_accuracy: 0.6886\n",
            "Epoch 9849/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8157\n",
            "Epoch 9849: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3600 - accuracy: 0.8162 - val_loss: 0.5822 - val_accuracy: 0.6320\n",
            "Epoch 9850/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3523 - accuracy: 0.8202\n",
            "Epoch 9850: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3538 - accuracy: 0.8193 - val_loss: 0.6783 - val_accuracy: 0.5747\n",
            "Epoch 9851/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3709 - accuracy: 0.8120\n",
            "Epoch 9851: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3689 - accuracy: 0.8127 - val_loss: 0.4771 - val_accuracy: 0.7202\n",
            "Epoch 9852/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3544 - accuracy: 0.8228\n",
            "Epoch 9852: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3533 - accuracy: 0.8233 - val_loss: 0.4932 - val_accuracy: 0.7088\n",
            "Epoch 9853/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3598 - accuracy: 0.8179\n",
            "Epoch 9853: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3610 - accuracy: 0.8173 - val_loss: 0.7080 - val_accuracy: 0.5609\n",
            "Epoch 9854/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3720 - accuracy: 0.8056\n",
            "Epoch 9854: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3700 - accuracy: 0.8076 - val_loss: 0.7293 - val_accuracy: 0.5407\n",
            "Epoch 9855/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3974 - accuracy: 0.7946\n",
            "Epoch 9855: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3957 - accuracy: 0.7958 - val_loss: 0.5071 - val_accuracy: 0.6996\n",
            "Epoch 9856/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8160\n",
            "Epoch 9856: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3660 - accuracy: 0.8132 - val_loss: 0.3691 - val_accuracy: 0.7931\n",
            "Epoch 9857/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3870 - accuracy: 0.8009\n",
            "Epoch 9857: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3840 - accuracy: 0.8022 - val_loss: 0.4853 - val_accuracy: 0.7112\n",
            "Epoch 9858/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8194\n",
            "Epoch 9858: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3588 - accuracy: 0.8200 - val_loss: 0.5938 - val_accuracy: 0.6265\n",
            "Epoch 9859/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8194\n",
            "Epoch 9859: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3571 - accuracy: 0.8216 - val_loss: 0.5691 - val_accuracy: 0.6436\n",
            "Epoch 9860/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8192\n",
            "Epoch 9860: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3598 - accuracy: 0.8168 - val_loss: 0.3844 - val_accuracy: 0.7799\n",
            "Epoch 9861/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3944 - accuracy: 0.7938\n",
            "Epoch 9861: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3916 - accuracy: 0.7967 - val_loss: 0.5411 - val_accuracy: 0.6763\n",
            "Epoch 9862/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3496 - accuracy: 0.8256\n",
            "Epoch 9862: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3531 - accuracy: 0.8235 - val_loss: 0.5539 - val_accuracy: 0.6623\n",
            "Epoch 9863/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3492 - accuracy: 0.8258\n",
            "Epoch 9863: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3511 - accuracy: 0.8245 - val_loss: 0.4969 - val_accuracy: 0.7035\n",
            "Epoch 9864/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3582 - accuracy: 0.8185\n",
            "Epoch 9864: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3580 - accuracy: 0.8182 - val_loss: 0.5905 - val_accuracy: 0.6241\n",
            "Epoch 9865/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8191\n",
            "Epoch 9865: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3599 - accuracy: 0.8179 - val_loss: 0.5871 - val_accuracy: 0.6430\n",
            "Epoch 9866/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3566 - accuracy: 0.8190\n",
            "Epoch 9866: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3600 - accuracy: 0.8174 - val_loss: 0.7672 - val_accuracy: 0.5267\n",
            "Epoch 9867/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3860 - accuracy: 0.8010\n",
            "Epoch 9867: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3868 - accuracy: 0.8008 - val_loss: 0.4258 - val_accuracy: 0.7588\n",
            "Epoch 9868/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8137\n",
            "Epoch 9868: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3608 - accuracy: 0.8153 - val_loss: 0.4942 - val_accuracy: 0.7033\n",
            "Epoch 9869/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8230\n",
            "Epoch 9869: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3558 - accuracy: 0.8217 - val_loss: 0.5018 - val_accuracy: 0.6978\n",
            "Epoch 9870/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8242\n",
            "Epoch 9870: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3516 - accuracy: 0.8232 - val_loss: 0.5895 - val_accuracy: 0.6291\n",
            "Epoch 9871/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8204\n",
            "Epoch 9871: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3527 - accuracy: 0.8203 - val_loss: 0.5208 - val_accuracy: 0.6816\n",
            "Epoch 9872/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3508 - accuracy: 0.8242\n",
            "Epoch 9872: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3513 - accuracy: 0.8241 - val_loss: 0.4853 - val_accuracy: 0.7092\n",
            "Epoch 9873/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8217\n",
            "Epoch 9873: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3524 - accuracy: 0.8207 - val_loss: 0.5852 - val_accuracy: 0.6285\n",
            "Epoch 9874/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8238\n",
            "Epoch 9874: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3516 - accuracy: 0.8226 - val_loss: 0.4596 - val_accuracy: 0.7244\n",
            "Epoch 9875/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3553 - accuracy: 0.8171\n",
            "Epoch 9875: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3577 - accuracy: 0.8161 - val_loss: 0.4722 - val_accuracy: 0.7202\n",
            "Epoch 9876/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3656 - accuracy: 0.8112\n",
            "Epoch 9876: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3693 - accuracy: 0.8090 - val_loss: 0.7962 - val_accuracy: 0.5027\n",
            "Epoch 9877/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3862 - accuracy: 0.8019\n",
            "Epoch 9877: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3823 - accuracy: 0.8035 - val_loss: 0.5721 - val_accuracy: 0.6449\n",
            "Epoch 9878/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3710 - accuracy: 0.8104\n",
            "Epoch 9878: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3745 - accuracy: 0.8076 - val_loss: 0.4678 - val_accuracy: 0.7187\n",
            "Epoch 9879/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3742 - accuracy: 0.8109\n",
            "Epoch 9879: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3721 - accuracy: 0.8117 - val_loss: 0.4266 - val_accuracy: 0.7538\n",
            "Epoch 9880/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3630 - accuracy: 0.8157\n",
            "Epoch 9880: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3638 - accuracy: 0.8151 - val_loss: 0.4698 - val_accuracy: 0.7279\n",
            "Epoch 9881/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3762 - accuracy: 0.8090\n",
            "Epoch 9881: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3809 - accuracy: 0.8054 - val_loss: 0.8373 - val_accuracy: 0.4926\n",
            "Epoch 9882/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3881 - accuracy: 0.7991\n",
            "Epoch 9882: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3826 - accuracy: 0.8030 - val_loss: 0.7339 - val_accuracy: 0.5449\n",
            "Epoch 9883/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3958 - accuracy: 0.7910\n",
            "Epoch 9883: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3933 - accuracy: 0.7931 - val_loss: 0.5782 - val_accuracy: 0.6419\n",
            "Epoch 9884/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3688 - accuracy: 0.8162\n",
            "Epoch 9884: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3752 - accuracy: 0.8121 - val_loss: 0.3655 - val_accuracy: 0.7904\n",
            "Epoch 9885/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4174 - accuracy: 0.7791\n",
            "Epoch 9885: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4123 - accuracy: 0.7828 - val_loss: 0.4927 - val_accuracy: 0.7097\n",
            "Epoch 9886/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3656 - accuracy: 0.8138\n",
            "Epoch 9886: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3663 - accuracy: 0.8143 - val_loss: 0.5652 - val_accuracy: 0.6531\n",
            "Epoch 9887/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3578 - accuracy: 0.8177\n",
            "Epoch 9887: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3605 - accuracy: 0.8162 - val_loss: 0.6422 - val_accuracy: 0.5947\n",
            "Epoch 9888/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8184\n",
            "Epoch 9888: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3575 - accuracy: 0.8176 - val_loss: 0.8114 - val_accuracy: 0.5111\n",
            "Epoch 9889/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3890 - accuracy: 0.8005\n",
            "Epoch 9889: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3848 - accuracy: 0.8027 - val_loss: 0.6464 - val_accuracy: 0.5868\n",
            "Epoch 9890/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3623 - accuracy: 0.8136\n",
            "Epoch 9890: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3614 - accuracy: 0.8145 - val_loss: 0.5759 - val_accuracy: 0.6454\n",
            "Epoch 9891/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3622 - accuracy: 0.8159\n",
            "Epoch 9891: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3658 - accuracy: 0.8127 - val_loss: 0.4317 - val_accuracy: 0.7540\n",
            "Epoch 9892/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3598 - accuracy: 0.8178\n",
            "Epoch 9892: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3584 - accuracy: 0.8193 - val_loss: 0.4617 - val_accuracy: 0.7327\n",
            "Epoch 9893/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8150\n",
            "Epoch 9893: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3656 - accuracy: 0.8144 - val_loss: 0.4949 - val_accuracy: 0.7033\n",
            "Epoch 9894/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3581 - accuracy: 0.8166\n",
            "Epoch 9894: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3602 - accuracy: 0.8156 - val_loss: 0.7088 - val_accuracy: 0.5561\n",
            "Epoch 9895/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3643 - accuracy: 0.8136\n",
            "Epoch 9895: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3638 - accuracy: 0.8131 - val_loss: 0.5883 - val_accuracy: 0.6309\n",
            "Epoch 9896/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3548 - accuracy: 0.8219\n",
            "Epoch 9896: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3544 - accuracy: 0.8217 - val_loss: 0.5179 - val_accuracy: 0.6805\n",
            "Epoch 9897/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3494 - accuracy: 0.8256\n",
            "Epoch 9897: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3496 - accuracy: 0.8252 - val_loss: 0.5290 - val_accuracy: 0.6754\n",
            "Epoch 9898/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3480 - accuracy: 0.8275\n",
            "Epoch 9898: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3489 - accuracy: 0.8261 - val_loss: 0.4428 - val_accuracy: 0.7454\n",
            "Epoch 9899/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8141\n",
            "Epoch 9899: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3603 - accuracy: 0.8147 - val_loss: 0.5625 - val_accuracy: 0.6506\n",
            "Epoch 9900/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3494 - accuracy: 0.8233\n",
            "Epoch 9900: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3514 - accuracy: 0.8219 - val_loss: 0.8040 - val_accuracy: 0.5065\n",
            "Epoch 9901/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3973 - accuracy: 0.7938\n",
            "Epoch 9901: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3936 - accuracy: 0.7956 - val_loss: 0.6150 - val_accuracy: 0.6147\n",
            "Epoch 9902/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8095\n",
            "Epoch 9902: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3793 - accuracy: 0.8071 - val_loss: 0.4718 - val_accuracy: 0.7242\n",
            "Epoch 9903/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8190\n",
            "Epoch 9903: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3669 - accuracy: 0.8143 - val_loss: 0.3783 - val_accuracy: 0.7825\n",
            "Epoch 9904/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3847 - accuracy: 0.8034\n",
            "Epoch 9904: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3830 - accuracy: 0.8044 - val_loss: 0.3835 - val_accuracy: 0.7674\n",
            "Epoch 9905/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4605 - accuracy: 0.7657\n",
            "Epoch 9905: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.4526 - accuracy: 0.7699 - val_loss: 0.4050 - val_accuracy: 0.7724\n",
            "Epoch 9906/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4309 - accuracy: 0.7785\n",
            "Epoch 9906: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4304 - accuracy: 0.7789 - val_loss: 0.5379 - val_accuracy: 0.6739\n",
            "Epoch 9907/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4030 - accuracy: 0.7940\n",
            "Epoch 9907: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.4108 - accuracy: 0.7906 - val_loss: 0.8056 - val_accuracy: 0.5192\n",
            "Epoch 9908/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3825 - accuracy: 0.8005\n",
            "Epoch 9908: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3887 - accuracy: 0.7988 - val_loss: 1.0805 - val_accuracy: 0.4207\n",
            "Epoch 9909/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4350 - accuracy: 0.7777\n",
            "Epoch 9909: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.4281 - accuracy: 0.7821 - val_loss: 0.8824 - val_accuracy: 0.4948\n",
            "Epoch 9910/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4114 - accuracy: 0.7891\n",
            "Epoch 9910: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4074 - accuracy: 0.7919 - val_loss: 0.8375 - val_accuracy: 0.5005\n",
            "Epoch 9911/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4085 - accuracy: 0.7899\n",
            "Epoch 9911: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.4040 - accuracy: 0.7925 - val_loss: 0.7418 - val_accuracy: 0.5471\n",
            "Epoch 9912/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4036 - accuracy: 0.7923\n",
            "Epoch 9912: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3994 - accuracy: 0.7939 - val_loss: 0.6949 - val_accuracy: 0.5591\n",
            "Epoch 9913/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8043\n",
            "Epoch 9913: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3839 - accuracy: 0.8046 - val_loss: 0.5369 - val_accuracy: 0.6684\n",
            "Epoch 9914/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3642 - accuracy: 0.8166\n",
            "Epoch 9914: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 0.3686 - accuracy: 0.8140 - val_loss: 0.4959 - val_accuracy: 0.7079\n",
            "Epoch 9915/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3577 - accuracy: 0.8227\n",
            "Epoch 9915: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3563 - accuracy: 0.8228 - val_loss: 0.5389 - val_accuracy: 0.6669\n",
            "Epoch 9916/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8208\n",
            "Epoch 9916: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3584 - accuracy: 0.8203 - val_loss: 0.5006 - val_accuracy: 0.7031\n",
            "Epoch 9917/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8256\n",
            "Epoch 9917: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3556 - accuracy: 0.8233 - val_loss: 0.4601 - val_accuracy: 0.7222\n",
            "Epoch 9918/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8196\n",
            "Epoch 9918: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3559 - accuracy: 0.8201 - val_loss: 0.4566 - val_accuracy: 0.7362\n",
            "Epoch 9919/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8168\n",
            "Epoch 9919: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3584 - accuracy: 0.8177 - val_loss: 0.6148 - val_accuracy: 0.6173\n",
            "Epoch 9920/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8222\n",
            "Epoch 9920: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3557 - accuracy: 0.8223 - val_loss: 0.6020 - val_accuracy: 0.6177\n",
            "Epoch 9921/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3530 - accuracy: 0.8237\n",
            "Epoch 9921: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3544 - accuracy: 0.8226 - val_loss: 0.5401 - val_accuracy: 0.6588\n",
            "Epoch 9922/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3483 - accuracy: 0.8263\n",
            "Epoch 9922: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3501 - accuracy: 0.8252 - val_loss: 0.4687 - val_accuracy: 0.7228\n",
            "Epoch 9923/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3643 - accuracy: 0.8171\n",
            "Epoch 9923: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3639 - accuracy: 0.8171 - val_loss: 0.5595 - val_accuracy: 0.6445\n",
            "Epoch 9924/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8244\n",
            "Epoch 9924: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3510 - accuracy: 0.8250 - val_loss: 0.5745 - val_accuracy: 0.6379\n",
            "Epoch 9925/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3503 - accuracy: 0.8247\n",
            "Epoch 9925: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3500 - accuracy: 0.8255 - val_loss: 0.5841 - val_accuracy: 0.6320\n",
            "Epoch 9926/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8219\n",
            "Epoch 9926: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3514 - accuracy: 0.8226 - val_loss: 0.5434 - val_accuracy: 0.6654\n",
            "Epoch 9927/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8247\n",
            "Epoch 9927: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3492 - accuracy: 0.8243 - val_loss: 0.5854 - val_accuracy: 0.6313\n",
            "Epoch 9928/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3534 - accuracy: 0.8229\n",
            "Epoch 9928: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3526 - accuracy: 0.8237 - val_loss: 0.4937 - val_accuracy: 0.7033\n",
            "Epoch 9929/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3474 - accuracy: 0.8268\n",
            "Epoch 9929: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3484 - accuracy: 0.8254 - val_loss: 0.4374 - val_accuracy: 0.7461\n",
            "Epoch 9930/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3673 - accuracy: 0.8128\n",
            "Epoch 9930: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3671 - accuracy: 0.8129 - val_loss: 0.5578 - val_accuracy: 0.6550\n",
            "Epoch 9931/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3493 - accuracy: 0.8241\n",
            "Epoch 9931: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3489 - accuracy: 0.8255 - val_loss: 0.6212 - val_accuracy: 0.6037\n",
            "Epoch 9932/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8199\n",
            "Epoch 9932: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3586 - accuracy: 0.8196 - val_loss: 0.4506 - val_accuracy: 0.7408\n",
            "Epoch 9933/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3564 - accuracy: 0.8210\n",
            "Epoch 9933: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3568 - accuracy: 0.8207 - val_loss: 0.4835 - val_accuracy: 0.7068\n",
            "Epoch 9934/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8138\n",
            "Epoch 9934: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3621 - accuracy: 0.8143 - val_loss: 0.6651 - val_accuracy: 0.5811\n",
            "Epoch 9935/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3549 - accuracy: 0.8204\n",
            "Epoch 9935: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3558 - accuracy: 0.8193 - val_loss: 0.6728 - val_accuracy: 0.5765\n",
            "Epoch 9936/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3670 - accuracy: 0.8133\n",
            "Epoch 9936: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3660 - accuracy: 0.8140 - val_loss: 0.5308 - val_accuracy: 0.6700\n",
            "Epoch 9937/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3526 - accuracy: 0.8237\n",
            "Epoch 9937: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3540 - accuracy: 0.8223 - val_loss: 0.4421 - val_accuracy: 0.7479\n",
            "Epoch 9938/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3616 - accuracy: 0.8153\n",
            "Epoch 9938: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3591 - accuracy: 0.8171 - val_loss: 0.5389 - val_accuracy: 0.6651\n",
            "Epoch 9939/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3488 - accuracy: 0.8259\n",
            "Epoch 9939: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3498 - accuracy: 0.8257 - val_loss: 0.5926 - val_accuracy: 0.6263\n",
            "Epoch 9940/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3484 - accuracy: 0.8260\n",
            "Epoch 9940: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3504 - accuracy: 0.8236 - val_loss: 0.5571 - val_accuracy: 0.6489\n",
            "Epoch 9941/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3464 - accuracy: 0.8253\n",
            "Epoch 9941: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3469 - accuracy: 0.8250 - val_loss: 0.6102 - val_accuracy: 0.6039\n",
            "Epoch 9942/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8179\n",
            "Epoch 9942: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3585 - accuracy: 0.8183 - val_loss: 0.4780 - val_accuracy: 0.7158\n",
            "Epoch 9943/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3492 - accuracy: 0.8256\n",
            "Epoch 9943: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3517 - accuracy: 0.8226 - val_loss: 0.3906 - val_accuracy: 0.7731\n",
            "Epoch 9944/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4104 - accuracy: 0.7859\n",
            "Epoch 9944: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4061 - accuracy: 0.7885 - val_loss: 0.5249 - val_accuracy: 0.6776\n",
            "Epoch 9945/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8189\n",
            "Epoch 9945: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3580 - accuracy: 0.8189 - val_loss: 0.7369 - val_accuracy: 0.5319\n",
            "Epoch 9946/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3669 - accuracy: 0.8121\n",
            "Epoch 9946: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3651 - accuracy: 0.8131 - val_loss: 0.6008 - val_accuracy: 0.6294\n",
            "Epoch 9947/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3594 - accuracy: 0.8183\n",
            "Epoch 9947: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3622 - accuracy: 0.8164 - val_loss: 0.4420 - val_accuracy: 0.7448\n",
            "Epoch 9948/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3734 - accuracy: 0.8088\n",
            "Epoch 9948: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3709 - accuracy: 0.8103 - val_loss: 0.5172 - val_accuracy: 0.6822\n",
            "Epoch 9949/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8245\n",
            "Epoch 9949: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3513 - accuracy: 0.8243 - val_loss: 0.6764 - val_accuracy: 0.5686\n",
            "Epoch 9950/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8163\n",
            "Epoch 9950: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3597 - accuracy: 0.8188 - val_loss: 0.6816 - val_accuracy: 0.5721\n",
            "Epoch 9951/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3958 - accuracy: 0.7963\n",
            "Epoch 9951: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3973 - accuracy: 0.7957 - val_loss: 0.4656 - val_accuracy: 0.7237\n",
            "Epoch 9952/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3600 - accuracy: 0.8195\n",
            "Epoch 9952: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3716 - accuracy: 0.8132 - val_loss: 0.3715 - val_accuracy: 0.7665\n",
            "Epoch 9953/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4559 - accuracy: 0.7662\n",
            "Epoch 9953: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.4465 - accuracy: 0.7714 - val_loss: 0.3708 - val_accuracy: 0.7902\n",
            "Epoch 9954/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4403 - accuracy: 0.7754\n",
            "Epoch 9954: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4340 - accuracy: 0.7787 - val_loss: 0.4393 - val_accuracy: 0.7360\n",
            "Epoch 9955/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4638 - accuracy: 0.7674\n",
            "Epoch 9955: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4586 - accuracy: 0.7690 - val_loss: 0.4612 - val_accuracy: 0.7266\n",
            "Epoch 9956/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3962 - accuracy: 0.7957\n",
            "Epoch 9956: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3981 - accuracy: 0.7950 - val_loss: 0.5363 - val_accuracy: 0.6704\n",
            "Epoch 9957/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3989 - accuracy: 0.7943\n",
            "Epoch 9957: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4012 - accuracy: 0.7941 - val_loss: 0.5251 - val_accuracy: 0.6836\n",
            "Epoch 9958/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3802 - accuracy: 0.8047\n",
            "Epoch 9958: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3821 - accuracy: 0.8032 - val_loss: 0.6076 - val_accuracy: 0.6190\n",
            "Epoch 9959/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3821 - accuracy: 0.8004\n",
            "Epoch 9959: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3854 - accuracy: 0.7992 - val_loss: 0.6244 - val_accuracy: 0.6302\n",
            "Epoch 9960/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3745 - accuracy: 0.8078\n",
            "Epoch 9960: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3752 - accuracy: 0.8088 - val_loss: 0.7062 - val_accuracy: 0.5488\n",
            "Epoch 9961/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3590 - accuracy: 0.8186\n",
            "Epoch 9961: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3614 - accuracy: 0.8177 - val_loss: 0.5303 - val_accuracy: 0.6818\n",
            "Epoch 9962/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3579 - accuracy: 0.8159\n",
            "Epoch 9962: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3591 - accuracy: 0.8164 - val_loss: 0.6818 - val_accuracy: 0.5651\n",
            "Epoch 9963/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3614 - accuracy: 0.8149\n",
            "Epoch 9963: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3620 - accuracy: 0.8147 - val_loss: 0.6770 - val_accuracy: 0.5815\n",
            "Epoch 9964/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3695 - accuracy: 0.8117\n",
            "Epoch 9964: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3675 - accuracy: 0.8131 - val_loss: 0.6608 - val_accuracy: 0.5859\n",
            "Epoch 9965/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3601 - accuracy: 0.8193\n",
            "Epoch 9965: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3609 - accuracy: 0.8197 - val_loss: 0.5662 - val_accuracy: 0.6531\n",
            "Epoch 9966/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3531 - accuracy: 0.8240\n",
            "Epoch 9966: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3533 - accuracy: 0.8235 - val_loss: 0.5669 - val_accuracy: 0.6447\n",
            "Epoch 9967/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8234\n",
            "Epoch 9967: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3523 - accuracy: 0.8222 - val_loss: 0.5293 - val_accuracy: 0.6704\n",
            "Epoch 9968/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3480 - accuracy: 0.8252\n",
            "Epoch 9968: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3490 - accuracy: 0.8241 - val_loss: 0.5297 - val_accuracy: 0.6798\n",
            "Epoch 9969/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8266\n",
            "Epoch 9969: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3504 - accuracy: 0.8255 - val_loss: 0.5091 - val_accuracy: 0.6965\n",
            "Epoch 9970/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3499 - accuracy: 0.8244\n",
            "Epoch 9970: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.3511 - accuracy: 0.8235 - val_loss: 0.6287 - val_accuracy: 0.6002\n",
            "Epoch 9971/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3676 - accuracy: 0.8114\n",
            "Epoch 9971: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3679 - accuracy: 0.8111 - val_loss: 0.4707 - val_accuracy: 0.7224\n",
            "Epoch 9972/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8262\n",
            "Epoch 9972: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3547 - accuracy: 0.8246 - val_loss: 0.4593 - val_accuracy: 0.7292\n",
            "Epoch 9973/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3551 - accuracy: 0.8201\n",
            "Epoch 9973: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3553 - accuracy: 0.8207 - val_loss: 0.5635 - val_accuracy: 0.6489\n",
            "Epoch 9974/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3521 - accuracy: 0.8226\n",
            "Epoch 9974: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3540 - accuracy: 0.8216 - val_loss: 0.6265 - val_accuracy: 0.6081\n",
            "Epoch 9975/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3602 - accuracy: 0.8177\n",
            "Epoch 9975: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3598 - accuracy: 0.8162 - val_loss: 0.5380 - val_accuracy: 0.6647\n",
            "Epoch 9976/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8267\n",
            "Epoch 9976: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3507 - accuracy: 0.8260 - val_loss: 0.4715 - val_accuracy: 0.7226\n",
            "Epoch 9977/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8258\n",
            "Epoch 9977: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3524 - accuracy: 0.8245 - val_loss: 0.4637 - val_accuracy: 0.7301\n",
            "Epoch 9978/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8221\n",
            "Epoch 9978: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3546 - accuracy: 0.8228 - val_loss: 0.6062 - val_accuracy: 0.6133\n",
            "Epoch 9979/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8191\n",
            "Epoch 9979: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3551 - accuracy: 0.8189 - val_loss: 0.5379 - val_accuracy: 0.6759\n",
            "Epoch 9980/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3498 - accuracy: 0.8271\n",
            "Epoch 9980: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3500 - accuracy: 0.8265 - val_loss: 0.5317 - val_accuracy: 0.6783\n",
            "Epoch 9981/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3484 - accuracy: 0.8274\n",
            "Epoch 9981: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3487 - accuracy: 0.8271 - val_loss: 0.5111 - val_accuracy: 0.6877\n",
            "Epoch 9982/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3475 - accuracy: 0.8278\n",
            "Epoch 9982: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3488 - accuracy: 0.8269 - val_loss: 0.6016 - val_accuracy: 0.6120\n",
            "Epoch 9983/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8222\n",
            "Epoch 9983: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3491 - accuracy: 0.8239 - val_loss: 0.5890 - val_accuracy: 0.6252\n",
            "Epoch 9984/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8242\n",
            "Epoch 9984: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3512 - accuracy: 0.8238 - val_loss: 0.4607 - val_accuracy: 0.7347\n",
            "Epoch 9985/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3502 - accuracy: 0.8263\n",
            "Epoch 9985: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3509 - accuracy: 0.8253 - val_loss: 0.5584 - val_accuracy: 0.6528\n",
            "Epoch 9986/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3486 - accuracy: 0.8264\n",
            "Epoch 9986: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3492 - accuracy: 0.8264 - val_loss: 0.6009 - val_accuracy: 0.6206\n",
            "Epoch 9987/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3516 - accuracy: 0.8215\n",
            "Epoch 9987: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3513 - accuracy: 0.8209 - val_loss: 0.6023 - val_accuracy: 0.6151\n",
            "Epoch 9988/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8211\n",
            "Epoch 9988: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3561 - accuracy: 0.8192 - val_loss: 0.4687 - val_accuracy: 0.7239\n",
            "Epoch 9989/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3499 - accuracy: 0.8253\n",
            "Epoch 9989: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3511 - accuracy: 0.8235 - val_loss: 0.4205 - val_accuracy: 0.7573\n",
            "Epoch 9990/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3837 - accuracy: 0.8027\n",
            "Epoch 9990: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3843 - accuracy: 0.8018 - val_loss: 0.6363 - val_accuracy: 0.5993\n",
            "Epoch 9991/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8233\n",
            "Epoch 9991: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3548 - accuracy: 0.8217 - val_loss: 0.6341 - val_accuracy: 0.5967\n",
            "Epoch 9992/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3658 - accuracy: 0.8109\n",
            "Epoch 9992: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3662 - accuracy: 0.8110 - val_loss: 0.4189 - val_accuracy: 0.7628\n",
            "Epoch 9993/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3661 - accuracy: 0.8136\n",
            "Epoch 9993: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3658 - accuracy: 0.8132 - val_loss: 0.4216 - val_accuracy: 0.7547\n",
            "Epoch 9994/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3834 - accuracy: 0.8030\n",
            "Epoch 9994: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3828 - accuracy: 0.8031 - val_loss: 0.6152 - val_accuracy: 0.6131\n",
            "Epoch 9995/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3565 - accuracy: 0.8182\n",
            "Epoch 9995: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3575 - accuracy: 0.8177 - val_loss: 0.8092 - val_accuracy: 0.5117\n",
            "Epoch 9996/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3970 - accuracy: 0.7943\n",
            "Epoch 9996: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3923 - accuracy: 0.7971 - val_loss: 0.6018 - val_accuracy: 0.6261\n",
            "Epoch 9997/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3700 - accuracy: 0.8115\n",
            "Epoch 9997: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3738 - accuracy: 0.8094 - val_loss: 0.4185 - val_accuracy: 0.7593\n",
            "Epoch 9998/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3730 - accuracy: 0.8087\n",
            "Epoch 9998: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3689 - accuracy: 0.8122 - val_loss: 0.4275 - val_accuracy: 0.7531\n",
            "Epoch 9999/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3720 - accuracy: 0.8103\n",
            "Epoch 9999: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3710 - accuracy: 0.8103 - val_loss: 0.6435 - val_accuracy: 0.5892\n",
            "Epoch 10000/10000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8231\n",
            "Epoch 10000: loss did not improve from 0.34623\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3547 - accuracy: 0.8224 - val_loss: 0.7593 - val_accuracy: 0.5350\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 2.2007 - accuracy: 0.5358\n",
            "[2.200653314590454, 0.5358083844184875]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.65      0.70      0.67      2848\n",
            "     class 1       0.22      0.17      0.19      1327\n",
            "\n",
            "    accuracy                           0.54      4175\n",
            "   macro avg       0.43      0.44      0.43      4175\n",
            "weighted avg       0.51      0.54      0.52      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "712/712 [==============================] - 2s 2ms/step - loss: 0.4646 - accuracy: 0.7411\n",
            "[0.46455439925193787, 0.7411113977432251]\n",
            "712/712 [==============================] - 1s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.68      0.92      0.78     11391\n",
            "     class 1       0.88      0.56      0.68     11391\n",
            "\n",
            "    accuracy                           0.74     22782\n",
            "   macro avg       0.78      0.74      0.73     22782\n",
            "weighted avg       0.78      0.74      0.73     22782\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Model Continue Learning***#\n",
        "#------------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 8224, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_2', 'class 1']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95sTy2url9fi",
        "outputId": "c441626e-9e52-43a6-8adf-2de4f1a22251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4960 - accuracy: 0.7510\n",
            "Epoch 3752: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.4827 - accuracy: 0.7579 - val_loss: 0.3581 - val_accuracy: 0.7713\n",
            "Epoch 3753/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4914 - accuracy: 0.7578\n",
            "Epoch 3753: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4777 - accuracy: 0.7645 - val_loss: 0.3559 - val_accuracy: 0.7900\n",
            "Epoch 3754/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4459 - accuracy: 0.7732\n",
            "Epoch 3754: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.4400 - accuracy: 0.7760 - val_loss: 0.4375 - val_accuracy: 0.7446\n",
            "Epoch 3755/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8131\n",
            "Epoch 3755: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3698 - accuracy: 0.8143 - val_loss: 0.4354 - val_accuracy: 0.7338\n",
            "Epoch 3756/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3684 - accuracy: 0.8108\n",
            "Epoch 3756: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3672 - accuracy: 0.8122 - val_loss: 0.4736 - val_accuracy: 0.7176\n",
            "Epoch 3757/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3544 - accuracy: 0.8211\n",
            "Epoch 3757: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3555 - accuracy: 0.8200 - val_loss: 0.5479 - val_accuracy: 0.6783\n",
            "Epoch 3758/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3427 - accuracy: 0.8296\n",
            "Epoch 3758: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3442 - accuracy: 0.8293 - val_loss: 0.5160 - val_accuracy: 0.6794\n",
            "Epoch 3759/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8300\n",
            "Epoch 3759: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3401 - accuracy: 0.8290 - val_loss: 0.4846 - val_accuracy: 0.7095\n",
            "Epoch 3760/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3377 - accuracy: 0.8295\n",
            "Epoch 3760: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3389 - accuracy: 0.8283 - val_loss: 0.4950 - val_accuracy: 0.7027\n",
            "Epoch 3761/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8324\n",
            "Epoch 3761: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3380 - accuracy: 0.8309 - val_loss: 0.4826 - val_accuracy: 0.7110\n",
            "Epoch 3762/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3396 - accuracy: 0.8303\n",
            "Epoch 3762: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3409 - accuracy: 0.8292 - val_loss: 0.5617 - val_accuracy: 0.6421\n",
            "Epoch 3763/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3423 - accuracy: 0.8254\n",
            "Epoch 3763: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3405 - accuracy: 0.8274 - val_loss: 0.5267 - val_accuracy: 0.6757\n",
            "Epoch 3764/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8335\n",
            "Epoch 3764: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3368 - accuracy: 0.8330 - val_loss: 0.5171 - val_accuracy: 0.6785\n",
            "Epoch 3765/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3367 - accuracy: 0.8326\n",
            "Epoch 3765: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3366 - accuracy: 0.8321 - val_loss: 0.4652 - val_accuracy: 0.7283\n",
            "Epoch 3766/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3375 - accuracy: 0.8298\n",
            "Epoch 3766: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3373 - accuracy: 0.8303 - val_loss: 0.4504 - val_accuracy: 0.7428\n",
            "Epoch 3767/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3402 - accuracy: 0.8287\n",
            "Epoch 3767: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3393 - accuracy: 0.8290 - val_loss: 0.5331 - val_accuracy: 0.6708\n",
            "Epoch 3768/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3343 - accuracy: 0.8326\n",
            "Epoch 3768: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3340 - accuracy: 0.8331 - val_loss: 0.5658 - val_accuracy: 0.6434\n",
            "Epoch 3769/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8307\n",
            "Epoch 3769: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3341 - accuracy: 0.8315 - val_loss: 0.6387 - val_accuracy: 0.5927\n",
            "Epoch 3770/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8177\n",
            "Epoch 3770: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3555 - accuracy: 0.8182 - val_loss: 0.5466 - val_accuracy: 0.6496\n",
            "Epoch 3771/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3464 - accuracy: 0.8270\n",
            "Epoch 3771: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3511 - accuracy: 0.8229 - val_loss: 0.3712 - val_accuracy: 0.7858\n",
            "Epoch 3772/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8165\n",
            "Epoch 3772: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3590 - accuracy: 0.8173 - val_loss: 0.3680 - val_accuracy: 0.7830\n",
            "Epoch 3773/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3794 - accuracy: 0.8075\n",
            "Epoch 3773: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3746 - accuracy: 0.8103 - val_loss: 0.4196 - val_accuracy: 0.7586\n",
            "Epoch 3774/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3784 - accuracy: 0.8045\n",
            "Epoch 3774: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3755 - accuracy: 0.8064 - val_loss: 0.5003 - val_accuracy: 0.6871\n",
            "Epoch 3775/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3420 - accuracy: 0.8253\n",
            "Epoch 3775: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3440 - accuracy: 0.8243 - val_loss: 0.5831 - val_accuracy: 0.6340\n",
            "Epoch 3776/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8298\n",
            "Epoch 3776: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3444 - accuracy: 0.8266 - val_loss: 0.8132 - val_accuracy: 0.5188\n",
            "Epoch 3777/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8048\n",
            "Epoch 3777: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3760 - accuracy: 0.8065 - val_loss: 0.6902 - val_accuracy: 0.5635\n",
            "Epoch 3778/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3709 - accuracy: 0.8101\n",
            "Epoch 3778: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3698 - accuracy: 0.8097 - val_loss: 0.4977 - val_accuracy: 0.7099\n",
            "Epoch 3779/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8306\n",
            "Epoch 3779: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3461 - accuracy: 0.8279 - val_loss: 0.4208 - val_accuracy: 0.7586\n",
            "Epoch 3780/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3515 - accuracy: 0.8235\n",
            "Epoch 3780: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3519 - accuracy: 0.8228 - val_loss: 0.4015 - val_accuracy: 0.7722\n",
            "Epoch 3781/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3518 - accuracy: 0.8202\n",
            "Epoch 3781: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3504 - accuracy: 0.8213 - val_loss: 0.4895 - val_accuracy: 0.7044\n",
            "Epoch 3782/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3485 - accuracy: 0.8236\n",
            "Epoch 3782: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3514 - accuracy: 0.8217 - val_loss: 0.6093 - val_accuracy: 0.6256\n",
            "Epoch 3783/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3379 - accuracy: 0.8287\n",
            "Epoch 3783: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3401 - accuracy: 0.8270 - val_loss: 0.5510 - val_accuracy: 0.6498\n",
            "Epoch 3784/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8329\n",
            "Epoch 3784: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3345 - accuracy: 0.8320 - val_loss: 0.6323 - val_accuracy: 0.6006\n",
            "Epoch 3785/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3453 - accuracy: 0.8272\n",
            "Epoch 3785: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3442 - accuracy: 0.8286 - val_loss: 0.5560 - val_accuracy: 0.6579\n",
            "Epoch 3786/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3394 - accuracy: 0.8307\n",
            "Epoch 3786: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3395 - accuracy: 0.8298 - val_loss: 0.5047 - val_accuracy: 0.6963\n",
            "Epoch 3787/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8361\n",
            "Epoch 3787: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3363 - accuracy: 0.8346 - val_loss: 0.4071 - val_accuracy: 0.7610\n",
            "Epoch 3788/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3480 - accuracy: 0.8237\n",
            "Epoch 3788: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3465 - accuracy: 0.8241 - val_loss: 0.4241 - val_accuracy: 0.7573\n",
            "Epoch 3789/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3410 - accuracy: 0.8259\n",
            "Epoch 3789: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3408 - accuracy: 0.8264 - val_loss: 0.4608 - val_accuracy: 0.7323\n",
            "Epoch 3790/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8258\n",
            "Epoch 3790: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3425 - accuracy: 0.8271 - val_loss: 0.5570 - val_accuracy: 0.6506\n",
            "Epoch 3791/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8317\n",
            "Epoch 3791: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3346 - accuracy: 0.8321 - val_loss: 0.6746 - val_accuracy: 0.5730\n",
            "Epoch 3792/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8221\n",
            "Epoch 3792: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3498 - accuracy: 0.8229 - val_loss: 0.5633 - val_accuracy: 0.6463\n",
            "Epoch 3793/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3496 - accuracy: 0.8230\n",
            "Epoch 3793: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3517 - accuracy: 0.8211 - val_loss: 0.4833 - val_accuracy: 0.7081\n",
            "Epoch 3794/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8334\n",
            "Epoch 3794: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3409 - accuracy: 0.8296 - val_loss: 0.3606 - val_accuracy: 0.7887\n",
            "Epoch 3795/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3650 - accuracy: 0.8107\n",
            "Epoch 3795: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3612 - accuracy: 0.8132 - val_loss: 0.4444 - val_accuracy: 0.7433\n",
            "Epoch 3796/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3383 - accuracy: 0.8296\n",
            "Epoch 3796: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3376 - accuracy: 0.8306 - val_loss: 0.4805 - val_accuracy: 0.7073\n",
            "Epoch 3797/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3340 - accuracy: 0.8347\n",
            "Epoch 3797: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3345 - accuracy: 0.8342 - val_loss: 0.5884 - val_accuracy: 0.6283\n",
            "Epoch 3798/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3385 - accuracy: 0.8281\n",
            "Epoch 3798: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3386 - accuracy: 0.8283 - val_loss: 0.5747 - val_accuracy: 0.6362\n",
            "Epoch 3799/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3451 - accuracy: 0.8240\n",
            "Epoch 3799: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3447 - accuracy: 0.8247 - val_loss: 0.4661 - val_accuracy: 0.7277\n",
            "Epoch 3800/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3361 - accuracy: 0.8346\n",
            "Epoch 3800: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3364 - accuracy: 0.8348 - val_loss: 0.4118 - val_accuracy: 0.7648\n",
            "Epoch 3801/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3391 - accuracy: 0.8310\n",
            "Epoch 3801: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3366 - accuracy: 0.8326 - val_loss: 0.5658 - val_accuracy: 0.6465\n",
            "Epoch 3802/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8330\n",
            "Epoch 3802: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3351 - accuracy: 0.8325 - val_loss: 0.4428 - val_accuracy: 0.7428\n",
            "Epoch 3803/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8378\n",
            "Epoch 3803: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3326 - accuracy: 0.8348 - val_loss: 0.3918 - val_accuracy: 0.7757\n",
            "Epoch 3804/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3610 - accuracy: 0.8148\n",
            "Epoch 3804: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3600 - accuracy: 0.8156 - val_loss: 0.5831 - val_accuracy: 0.6261\n",
            "Epoch 3805/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8372\n",
            "Epoch 3805: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3333 - accuracy: 0.8344 - val_loss: 0.5922 - val_accuracy: 0.6254\n",
            "Epoch 3806/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3391 - accuracy: 0.8288\n",
            "Epoch 3806: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3385 - accuracy: 0.8299 - val_loss: 0.5716 - val_accuracy: 0.6388\n",
            "Epoch 3807/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3364 - accuracy: 0.8329\n",
            "Epoch 3807: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3386 - accuracy: 0.8306 - val_loss: 0.3831 - val_accuracy: 0.7812\n",
            "Epoch 3808/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3672 - accuracy: 0.8130\n",
            "Epoch 3808: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3641 - accuracy: 0.8146 - val_loss: 0.4983 - val_accuracy: 0.7020\n",
            "Epoch 3809/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8340\n",
            "Epoch 3809: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3361 - accuracy: 0.8320 - val_loss: 0.4933 - val_accuracy: 0.7018\n",
            "Epoch 3810/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3313 - accuracy: 0.8354\n",
            "Epoch 3810: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3340 - accuracy: 0.8329 - val_loss: 0.5139 - val_accuracy: 0.6847\n",
            "Epoch 3811/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8321\n",
            "Epoch 3811: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3359 - accuracy: 0.8303 - val_loss: 0.6306 - val_accuracy: 0.5949\n",
            "Epoch 3812/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3389 - accuracy: 0.8294\n",
            "Epoch 3812: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3381 - accuracy: 0.8302 - val_loss: 0.5029 - val_accuracy: 0.6932\n",
            "Epoch 3813/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3302 - accuracy: 0.8358\n",
            "Epoch 3813: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3311 - accuracy: 0.8346 - val_loss: 0.4379 - val_accuracy: 0.7474\n",
            "Epoch 3814/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3560 - accuracy: 0.8192\n",
            "Epoch 3814: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3546 - accuracy: 0.8204 - val_loss: 0.6408 - val_accuracy: 0.5997\n",
            "Epoch 3815/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3500 - accuracy: 0.8179\n",
            "Epoch 3815: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3482 - accuracy: 0.8196 - val_loss: 0.8409 - val_accuracy: 0.4999\n",
            "Epoch 3816/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4106 - accuracy: 0.7871\n",
            "Epoch 3816: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4093 - accuracy: 0.7882 - val_loss: 0.3739 - val_accuracy: 0.7917\n",
            "Epoch 3817/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8160\n",
            "Epoch 3817: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3564 - accuracy: 0.8196 - val_loss: 0.4750 - val_accuracy: 0.7136\n",
            "Epoch 3818/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3476 - accuracy: 0.8265\n",
            "Epoch 3818: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3469 - accuracy: 0.8270 - val_loss: 0.4889 - val_accuracy: 0.7079\n",
            "Epoch 3819/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8283\n",
            "Epoch 3819: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3432 - accuracy: 0.8261 - val_loss: 0.8087 - val_accuracy: 0.5113\n",
            "Epoch 3820/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8073\n",
            "Epoch 3820: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3673 - accuracy: 0.8099 - val_loss: 0.7473 - val_accuracy: 0.5414\n",
            "Epoch 3821/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4108 - accuracy: 0.7859\n",
            "Epoch 3821: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.4103 - accuracy: 0.7858 - val_loss: 0.5514 - val_accuracy: 0.6623\n",
            "Epoch 3822/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8132\n",
            "Epoch 3822: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3817 - accuracy: 0.8071 - val_loss: 0.3723 - val_accuracy: 0.7889\n",
            "Epoch 3823/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3554 - accuracy: 0.8211\n",
            "Epoch 3823: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3632 - accuracy: 0.8157 - val_loss: 0.3543 - val_accuracy: 0.7937\n",
            "Epoch 3824/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4043 - accuracy: 0.7968\n",
            "Epoch 3824: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3988 - accuracy: 0.7982 - val_loss: 0.4524 - val_accuracy: 0.7345\n",
            "Epoch 3825/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3435 - accuracy: 0.8275\n",
            "Epoch 3825: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3442 - accuracy: 0.8283 - val_loss: 0.5300 - val_accuracy: 0.6724\n",
            "Epoch 3826/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8309\n",
            "Epoch 3826: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3389 - accuracy: 0.8297 - val_loss: 0.4746 - val_accuracy: 0.7178\n",
            "Epoch 3827/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3418 - accuracy: 0.8266\n",
            "Epoch 3827: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.3408 - accuracy: 0.8281 - val_loss: 0.5553 - val_accuracy: 0.6520\n",
            "Epoch 3828/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3357 - accuracy: 0.8349\n",
            "Epoch 3828: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3359 - accuracy: 0.8342 - val_loss: 0.5849 - val_accuracy: 0.6272\n",
            "Epoch 3829/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3426 - accuracy: 0.8296\n",
            "Epoch 3829: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3430 - accuracy: 0.8283 - val_loss: 0.5459 - val_accuracy: 0.6654\n",
            "Epoch 3830/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3406 - accuracy: 0.8308\n",
            "Epoch 3830: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3408 - accuracy: 0.8307 - val_loss: 0.4775 - val_accuracy: 0.7134\n",
            "Epoch 3831/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3359 - accuracy: 0.8335\n",
            "Epoch 3831: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3366 - accuracy: 0.8325 - val_loss: 0.4330 - val_accuracy: 0.7496\n",
            "Epoch 3832/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3427 - accuracy: 0.8281\n",
            "Epoch 3832: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3422 - accuracy: 0.8289 - val_loss: 0.4820 - val_accuracy: 0.7114\n",
            "Epoch 3833/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3435 - accuracy: 0.8276\n",
            "Epoch 3833: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3430 - accuracy: 0.8271 - val_loss: 0.6765 - val_accuracy: 0.5820\n",
            "Epoch 3834/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3638 - accuracy: 0.8120\n",
            "Epoch 3834: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3606 - accuracy: 0.8140 - val_loss: 0.5601 - val_accuracy: 0.6522\n",
            "Epoch 3835/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3393 - accuracy: 0.8289\n",
            "Epoch 3835: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3398 - accuracy: 0.8281 - val_loss: 0.3960 - val_accuracy: 0.7817\n",
            "Epoch 3836/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3437 - accuracy: 0.8256\n",
            "Epoch 3836: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3433 - accuracy: 0.8260 - val_loss: 0.5425 - val_accuracy: 0.6570\n",
            "Epoch 3837/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3336 - accuracy: 0.8334\n",
            "Epoch 3837: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3325 - accuracy: 0.8333 - val_loss: 0.4372 - val_accuracy: 0.7426\n",
            "Epoch 3838/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8250\n",
            "Epoch 3838: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3471 - accuracy: 0.8236 - val_loss: 0.6376 - val_accuracy: 0.5958\n",
            "Epoch 3839/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3384 - accuracy: 0.8285\n",
            "Epoch 3839: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3389 - accuracy: 0.8280 - val_loss: 0.7027 - val_accuracy: 0.5607\n",
            "Epoch 3840/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3683 - accuracy: 0.8110\n",
            "Epoch 3840: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3686 - accuracy: 0.8104 - val_loss: 0.4657 - val_accuracy: 0.7231\n",
            "Epoch 3841/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8328\n",
            "Epoch 3841: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3403 - accuracy: 0.8307 - val_loss: 0.4267 - val_accuracy: 0.7533\n",
            "Epoch 3842/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8346\n",
            "Epoch 3842: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.3377 - accuracy: 0.8322 - val_loss: 0.3598 - val_accuracy: 0.8032\n",
            "Epoch 3843/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3639 - accuracy: 0.8113\n",
            "Epoch 3843: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3620 - accuracy: 0.8123 - val_loss: 0.5431 - val_accuracy: 0.6647\n",
            "Epoch 3844/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3330 - accuracy: 0.8332\n",
            "Epoch 3844: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3357 - accuracy: 0.8319 - val_loss: 0.7533 - val_accuracy: 0.5357\n",
            "Epoch 3845/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3722 - accuracy: 0.8093\n",
            "Epoch 3845: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3689 - accuracy: 0.8108 - val_loss: 0.4728 - val_accuracy: 0.7167\n",
            "Epoch 3846/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3291 - accuracy: 0.8367\n",
            "Epoch 3846: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3299 - accuracy: 0.8363 - val_loss: 0.4355 - val_accuracy: 0.7406\n",
            "Epoch 3847/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3352 - accuracy: 0.8293\n",
            "Epoch 3847: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3349 - accuracy: 0.8308 - val_loss: 0.4170 - val_accuracy: 0.7571\n",
            "Epoch 3848/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8186\n",
            "Epoch 3848: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3559 - accuracy: 0.8188 - val_loss: 0.5016 - val_accuracy: 0.6974\n",
            "Epoch 3849/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3412 - accuracy: 0.8284\n",
            "Epoch 3849: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3440 - accuracy: 0.8269 - val_loss: 0.7367 - val_accuracy: 0.5387\n",
            "Epoch 3850/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3525 - accuracy: 0.8211\n",
            "Epoch 3850: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3524 - accuracy: 0.8210 - val_loss: 0.7596 - val_accuracy: 0.5372\n",
            "Epoch 3851/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3903 - accuracy: 0.7974\n",
            "Epoch 3851: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3875 - accuracy: 0.7985 - val_loss: 0.6479 - val_accuracy: 0.5804\n",
            "Epoch 3852/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3917 - accuracy: 0.7980\n",
            "Epoch 3852: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3977 - accuracy: 0.7950 - val_loss: 0.3782 - val_accuracy: 0.7799\n",
            "Epoch 3853/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8208\n",
            "Epoch 3853: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3606 - accuracy: 0.8180 - val_loss: 0.3768 - val_accuracy: 0.7716\n",
            "Epoch 3854/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3993 - accuracy: 0.7977\n",
            "Epoch 3854: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3936 - accuracy: 0.7999 - val_loss: 0.3520 - val_accuracy: 0.7986\n",
            "Epoch 3855/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4137 - accuracy: 0.7885\n",
            "Epoch 3855: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4075 - accuracy: 0.7920 - val_loss: 0.5188 - val_accuracy: 0.6783\n",
            "Epoch 3856/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3459 - accuracy: 0.8251\n",
            "Epoch 3856: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3481 - accuracy: 0.8237 - val_loss: 0.5294 - val_accuracy: 0.6728\n",
            "Epoch 3857/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3420 - accuracy: 0.8244\n",
            "Epoch 3857: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3433 - accuracy: 0.8233 - val_loss: 0.6912 - val_accuracy: 0.5631\n",
            "Epoch 3858/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3472 - accuracy: 0.8254\n",
            "Epoch 3858: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3476 - accuracy: 0.8249 - val_loss: 0.5250 - val_accuracy: 0.6761\n",
            "Epoch 3859/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8317\n",
            "Epoch 3859: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3365 - accuracy: 0.8313 - val_loss: 0.5348 - val_accuracy: 0.6621\n",
            "Epoch 3860/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3336 - accuracy: 0.8330\n",
            "Epoch 3860: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3343 - accuracy: 0.8329 - val_loss: 0.4952 - val_accuracy: 0.7035\n",
            "Epoch 3861/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3342 - accuracy: 0.8349\n",
            "Epoch 3861: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3340 - accuracy: 0.8351 - val_loss: 0.5032 - val_accuracy: 0.6965\n",
            "Epoch 3862/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3336 - accuracy: 0.8342\n",
            "Epoch 3862: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3329 - accuracy: 0.8343 - val_loss: 0.4722 - val_accuracy: 0.7114\n",
            "Epoch 3863/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3319 - accuracy: 0.8340\n",
            "Epoch 3863: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3319 - accuracy: 0.8350 - val_loss: 0.5362 - val_accuracy: 0.6585\n",
            "Epoch 3864/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3354 - accuracy: 0.8330\n",
            "Epoch 3864: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3341 - accuracy: 0.8340 - val_loss: 0.4656 - val_accuracy: 0.7279\n",
            "Epoch 3865/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8351\n",
            "Epoch 3865: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3326 - accuracy: 0.8341 - val_loss: 0.4337 - val_accuracy: 0.7483\n",
            "Epoch 3866/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8326\n",
            "Epoch 3866: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3331 - accuracy: 0.8336 - val_loss: 0.5242 - val_accuracy: 0.6807\n",
            "Epoch 3867/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8346\n",
            "Epoch 3867: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3349 - accuracy: 0.8347 - val_loss: 0.4855 - val_accuracy: 0.7108\n",
            "Epoch 3868/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3392 - accuracy: 0.8290\n",
            "Epoch 3868: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3365 - accuracy: 0.8303 - val_loss: 0.4622 - val_accuracy: 0.7246\n",
            "Epoch 3869/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8265\n",
            "Epoch 3869: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3395 - accuracy: 0.8278 - val_loss: 0.5971 - val_accuracy: 0.6184\n",
            "Epoch 3870/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8315\n",
            "Epoch 3870: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3351 - accuracy: 0.8293 - val_loss: 0.7627 - val_accuracy: 0.5343\n",
            "Epoch 3871/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8005\n",
            "Epoch 3871: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3818 - accuracy: 0.8023 - val_loss: 0.5693 - val_accuracy: 0.6458\n",
            "Epoch 3872/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8213\n",
            "Epoch 3872: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3614 - accuracy: 0.8179 - val_loss: 0.4055 - val_accuracy: 0.7623\n",
            "Epoch 3873/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8236\n",
            "Epoch 3873: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3522 - accuracy: 0.8217 - val_loss: 0.3834 - val_accuracy: 0.7718\n",
            "Epoch 3874/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3955 - accuracy: 0.7968\n",
            "Epoch 3874: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3919 - accuracy: 0.7991 - val_loss: 0.4525 - val_accuracy: 0.7312\n",
            "Epoch 3875/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3666 - accuracy: 0.8127\n",
            "Epoch 3875: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3683 - accuracy: 0.8114 - val_loss: 0.6881 - val_accuracy: 0.5802\n",
            "Epoch 3876/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3540 - accuracy: 0.8218\n",
            "Epoch 3876: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3520 - accuracy: 0.8238 - val_loss: 0.6789 - val_accuracy: 0.5774\n",
            "Epoch 3877/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3496 - accuracy: 0.8233\n",
            "Epoch 3877: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3496 - accuracy: 0.8229 - val_loss: 0.7631 - val_accuracy: 0.5350\n",
            "Epoch 3878/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3960 - accuracy: 0.7947\n",
            "Epoch 3878: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3933 - accuracy: 0.7976 - val_loss: 0.4962 - val_accuracy: 0.6976\n",
            "Epoch 3879/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8287\n",
            "Epoch 3879: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3452 - accuracy: 0.8264 - val_loss: 0.4042 - val_accuracy: 0.7630\n",
            "Epoch 3880/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3487 - accuracy: 0.8238\n",
            "Epoch 3880: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3501 - accuracy: 0.8222 - val_loss: 0.3730 - val_accuracy: 0.7799\n",
            "Epoch 3881/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3880 - accuracy: 0.7982\n",
            "Epoch 3881: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3849 - accuracy: 0.8012 - val_loss: 0.4418 - val_accuracy: 0.7397\n",
            "Epoch 3882/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3497 - accuracy: 0.8218\n",
            "Epoch 3882: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3535 - accuracy: 0.8188 - val_loss: 0.6633 - val_accuracy: 0.5850\n",
            "Epoch 3883/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3412 - accuracy: 0.8277\n",
            "Epoch 3883: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3441 - accuracy: 0.8268 - val_loss: 0.7021 - val_accuracy: 0.5657\n",
            "Epoch 3884/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3552 - accuracy: 0.8185\n",
            "Epoch 3884: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3533 - accuracy: 0.8204 - val_loss: 0.6959 - val_accuracy: 0.5618\n",
            "Epoch 3885/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3579 - accuracy: 0.8166\n",
            "Epoch 3885: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3561 - accuracy: 0.8176 - val_loss: 0.4715 - val_accuracy: 0.7158\n",
            "Epoch 3886/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3389 - accuracy: 0.8299\n",
            "Epoch 3886: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3388 - accuracy: 0.8300 - val_loss: 0.4579 - val_accuracy: 0.7343\n",
            "Epoch 3887/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8340\n",
            "Epoch 3887: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3339 - accuracy: 0.8329 - val_loss: 0.4221 - val_accuracy: 0.7626\n",
            "Epoch 3888/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3498 - accuracy: 0.8223\n",
            "Epoch 3888: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3484 - accuracy: 0.8236 - val_loss: 0.3764 - val_accuracy: 0.7885\n",
            "Epoch 3889/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3785 - accuracy: 0.8047\n",
            "Epoch 3889: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3757 - accuracy: 0.8055 - val_loss: 0.4909 - val_accuracy: 0.7053\n",
            "Epoch 3890/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8112\n",
            "Epoch 3890: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3739 - accuracy: 0.8095 - val_loss: 0.6579 - val_accuracy: 0.6052\n",
            "Epoch 3891/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3520 - accuracy: 0.8188\n",
            "Epoch 3891: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3523 - accuracy: 0.8196 - val_loss: 0.6665 - val_accuracy: 0.5774\n",
            "Epoch 3892/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3433 - accuracy: 0.8259\n",
            "Epoch 3892: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3439 - accuracy: 0.8258 - val_loss: 0.6993 - val_accuracy: 0.5769\n",
            "Epoch 3893/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3700 - accuracy: 0.8086\n",
            "Epoch 3893: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3668 - accuracy: 0.8104 - val_loss: 0.6548 - val_accuracy: 0.5820\n",
            "Epoch 3894/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3701 - accuracy: 0.8113\n",
            "Epoch 3894: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3726 - accuracy: 0.8103 - val_loss: 0.3544 - val_accuracy: 0.8003\n",
            "Epoch 3895/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3637 - accuracy: 0.8112\n",
            "Epoch 3895: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3603 - accuracy: 0.8142 - val_loss: 0.4188 - val_accuracy: 0.7584\n",
            "Epoch 3896/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3540 - accuracy: 0.8236\n",
            "Epoch 3896: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3510 - accuracy: 0.8256 - val_loss: 0.3980 - val_accuracy: 0.7746\n",
            "Epoch 3897/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8185\n",
            "Epoch 3897: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3527 - accuracy: 0.8191 - val_loss: 0.4532 - val_accuracy: 0.7332\n",
            "Epoch 3898/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3666 - accuracy: 0.8134\n",
            "Epoch 3898: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3673 - accuracy: 0.8131 - val_loss: 0.5054 - val_accuracy: 0.6915\n",
            "Epoch 3899/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3451 - accuracy: 0.8247\n",
            "Epoch 3899: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3485 - accuracy: 0.8231 - val_loss: 0.6210 - val_accuracy: 0.6079\n",
            "Epoch 3900/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3394 - accuracy: 0.8262\n",
            "Epoch 3900: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3419 - accuracy: 0.8256 - val_loss: 0.6575 - val_accuracy: 0.5927\n",
            "Epoch 3901/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3484 - accuracy: 0.8214\n",
            "Epoch 3901: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3474 - accuracy: 0.8224 - val_loss: 0.5871 - val_accuracy: 0.6322\n",
            "Epoch 3902/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3382 - accuracy: 0.8317\n",
            "Epoch 3902: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3371 - accuracy: 0.8325 - val_loss: 0.5502 - val_accuracy: 0.6561\n",
            "Epoch 3903/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8315\n",
            "Epoch 3903: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3377 - accuracy: 0.8296 - val_loss: 0.5861 - val_accuracy: 0.6287\n",
            "Epoch 3904/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3470 - accuracy: 0.8262\n",
            "Epoch 3904: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3454 - accuracy: 0.8265 - val_loss: 0.5131 - val_accuracy: 0.6754\n",
            "Epoch 3905/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8286\n",
            "Epoch 3905: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3411 - accuracy: 0.8273 - val_loss: 0.5242 - val_accuracy: 0.6752\n",
            "Epoch 3906/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3421 - accuracy: 0.8295\n",
            "Epoch 3906: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3479 - accuracy: 0.8258 - val_loss: 0.3751 - val_accuracy: 0.7830\n",
            "Epoch 3907/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8177\n",
            "Epoch 3907: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3563 - accuracy: 0.8177 - val_loss: 0.4028 - val_accuracy: 0.7654\n",
            "Epoch 3908/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3584 - accuracy: 0.8171\n",
            "Epoch 3908: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3546 - accuracy: 0.8202 - val_loss: 0.4812 - val_accuracy: 0.7097\n",
            "Epoch 3909/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3333 - accuracy: 0.8342\n",
            "Epoch 3909: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3333 - accuracy: 0.8345 - val_loss: 0.5166 - val_accuracy: 0.6847\n",
            "Epoch 3910/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3322 - accuracy: 0.8343\n",
            "Epoch 3910: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3324 - accuracy: 0.8340 - val_loss: 0.5546 - val_accuracy: 0.6548\n",
            "Epoch 3911/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3338 - accuracy: 0.8355\n",
            "Epoch 3911: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3350 - accuracy: 0.8343 - val_loss: 0.5165 - val_accuracy: 0.6864\n",
            "Epoch 3912/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3364 - accuracy: 0.8313\n",
            "Epoch 3912: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3350 - accuracy: 0.8324 - val_loss: 0.5073 - val_accuracy: 0.6871\n",
            "Epoch 3913/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3359 - accuracy: 0.8303\n",
            "Epoch 3913: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3355 - accuracy: 0.8314 - val_loss: 0.5080 - val_accuracy: 0.6873\n",
            "Epoch 3914/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3417 - accuracy: 0.8281\n",
            "Epoch 3914: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3402 - accuracy: 0.8291 - val_loss: 0.5056 - val_accuracy: 0.6978\n",
            "Epoch 3915/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3408 - accuracy: 0.8287\n",
            "Epoch 3915: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3421 - accuracy: 0.8280 - val_loss: 0.4005 - val_accuracy: 0.7648\n",
            "Epoch 3916/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3624 - accuracy: 0.8147\n",
            "Epoch 3916: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.3596 - accuracy: 0.8168 - val_loss: 0.5001 - val_accuracy: 0.6989\n",
            "Epoch 3917/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3414 - accuracy: 0.8278\n",
            "Epoch 3917: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3427 - accuracy: 0.8269 - val_loss: 0.8757 - val_accuracy: 0.4847\n",
            "Epoch 3918/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4169 - accuracy: 0.7853\n",
            "Epoch 3918: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.4119 - accuracy: 0.7879 - val_loss: 0.4438 - val_accuracy: 0.7329\n",
            "Epoch 3919/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8318\n",
            "Epoch 3919: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3412 - accuracy: 0.8288 - val_loss: 0.3815 - val_accuracy: 0.7740\n",
            "Epoch 3920/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3699 - accuracy: 0.8109\n",
            "Epoch 3920: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3665 - accuracy: 0.8136 - val_loss: 0.3905 - val_accuracy: 0.7711\n",
            "Epoch 3921/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3916 - accuracy: 0.7991\n",
            "Epoch 3921: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3902 - accuracy: 0.7994 - val_loss: 0.5028 - val_accuracy: 0.6954\n",
            "Epoch 3922/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3435 - accuracy: 0.8227\n",
            "Epoch 3922: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3445 - accuracy: 0.8232 - val_loss: 0.7192 - val_accuracy: 0.5539\n",
            "Epoch 3923/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8168\n",
            "Epoch 3923: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3557 - accuracy: 0.8182 - val_loss: 0.5782 - val_accuracy: 0.6555\n",
            "Epoch 3924/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3483 - accuracy: 0.8253\n",
            "Epoch 3924: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3464 - accuracy: 0.8271 - val_loss: 0.4999 - val_accuracy: 0.6937\n",
            "Epoch 3925/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3374 - accuracy: 0.8313\n",
            "Epoch 3925: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3380 - accuracy: 0.8309 - val_loss: 0.5352 - val_accuracy: 0.6711\n",
            "Epoch 3926/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3384 - accuracy: 0.8302\n",
            "Epoch 3926: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3364 - accuracy: 0.8326 - val_loss: 0.5300 - val_accuracy: 0.6713\n",
            "Epoch 3927/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3314 - accuracy: 0.8346\n",
            "Epoch 3927: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3323 - accuracy: 0.8335 - val_loss: 0.5363 - val_accuracy: 0.6700\n",
            "Epoch 3928/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3351 - accuracy: 0.8324\n",
            "Epoch 3928: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3373 - accuracy: 0.8308 - val_loss: 0.4216 - val_accuracy: 0.7566\n",
            "Epoch 3929/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3333 - accuracy: 0.8338\n",
            "Epoch 3929: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3337 - accuracy: 0.8337 - val_loss: 0.5432 - val_accuracy: 0.6557\n",
            "Epoch 3930/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8358\n",
            "Epoch 3930: loss did not improve from 0.32941\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3326 - accuracy: 0.8348 - val_loss: 0.4723 - val_accuracy: 0.7182\n",
            "Epoch 3931/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3277 - accuracy: 0.8373\n",
            "Epoch 3931: loss improved from 0.32941 to 0.32929, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 222ms/step - loss: 0.3293 - accuracy: 0.8356 - val_loss: 0.4462 - val_accuracy: 0.7417\n",
            "Epoch 3932/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8327\n",
            "Epoch 3932: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3332 - accuracy: 0.8330 - val_loss: 0.4436 - val_accuracy: 0.7402\n",
            "Epoch 3933/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3372 - accuracy: 0.8306\n",
            "Epoch 3933: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3386 - accuracy: 0.8301 - val_loss: 0.5317 - val_accuracy: 0.6757\n",
            "Epoch 3934/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3340 - accuracy: 0.8341\n",
            "Epoch 3934: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3343 - accuracy: 0.8340 - val_loss: 0.5831 - val_accuracy: 0.6335\n",
            "Epoch 3935/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3384 - accuracy: 0.8299\n",
            "Epoch 3935: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3384 - accuracy: 0.8294 - val_loss: 0.4432 - val_accuracy: 0.7369\n",
            "Epoch 3936/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8320\n",
            "Epoch 3936: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3352 - accuracy: 0.8320 - val_loss: 0.4648 - val_accuracy: 0.7257\n",
            "Epoch 3937/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3440 - accuracy: 0.8230\n",
            "Epoch 3937: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3455 - accuracy: 0.8224 - val_loss: 0.6913 - val_accuracy: 0.5629\n",
            "Epoch 3938/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8182\n",
            "Epoch 3938: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3530 - accuracy: 0.8193 - val_loss: 0.4336 - val_accuracy: 0.7430\n",
            "Epoch 3939/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8295\n",
            "Epoch 3939: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3385 - accuracy: 0.8286 - val_loss: 0.6692 - val_accuracy: 0.5776\n",
            "Epoch 3940/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8210\n",
            "Epoch 3940: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3515 - accuracy: 0.8207 - val_loss: 0.4958 - val_accuracy: 0.7040\n",
            "Epoch 3941/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8276\n",
            "Epoch 3941: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3452 - accuracy: 0.8242 - val_loss: 0.3862 - val_accuracy: 0.7678\n",
            "Epoch 3942/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3795 - accuracy: 0.8054\n",
            "Epoch 3942: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3751 - accuracy: 0.8076 - val_loss: 0.3841 - val_accuracy: 0.7768\n",
            "Epoch 3943/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4066 - accuracy: 0.7918\n",
            "Epoch 3943: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4076 - accuracy: 0.7925 - val_loss: 0.7175 - val_accuracy: 0.5596\n",
            "Epoch 3944/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8165\n",
            "Epoch 3944: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3578 - accuracy: 0.8149 - val_loss: 1.0176 - val_accuracy: 0.4488\n",
            "Epoch 3945/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4431 - accuracy: 0.7765\n",
            "Epoch 3945: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.4342 - accuracy: 0.7809 - val_loss: 0.7131 - val_accuracy: 0.5644\n",
            "Epoch 3946/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4237 - accuracy: 0.7842\n",
            "Epoch 3946: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4282 - accuracy: 0.7816 - val_loss: 0.4347 - val_accuracy: 0.7430\n",
            "Epoch 3947/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3617 - accuracy: 0.8191\n",
            "Epoch 3947: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3692 - accuracy: 0.8146 - val_loss: 0.3893 - val_accuracy: 0.7724\n",
            "Epoch 3948/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3668 - accuracy: 0.8127\n",
            "Epoch 3948: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3810 - accuracy: 0.8055 - val_loss: 0.3685 - val_accuracy: 0.7632\n",
            "Epoch 3949/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4550 - accuracy: 0.7708\n",
            "Epoch 3949: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4521 - accuracy: 0.7709 - val_loss: 0.2989 - val_accuracy: 0.8187\n",
            "Epoch 3950/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5071 - accuracy: 0.7470\n",
            "Epoch 3950: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4933 - accuracy: 0.7537 - val_loss: 0.3805 - val_accuracy: 0.7443\n",
            "Epoch 3951/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4849 - accuracy: 0.7653\n",
            "Epoch 3951: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.4762 - accuracy: 0.7676 - val_loss: 0.3021 - val_accuracy: 0.8065\n",
            "Epoch 3952/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5192 - accuracy: 0.7451\n",
            "Epoch 3952: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.5092 - accuracy: 0.7499 - val_loss: 0.3782 - val_accuracy: 0.7531\n",
            "Epoch 3953/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4614 - accuracy: 0.7725\n",
            "Epoch 3953: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4613 - accuracy: 0.7722 - val_loss: 0.3622 - val_accuracy: 0.7858\n",
            "Epoch 3954/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4049 - accuracy: 0.7952\n",
            "Epoch 3954: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4076 - accuracy: 0.7933 - val_loss: 0.4219 - val_accuracy: 0.7391\n",
            "Epoch 3955/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3849 - accuracy: 0.8068\n",
            "Epoch 3955: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3869 - accuracy: 0.8055 - val_loss: 0.3258 - val_accuracy: 0.8148\n",
            "Epoch 3956/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4022 - accuracy: 0.7924\n",
            "Epoch 3956: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.4010 - accuracy: 0.7928 - val_loss: 0.3880 - val_accuracy: 0.7672\n",
            "Epoch 3957/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4007 - accuracy: 0.7976\n",
            "Epoch 3957: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3976 - accuracy: 0.7986 - val_loss: 0.3375 - val_accuracy: 0.8080\n",
            "Epoch 3958/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3843 - accuracy: 0.8023\n",
            "Epoch 3958: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3823 - accuracy: 0.8027 - val_loss: 0.4432 - val_accuracy: 0.7404\n",
            "Epoch 3959/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3558 - accuracy: 0.8236\n",
            "Epoch 3959: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3550 - accuracy: 0.8237 - val_loss: 0.4308 - val_accuracy: 0.7397\n",
            "Epoch 3960/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3458 - accuracy: 0.8252\n",
            "Epoch 3960: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3456 - accuracy: 0.8258 - val_loss: 0.4281 - val_accuracy: 0.7470\n",
            "Epoch 3961/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8209\n",
            "Epoch 3961: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3496 - accuracy: 0.8224 - val_loss: 0.4321 - val_accuracy: 0.7481\n",
            "Epoch 3962/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8286\n",
            "Epoch 3962: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3426 - accuracy: 0.8287 - val_loss: 0.4850 - val_accuracy: 0.7073\n",
            "Epoch 3963/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3405 - accuracy: 0.8303\n",
            "Epoch 3963: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3385 - accuracy: 0.8319 - val_loss: 0.5744 - val_accuracy: 0.6252\n",
            "Epoch 3964/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3408 - accuracy: 0.8265\n",
            "Epoch 3964: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3382 - accuracy: 0.8300 - val_loss: 0.5424 - val_accuracy: 0.6596\n",
            "Epoch 3965/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3373 - accuracy: 0.8308\n",
            "Epoch 3965: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3374 - accuracy: 0.8308 - val_loss: 0.6261 - val_accuracy: 0.5962\n",
            "Epoch 3966/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3538 - accuracy: 0.8202\n",
            "Epoch 3966: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3532 - accuracy: 0.8200 - val_loss: 0.5319 - val_accuracy: 0.6658\n",
            "Epoch 3967/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3426 - accuracy: 0.8329\n",
            "Epoch 3967: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3421 - accuracy: 0.8318 - val_loss: 0.6363 - val_accuracy: 0.5901\n",
            "Epoch 3968/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8120\n",
            "Epoch 3968: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3623 - accuracy: 0.8142 - val_loss: 0.6076 - val_accuracy: 0.6098\n",
            "Epoch 3969/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3626 - accuracy: 0.8185\n",
            "Epoch 3969: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3645 - accuracy: 0.8155 - val_loss: 0.4856 - val_accuracy: 0.7079\n",
            "Epoch 3970/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3393 - accuracy: 0.8320\n",
            "Epoch 3970: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3423 - accuracy: 0.8299 - val_loss: 0.4764 - val_accuracy: 0.7178\n",
            "Epoch 3971/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3376 - accuracy: 0.8335\n",
            "Epoch 3971: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3393 - accuracy: 0.8308 - val_loss: 0.4324 - val_accuracy: 0.7446\n",
            "Epoch 3972/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3425 - accuracy: 0.8288\n",
            "Epoch 3972: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3456 - accuracy: 0.8261 - val_loss: 0.3734 - val_accuracy: 0.7858\n",
            "Epoch 3973/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3755 - accuracy: 0.8098\n",
            "Epoch 3973: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3719 - accuracy: 0.8114 - val_loss: 0.4079 - val_accuracy: 0.7606\n",
            "Epoch 3974/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8158\n",
            "Epoch 3974: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3591 - accuracy: 0.8165 - val_loss: 0.4026 - val_accuracy: 0.7762\n",
            "Epoch 3975/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3630 - accuracy: 0.8136\n",
            "Epoch 3975: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3611 - accuracy: 0.8157 - val_loss: 0.5165 - val_accuracy: 0.6860\n",
            "Epoch 3976/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3412 - accuracy: 0.8290\n",
            "Epoch 3976: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3418 - accuracy: 0.8291 - val_loss: 0.5623 - val_accuracy: 0.6454\n",
            "Epoch 3977/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3384 - accuracy: 0.8314\n",
            "Epoch 3977: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3389 - accuracy: 0.8311 - val_loss: 0.6664 - val_accuracy: 0.5725\n",
            "Epoch 3978/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3435 - accuracy: 0.8247\n",
            "Epoch 3978: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3440 - accuracy: 0.8242 - val_loss: 0.7000 - val_accuracy: 0.5596\n",
            "Epoch 3979/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.8202\n",
            "Epoch 3979: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3520 - accuracy: 0.8211 - val_loss: 0.6841 - val_accuracy: 0.5695\n",
            "Epoch 3980/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3685 - accuracy: 0.8079\n",
            "Epoch 3980: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3663 - accuracy: 0.8092 - val_loss: 0.5621 - val_accuracy: 0.6504\n",
            "Epoch 3981/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3483 - accuracy: 0.8255\n",
            "Epoch 3981: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3489 - accuracy: 0.8242 - val_loss: 0.4993 - val_accuracy: 0.6972\n",
            "Epoch 3982/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3354 - accuracy: 0.8344\n",
            "Epoch 3982: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3356 - accuracy: 0.8343 - val_loss: 0.5043 - val_accuracy: 0.6904\n",
            "Epoch 3983/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3351 - accuracy: 0.8331\n",
            "Epoch 3983: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3352 - accuracy: 0.8330 - val_loss: 0.4814 - val_accuracy: 0.7121\n",
            "Epoch 3984/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8329\n",
            "Epoch 3984: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3354 - accuracy: 0.8328 - val_loss: 0.5196 - val_accuracy: 0.6844\n",
            "Epoch 3985/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8276\n",
            "Epoch 3985: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3415 - accuracy: 0.8273 - val_loss: 0.4844 - val_accuracy: 0.7075\n",
            "Epoch 3986/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3339 - accuracy: 0.8358\n",
            "Epoch 3986: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3358 - accuracy: 0.8336 - val_loss: 0.4551 - val_accuracy: 0.7292\n",
            "Epoch 3987/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8359\n",
            "Epoch 3987: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3335 - accuracy: 0.8359 - val_loss: 0.5255 - val_accuracy: 0.6732\n",
            "Epoch 3988/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8350\n",
            "Epoch 3988: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3356 - accuracy: 0.8340 - val_loss: 0.4435 - val_accuracy: 0.7402\n",
            "Epoch 3989/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3306 - accuracy: 0.8367\n",
            "Epoch 3989: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3327 - accuracy: 0.8346 - val_loss: 0.4361 - val_accuracy: 0.7503\n",
            "Epoch 3990/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3354 - accuracy: 0.8320\n",
            "Epoch 3990: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3364 - accuracy: 0.8314 - val_loss: 0.3752 - val_accuracy: 0.7909\n",
            "Epoch 3991/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8158\n",
            "Epoch 3991: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3582 - accuracy: 0.8179 - val_loss: 0.3893 - val_accuracy: 0.7744\n",
            "Epoch 3992/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3796 - accuracy: 0.8023\n",
            "Epoch 3992: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3792 - accuracy: 0.8030 - val_loss: 0.4880 - val_accuracy: 0.7064\n",
            "Epoch 3993/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8253\n",
            "Epoch 3993: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3490 - accuracy: 0.8230 - val_loss: 0.7668 - val_accuracy: 0.5416\n",
            "Epoch 3994/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3646 - accuracy: 0.8149\n",
            "Epoch 3994: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3625 - accuracy: 0.8166 - val_loss: 0.7676 - val_accuracy: 0.5304\n",
            "Epoch 3995/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3811 - accuracy: 0.8036\n",
            "Epoch 3995: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3785 - accuracy: 0.8046 - val_loss: 0.5472 - val_accuracy: 0.6564\n",
            "Epoch 3996/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3453 - accuracy: 0.8261\n",
            "Epoch 3996: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3477 - accuracy: 0.8232 - val_loss: 0.4563 - val_accuracy: 0.7310\n",
            "Epoch 3997/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3364 - accuracy: 0.8352\n",
            "Epoch 3997: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3381 - accuracy: 0.8334 - val_loss: 0.4147 - val_accuracy: 0.7621\n",
            "Epoch 3998/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.8208\n",
            "Epoch 3998: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3548 - accuracy: 0.8213 - val_loss: 0.3937 - val_accuracy: 0.7792\n",
            "Epoch 3999/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3719 - accuracy: 0.8096\n",
            "Epoch 3999: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3683 - accuracy: 0.8121 - val_loss: 0.4318 - val_accuracy: 0.7542\n",
            "Epoch 4000/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3658 - accuracy: 0.8120\n",
            "Epoch 4000: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3661 - accuracy: 0.8124 - val_loss: 0.4961 - val_accuracy: 0.6991\n",
            "Epoch 4001/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3414 - accuracy: 0.8271\n",
            "Epoch 4001: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3442 - accuracy: 0.8251 - val_loss: 0.6827 - val_accuracy: 0.5618\n",
            "Epoch 4002/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3474 - accuracy: 0.8232\n",
            "Epoch 4002: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3458 - accuracy: 0.8247 - val_loss: 0.6958 - val_accuracy: 0.5616\n",
            "Epoch 4003/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3602 - accuracy: 0.8129\n",
            "Epoch 4003: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3586 - accuracy: 0.8146 - val_loss: 0.6122 - val_accuracy: 0.6043\n",
            "Epoch 4004/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3525 - accuracy: 0.8200\n",
            "Epoch 4004: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3502 - accuracy: 0.8216 - val_loss: 0.6021 - val_accuracy: 0.6116\n",
            "Epoch 4005/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3434 - accuracy: 0.8268\n",
            "Epoch 4005: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3448 - accuracy: 0.8253 - val_loss: 0.5032 - val_accuracy: 0.6932\n",
            "Epoch 4006/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3407 - accuracy: 0.8266\n",
            "Epoch 4006: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3412 - accuracy: 0.8272 - val_loss: 0.5092 - val_accuracy: 0.6915\n",
            "Epoch 4007/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8342\n",
            "Epoch 4007: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3397 - accuracy: 0.8320 - val_loss: 0.3641 - val_accuracy: 0.7983\n",
            "Epoch 4008/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3504 - accuracy: 0.8211\n",
            "Epoch 4008: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3503 - accuracy: 0.8204 - val_loss: 0.4464 - val_accuracy: 0.7417\n",
            "Epoch 4009/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3577 - accuracy: 0.8165\n",
            "Epoch 4009: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3571 - accuracy: 0.8175 - val_loss: 0.5530 - val_accuracy: 0.6434\n",
            "Epoch 4010/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8292\n",
            "Epoch 4010: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3377 - accuracy: 0.8286 - val_loss: 0.5517 - val_accuracy: 0.6467\n",
            "Epoch 4011/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3304 - accuracy: 0.8340\n",
            "Epoch 4011: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3317 - accuracy: 0.8339 - val_loss: 0.5309 - val_accuracy: 0.6728\n",
            "Epoch 4012/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8317\n",
            "Epoch 4012: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3354 - accuracy: 0.8324 - val_loss: 0.6787 - val_accuracy: 0.5703\n",
            "Epoch 4013/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3500 - accuracy: 0.8222\n",
            "Epoch 4013: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3475 - accuracy: 0.8240 - val_loss: 0.6411 - val_accuracy: 0.5857\n",
            "Epoch 4014/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8185\n",
            "Epoch 4014: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3522 - accuracy: 0.8195 - val_loss: 0.5476 - val_accuracy: 0.6614\n",
            "Epoch 4015/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3450 - accuracy: 0.8268\n",
            "Epoch 4015: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3464 - accuracy: 0.8247 - val_loss: 0.4756 - val_accuracy: 0.7123\n",
            "Epoch 4016/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3393 - accuracy: 0.8308\n",
            "Epoch 4016: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3415 - accuracy: 0.8293 - val_loss: 0.3998 - val_accuracy: 0.7716\n",
            "Epoch 4017/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8326\n",
            "Epoch 4017: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3401 - accuracy: 0.8307 - val_loss: 0.3674 - val_accuracy: 0.7988\n",
            "Epoch 4018/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8139\n",
            "Epoch 4018: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3586 - accuracy: 0.8164 - val_loss: 0.4091 - val_accuracy: 0.7680\n",
            "Epoch 4019/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3687 - accuracy: 0.8126\n",
            "Epoch 4019: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3668 - accuracy: 0.8138 - val_loss: 0.4729 - val_accuracy: 0.7231\n",
            "Epoch 4020/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8219\n",
            "Epoch 4020: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3508 - accuracy: 0.8220 - val_loss: 0.4618 - val_accuracy: 0.7244\n",
            "Epoch 4021/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8158\n",
            "Epoch 4021: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3640 - accuracy: 0.8125 - val_loss: 0.7443 - val_accuracy: 0.5411\n",
            "Epoch 4022/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3530 - accuracy: 0.8213\n",
            "Epoch 4022: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3535 - accuracy: 0.8204 - val_loss: 0.8278 - val_accuracy: 0.5179\n",
            "Epoch 4023/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3844 - accuracy: 0.8011\n",
            "Epoch 4023: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3786 - accuracy: 0.8042 - val_loss: 0.6437 - val_accuracy: 0.5848\n",
            "Epoch 4024/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3476 - accuracy: 0.8228\n",
            "Epoch 4024: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3458 - accuracy: 0.8240 - val_loss: 0.4590 - val_accuracy: 0.7233\n",
            "Epoch 4025/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8304\n",
            "Epoch 4025: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3375 - accuracy: 0.8295 - val_loss: 0.4836 - val_accuracy: 0.7062\n",
            "Epoch 4026/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3371 - accuracy: 0.8320\n",
            "Epoch 4026: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3367 - accuracy: 0.8324 - val_loss: 0.5204 - val_accuracy: 0.6847\n",
            "Epoch 4027/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8342\n",
            "Epoch 4027: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3336 - accuracy: 0.8338 - val_loss: 0.5688 - val_accuracy: 0.6401\n",
            "Epoch 4028/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3386 - accuracy: 0.8299\n",
            "Epoch 4028: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3405 - accuracy: 0.8285 - val_loss: 0.4670 - val_accuracy: 0.7246\n",
            "Epoch 4029/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8354\n",
            "Epoch 4029: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3334 - accuracy: 0.8354 - val_loss: 0.4432 - val_accuracy: 0.7371\n",
            "Epoch 4030/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8346\n",
            "Epoch 4030: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3349 - accuracy: 0.8330 - val_loss: 0.4594 - val_accuracy: 0.7285\n",
            "Epoch 4031/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3336 - accuracy: 0.8338\n",
            "Epoch 4031: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3333 - accuracy: 0.8332 - val_loss: 0.4470 - val_accuracy: 0.7437\n",
            "Epoch 4032/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3369 - accuracy: 0.8307\n",
            "Epoch 4032: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3362 - accuracy: 0.8320 - val_loss: 0.5609 - val_accuracy: 0.6449\n",
            "Epoch 4033/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3343 - accuracy: 0.8347\n",
            "Epoch 4033: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3345 - accuracy: 0.8345 - val_loss: 0.6553 - val_accuracy: 0.5822\n",
            "Epoch 4034/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3484 - accuracy: 0.8247\n",
            "Epoch 4034: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3460 - accuracy: 0.8269 - val_loss: 0.5914 - val_accuracy: 0.6254\n",
            "Epoch 4035/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3539 - accuracy: 0.8197\n",
            "Epoch 4035: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3559 - accuracy: 0.8174 - val_loss: 0.4089 - val_accuracy: 0.7694\n",
            "Epoch 4036/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3423 - accuracy: 0.8270\n",
            "Epoch 4036: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3428 - accuracy: 0.8268 - val_loss: 0.4030 - val_accuracy: 0.7687\n",
            "Epoch 4037/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8168\n",
            "Epoch 4037: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3552 - accuracy: 0.8183 - val_loss: 0.4999 - val_accuracy: 0.6998\n",
            "Epoch 4038/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3435 - accuracy: 0.8230\n",
            "Epoch 4038: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3448 - accuracy: 0.8232 - val_loss: 0.7080 - val_accuracy: 0.5607\n",
            "Epoch 4039/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3513 - accuracy: 0.8224\n",
            "Epoch 4039: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3505 - accuracy: 0.8233 - val_loss: 0.6821 - val_accuracy: 0.5712\n",
            "Epoch 4040/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8211\n",
            "Epoch 4040: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3537 - accuracy: 0.8215 - val_loss: 0.4606 - val_accuracy: 0.7303\n",
            "Epoch 4041/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3311 - accuracy: 0.8366\n",
            "Epoch 4041: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3316 - accuracy: 0.8362 - val_loss: 0.4628 - val_accuracy: 0.7261\n",
            "Epoch 4042/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8353\n",
            "Epoch 4042: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3328 - accuracy: 0.8337 - val_loss: 0.4234 - val_accuracy: 0.7496\n",
            "Epoch 4043/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8225\n",
            "Epoch 4043: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3491 - accuracy: 0.8226 - val_loss: 0.4415 - val_accuracy: 0.7382\n",
            "Epoch 4044/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3366 - accuracy: 0.8290\n",
            "Epoch 4044: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3381 - accuracy: 0.8285 - val_loss: 0.6580 - val_accuracy: 0.5831\n",
            "Epoch 4045/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3455 - accuracy: 0.8253\n",
            "Epoch 4045: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3429 - accuracy: 0.8267 - val_loss: 0.6811 - val_accuracy: 0.5760\n",
            "Epoch 4046/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3635 - accuracy: 0.8125\n",
            "Epoch 4046: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3632 - accuracy: 0.8129 - val_loss: 0.4743 - val_accuracy: 0.7167\n",
            "Epoch 4047/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3322 - accuracy: 0.8368\n",
            "Epoch 4047: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3314 - accuracy: 0.8367 - val_loss: 0.4802 - val_accuracy: 0.7053\n",
            "Epoch 4048/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8349\n",
            "Epoch 4048: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3304 - accuracy: 0.8347 - val_loss: 0.4466 - val_accuracy: 0.7411\n",
            "Epoch 4049/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8336\n",
            "Epoch 4049: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3342 - accuracy: 0.8330 - val_loss: 0.4800 - val_accuracy: 0.7123\n",
            "Epoch 4050/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3403 - accuracy: 0.8295\n",
            "Epoch 4050: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3401 - accuracy: 0.8293 - val_loss: 0.6371 - val_accuracy: 0.5846\n",
            "Epoch 4051/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3416 - accuracy: 0.8234\n",
            "Epoch 4051: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3397 - accuracy: 0.8250 - val_loss: 0.5796 - val_accuracy: 0.6289\n",
            "Epoch 4052/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3403 - accuracy: 0.8275\n",
            "Epoch 4052: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3396 - accuracy: 0.8274 - val_loss: 0.4862 - val_accuracy: 0.7073\n",
            "Epoch 4053/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8360\n",
            "Epoch 4053: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3328 - accuracy: 0.8345 - val_loss: 0.4431 - val_accuracy: 0.7448\n",
            "Epoch 4054/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3358 - accuracy: 0.8327\n",
            "Epoch 4054: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3363 - accuracy: 0.8316 - val_loss: 0.4678 - val_accuracy: 0.7211\n",
            "Epoch 4055/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3487 - accuracy: 0.8210\n",
            "Epoch 4055: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3503 - accuracy: 0.8204 - val_loss: 0.5898 - val_accuracy: 0.6452\n",
            "Epoch 4056/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8303\n",
            "Epoch 4056: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3385 - accuracy: 0.8301 - val_loss: 0.6660 - val_accuracy: 0.5864\n",
            "Epoch 4057/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3488 - accuracy: 0.8236\n",
            "Epoch 4057: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3475 - accuracy: 0.8243 - val_loss: 0.5419 - val_accuracy: 0.6675\n",
            "Epoch 4058/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3475 - accuracy: 0.8258\n",
            "Epoch 4058: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3483 - accuracy: 0.8247 - val_loss: 0.4691 - val_accuracy: 0.7193\n",
            "Epoch 4059/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3440 - accuracy: 0.8282\n",
            "Epoch 4059: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3447 - accuracy: 0.8269 - val_loss: 0.4225 - val_accuracy: 0.7485\n",
            "Epoch 4060/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3548 - accuracy: 0.8195\n",
            "Epoch 4060: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3517 - accuracy: 0.8216 - val_loss: 0.4560 - val_accuracy: 0.7250\n",
            "Epoch 4061/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3563 - accuracy: 0.8185\n",
            "Epoch 4061: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3563 - accuracy: 0.8184 - val_loss: 0.5912 - val_accuracy: 0.6327\n",
            "Epoch 4062/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3428 - accuracy: 0.8283\n",
            "Epoch 4062: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3407 - accuracy: 0.8298 - val_loss: 0.5907 - val_accuracy: 0.6261\n",
            "Epoch 4063/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3398 - accuracy: 0.8293\n",
            "Epoch 4063: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3398 - accuracy: 0.8289 - val_loss: 0.4870 - val_accuracy: 0.7016\n",
            "Epoch 4064/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8316\n",
            "Epoch 4064: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3343 - accuracy: 0.8313 - val_loss: 0.4118 - val_accuracy: 0.7637\n",
            "Epoch 4065/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3489 - accuracy: 0.8220\n",
            "Epoch 4065: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3474 - accuracy: 0.8228 - val_loss: 0.5773 - val_accuracy: 0.6366\n",
            "Epoch 4066/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3310 - accuracy: 0.8355\n",
            "Epoch 4066: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3315 - accuracy: 0.8356 - val_loss: 0.6159 - val_accuracy: 0.6035\n",
            "Epoch 4067/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3423 - accuracy: 0.8253\n",
            "Epoch 4067: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3411 - accuracy: 0.8273 - val_loss: 0.5376 - val_accuracy: 0.6680\n",
            "Epoch 4068/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3330 - accuracy: 0.8337\n",
            "Epoch 4068: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3325 - accuracy: 0.8341 - val_loss: 0.4573 - val_accuracy: 0.7255\n",
            "Epoch 4069/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8348\n",
            "Epoch 4069: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3320 - accuracy: 0.8352 - val_loss: 0.4370 - val_accuracy: 0.7463\n",
            "Epoch 4070/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3444 - accuracy: 0.8266\n",
            "Epoch 4070: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3443 - accuracy: 0.8273 - val_loss: 0.6771 - val_accuracy: 0.5721\n",
            "Epoch 4071/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8195\n",
            "Epoch 4071: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3512 - accuracy: 0.8207 - val_loss: 0.5218 - val_accuracy: 0.6869\n",
            "Epoch 4072/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8282\n",
            "Epoch 4072: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3449 - accuracy: 0.8269 - val_loss: 0.4221 - val_accuracy: 0.7547\n",
            "Epoch 4073/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3371 - accuracy: 0.8320\n",
            "Epoch 4073: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3381 - accuracy: 0.8312 - val_loss: 0.3918 - val_accuracy: 0.7744\n",
            "Epoch 4074/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3593 - accuracy: 0.8149\n",
            "Epoch 4074: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3570 - accuracy: 0.8164 - val_loss: 0.5307 - val_accuracy: 0.6779\n",
            "Epoch 4075/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8319\n",
            "Epoch 4075: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3331 - accuracy: 0.8335 - val_loss: 0.6148 - val_accuracy: 0.6103\n",
            "Epoch 4076/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3387 - accuracy: 0.8274\n",
            "Epoch 4076: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3381 - accuracy: 0.8280 - val_loss: 0.5007 - val_accuracy: 0.6983\n",
            "Epoch 4077/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3332 - accuracy: 0.8324\n",
            "Epoch 4077: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3315 - accuracy: 0.8340 - val_loss: 0.4754 - val_accuracy: 0.7191\n",
            "Epoch 4078/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3329 - accuracy: 0.8334\n",
            "Epoch 4078: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3334 - accuracy: 0.8331 - val_loss: 0.4221 - val_accuracy: 0.7566\n",
            "Epoch 4079/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3366 - accuracy: 0.8326\n",
            "Epoch 4079: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3362 - accuracy: 0.8323 - val_loss: 0.4834 - val_accuracy: 0.7053\n",
            "Epoch 4080/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8365\n",
            "Epoch 4080: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3327 - accuracy: 0.8374 - val_loss: 0.5248 - val_accuracy: 0.6785\n",
            "Epoch 4081/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8348\n",
            "Epoch 4081: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3326 - accuracy: 0.8352 - val_loss: 0.4866 - val_accuracy: 0.7084\n",
            "Epoch 4082/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8357\n",
            "Epoch 4082: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3316 - accuracy: 0.8346 - val_loss: 0.7461 - val_accuracy: 0.5346\n",
            "Epoch 4083/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8037\n",
            "Epoch 4083: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3877 - accuracy: 0.8019 - val_loss: 0.3375 - val_accuracy: 0.8086\n",
            "Epoch 4084/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3869 - accuracy: 0.7991\n",
            "Epoch 4084: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3819 - accuracy: 0.8023 - val_loss: 0.4753 - val_accuracy: 0.7149\n",
            "Epoch 4085/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3522 - accuracy: 0.8216\n",
            "Epoch 4085: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3569 - accuracy: 0.8182 - val_loss: 0.9934 - val_accuracy: 0.4505\n",
            "Epoch 4086/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4413 - accuracy: 0.7771\n",
            "Epoch 4086: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4353 - accuracy: 0.7797 - val_loss: 0.4841 - val_accuracy: 0.7132\n",
            "Epoch 4087/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3436 - accuracy: 0.8303\n",
            "Epoch 4087: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3473 - accuracy: 0.8273 - val_loss: 0.4121 - val_accuracy: 0.7641\n",
            "Epoch 4088/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8268\n",
            "Epoch 4088: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3417 - accuracy: 0.8278 - val_loss: 0.4537 - val_accuracy: 0.7351\n",
            "Epoch 4089/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3384 - accuracy: 0.8310\n",
            "Epoch 4089: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3381 - accuracy: 0.8313 - val_loss: 0.4601 - val_accuracy: 0.7275\n",
            "Epoch 4090/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3413 - accuracy: 0.8265\n",
            "Epoch 4090: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3421 - accuracy: 0.8278 - val_loss: 0.6195 - val_accuracy: 0.6098\n",
            "Epoch 4091/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3387 - accuracy: 0.8277\n",
            "Epoch 4091: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3392 - accuracy: 0.8285 - val_loss: 0.6899 - val_accuracy: 0.5725\n",
            "Epoch 4092/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3614 - accuracy: 0.8143\n",
            "Epoch 4092: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3590 - accuracy: 0.8150 - val_loss: 0.5511 - val_accuracy: 0.6515\n",
            "Epoch 4093/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8208\n",
            "Epoch 4093: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3569 - accuracy: 0.8166 - val_loss: 0.3450 - val_accuracy: 0.7957\n",
            "Epoch 4094/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3919 - accuracy: 0.7996\n",
            "Epoch 4094: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3861 - accuracy: 0.8036 - val_loss: 0.3831 - val_accuracy: 0.7860\n",
            "Epoch 4095/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3971 - accuracy: 0.7963\n",
            "Epoch 4095: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3950 - accuracy: 0.7975 - val_loss: 0.4800 - val_accuracy: 0.7077\n",
            "Epoch 4096/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8202\n",
            "Epoch 4096: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3631 - accuracy: 0.8181 - val_loss: 0.7344 - val_accuracy: 0.5438\n",
            "Epoch 4097/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8215\n",
            "Epoch 4097: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3529 - accuracy: 0.8201 - val_loss: 0.8634 - val_accuracy: 0.4970\n",
            "Epoch 4098/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3834 - accuracy: 0.8011\n",
            "Epoch 4098: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3777 - accuracy: 0.8053 - val_loss: 0.6549 - val_accuracy: 0.5833\n",
            "Epoch 4099/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8168\n",
            "Epoch 4099: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3544 - accuracy: 0.8175 - val_loss: 0.4435 - val_accuracy: 0.7369\n",
            "Epoch 4100/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3379 - accuracy: 0.8312\n",
            "Epoch 4100: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3384 - accuracy: 0.8301 - val_loss: 0.3888 - val_accuracy: 0.7753\n",
            "Epoch 4101/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3676 - accuracy: 0.8120\n",
            "Epoch 4101: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3627 - accuracy: 0.8156 - val_loss: 0.4042 - val_accuracy: 0.7711\n",
            "Epoch 4102/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3639 - accuracy: 0.8110\n",
            "Epoch 4102: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3642 - accuracy: 0.8115 - val_loss: 0.6806 - val_accuracy: 0.5666\n",
            "Epoch 4103/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3446 - accuracy: 0.8244\n",
            "Epoch 4103: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3455 - accuracy: 0.8240 - val_loss: 0.8010 - val_accuracy: 0.5357\n",
            "Epoch 4104/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3794 - accuracy: 0.8074\n",
            "Epoch 4104: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3739 - accuracy: 0.8109 - val_loss: 0.6327 - val_accuracy: 0.5943\n",
            "Epoch 4105/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3554 - accuracy: 0.8177\n",
            "Epoch 4105: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3552 - accuracy: 0.8176 - val_loss: 0.4170 - val_accuracy: 0.7591\n",
            "Epoch 4106/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3403 - accuracy: 0.8295\n",
            "Epoch 4106: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3410 - accuracy: 0.8296 - val_loss: 0.3913 - val_accuracy: 0.7759\n",
            "Epoch 4107/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3587 - accuracy: 0.8155\n",
            "Epoch 4107: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3554 - accuracy: 0.8173 - val_loss: 0.4375 - val_accuracy: 0.7430\n",
            "Epoch 4108/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3675 - accuracy: 0.8106\n",
            "Epoch 4108: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3705 - accuracy: 0.8086 - val_loss: 0.8094 - val_accuracy: 0.5155\n",
            "Epoch 4109/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3634 - accuracy: 0.8134\n",
            "Epoch 4109: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.3632 - accuracy: 0.8143 - val_loss: 0.9512 - val_accuracy: 0.4751\n",
            "Epoch 4110/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4190 - accuracy: 0.7848\n",
            "Epoch 4110: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.4111 - accuracy: 0.7880 - val_loss: 0.6600 - val_accuracy: 0.5853\n",
            "Epoch 4111/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3708 - accuracy: 0.8115\n",
            "Epoch 4111: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3704 - accuracy: 0.8115 - val_loss: 0.4970 - val_accuracy: 0.6948\n",
            "Epoch 4112/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8255\n",
            "Epoch 4112: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3567 - accuracy: 0.8218 - val_loss: 0.3502 - val_accuracy: 0.8091\n",
            "Epoch 4113/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3619 - accuracy: 0.8141\n",
            "Epoch 4113: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3601 - accuracy: 0.8149 - val_loss: 0.4403 - val_accuracy: 0.7391\n",
            "Epoch 4114/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8225\n",
            "Epoch 4114: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3532 - accuracy: 0.8229 - val_loss: 0.3598 - val_accuracy: 0.8016\n",
            "Epoch 4115/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3678 - accuracy: 0.8112\n",
            "Epoch 4115: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3652 - accuracy: 0.8139 - val_loss: 0.4815 - val_accuracy: 0.7062\n",
            "Epoch 4116/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3453 - accuracy: 0.8263\n",
            "Epoch 4116: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3455 - accuracy: 0.8271 - val_loss: 0.4638 - val_accuracy: 0.7294\n",
            "Epoch 4117/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3358 - accuracy: 0.8320\n",
            "Epoch 4117: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3353 - accuracy: 0.8329 - val_loss: 0.4950 - val_accuracy: 0.7029\n",
            "Epoch 4118/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3410 - accuracy: 0.8295\n",
            "Epoch 4118: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3398 - accuracy: 0.8308 - val_loss: 0.4198 - val_accuracy: 0.7612\n",
            "Epoch 4119/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3445 - accuracy: 0.8263\n",
            "Epoch 4119: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3437 - accuracy: 0.8258 - val_loss: 0.5075 - val_accuracy: 0.6926\n",
            "Epoch 4120/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8338\n",
            "Epoch 4120: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3351 - accuracy: 0.8329 - val_loss: 0.5881 - val_accuracy: 0.6180\n",
            "Epoch 4121/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3424 - accuracy: 0.8282\n",
            "Epoch 4121: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3427 - accuracy: 0.8278 - val_loss: 0.4979 - val_accuracy: 0.6952\n",
            "Epoch 4122/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3304 - accuracy: 0.8352\n",
            "Epoch 4122: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3309 - accuracy: 0.8337 - val_loss: 0.4547 - val_accuracy: 0.7369\n",
            "Epoch 4123/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8325\n",
            "Epoch 4123: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3360 - accuracy: 0.8322 - val_loss: 0.6148 - val_accuracy: 0.6129\n",
            "Epoch 4124/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3477 - accuracy: 0.8275\n",
            "Epoch 4124: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3472 - accuracy: 0.8271 - val_loss: 0.5089 - val_accuracy: 0.6871\n",
            "Epoch 4125/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8320\n",
            "Epoch 4125: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3420 - accuracy: 0.8289 - val_loss: 0.4219 - val_accuracy: 0.7549\n",
            "Epoch 4126/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3430 - accuracy: 0.8296\n",
            "Epoch 4126: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3443 - accuracy: 0.8277 - val_loss: 0.3982 - val_accuracy: 0.7753\n",
            "Epoch 4127/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3461 - accuracy: 0.8223\n",
            "Epoch 4127: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3446 - accuracy: 0.8235 - val_loss: 0.5125 - val_accuracy: 0.6880\n",
            "Epoch 4128/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3408 - accuracy: 0.8273\n",
            "Epoch 4128: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3385 - accuracy: 0.8289 - val_loss: 0.4696 - val_accuracy: 0.7169\n",
            "Epoch 4129/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3399 - accuracy: 0.8252\n",
            "Epoch 4129: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3423 - accuracy: 0.8248 - val_loss: 0.9226 - val_accuracy: 0.4742\n",
            "Epoch 4130/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3973 - accuracy: 0.7966\n",
            "Epoch 4130: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3905 - accuracy: 0.8001 - val_loss: 0.5502 - val_accuracy: 0.6616\n",
            "Epoch 4131/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8325\n",
            "Epoch 4131: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3371 - accuracy: 0.8314 - val_loss: 0.4910 - val_accuracy: 0.7044\n",
            "Epoch 4132/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3323 - accuracy: 0.8356\n",
            "Epoch 4132: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3324 - accuracy: 0.8354 - val_loss: 0.4592 - val_accuracy: 0.7270\n",
            "Epoch 4133/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8352\n",
            "Epoch 4133: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3308 - accuracy: 0.8352 - val_loss: 0.6802 - val_accuracy: 0.5684\n",
            "Epoch 4134/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8170\n",
            "Epoch 4134: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3569 - accuracy: 0.8166 - val_loss: 0.4529 - val_accuracy: 0.7345\n",
            "Epoch 4135/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8348\n",
            "Epoch 4135: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3352 - accuracy: 0.8328 - val_loss: 0.3950 - val_accuracy: 0.7751\n",
            "Epoch 4136/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8195\n",
            "Epoch 4136: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3488 - accuracy: 0.8208 - val_loss: 0.5453 - val_accuracy: 0.6581\n",
            "Epoch 4137/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3305 - accuracy: 0.8341\n",
            "Epoch 4137: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3314 - accuracy: 0.8336 - val_loss: 0.4867 - val_accuracy: 0.7059\n",
            "Epoch 4138/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8368\n",
            "Epoch 4138: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3303 - accuracy: 0.8359 - val_loss: 0.5477 - val_accuracy: 0.6561\n",
            "Epoch 4139/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8320\n",
            "Epoch 4139: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3379 - accuracy: 0.8299 - val_loss: 0.3900 - val_accuracy: 0.7759\n",
            "Epoch 4140/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3515 - accuracy: 0.8208\n",
            "Epoch 4140: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3499 - accuracy: 0.8222 - val_loss: 0.5042 - val_accuracy: 0.6869\n",
            "Epoch 4141/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3316 - accuracy: 0.8332\n",
            "Epoch 4141: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3334 - accuracy: 0.8329 - val_loss: 0.6248 - val_accuracy: 0.6002\n",
            "Epoch 4142/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3488 - accuracy: 0.8208\n",
            "Epoch 4142: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3466 - accuracy: 0.8223 - val_loss: 0.4418 - val_accuracy: 0.7400\n",
            "Epoch 4143/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8350\n",
            "Epoch 4143: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3342 - accuracy: 0.8342 - val_loss: 0.4103 - val_accuracy: 0.7623\n",
            "Epoch 4144/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3578 - accuracy: 0.8159\n",
            "Epoch 4144: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3571 - accuracy: 0.8172 - val_loss: 0.4382 - val_accuracy: 0.7459\n",
            "Epoch 4145/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3446 - accuracy: 0.8237\n",
            "Epoch 4145: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3446 - accuracy: 0.8238 - val_loss: 0.5316 - val_accuracy: 0.6682\n",
            "Epoch 4146/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3448 - accuracy: 0.8227\n",
            "Epoch 4146: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3490 - accuracy: 0.8212 - val_loss: 0.8082 - val_accuracy: 0.5275\n",
            "Epoch 4147/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3872 - accuracy: 0.8022\n",
            "Epoch 4147: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3824 - accuracy: 0.8043 - val_loss: 0.7650 - val_accuracy: 0.5319\n",
            "Epoch 4148/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4114 - accuracy: 0.7893\n",
            "Epoch 4148: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4130 - accuracy: 0.7889 - val_loss: 0.4415 - val_accuracy: 0.7450\n",
            "Epoch 4149/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8228\n",
            "Epoch 4149: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3605 - accuracy: 0.8174 - val_loss: 0.3440 - val_accuracy: 0.8007\n",
            "Epoch 4150/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3899 - accuracy: 0.7975\n",
            "Epoch 4150: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3856 - accuracy: 0.8007 - val_loss: 0.4277 - val_accuracy: 0.7531\n",
            "Epoch 4151/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3689 - accuracy: 0.8126\n",
            "Epoch 4151: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3658 - accuracy: 0.8140 - val_loss: 0.5216 - val_accuracy: 0.6923\n",
            "Epoch 4152/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3398 - accuracy: 0.8310\n",
            "Epoch 4152: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3425 - accuracy: 0.8291 - val_loss: 0.6418 - val_accuracy: 0.5927\n",
            "Epoch 4153/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3418 - accuracy: 0.8258\n",
            "Epoch 4153: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3417 - accuracy: 0.8258 - val_loss: 0.5137 - val_accuracy: 0.6851\n",
            "Epoch 4154/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8360\n",
            "Epoch 4154: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3344 - accuracy: 0.8339 - val_loss: 0.4504 - val_accuracy: 0.7391\n",
            "Epoch 4155/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3339 - accuracy: 0.8343\n",
            "Epoch 4155: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3343 - accuracy: 0.8342 - val_loss: 0.4241 - val_accuracy: 0.7586\n",
            "Epoch 4156/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8174\n",
            "Epoch 4156: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3553 - accuracy: 0.8185 - val_loss: 0.5565 - val_accuracy: 0.6594\n",
            "Epoch 4157/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8315\n",
            "Epoch 4157: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3346 - accuracy: 0.8325 - val_loss: 0.7369 - val_accuracy: 0.5539\n",
            "Epoch 4158/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3762 - accuracy: 0.8083\n",
            "Epoch 4158: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3753 - accuracy: 0.8080 - val_loss: 0.4323 - val_accuracy: 0.7463\n",
            "Epoch 4159/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8324\n",
            "Epoch 4159: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3382 - accuracy: 0.8303 - val_loss: 0.4033 - val_accuracy: 0.7707\n",
            "Epoch 4160/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3499 - accuracy: 0.8239\n",
            "Epoch 4160: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3451 - accuracy: 0.8273 - val_loss: 0.4644 - val_accuracy: 0.7217\n",
            "Epoch 4161/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3324 - accuracy: 0.8334\n",
            "Epoch 4161: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3321 - accuracy: 0.8338 - val_loss: 0.4436 - val_accuracy: 0.7406\n",
            "Epoch 4162/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3434 - accuracy: 0.8276\n",
            "Epoch 4162: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3437 - accuracy: 0.8276 - val_loss: 0.6665 - val_accuracy: 0.5853\n",
            "Epoch 4163/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3486 - accuracy: 0.8262\n",
            "Epoch 4163: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3467 - accuracy: 0.8270 - val_loss: 0.5682 - val_accuracy: 0.6498\n",
            "Epoch 4164/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3432 - accuracy: 0.8258\n",
            "Epoch 4164: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3431 - accuracy: 0.8260 - val_loss: 0.5004 - val_accuracy: 0.7009\n",
            "Epoch 4165/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3354 - accuracy: 0.8341\n",
            "Epoch 4165: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3371 - accuracy: 0.8325 - val_loss: 0.4337 - val_accuracy: 0.7450\n",
            "Epoch 4166/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8314\n",
            "Epoch 4166: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3364 - accuracy: 0.8319 - val_loss: 0.4676 - val_accuracy: 0.7307\n",
            "Epoch 4167/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3309 - accuracy: 0.8353\n",
            "Epoch 4167: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3318 - accuracy: 0.8351 - val_loss: 0.4656 - val_accuracy: 0.7220\n",
            "Epoch 4168/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8308\n",
            "Epoch 4168: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3341 - accuracy: 0.8303 - val_loss: 0.5217 - val_accuracy: 0.6752\n",
            "Epoch 4169/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8304\n",
            "Epoch 4169: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3363 - accuracy: 0.8302 - val_loss: 0.7059 - val_accuracy: 0.5587\n",
            "Epoch 4170/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3500 - accuracy: 0.8188\n",
            "Epoch 4170: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3489 - accuracy: 0.8206 - val_loss: 0.5132 - val_accuracy: 0.6816\n",
            "Epoch 4171/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3349 - accuracy: 0.8330\n",
            "Epoch 4171: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3339 - accuracy: 0.8337 - val_loss: 0.5681 - val_accuracy: 0.6386\n",
            "Epoch 4172/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3364 - accuracy: 0.8315\n",
            "Epoch 4172: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3405 - accuracy: 0.8290 - val_loss: 0.3755 - val_accuracy: 0.7880\n",
            "Epoch 4173/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8126\n",
            "Epoch 4173: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3569 - accuracy: 0.8148 - val_loss: 0.4552 - val_accuracy: 0.7296\n",
            "Epoch 4174/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8226\n",
            "Epoch 4174: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3522 - accuracy: 0.8222 - val_loss: 0.6168 - val_accuracy: 0.6193\n",
            "Epoch 4175/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3418 - accuracy: 0.8291\n",
            "Epoch 4175: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3428 - accuracy: 0.8289 - val_loss: 0.7534 - val_accuracy: 0.5427\n",
            "Epoch 4176/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3629 - accuracy: 0.8155\n",
            "Epoch 4176: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3601 - accuracy: 0.8169 - val_loss: 0.4673 - val_accuracy: 0.7215\n",
            "Epoch 4177/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3339 - accuracy: 0.8348\n",
            "Epoch 4177: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3329 - accuracy: 0.8348 - val_loss: 0.4201 - val_accuracy: 0.7582\n",
            "Epoch 4178/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8312\n",
            "Epoch 4178: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3362 - accuracy: 0.8320 - val_loss: 0.5694 - val_accuracy: 0.6412\n",
            "Epoch 4179/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8331\n",
            "Epoch 4179: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.3354 - accuracy: 0.8322 - val_loss: 0.4268 - val_accuracy: 0.7551\n",
            "Epoch 4180/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3316 - accuracy: 0.8351\n",
            "Epoch 4180: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3326 - accuracy: 0.8340 - val_loss: 0.4734 - val_accuracy: 0.7174\n",
            "Epoch 4181/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3287 - accuracy: 0.8368\n",
            "Epoch 4181: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3310 - accuracy: 0.8348 - val_loss: 0.4602 - val_accuracy: 0.7285\n",
            "Epoch 4182/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3293 - accuracy: 0.8362\n",
            "Epoch 4182: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3310 - accuracy: 0.8350 - val_loss: 0.5248 - val_accuracy: 0.6787\n",
            "Epoch 4183/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3276 - accuracy: 0.8378\n",
            "Epoch 4183: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3304 - accuracy: 0.8351 - val_loss: 0.5805 - val_accuracy: 0.6364\n",
            "Epoch 4184/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3396 - accuracy: 0.8282\n",
            "Epoch 4184: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3401 - accuracy: 0.8292 - val_loss: 0.4788 - val_accuracy: 0.7165\n",
            "Epoch 4185/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8327\n",
            "Epoch 4185: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3365 - accuracy: 0.8297 - val_loss: 0.3676 - val_accuracy: 0.7834\n",
            "Epoch 4186/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3805 - accuracy: 0.8073\n",
            "Epoch 4186: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3782 - accuracy: 0.8077 - val_loss: 0.5294 - val_accuracy: 0.6649\n",
            "Epoch 4187/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8336\n",
            "Epoch 4187: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3332 - accuracy: 0.8341 - val_loss: 0.6320 - val_accuracy: 0.6002\n",
            "Epoch 4188/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8244\n",
            "Epoch 4188: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3446 - accuracy: 0.8240 - val_loss: 0.4794 - val_accuracy: 0.7191\n",
            "Epoch 4189/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3357 - accuracy: 0.8318\n",
            "Epoch 4189: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3361 - accuracy: 0.8324 - val_loss: 0.4818 - val_accuracy: 0.7073\n",
            "Epoch 4190/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3319 - accuracy: 0.8318\n",
            "Epoch 4190: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3359 - accuracy: 0.8305 - val_loss: 0.5988 - val_accuracy: 0.6223\n",
            "Epoch 4191/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8309\n",
            "Epoch 4191: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3348 - accuracy: 0.8308 - val_loss: 0.6689 - val_accuracy: 0.5846\n",
            "Epoch 4192/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8161\n",
            "Epoch 4192: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3590 - accuracy: 0.8182 - val_loss: 0.5236 - val_accuracy: 0.6794\n",
            "Epoch 4193/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3417 - accuracy: 0.8310\n",
            "Epoch 4193: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3476 - accuracy: 0.8274 - val_loss: 0.4264 - val_accuracy: 0.7586\n",
            "Epoch 4194/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3403 - accuracy: 0.8296\n",
            "Epoch 4194: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3395 - accuracy: 0.8305 - val_loss: 0.4416 - val_accuracy: 0.7441\n",
            "Epoch 4195/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3387 - accuracy: 0.8307\n",
            "Epoch 4195: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3363 - accuracy: 0.8328 - val_loss: 0.5347 - val_accuracy: 0.6704\n",
            "Epoch 4196/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8349\n",
            "Epoch 4196: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3308 - accuracy: 0.8358 - val_loss: 0.5194 - val_accuracy: 0.6858\n",
            "Epoch 4197/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3371 - accuracy: 0.8301\n",
            "Epoch 4197: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3373 - accuracy: 0.8303 - val_loss: 0.4195 - val_accuracy: 0.7601\n",
            "Epoch 4198/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3572 - accuracy: 0.8180\n",
            "Epoch 4198: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3558 - accuracy: 0.8194 - val_loss: 0.4967 - val_accuracy: 0.6978\n",
            "Epoch 4199/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8239\n",
            "Epoch 4199: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3501 - accuracy: 0.8205 - val_loss: 0.9770 - val_accuracy: 0.4661\n",
            "Epoch 4200/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4080 - accuracy: 0.7909\n",
            "Epoch 4200: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4039 - accuracy: 0.7927 - val_loss: 0.6930 - val_accuracy: 0.5598\n",
            "Epoch 4201/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3719 - accuracy: 0.8092\n",
            "Epoch 4201: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3724 - accuracy: 0.8088 - val_loss: 0.3906 - val_accuracy: 0.7790\n",
            "Epoch 4202/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8273\n",
            "Epoch 4202: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3421 - accuracy: 0.8281 - val_loss: 0.3968 - val_accuracy: 0.7700\n",
            "Epoch 4203/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8155\n",
            "Epoch 4203: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3571 - accuracy: 0.8178 - val_loss: 0.5619 - val_accuracy: 0.6430\n",
            "Epoch 4204/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3351 - accuracy: 0.8312\n",
            "Epoch 4204: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3341 - accuracy: 0.8319 - val_loss: 0.5740 - val_accuracy: 0.6526\n",
            "Epoch 4205/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3359 - accuracy: 0.8346\n",
            "Epoch 4205: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3349 - accuracy: 0.8343 - val_loss: 0.5307 - val_accuracy: 0.6680\n",
            "Epoch 4206/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3396 - accuracy: 0.8282\n",
            "Epoch 4206: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3400 - accuracy: 0.8285 - val_loss: 0.4664 - val_accuracy: 0.7255\n",
            "Epoch 4207/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8352\n",
            "Epoch 4207: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3372 - accuracy: 0.8339 - val_loss: 0.4227 - val_accuracy: 0.7525\n",
            "Epoch 4208/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3412 - accuracy: 0.8296\n",
            "Epoch 4208: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3423 - accuracy: 0.8281 - val_loss: 0.6273 - val_accuracy: 0.5971\n",
            "Epoch 4209/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3424 - accuracy: 0.8237\n",
            "Epoch 4209: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3429 - accuracy: 0.8238 - val_loss: 0.4267 - val_accuracy: 0.7468\n",
            "Epoch 4210/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3310 - accuracy: 0.8358\n",
            "Epoch 4210: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3321 - accuracy: 0.8340 - val_loss: 0.4630 - val_accuracy: 0.7250\n",
            "Epoch 4211/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8360\n",
            "Epoch 4211: loss did not improve from 0.32929\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3307 - accuracy: 0.8362 - val_loss: 0.4647 - val_accuracy: 0.7226\n",
            "Epoch 4212/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3284 - accuracy: 0.8337\n",
            "Epoch 4212: loss improved from 0.32929 to 0.32887, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 1s 224ms/step - loss: 0.3289 - accuracy: 0.8338 - val_loss: 0.5005 - val_accuracy: 0.6915\n",
            "Epoch 4213/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3300 - accuracy: 0.8346\n",
            "Epoch 4213: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3304 - accuracy: 0.8347 - val_loss: 0.5199 - val_accuracy: 0.6836\n",
            "Epoch 4214/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3303 - accuracy: 0.8346\n",
            "Epoch 4214: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3302 - accuracy: 0.8347 - val_loss: 0.6668 - val_accuracy: 0.5791\n",
            "Epoch 4215/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8223\n",
            "Epoch 4215: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3449 - accuracy: 0.8220 - val_loss: 0.4128 - val_accuracy: 0.7634\n",
            "Epoch 4216/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3339 - accuracy: 0.8310\n",
            "Epoch 4216: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3322 - accuracy: 0.8329 - val_loss: 0.4804 - val_accuracy: 0.7136\n",
            "Epoch 4217/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8299\n",
            "Epoch 4217: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3415 - accuracy: 0.8257 - val_loss: 0.7396 - val_accuracy: 0.5438\n",
            "Epoch 4218/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3702 - accuracy: 0.8078\n",
            "Epoch 4218: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3661 - accuracy: 0.8103 - val_loss: 0.6165 - val_accuracy: 0.6114\n",
            "Epoch 4219/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3764 - accuracy: 0.8086\n",
            "Epoch 4219: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3893 - accuracy: 0.8021 - val_loss: 0.3555 - val_accuracy: 0.7698\n",
            "Epoch 4220/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4405 - accuracy: 0.7781\n",
            "Epoch 4220: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.4290 - accuracy: 0.7840 - val_loss: 0.3601 - val_accuracy: 0.7964\n",
            "Epoch 4221/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4156 - accuracy: 0.7844\n",
            "Epoch 4221: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4178 - accuracy: 0.7835 - val_loss: 0.8347 - val_accuracy: 0.5181\n",
            "Epoch 4222/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3790 - accuracy: 0.8065\n",
            "Epoch 4222: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3799 - accuracy: 0.8067 - val_loss: 0.7853 - val_accuracy: 0.5673\n",
            "Epoch 4223/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3744 - accuracy: 0.8127\n",
            "Epoch 4223: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3713 - accuracy: 0.8147 - val_loss: 0.6677 - val_accuracy: 0.5793\n",
            "Epoch 4224/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3555 - accuracy: 0.8211\n",
            "Epoch 4224: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3553 - accuracy: 0.8204 - val_loss: 0.5443 - val_accuracy: 0.6700\n",
            "Epoch 4225/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8286\n",
            "Epoch 4225: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3484 - accuracy: 0.8271 - val_loss: 0.4480 - val_accuracy: 0.7393\n",
            "Epoch 4226/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3379 - accuracy: 0.8306\n",
            "Epoch 4226: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3408 - accuracy: 0.8287 - val_loss: 0.3710 - val_accuracy: 0.7823\n",
            "Epoch 4227/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3846 - accuracy: 0.8017\n",
            "Epoch 4227: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3798 - accuracy: 0.8049 - val_loss: 0.5045 - val_accuracy: 0.7009\n",
            "Epoch 4228/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3439 - accuracy: 0.8276\n",
            "Epoch 4228: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3453 - accuracy: 0.8266 - val_loss: 0.7685 - val_accuracy: 0.5374\n",
            "Epoch 4229/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3709 - accuracy: 0.8121\n",
            "Epoch 4229: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3655 - accuracy: 0.8154 - val_loss: 0.7822 - val_accuracy: 0.5455\n",
            "Epoch 4230/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4292 - accuracy: 0.7805\n",
            "Epoch 4230: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4296 - accuracy: 0.7796 - val_loss: 0.4813 - val_accuracy: 0.7143\n",
            "Epoch 4231/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3550 - accuracy: 0.8219\n",
            "Epoch 4231: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3639 - accuracy: 0.8166 - val_loss: 0.3530 - val_accuracy: 0.8071\n",
            "Epoch 4232/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3595 - accuracy: 0.8169\n",
            "Epoch 4232: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3644 - accuracy: 0.8139 - val_loss: 0.3614 - val_accuracy: 0.7713\n",
            "Epoch 4233/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4287 - accuracy: 0.7821\n",
            "Epoch 4233: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.4209 - accuracy: 0.7858 - val_loss: 0.4453 - val_accuracy: 0.7404\n",
            "Epoch 4234/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8188\n",
            "Epoch 4234: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3563 - accuracy: 0.8184 - val_loss: 0.5850 - val_accuracy: 0.6364\n",
            "Epoch 4235/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3436 - accuracy: 0.8273\n",
            "Epoch 4235: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3443 - accuracy: 0.8266 - val_loss: 0.5624 - val_accuracy: 0.6458\n",
            "Epoch 4236/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3406 - accuracy: 0.8268\n",
            "Epoch 4236: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3424 - accuracy: 0.8260 - val_loss: 0.6785 - val_accuracy: 0.5826\n",
            "Epoch 4237/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3515 - accuracy: 0.8206\n",
            "Epoch 4237: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3497 - accuracy: 0.8223 - val_loss: 0.6415 - val_accuracy: 0.5945\n",
            "Epoch 4238/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3492 - accuracy: 0.8220\n",
            "Epoch 4238: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3482 - accuracy: 0.8228 - val_loss: 0.5417 - val_accuracy: 0.6717\n",
            "Epoch 4239/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3416 - accuracy: 0.8276\n",
            "Epoch 4239: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3454 - accuracy: 0.8245 - val_loss: 0.3779 - val_accuracy: 0.7838\n",
            "Epoch 4240/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3672 - accuracy: 0.8118\n",
            "Epoch 4240: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3652 - accuracy: 0.8130 - val_loss: 0.3892 - val_accuracy: 0.7806\n",
            "Epoch 4241/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3752 - accuracy: 0.8065\n",
            "Epoch 4241: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3765 - accuracy: 0.8061 - val_loss: 0.6750 - val_accuracy: 0.5760\n",
            "Epoch 4242/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3459 - accuracy: 0.8225\n",
            "Epoch 4242: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3484 - accuracy: 0.8208 - val_loss: 0.7628 - val_accuracy: 0.5289\n",
            "Epoch 4243/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3684 - accuracy: 0.8113\n",
            "Epoch 4243: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3659 - accuracy: 0.8123 - val_loss: 0.5718 - val_accuracy: 0.6425\n",
            "Epoch 4244/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3444 - accuracy: 0.8243\n",
            "Epoch 4244: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3432 - accuracy: 0.8249 - val_loss: 0.5299 - val_accuracy: 0.6715\n",
            "Epoch 4245/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3305 - accuracy: 0.8361\n",
            "Epoch 4245: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3310 - accuracy: 0.8358 - val_loss: 0.4282 - val_accuracy: 0.7483\n",
            "Epoch 4246/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8298\n",
            "Epoch 4246: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3384 - accuracy: 0.8307 - val_loss: 0.5762 - val_accuracy: 0.6329\n",
            "Epoch 4247/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3332 - accuracy: 0.8324\n",
            "Epoch 4247: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3330 - accuracy: 0.8328 - val_loss: 0.5103 - val_accuracy: 0.6822\n",
            "Epoch 4248/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3357 - accuracy: 0.8301\n",
            "Epoch 4248: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 0.3356 - accuracy: 0.8299 - val_loss: 0.4658 - val_accuracy: 0.7233\n",
            "Epoch 4249/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8322\n",
            "Epoch 4249: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3366 - accuracy: 0.8322 - val_loss: 0.4124 - val_accuracy: 0.7560\n",
            "Epoch 4250/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3522 - accuracy: 0.8196\n",
            "Epoch 4250: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3519 - accuracy: 0.8198 - val_loss: 0.5359 - val_accuracy: 0.6649\n",
            "Epoch 4251/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8299\n",
            "Epoch 4251: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3358 - accuracy: 0.8305 - val_loss: 0.6110 - val_accuracy: 0.6188\n",
            "Epoch 4252/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3447 - accuracy: 0.8261\n",
            "Epoch 4252: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3443 - accuracy: 0.8275 - val_loss: 0.4967 - val_accuracy: 0.6976\n",
            "Epoch 4253/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8348\n",
            "Epoch 4253: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3306 - accuracy: 0.8353 - val_loss: 0.5028 - val_accuracy: 0.6877\n",
            "Epoch 4254/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3304 - accuracy: 0.8379\n",
            "Epoch 4254: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3308 - accuracy: 0.8373 - val_loss: 0.4561 - val_accuracy: 0.7325\n",
            "Epoch 4255/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3348 - accuracy: 0.8327\n",
            "Epoch 4255: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3334 - accuracy: 0.8333 - val_loss: 0.5603 - val_accuracy: 0.6493\n",
            "Epoch 4256/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3378 - accuracy: 0.8306\n",
            "Epoch 4256: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3375 - accuracy: 0.8301 - val_loss: 0.4754 - val_accuracy: 0.7248\n",
            "Epoch 4257/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8374\n",
            "Epoch 4257: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3297 - accuracy: 0.8375 - val_loss: 0.4192 - val_accuracy: 0.7637\n",
            "Epoch 4258/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3332 - accuracy: 0.8335\n",
            "Epoch 4258: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3345 - accuracy: 0.8324 - val_loss: 0.5265 - val_accuracy: 0.6737\n",
            "Epoch 4259/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3311 - accuracy: 0.8338\n",
            "Epoch 4259: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3328 - accuracy: 0.8331 - val_loss: 0.6056 - val_accuracy: 0.6098\n",
            "Epoch 4260/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3366 - accuracy: 0.8298\n",
            "Epoch 4260: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3359 - accuracy: 0.8305 - val_loss: 0.5381 - val_accuracy: 0.6649\n",
            "Epoch 4261/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8346\n",
            "Epoch 4261: loss did not improve from 0.32887\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3317 - accuracy: 0.8336 - val_loss: 0.4941 - val_accuracy: 0.6980\n",
            "Epoch 4262/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3256 - accuracy: 0.8385\n",
            "Epoch 4262: loss improved from 0.32887 to 0.32687, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.3269 - accuracy: 0.8372 - val_loss: 0.4634 - val_accuracy: 0.7325\n",
            "Epoch 4263/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3415 - accuracy: 0.8275\n",
            "Epoch 4263: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3419 - accuracy: 0.8275 - val_loss: 0.6099 - val_accuracy: 0.6085\n",
            "Epoch 4264/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3372 - accuracy: 0.8304\n",
            "Epoch 4264: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3358 - accuracy: 0.8315 - val_loss: 0.5121 - val_accuracy: 0.6822\n",
            "Epoch 4265/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8354\n",
            "Epoch 4265: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3319 - accuracy: 0.8336 - val_loss: 0.4080 - val_accuracy: 0.7650\n",
            "Epoch 4266/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8267\n",
            "Epoch 4266: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3420 - accuracy: 0.8284 - val_loss: 0.5160 - val_accuracy: 0.6822\n",
            "Epoch 4267/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3332 - accuracy: 0.8317\n",
            "Epoch 4267: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3320 - accuracy: 0.8333 - val_loss: 0.5877 - val_accuracy: 0.6252\n",
            "Epoch 4268/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8303\n",
            "Epoch 4268: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3329 - accuracy: 0.8306 - val_loss: 0.5978 - val_accuracy: 0.6180\n",
            "Epoch 4269/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3436 - accuracy: 0.8239\n",
            "Epoch 4269: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3457 - accuracy: 0.8236 - val_loss: 0.4348 - val_accuracy: 0.7397\n",
            "Epoch 4270/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3471 - accuracy: 0.8230\n",
            "Epoch 4270: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3474 - accuracy: 0.8229 - val_loss: 0.4934 - val_accuracy: 0.6950\n",
            "Epoch 4271/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8318\n",
            "Epoch 4271: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3360 - accuracy: 0.8308 - val_loss: 0.8056 - val_accuracy: 0.5148\n",
            "Epoch 4272/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3950 - accuracy: 0.7947\n",
            "Epoch 4272: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3955 - accuracy: 0.7941 - val_loss: 0.4424 - val_accuracy: 0.7402\n",
            "Epoch 4273/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3375 - accuracy: 0.8335\n",
            "Epoch 4273: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3387 - accuracy: 0.8328 - val_loss: 0.3944 - val_accuracy: 0.7705\n",
            "Epoch 4274/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8208\n",
            "Epoch 4274: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3518 - accuracy: 0.8215 - val_loss: 0.4640 - val_accuracy: 0.7275\n",
            "Epoch 4275/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8340\n",
            "Epoch 4275: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3323 - accuracy: 0.8330 - val_loss: 0.5028 - val_accuracy: 0.6919\n",
            "Epoch 4276/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3313 - accuracy: 0.8342\n",
            "Epoch 4276: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3319 - accuracy: 0.8339 - val_loss: 0.5113 - val_accuracy: 0.6779\n",
            "Epoch 4277/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8324\n",
            "Epoch 4277: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3321 - accuracy: 0.8319 - val_loss: 0.5596 - val_accuracy: 0.6487\n",
            "Epoch 4278/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3323 - accuracy: 0.8318\n",
            "Epoch 4278: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3314 - accuracy: 0.8321 - val_loss: 0.6013 - val_accuracy: 0.6217\n",
            "Epoch 4279/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3394 - accuracy: 0.8287\n",
            "Epoch 4279: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3402 - accuracy: 0.8274 - val_loss: 0.3927 - val_accuracy: 0.7781\n",
            "Epoch 4280/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3529 - accuracy: 0.8181\n",
            "Epoch 4280: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3507 - accuracy: 0.8200 - val_loss: 0.5933 - val_accuracy: 0.6232\n",
            "Epoch 4281/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3309 - accuracy: 0.8336\n",
            "Epoch 4281: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3319 - accuracy: 0.8329 - val_loss: 0.5236 - val_accuracy: 0.6737\n",
            "Epoch 4282/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3352 - accuracy: 0.8307\n",
            "Epoch 4282: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3354 - accuracy: 0.8299 - val_loss: 0.5100 - val_accuracy: 0.6908\n",
            "Epoch 4283/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3288 - accuracy: 0.8397\n",
            "Epoch 4283: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3322 - accuracy: 0.8363 - val_loss: 0.3645 - val_accuracy: 0.7977\n",
            "Epoch 4284/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8117\n",
            "Epoch 4284: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3598 - accuracy: 0.8138 - val_loss: 0.5778 - val_accuracy: 0.6410\n",
            "Epoch 4285/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3310 - accuracy: 0.8346\n",
            "Epoch 4285: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3324 - accuracy: 0.8336 - val_loss: 0.4401 - val_accuracy: 0.7400\n",
            "Epoch 4286/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8329\n",
            "Epoch 4286: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3340 - accuracy: 0.8320 - val_loss: 0.6790 - val_accuracy: 0.5785\n",
            "Epoch 4287/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8204\n",
            "Epoch 4287: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3487 - accuracy: 0.8228 - val_loss: 0.5554 - val_accuracy: 0.6533\n",
            "Epoch 4288/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3377 - accuracy: 0.8315\n",
            "Epoch 4288: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3409 - accuracy: 0.8297 - val_loss: 0.4169 - val_accuracy: 0.7577\n",
            "Epoch 4289/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3372 - accuracy: 0.8317\n",
            "Epoch 4289: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3368 - accuracy: 0.8318 - val_loss: 0.4150 - val_accuracy: 0.7634\n",
            "Epoch 4290/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3546 - accuracy: 0.8169\n",
            "Epoch 4290: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3575 - accuracy: 0.8163 - val_loss: 0.8473 - val_accuracy: 0.5100\n",
            "Epoch 4291/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3957 - accuracy: 0.7937\n",
            "Epoch 4291: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3890 - accuracy: 0.7969 - val_loss: 0.4547 - val_accuracy: 0.7292\n",
            "Epoch 4292/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8293\n",
            "Epoch 4292: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3368 - accuracy: 0.8303 - val_loss: 0.4547 - val_accuracy: 0.7369\n",
            "Epoch 4293/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3348 - accuracy: 0.8307\n",
            "Epoch 4293: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3368 - accuracy: 0.8300 - val_loss: 0.6062 - val_accuracy: 0.6199\n",
            "Epoch 4294/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8328\n",
            "Epoch 4294: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3350 - accuracy: 0.8324 - val_loss: 0.6028 - val_accuracy: 0.6153\n",
            "Epoch 4295/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3489 - accuracy: 0.8221\n",
            "Epoch 4295: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3517 - accuracy: 0.8196 - val_loss: 0.3423 - val_accuracy: 0.8163\n",
            "Epoch 4296/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3889 - accuracy: 0.7972\n",
            "Epoch 4296: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3868 - accuracy: 0.7993 - val_loss: 0.7241 - val_accuracy: 0.5526\n",
            "Epoch 4297/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8207\n",
            "Epoch 4297: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3524 - accuracy: 0.8219 - val_loss: 0.6549 - val_accuracy: 0.6085\n",
            "Epoch 4298/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3815 - accuracy: 0.8064\n",
            "Epoch 4298: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3846 - accuracy: 0.8045 - val_loss: 0.4436 - val_accuracy: 0.7397\n",
            "Epoch 4299/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8245\n",
            "Epoch 4299: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3552 - accuracy: 0.8217 - val_loss: 0.3872 - val_accuracy: 0.7803\n",
            "Epoch 4300/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8220\n",
            "Epoch 4300: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3504 - accuracy: 0.8210 - val_loss: 0.3760 - val_accuracy: 0.7792\n",
            "Epoch 4301/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3797 - accuracy: 0.8037\n",
            "Epoch 4301: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3766 - accuracy: 0.8067 - val_loss: 0.4809 - val_accuracy: 0.7136\n",
            "Epoch 4302/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3471 - accuracy: 0.8234\n",
            "Epoch 4302: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3508 - accuracy: 0.8217 - val_loss: 0.5873 - val_accuracy: 0.6316\n",
            "Epoch 4303/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3380 - accuracy: 0.8309\n",
            "Epoch 4303: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3396 - accuracy: 0.8293 - val_loss: 0.6678 - val_accuracy: 0.5809\n",
            "Epoch 4304/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3467 - accuracy: 0.8233\n",
            "Epoch 4304: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3449 - accuracy: 0.8243 - val_loss: 0.5315 - val_accuracy: 0.6715\n",
            "Epoch 4305/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8343\n",
            "Epoch 4305: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3345 - accuracy: 0.8330 - val_loss: 0.4584 - val_accuracy: 0.7250\n",
            "Epoch 4306/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3372 - accuracy: 0.8296\n",
            "Epoch 4306: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 111ms/step - loss: 0.3371 - accuracy: 0.8303 - val_loss: 0.5685 - val_accuracy: 0.6381\n",
            "Epoch 4307/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8330\n",
            "Epoch 4307: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3319 - accuracy: 0.8339 - val_loss: 0.5646 - val_accuracy: 0.6419\n",
            "Epoch 4308/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3380 - accuracy: 0.8267\n",
            "Epoch 4308: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3368 - accuracy: 0.8275 - val_loss: 0.4974 - val_accuracy: 0.6994\n",
            "Epoch 4309/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3314 - accuracy: 0.8363\n",
            "Epoch 4309: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3323 - accuracy: 0.8348 - val_loss: 0.3682 - val_accuracy: 0.7933\n",
            "Epoch 4310/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8133\n",
            "Epoch 4310: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3566 - accuracy: 0.8151 - val_loss: 0.6057 - val_accuracy: 0.6162\n",
            "Epoch 4311/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8312\n",
            "Epoch 4311: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3351 - accuracy: 0.8318 - val_loss: 0.5556 - val_accuracy: 0.6482\n",
            "Epoch 4312/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3400 - accuracy: 0.8292\n",
            "Epoch 4312: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3380 - accuracy: 0.8313 - val_loss: 0.4526 - val_accuracy: 0.7419\n",
            "Epoch 4313/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8334\n",
            "Epoch 4313: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3315 - accuracy: 0.8332 - val_loss: 0.4111 - val_accuracy: 0.7667\n",
            "Epoch 4314/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3349 - accuracy: 0.8298\n",
            "Epoch 4314: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3348 - accuracy: 0.8298 - val_loss: 0.5025 - val_accuracy: 0.6952\n",
            "Epoch 4315/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8344\n",
            "Epoch 4315: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3334 - accuracy: 0.8331 - val_loss: 0.4747 - val_accuracy: 0.7112\n",
            "Epoch 4316/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8309\n",
            "Epoch 4316: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3314 - accuracy: 0.8313 - val_loss: 0.6156 - val_accuracy: 0.6190\n",
            "Epoch 4317/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3352 - accuracy: 0.8315\n",
            "Epoch 4317: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3361 - accuracy: 0.8307 - val_loss: 0.4646 - val_accuracy: 0.7196\n",
            "Epoch 4318/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3290 - accuracy: 0.8362\n",
            "Epoch 4318: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3297 - accuracy: 0.8352 - val_loss: 0.4905 - val_accuracy: 0.7009\n",
            "Epoch 4319/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3288 - accuracy: 0.8361\n",
            "Epoch 4319: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3276 - accuracy: 0.8367 - val_loss: 0.5112 - val_accuracy: 0.6855\n",
            "Epoch 4320/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8344\n",
            "Epoch 4320: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3323 - accuracy: 0.8350 - val_loss: 0.4034 - val_accuracy: 0.7716\n",
            "Epoch 4321/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3482 - accuracy: 0.8214\n",
            "Epoch 4321: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3481 - accuracy: 0.8219 - val_loss: 0.5537 - val_accuracy: 0.6544\n",
            "Epoch 4322/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3288 - accuracy: 0.8346\n",
            "Epoch 4322: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3301 - accuracy: 0.8337 - val_loss: 0.5502 - val_accuracy: 0.6623\n",
            "Epoch 4323/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3293 - accuracy: 0.8375\n",
            "Epoch 4323: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3293 - accuracy: 0.8373 - val_loss: 0.5592 - val_accuracy: 0.6498\n",
            "Epoch 4324/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3338 - accuracy: 0.8337\n",
            "Epoch 4324: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3349 - accuracy: 0.8326 - val_loss: 0.3970 - val_accuracy: 0.7687\n",
            "Epoch 4325/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8162\n",
            "Epoch 4325: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3571 - accuracy: 0.8160 - val_loss: 0.5837 - val_accuracy: 0.6291\n",
            "Epoch 4326/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8282\n",
            "Epoch 4326: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3393 - accuracy: 0.8274 - val_loss: 0.8924 - val_accuracy: 0.4856\n",
            "Epoch 4327/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4052 - accuracy: 0.7912\n",
            "Epoch 4327: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4007 - accuracy: 0.7933 - val_loss: 0.4715 - val_accuracy: 0.7196\n",
            "Epoch 4328/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8325\n",
            "Epoch 4328: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3396 - accuracy: 0.8307 - val_loss: 0.4191 - val_accuracy: 0.7544\n",
            "Epoch 4329/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3448 - accuracy: 0.8279\n",
            "Epoch 4329: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3442 - accuracy: 0.8277 - val_loss: 0.4721 - val_accuracy: 0.7088\n",
            "Epoch 4330/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3290 - accuracy: 0.8342\n",
            "Epoch 4330: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3308 - accuracy: 0.8340 - val_loss: 0.4797 - val_accuracy: 0.7154\n",
            "Epoch 4331/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3292 - accuracy: 0.8377\n",
            "Epoch 4331: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3305 - accuracy: 0.8365 - val_loss: 0.5995 - val_accuracy: 0.6300\n",
            "Epoch 4332/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3448 - accuracy: 0.8266\n",
            "Epoch 4332: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3451 - accuracy: 0.8256 - val_loss: 0.4333 - val_accuracy: 0.7461\n",
            "Epoch 4333/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3356 - accuracy: 0.8312\n",
            "Epoch 4333: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3353 - accuracy: 0.8313 - val_loss: 0.5190 - val_accuracy: 0.6785\n",
            "Epoch 4334/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8346\n",
            "Epoch 4334: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3301 - accuracy: 0.8357 - val_loss: 0.4986 - val_accuracy: 0.6978\n",
            "Epoch 4335/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8365\n",
            "Epoch 4335: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3302 - accuracy: 0.8360 - val_loss: 0.7079 - val_accuracy: 0.5578\n",
            "Epoch 4336/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3617 - accuracy: 0.8139\n",
            "Epoch 4336: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3596 - accuracy: 0.8151 - val_loss: 0.5240 - val_accuracy: 0.6728\n",
            "Epoch 4337/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3420 - accuracy: 0.8291\n",
            "Epoch 4337: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3489 - accuracy: 0.8241 - val_loss: 0.3611 - val_accuracy: 0.7825\n",
            "Epoch 4338/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3922 - accuracy: 0.7964\n",
            "Epoch 4338: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3898 - accuracy: 0.7981 - val_loss: 0.4680 - val_accuracy: 0.7200\n",
            "Epoch 4339/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3468 - accuracy: 0.8213\n",
            "Epoch 4339: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3564 - accuracy: 0.8167 - val_loss: 1.1937 - val_accuracy: 0.4000\n",
            "Epoch 4340/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4592 - accuracy: 0.7691\n",
            "Epoch 4340: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.4466 - accuracy: 0.7748 - val_loss: 0.6491 - val_accuracy: 0.5969\n",
            "Epoch 4341/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3879 - accuracy: 0.8009\n",
            "Epoch 4341: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3962 - accuracy: 0.7974 - val_loss: 0.3752 - val_accuracy: 0.7735\n",
            "Epoch 4342/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3777 - accuracy: 0.8073\n",
            "Epoch 4342: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3762 - accuracy: 0.8076 - val_loss: 0.3437 - val_accuracy: 0.7992\n",
            "Epoch 4343/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4082 - accuracy: 0.7901\n",
            "Epoch 4343: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.4012 - accuracy: 0.7949 - val_loss: 0.4477 - val_accuracy: 0.7343\n",
            "Epoch 4344/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3786 - accuracy: 0.8078\n",
            "Epoch 4344: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3804 - accuracy: 0.8065 - val_loss: 0.5615 - val_accuracy: 0.6599\n",
            "Epoch 4345/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3472 - accuracy: 0.8221\n",
            "Epoch 4345: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3521 - accuracy: 0.8198 - val_loss: 0.7872 - val_accuracy: 0.5236\n",
            "Epoch 4346/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3579 - accuracy: 0.8174\n",
            "Epoch 4346: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3582 - accuracy: 0.8181 - val_loss: 0.8270 - val_accuracy: 0.5126\n",
            "Epoch 4347/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3915 - accuracy: 0.7970\n",
            "Epoch 4347: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3879 - accuracy: 0.7996 - val_loss: 0.5631 - val_accuracy: 0.6487\n",
            "Epoch 4348/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8257\n",
            "Epoch 4348: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3450 - accuracy: 0.8240 - val_loss: 0.4184 - val_accuracy: 0.7569\n",
            "Epoch 4349/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3417 - accuracy: 0.8295\n",
            "Epoch 4349: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3419 - accuracy: 0.8292 - val_loss: 0.4124 - val_accuracy: 0.7632\n",
            "Epoch 4350/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3593 - accuracy: 0.8153\n",
            "Epoch 4350: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 0.3572 - accuracy: 0.8177 - val_loss: 0.4703 - val_accuracy: 0.7222\n",
            "Epoch 4351/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3486 - accuracy: 0.8211\n",
            "Epoch 4351: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3480 - accuracy: 0.8221 - val_loss: 0.4947 - val_accuracy: 0.7016\n",
            "Epoch 4352/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3396 - accuracy: 0.8295\n",
            "Epoch 4352: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3398 - accuracy: 0.8292 - val_loss: 0.5643 - val_accuracy: 0.6485\n",
            "Epoch 4353/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8268\n",
            "Epoch 4353: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3446 - accuracy: 0.8258 - val_loss: 0.6494 - val_accuracy: 0.5964\n",
            "Epoch 4354/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8214\n",
            "Epoch 4354: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3465 - accuracy: 0.8238 - val_loss: 0.6081 - val_accuracy: 0.6114\n",
            "Epoch 4355/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3408 - accuracy: 0.8256\n",
            "Epoch 4355: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3390 - accuracy: 0.8269 - val_loss: 0.5072 - val_accuracy: 0.6858\n",
            "Epoch 4356/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3375 - accuracy: 0.8310\n",
            "Epoch 4356: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3385 - accuracy: 0.8306 - val_loss: 0.4923 - val_accuracy: 0.7035\n",
            "Epoch 4357/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3325 - accuracy: 0.8332\n",
            "Epoch 4357: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3345 - accuracy: 0.8314 - val_loss: 0.3744 - val_accuracy: 0.7784\n",
            "Epoch 4358/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4011 - accuracy: 0.7914\n",
            "Epoch 4358: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3954 - accuracy: 0.7965 - val_loss: 0.4563 - val_accuracy: 0.7281\n",
            "Epoch 4359/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3563 - accuracy: 0.8169\n",
            "Epoch 4359: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3588 - accuracy: 0.8163 - val_loss: 0.7155 - val_accuracy: 0.5572\n",
            "Epoch 4360/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3508 - accuracy: 0.8217\n",
            "Epoch 4360: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3516 - accuracy: 0.8205 - val_loss: 0.7869 - val_accuracy: 0.5271\n",
            "Epoch 4361/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4047 - accuracy: 0.7920\n",
            "Epoch 4361: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4021 - accuracy: 0.7933 - val_loss: 0.5244 - val_accuracy: 0.6735\n",
            "Epoch 4362/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8177\n",
            "Epoch 4362: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3611 - accuracy: 0.8150 - val_loss: 0.4173 - val_accuracy: 0.7522\n",
            "Epoch 4363/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3490 - accuracy: 0.8237\n",
            "Epoch 4363: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3516 - accuracy: 0.8216 - val_loss: 0.3541 - val_accuracy: 0.7909\n",
            "Epoch 4364/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4063 - accuracy: 0.7909\n",
            "Epoch 4364: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.4024 - accuracy: 0.7934 - val_loss: 0.5345 - val_accuracy: 0.6684\n",
            "Epoch 4365/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8216\n",
            "Epoch 4365: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3472 - accuracy: 0.8208 - val_loss: 0.7250 - val_accuracy: 0.5745\n",
            "Epoch 4366/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3573 - accuracy: 0.8191\n",
            "Epoch 4366: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3569 - accuracy: 0.8201 - val_loss: 0.6639 - val_accuracy: 0.5853\n",
            "Epoch 4367/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8250\n",
            "Epoch 4367: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3490 - accuracy: 0.8248 - val_loss: 0.5213 - val_accuracy: 0.6959\n",
            "Epoch 4368/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8259\n",
            "Epoch 4368: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3424 - accuracy: 0.8261 - val_loss: 0.4822 - val_accuracy: 0.7064\n",
            "Epoch 4369/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3379 - accuracy: 0.8300\n",
            "Epoch 4369: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3376 - accuracy: 0.8294 - val_loss: 0.3886 - val_accuracy: 0.7755\n",
            "Epoch 4370/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3782 - accuracy: 0.8017\n",
            "Epoch 4370: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3776 - accuracy: 0.8014 - val_loss: 0.5415 - val_accuracy: 0.6607\n",
            "Epoch 4371/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3366 - accuracy: 0.8274\n",
            "Epoch 4371: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3428 - accuracy: 0.8240 - val_loss: 0.8203 - val_accuracy: 0.5128\n",
            "Epoch 4372/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3730 - accuracy: 0.8085\n",
            "Epoch 4372: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3699 - accuracy: 0.8105 - val_loss: 0.6378 - val_accuracy: 0.5921\n",
            "Epoch 4373/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8184\n",
            "Epoch 4373: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3540 - accuracy: 0.8191 - val_loss: 0.5555 - val_accuracy: 0.6513\n",
            "Epoch 4374/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3451 - accuracy: 0.8300\n",
            "Epoch 4374: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3457 - accuracy: 0.8285 - val_loss: 0.4426 - val_accuracy: 0.7417\n",
            "Epoch 4375/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3353 - accuracy: 0.8335\n",
            "Epoch 4375: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3358 - accuracy: 0.8329 - val_loss: 0.4401 - val_accuracy: 0.7349\n",
            "Epoch 4376/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3417 - accuracy: 0.8276\n",
            "Epoch 4376: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3414 - accuracy: 0.8281 - val_loss: 0.4191 - val_accuracy: 0.7591\n",
            "Epoch 4377/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8248\n",
            "Epoch 4377: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3425 - accuracy: 0.8263 - val_loss: 0.5084 - val_accuracy: 0.6801\n",
            "Epoch 4378/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8335\n",
            "Epoch 4378: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3371 - accuracy: 0.8321 - val_loss: 0.6324 - val_accuracy: 0.6052\n",
            "Epoch 4379/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3423 - accuracy: 0.8234\n",
            "Epoch 4379: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3422 - accuracy: 0.8247 - val_loss: 0.7589 - val_accuracy: 0.5405\n",
            "Epoch 4380/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8142\n",
            "Epoch 4380: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3635 - accuracy: 0.8145 - val_loss: 0.4547 - val_accuracy: 0.7250\n",
            "Epoch 4381/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8330\n",
            "Epoch 4381: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3342 - accuracy: 0.8328 - val_loss: 0.4943 - val_accuracy: 0.6978\n",
            "Epoch 4382/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8335\n",
            "Epoch 4382: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3340 - accuracy: 0.8334 - val_loss: 0.4326 - val_accuracy: 0.7452\n",
            "Epoch 4383/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8340\n",
            "Epoch 4383: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3333 - accuracy: 0.8317 - val_loss: 0.4475 - val_accuracy: 0.7422\n",
            "Epoch 4384/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3428 - accuracy: 0.8257\n",
            "Epoch 4384: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3423 - accuracy: 0.8264 - val_loss: 0.5603 - val_accuracy: 0.6432\n",
            "Epoch 4385/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3324 - accuracy: 0.8308\n",
            "Epoch 4385: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.3327 - accuracy: 0.8320 - val_loss: 0.6852 - val_accuracy: 0.5651\n",
            "Epoch 4386/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8217\n",
            "Epoch 4386: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3517 - accuracy: 0.8224 - val_loss: 0.4567 - val_accuracy: 0.7266\n",
            "Epoch 4387/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.8354\n",
            "Epoch 4387: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 0.3321 - accuracy: 0.8341 - val_loss: 0.4538 - val_accuracy: 0.7345\n",
            "Epoch 4388/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8360\n",
            "Epoch 4388: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3313 - accuracy: 0.8362 - val_loss: 0.4459 - val_accuracy: 0.7338\n",
            "Epoch 4389/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3280 - accuracy: 0.8377\n",
            "Epoch 4389: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3292 - accuracy: 0.8369 - val_loss: 0.4700 - val_accuracy: 0.7193\n",
            "Epoch 4390/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3317 - accuracy: 0.8346\n",
            "Epoch 4390: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3323 - accuracy: 0.8343 - val_loss: 0.4781 - val_accuracy: 0.7108\n",
            "Epoch 4391/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3351 - accuracy: 0.8295\n",
            "Epoch 4391: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3322 - accuracy: 0.8322 - val_loss: 0.5601 - val_accuracy: 0.6480\n",
            "Epoch 4392/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3343 - accuracy: 0.8316\n",
            "Epoch 4392: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3329 - accuracy: 0.8317 - val_loss: 0.5199 - val_accuracy: 0.6840\n",
            "Epoch 4393/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3353 - accuracy: 0.8299\n",
            "Epoch 4393: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3338 - accuracy: 0.8305 - val_loss: 0.4319 - val_accuracy: 0.7494\n",
            "Epoch 4394/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8331\n",
            "Epoch 4394: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3356 - accuracy: 0.8331 - val_loss: 0.4880 - val_accuracy: 0.7064\n",
            "Epoch 4395/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3300 - accuracy: 0.8359\n",
            "Epoch 4395: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3290 - accuracy: 0.8374 - val_loss: 0.5529 - val_accuracy: 0.6539\n",
            "Epoch 4396/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8362\n",
            "Epoch 4396: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3316 - accuracy: 0.8339 - val_loss: 0.3982 - val_accuracy: 0.7753\n",
            "Epoch 4397/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8236\n",
            "Epoch 4397: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3429 - accuracy: 0.8256 - val_loss: 0.5238 - val_accuracy: 0.6829\n",
            "Epoch 4398/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3333 - accuracy: 0.8318\n",
            "Epoch 4398: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3332 - accuracy: 0.8328 - val_loss: 0.5645 - val_accuracy: 0.6445\n",
            "Epoch 4399/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8330\n",
            "Epoch 4399: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3323 - accuracy: 0.8322 - val_loss: 0.4704 - val_accuracy: 0.7169\n",
            "Epoch 4400/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8352\n",
            "Epoch 4400: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3325 - accuracy: 0.8350 - val_loss: 0.4380 - val_accuracy: 0.7465\n",
            "Epoch 4401/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3393 - accuracy: 0.8302\n",
            "Epoch 4401: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3406 - accuracy: 0.8290 - val_loss: 0.7469 - val_accuracy: 0.5471\n",
            "Epoch 4402/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3602 - accuracy: 0.8146\n",
            "Epoch 4402: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3582 - accuracy: 0.8157 - val_loss: 0.5703 - val_accuracy: 0.6403\n",
            "Epoch 4403/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3478 - accuracy: 0.8231\n",
            "Epoch 4403: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3513 - accuracy: 0.8207 - val_loss: 0.3524 - val_accuracy: 0.7928\n",
            "Epoch 4404/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4008 - accuracy: 0.7952\n",
            "Epoch 4404: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3949 - accuracy: 0.7978 - val_loss: 0.4866 - val_accuracy: 0.7040\n",
            "Epoch 4405/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3470 - accuracy: 0.8217\n",
            "Epoch 4405: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3517 - accuracy: 0.8195 - val_loss: 0.8340 - val_accuracy: 0.5124\n",
            "Epoch 4406/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3843 - accuracy: 0.8017\n",
            "Epoch 4406: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3788 - accuracy: 0.8048 - val_loss: 0.6388 - val_accuracy: 0.6043\n",
            "Epoch 4407/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8178\n",
            "Epoch 4407: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3552 - accuracy: 0.8187 - val_loss: 0.4680 - val_accuracy: 0.7176\n",
            "Epoch 4408/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8260\n",
            "Epoch 4408: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3506 - accuracy: 0.8235 - val_loss: 0.4129 - val_accuracy: 0.7591\n",
            "Epoch 4409/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3460 - accuracy: 0.8296\n",
            "Epoch 4409: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3480 - accuracy: 0.8277 - val_loss: 0.3211 - val_accuracy: 0.8104\n",
            "Epoch 4410/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4373 - accuracy: 0.7734\n",
            "Epoch 4410: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4294 - accuracy: 0.7789 - val_loss: 0.4821 - val_accuracy: 0.7127\n",
            "Epoch 4411/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8193\n",
            "Epoch 4411: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3583 - accuracy: 0.8188 - val_loss: 0.7413 - val_accuracy: 0.5578\n",
            "Epoch 4412/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3621 - accuracy: 0.8167\n",
            "Epoch 4412: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3603 - accuracy: 0.8176 - val_loss: 0.6166 - val_accuracy: 0.6105\n",
            "Epoch 4413/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3543 - accuracy: 0.8204\n",
            "Epoch 4413: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3548 - accuracy: 0.8198 - val_loss: 0.4580 - val_accuracy: 0.7314\n",
            "Epoch 4414/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8330\n",
            "Epoch 4414: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3377 - accuracy: 0.8312 - val_loss: 0.3798 - val_accuracy: 0.7817\n",
            "Epoch 4415/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3551 - accuracy: 0.8207\n",
            "Epoch 4415: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3534 - accuracy: 0.8204 - val_loss: 0.4524 - val_accuracy: 0.7332\n",
            "Epoch 4416/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3496 - accuracy: 0.8216\n",
            "Epoch 4416: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3504 - accuracy: 0.8219 - val_loss: 0.5830 - val_accuracy: 0.6320\n",
            "Epoch 4417/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8313\n",
            "Epoch 4417: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3328 - accuracy: 0.8307 - val_loss: 0.5814 - val_accuracy: 0.6228\n",
            "Epoch 4418/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8289\n",
            "Epoch 4418: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3352 - accuracy: 0.8298 - val_loss: 0.6109 - val_accuracy: 0.6079\n",
            "Epoch 4419/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3419 - accuracy: 0.8299\n",
            "Epoch 4419: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3416 - accuracy: 0.8295 - val_loss: 0.4771 - val_accuracy: 0.7064\n",
            "Epoch 4420/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8351\n",
            "Epoch 4420: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3318 - accuracy: 0.8337 - val_loss: 0.4472 - val_accuracy: 0.7373\n",
            "Epoch 4421/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3400 - accuracy: 0.8282\n",
            "Epoch 4421: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3386 - accuracy: 0.8296 - val_loss: 0.5619 - val_accuracy: 0.6476\n",
            "Epoch 4422/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8280\n",
            "Epoch 4422: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3378 - accuracy: 0.8292 - val_loss: 0.4557 - val_accuracy: 0.7312\n",
            "Epoch 4423/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3314 - accuracy: 0.8351\n",
            "Epoch 4423: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3317 - accuracy: 0.8345 - val_loss: 0.4431 - val_accuracy: 0.7397\n",
            "Epoch 4424/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3353 - accuracy: 0.8303\n",
            "Epoch 4424: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3356 - accuracy: 0.8304 - val_loss: 0.5476 - val_accuracy: 0.6614\n",
            "Epoch 4425/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3302 - accuracy: 0.8351\n",
            "Epoch 4425: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3304 - accuracy: 0.8349 - val_loss: 0.6413 - val_accuracy: 0.6019\n",
            "Epoch 4426/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3517 - accuracy: 0.8197\n",
            "Epoch 4426: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3513 - accuracy: 0.8204 - val_loss: 0.4623 - val_accuracy: 0.7292\n",
            "Epoch 4427/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3325 - accuracy: 0.8368\n",
            "Epoch 4427: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3365 - accuracy: 0.8335 - val_loss: 0.3551 - val_accuracy: 0.7986\n",
            "Epoch 4428/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3930 - accuracy: 0.7965\n",
            "Epoch 4428: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3929 - accuracy: 0.7960 - val_loss: 0.6804 - val_accuracy: 0.5789\n",
            "Epoch 4429/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3424 - accuracy: 0.8281\n",
            "Epoch 4429: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3413 - accuracy: 0.8286 - val_loss: 0.5985 - val_accuracy: 0.6248\n",
            "Epoch 4430/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8170\n",
            "Epoch 4430: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3543 - accuracy: 0.8168 - val_loss: 0.4159 - val_accuracy: 0.7593\n",
            "Epoch 4431/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3501 - accuracy: 0.8220\n",
            "Epoch 4431: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3471 - accuracy: 0.8240 - val_loss: 0.4103 - val_accuracy: 0.7630\n",
            "Epoch 4432/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8202\n",
            "Epoch 4432: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3577 - accuracy: 0.8173 - val_loss: 0.7790 - val_accuracy: 0.5249\n",
            "Epoch 4433/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3569 - accuracy: 0.8166\n",
            "Epoch 4433: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3580 - accuracy: 0.8162 - val_loss: 0.5917 - val_accuracy: 0.6333\n",
            "Epoch 4434/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3476 - accuracy: 0.8239\n",
            "Epoch 4434: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3461 - accuracy: 0.8247 - val_loss: 0.4950 - val_accuracy: 0.6983\n",
            "Epoch 4435/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3340 - accuracy: 0.8337\n",
            "Epoch 4435: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3378 - accuracy: 0.8309 - val_loss: 0.3756 - val_accuracy: 0.7915\n",
            "Epoch 4436/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3639 - accuracy: 0.8161\n",
            "Epoch 4436: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3613 - accuracy: 0.8178 - val_loss: 0.4627 - val_accuracy: 0.7228\n",
            "Epoch 4437/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3612 - accuracy: 0.8143\n",
            "Epoch 4437: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3693 - accuracy: 0.8100 - val_loss: 0.8359 - val_accuracy: 0.5043\n",
            "Epoch 4438/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3797 - accuracy: 0.8034\n",
            "Epoch 4438: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3762 - accuracy: 0.8054 - val_loss: 0.5871 - val_accuracy: 0.6401\n",
            "Epoch 4439/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8233\n",
            "Epoch 4439: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3547 - accuracy: 0.8221 - val_loss: 0.5104 - val_accuracy: 0.6801\n",
            "Epoch 4440/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3504 - accuracy: 0.8237\n",
            "Epoch 4440: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3510 - accuracy: 0.8234 - val_loss: 0.3966 - val_accuracy: 0.7790\n",
            "Epoch 4441/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3517 - accuracy: 0.8219\n",
            "Epoch 4441: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3504 - accuracy: 0.8217 - val_loss: 0.4037 - val_accuracy: 0.7553\n",
            "Epoch 4442/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3948 - accuracy: 0.7988\n",
            "Epoch 4442: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3950 - accuracy: 0.7986 - val_loss: 0.5340 - val_accuracy: 0.6695\n",
            "Epoch 4443/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3421 - accuracy: 0.8215\n",
            "Epoch 4443: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3458 - accuracy: 0.8210 - val_loss: 0.7983 - val_accuracy: 0.5232\n",
            "Epoch 4444/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3662 - accuracy: 0.8140\n",
            "Epoch 4444: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3639 - accuracy: 0.8153 - val_loss: 0.8042 - val_accuracy: 0.5212\n",
            "Epoch 4445/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3920 - accuracy: 0.7997\n",
            "Epoch 4445: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3870 - accuracy: 0.8022 - val_loss: 0.5944 - val_accuracy: 0.6204\n",
            "Epoch 4446/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3713 - accuracy: 0.8066\n",
            "Epoch 4446: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.3728 - accuracy: 0.8063 - val_loss: 0.5156 - val_accuracy: 0.6862\n",
            "Epoch 4447/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3784 - accuracy: 0.8090\n",
            "Epoch 4447: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3883 - accuracy: 0.8037 - val_loss: 0.4251 - val_accuracy: 0.7492\n",
            "Epoch 4448/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3715 - accuracy: 0.8140\n",
            "Epoch 4448: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3854 - accuracy: 0.8073 - val_loss: 0.3624 - val_accuracy: 0.7770\n",
            "Epoch 4449/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3860 - accuracy: 0.8071\n",
            "Epoch 4449: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3922 - accuracy: 0.8048 - val_loss: 0.3646 - val_accuracy: 0.7878\n",
            "Epoch 4450/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3861 - accuracy: 0.8054\n",
            "Epoch 4450: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3860 - accuracy: 0.8051 - val_loss: 0.3673 - val_accuracy: 0.7856\n",
            "Epoch 4451/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3810 - accuracy: 0.8046\n",
            "Epoch 4451: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3783 - accuracy: 0.8066 - val_loss: 0.3644 - val_accuracy: 0.7990\n",
            "Epoch 4452/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3871 - accuracy: 0.8022\n",
            "Epoch 4452: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.3840 - accuracy: 0.8041 - val_loss: 0.4883 - val_accuracy: 0.7005\n",
            "Epoch 4453/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3574 - accuracy: 0.8162\n",
            "Epoch 4453: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3584 - accuracy: 0.8156 - val_loss: 0.5722 - val_accuracy: 0.6421\n",
            "Epoch 4454/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3390 - accuracy: 0.8270\n",
            "Epoch 4454: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3407 - accuracy: 0.8256 - val_loss: 0.6707 - val_accuracy: 0.5727\n",
            "Epoch 4455/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3449 - accuracy: 0.8241\n",
            "Epoch 4455: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3454 - accuracy: 0.8241 - val_loss: 0.6607 - val_accuracy: 0.6050\n",
            "Epoch 4456/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3605 - accuracy: 0.8151\n",
            "Epoch 4456: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3578 - accuracy: 0.8181 - val_loss: 0.6181 - val_accuracy: 0.6054\n",
            "Epoch 4457/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3510 - accuracy: 0.8220\n",
            "Epoch 4457: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3501 - accuracy: 0.8212 - val_loss: 0.5123 - val_accuracy: 0.6825\n",
            "Epoch 4458/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8258\n",
            "Epoch 4458: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3529 - accuracy: 0.8237 - val_loss: 0.4308 - val_accuracy: 0.7419\n",
            "Epoch 4459/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3416 - accuracy: 0.8290\n",
            "Epoch 4459: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3436 - accuracy: 0.8271 - val_loss: 0.3977 - val_accuracy: 0.7707\n",
            "Epoch 4460/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3484 - accuracy: 0.8241\n",
            "Epoch 4460: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3485 - accuracy: 0.8245 - val_loss: 0.3772 - val_accuracy: 0.7841\n",
            "Epoch 4461/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3621 - accuracy: 0.8134\n",
            "Epoch 4461: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3580 - accuracy: 0.8161 - val_loss: 0.4745 - val_accuracy: 0.7145\n",
            "Epoch 4462/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3319 - accuracy: 0.8350\n",
            "Epoch 4462: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3333 - accuracy: 0.8331 - val_loss: 0.5074 - val_accuracy: 0.6882\n",
            "Epoch 4463/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3310 - accuracy: 0.8340\n",
            "Epoch 4463: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3308 - accuracy: 0.8352 - val_loss: 0.4775 - val_accuracy: 0.7169\n",
            "Epoch 4464/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8334\n",
            "Epoch 4464: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3335 - accuracy: 0.8338 - val_loss: 0.5296 - val_accuracy: 0.6726\n",
            "Epoch 4465/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3282 - accuracy: 0.8362\n",
            "Epoch 4465: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3300 - accuracy: 0.8355 - val_loss: 0.4891 - val_accuracy: 0.7053\n",
            "Epoch 4466/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3306 - accuracy: 0.8368\n",
            "Epoch 4466: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3299 - accuracy: 0.8374 - val_loss: 0.5482 - val_accuracy: 0.6493\n",
            "Epoch 4467/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3283 - accuracy: 0.8366\n",
            "Epoch 4467: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3297 - accuracy: 0.8356 - val_loss: 0.6067 - val_accuracy: 0.6133\n",
            "Epoch 4468/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3399 - accuracy: 0.8288\n",
            "Epoch 4468: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3379 - accuracy: 0.8307 - val_loss: 0.6162 - val_accuracy: 0.6070\n",
            "Epoch 4469/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3516 - accuracy: 0.8208\n",
            "Epoch 4469: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3523 - accuracy: 0.8195 - val_loss: 0.4927 - val_accuracy: 0.7002\n",
            "Epoch 4470/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3363 - accuracy: 0.8348\n",
            "Epoch 4470: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3396 - accuracy: 0.8321 - val_loss: 0.4284 - val_accuracy: 0.7503\n",
            "Epoch 4471/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8363\n",
            "Epoch 4471: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3368 - accuracy: 0.8342 - val_loss: 0.3869 - val_accuracy: 0.7759\n",
            "Epoch 4472/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3487 - accuracy: 0.8238\n",
            "Epoch 4472: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3481 - accuracy: 0.8238 - val_loss: 0.3700 - val_accuracy: 0.7961\n",
            "Epoch 4473/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3658 - accuracy: 0.8112\n",
            "Epoch 4473: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3632 - accuracy: 0.8135 - val_loss: 0.4622 - val_accuracy: 0.7277\n",
            "Epoch 4474/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3511 - accuracy: 0.8217\n",
            "Epoch 4474: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3527 - accuracy: 0.8212 - val_loss: 0.6799 - val_accuracy: 0.5741\n",
            "Epoch 4475/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3453 - accuracy: 0.8225\n",
            "Epoch 4475: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3455 - accuracy: 0.8232 - val_loss: 0.6046 - val_accuracy: 0.6186\n",
            "Epoch 4476/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3386 - accuracy: 0.8317\n",
            "Epoch 4476: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3380 - accuracy: 0.8312 - val_loss: 0.5659 - val_accuracy: 0.6401\n",
            "Epoch 4477/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3351 - accuracy: 0.8332\n",
            "Epoch 4477: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3363 - accuracy: 0.8322 - val_loss: 0.4788 - val_accuracy: 0.7158\n",
            "Epoch 4478/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3300 - accuracy: 0.8363\n",
            "Epoch 4478: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3317 - accuracy: 0.8347 - val_loss: 0.3652 - val_accuracy: 0.7983\n",
            "Epoch 4479/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3692 - accuracy: 0.8095\n",
            "Epoch 4479: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3671 - accuracy: 0.8114 - val_loss: 0.5362 - val_accuracy: 0.6689\n",
            "Epoch 4480/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8329\n",
            "Epoch 4480: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3345 - accuracy: 0.8318 - val_loss: 0.6920 - val_accuracy: 0.5675\n",
            "Epoch 4481/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8216\n",
            "Epoch 4481: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3501 - accuracy: 0.8209 - val_loss: 0.5305 - val_accuracy: 0.6669\n",
            "Epoch 4482/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3273 - accuracy: 0.8378\n",
            "Epoch 4482: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3291 - accuracy: 0.8363 - val_loss: 0.4780 - val_accuracy: 0.7125\n",
            "Epoch 4483/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3278 - accuracy: 0.8375\n",
            "Epoch 4483: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3289 - accuracy: 0.8365 - val_loss: 0.4903 - val_accuracy: 0.7005\n",
            "Epoch 4484/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3265 - accuracy: 0.8379\n",
            "Epoch 4484: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3284 - accuracy: 0.8368 - val_loss: 0.5308 - val_accuracy: 0.6647\n",
            "Epoch 4485/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3278 - accuracy: 0.8395\n",
            "Epoch 4485: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3305 - accuracy: 0.8360 - val_loss: 0.4804 - val_accuracy: 0.7121\n",
            "Epoch 4486/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8330\n",
            "Epoch 4486: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3330 - accuracy: 0.8337 - val_loss: 0.4170 - val_accuracy: 0.7593\n",
            "Epoch 4487/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3425 - accuracy: 0.8286\n",
            "Epoch 4487: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3435 - accuracy: 0.8268 - val_loss: 0.4220 - val_accuracy: 0.7566\n",
            "Epoch 4488/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3487 - accuracy: 0.8237\n",
            "Epoch 4488: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3492 - accuracy: 0.8236 - val_loss: 0.5656 - val_accuracy: 0.6482\n",
            "Epoch 4489/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8341\n",
            "Epoch 4489: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3327 - accuracy: 0.8336 - val_loss: 0.5652 - val_accuracy: 0.6386\n",
            "Epoch 4490/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3287 - accuracy: 0.8350\n",
            "Epoch 4490: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3295 - accuracy: 0.8354 - val_loss: 0.5758 - val_accuracy: 0.6280\n",
            "Epoch 4491/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3305 - accuracy: 0.8348\n",
            "Epoch 4491: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3298 - accuracy: 0.8356 - val_loss: 0.5745 - val_accuracy: 0.6357\n",
            "Epoch 4492/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8311\n",
            "Epoch 4492: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3359 - accuracy: 0.8311 - val_loss: 0.4121 - val_accuracy: 0.7687\n",
            "Epoch 4493/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8312\n",
            "Epoch 4493: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3335 - accuracy: 0.8319 - val_loss: 0.4682 - val_accuracy: 0.7209\n",
            "Epoch 4494/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8316\n",
            "Epoch 4494: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3359 - accuracy: 0.8304 - val_loss: 0.8718 - val_accuracy: 0.4883\n",
            "Epoch 4495/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4074 - accuracy: 0.7888\n",
            "Epoch 4495: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4032 - accuracy: 0.7914 - val_loss: 0.4423 - val_accuracy: 0.7452\n",
            "Epoch 4496/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3318 - accuracy: 0.8337\n",
            "Epoch 4496: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3337 - accuracy: 0.8324 - val_loss: 0.3901 - val_accuracy: 0.7762\n",
            "Epoch 4497/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3693 - accuracy: 0.8083\n",
            "Epoch 4497: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3675 - accuracy: 0.8095 - val_loss: 0.4962 - val_accuracy: 0.7020\n",
            "Epoch 4498/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8309\n",
            "Epoch 4498: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3377 - accuracy: 0.8307 - val_loss: 0.6034 - val_accuracy: 0.6186\n",
            "Epoch 4499/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3324 - accuracy: 0.8326\n",
            "Epoch 4499: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3331 - accuracy: 0.8315 - val_loss: 0.5691 - val_accuracy: 0.6452\n",
            "Epoch 4500/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3404 - accuracy: 0.8276\n",
            "Epoch 4500: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3397 - accuracy: 0.8280 - val_loss: 0.5085 - val_accuracy: 0.6893\n",
            "Epoch 4501/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8333\n",
            "Epoch 4501: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3383 - accuracy: 0.8307 - val_loss: 0.3377 - val_accuracy: 0.8023\n",
            "Epoch 4502/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4089 - accuracy: 0.7856\n",
            "Epoch 4502: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4030 - accuracy: 0.7894 - val_loss: 0.5353 - val_accuracy: 0.6752\n",
            "Epoch 4503/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3474 - accuracy: 0.8226\n",
            "Epoch 4503: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3558 - accuracy: 0.8178 - val_loss: 0.9695 - val_accuracy: 0.4529\n",
            "Epoch 4504/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3942 - accuracy: 0.7985\n",
            "Epoch 4504: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3907 - accuracy: 0.8005 - val_loss: 0.9405 - val_accuracy: 0.4703\n",
            "Epoch 4505/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4288 - accuracy: 0.7778\n",
            "Epoch 4505: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4196 - accuracy: 0.7838 - val_loss: 0.6869 - val_accuracy: 0.5774\n",
            "Epoch 4506/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3962 - accuracy: 0.8025\n",
            "Epoch 4506: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3994 - accuracy: 0.7996 - val_loss: 0.4013 - val_accuracy: 0.7705\n",
            "Epoch 4507/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3500 - accuracy: 0.8231\n",
            "Epoch 4507: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3533 - accuracy: 0.8208 - val_loss: 0.4324 - val_accuracy: 0.7433\n",
            "Epoch 4508/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3459 - accuracy: 0.8289\n",
            "Epoch 4508: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3484 - accuracy: 0.8263 - val_loss: 0.3530 - val_accuracy: 0.7990\n",
            "Epoch 4509/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3770 - accuracy: 0.8082\n",
            "Epoch 4509: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3744 - accuracy: 0.8097 - val_loss: 0.4927 - val_accuracy: 0.7029\n",
            "Epoch 4510/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3415 - accuracy: 0.8286\n",
            "Epoch 4510: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3406 - accuracy: 0.8285 - val_loss: 0.5091 - val_accuracy: 0.6836\n",
            "Epoch 4511/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3323 - accuracy: 0.8321\n",
            "Epoch 4511: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3354 - accuracy: 0.8304 - val_loss: 0.6465 - val_accuracy: 0.5956\n",
            "Epoch 4512/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3436 - accuracy: 0.8250\n",
            "Epoch 4512: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3416 - accuracy: 0.8269 - val_loss: 0.6817 - val_accuracy: 0.5712\n",
            "Epoch 4513/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8208\n",
            "Epoch 4513: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3519 - accuracy: 0.8205 - val_loss: 0.4372 - val_accuracy: 0.7512\n",
            "Epoch 4514/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3296 - accuracy: 0.8364\n",
            "Epoch 4514: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3315 - accuracy: 0.8341 - val_loss: 0.3840 - val_accuracy: 0.7768\n",
            "Epoch 4515/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3693 - accuracy: 0.8111\n",
            "Epoch 4515: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3667 - accuracy: 0.8124 - val_loss: 0.4573 - val_accuracy: 0.7301\n",
            "Epoch 4516/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3538 - accuracy: 0.8187\n",
            "Epoch 4516: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3591 - accuracy: 0.8157 - val_loss: 0.8251 - val_accuracy: 0.5190\n",
            "Epoch 4517/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8098\n",
            "Epoch 4517: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3722 - accuracy: 0.8091 - val_loss: 0.9245 - val_accuracy: 0.4672\n",
            "Epoch 4518/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4046 - accuracy: 0.7932\n",
            "Epoch 4518: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3973 - accuracy: 0.7969 - val_loss: 0.7686 - val_accuracy: 0.5396\n",
            "Epoch 4519/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3964 - accuracy: 0.7957\n",
            "Epoch 4519: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3949 - accuracy: 0.7966 - val_loss: 0.5362 - val_accuracy: 0.6675\n",
            "Epoch 4520/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3474 - accuracy: 0.8234\n",
            "Epoch 4520: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3484 - accuracy: 0.8224 - val_loss: 0.4147 - val_accuracy: 0.7601\n",
            "Epoch 4521/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8307\n",
            "Epoch 4521: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3367 - accuracy: 0.8311 - val_loss: 0.4446 - val_accuracy: 0.7437\n",
            "Epoch 4522/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3439 - accuracy: 0.8277\n",
            "Epoch 4522: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3444 - accuracy: 0.8266 - val_loss: 0.4241 - val_accuracy: 0.7569\n",
            "Epoch 4523/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8271\n",
            "Epoch 4523: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3425 - accuracy: 0.8262 - val_loss: 0.5718 - val_accuracy: 0.6370\n",
            "Epoch 4524/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3317 - accuracy: 0.8344\n",
            "Epoch 4524: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3327 - accuracy: 0.8342 - val_loss: 0.4818 - val_accuracy: 0.7079\n",
            "Epoch 4525/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3361 - accuracy: 0.8309\n",
            "Epoch 4525: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3364 - accuracy: 0.8311 - val_loss: 0.5718 - val_accuracy: 0.6406\n",
            "Epoch 4526/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8323\n",
            "Epoch 4526: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3334 - accuracy: 0.8321 - val_loss: 0.4897 - val_accuracy: 0.6956\n",
            "Epoch 4527/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3299 - accuracy: 0.8358\n",
            "Epoch 4527: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3319 - accuracy: 0.8340 - val_loss: 0.6272 - val_accuracy: 0.6081\n",
            "Epoch 4528/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3450 - accuracy: 0.8278\n",
            "Epoch 4528: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3445 - accuracy: 0.8275 - val_loss: 0.4537 - val_accuracy: 0.7343\n",
            "Epoch 4529/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3341 - accuracy: 0.8321\n",
            "Epoch 4529: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3341 - accuracy: 0.8319 - val_loss: 0.5335 - val_accuracy: 0.6717\n",
            "Epoch 4530/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.8369\n",
            "Epoch 4530: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3288 - accuracy: 0.8376 - val_loss: 0.5673 - val_accuracy: 0.6373\n",
            "Epoch 4531/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8267\n",
            "Epoch 4531: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3437 - accuracy: 0.8239 - val_loss: 0.4300 - val_accuracy: 0.7468\n",
            "Epoch 4532/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3463 - accuracy: 0.8260\n",
            "Epoch 4532: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3440 - accuracy: 0.8273 - val_loss: 0.3773 - val_accuracy: 0.7874\n",
            "Epoch 4533/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8130\n",
            "Epoch 4533: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3547 - accuracy: 0.8147 - val_loss: 0.5124 - val_accuracy: 0.6842\n",
            "Epoch 4534/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3612 - accuracy: 0.8188\n",
            "Epoch 4534: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3632 - accuracy: 0.8176 - val_loss: 0.8230 - val_accuracy: 0.5330\n",
            "Epoch 4535/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3976 - accuracy: 0.7959\n",
            "Epoch 4535: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3917 - accuracy: 0.7990 - val_loss: 0.7208 - val_accuracy: 0.5526\n",
            "Epoch 4536/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3883 - accuracy: 0.8005\n",
            "Epoch 4536: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3885 - accuracy: 0.8007 - val_loss: 0.5038 - val_accuracy: 0.6937\n",
            "Epoch 4537/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3626 - accuracy: 0.8197\n",
            "Epoch 4537: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3690 - accuracy: 0.8158 - val_loss: 0.3857 - val_accuracy: 0.7738\n",
            "Epoch 4538/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3596 - accuracy: 0.8168\n",
            "Epoch 4538: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3570 - accuracy: 0.8187 - val_loss: 0.4240 - val_accuracy: 0.7573\n",
            "Epoch 4539/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8281\n",
            "Epoch 4539: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3417 - accuracy: 0.8281 - val_loss: 0.4143 - val_accuracy: 0.7615\n",
            "Epoch 4540/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3457 - accuracy: 0.8276\n",
            "Epoch 4540: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3446 - accuracy: 0.8279 - val_loss: 0.5651 - val_accuracy: 0.6463\n",
            "Epoch 4541/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8340\n",
            "Epoch 4541: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3331 - accuracy: 0.8339 - val_loss: 0.4811 - val_accuracy: 0.7099\n",
            "Epoch 4542/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8334\n",
            "Epoch 4542: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3343 - accuracy: 0.8329 - val_loss: 0.5128 - val_accuracy: 0.6833\n",
            "Epoch 4543/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3354 - accuracy: 0.8317\n",
            "Epoch 4543: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3349 - accuracy: 0.8319 - val_loss: 0.4721 - val_accuracy: 0.7171\n",
            "Epoch 4544/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3322 - accuracy: 0.8340\n",
            "Epoch 4544: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3328 - accuracy: 0.8336 - val_loss: 0.4773 - val_accuracy: 0.7167\n",
            "Epoch 4545/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3338 - accuracy: 0.8342\n",
            "Epoch 4545: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3337 - accuracy: 0.8334 - val_loss: 0.4990 - val_accuracy: 0.6932\n",
            "Epoch 4546/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8349\n",
            "Epoch 4546: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3324 - accuracy: 0.8331 - val_loss: 0.4555 - val_accuracy: 0.7303\n",
            "Epoch 4547/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8333\n",
            "Epoch 4547: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3337 - accuracy: 0.8341 - val_loss: 0.4960 - val_accuracy: 0.6974\n",
            "Epoch 4548/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3275 - accuracy: 0.8372\n",
            "Epoch 4548: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3291 - accuracy: 0.8356 - val_loss: 0.4168 - val_accuracy: 0.7656\n",
            "Epoch 4549/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3509 - accuracy: 0.8219\n",
            "Epoch 4549: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3513 - accuracy: 0.8226 - val_loss: 0.5429 - val_accuracy: 0.6675\n",
            "Epoch 4550/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3306 - accuracy: 0.8334\n",
            "Epoch 4550: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3302 - accuracy: 0.8343 - val_loss: 0.6160 - val_accuracy: 0.6050\n",
            "Epoch 4551/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3415 - accuracy: 0.8269\n",
            "Epoch 4551: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3409 - accuracy: 0.8272 - val_loss: 0.5161 - val_accuracy: 0.6842\n",
            "Epoch 4552/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8309\n",
            "Epoch 4552: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3412 - accuracy: 0.8292 - val_loss: 0.4469 - val_accuracy: 0.7395\n",
            "Epoch 4553/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3369 - accuracy: 0.8320\n",
            "Epoch 4553: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3386 - accuracy: 0.8300 - val_loss: 0.4062 - val_accuracy: 0.7687\n",
            "Epoch 4554/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3407 - accuracy: 0.8305\n",
            "Epoch 4554: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3392 - accuracy: 0.8312 - val_loss: 0.4480 - val_accuracy: 0.7362\n",
            "Epoch 4555/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3352 - accuracy: 0.8316\n",
            "Epoch 4555: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3349 - accuracy: 0.8322 - val_loss: 0.5105 - val_accuracy: 0.6825\n",
            "Epoch 4556/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3280 - accuracy: 0.8363\n",
            "Epoch 4556: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3283 - accuracy: 0.8354 - val_loss: 0.6307 - val_accuracy: 0.6030\n",
            "Epoch 4557/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3502 - accuracy: 0.8205\n",
            "Epoch 4557: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3519 - accuracy: 0.8194 - val_loss: 0.3951 - val_accuracy: 0.7799\n",
            "Epoch 4558/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3389 - accuracy: 0.8302\n",
            "Epoch 4558: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3386 - accuracy: 0.8296 - val_loss: 0.4123 - val_accuracy: 0.7656\n",
            "Epoch 4559/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8169\n",
            "Epoch 4559: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3572 - accuracy: 0.8165 - val_loss: 0.5872 - val_accuracy: 0.6318\n",
            "Epoch 4560/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8320\n",
            "Epoch 4560: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3335 - accuracy: 0.8320 - val_loss: 0.6351 - val_accuracy: 0.5947\n",
            "Epoch 4561/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3467 - accuracy: 0.8250\n",
            "Epoch 4561: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3475 - accuracy: 0.8243 - val_loss: 0.3949 - val_accuracy: 0.7759\n",
            "Epoch 4562/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3470 - accuracy: 0.8249\n",
            "Epoch 4562: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3447 - accuracy: 0.8269 - val_loss: 0.4653 - val_accuracy: 0.7279\n",
            "Epoch 4563/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3339 - accuracy: 0.8350\n",
            "Epoch 4563: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3363 - accuracy: 0.8329 - val_loss: 0.5149 - val_accuracy: 0.6790\n",
            "Epoch 4564/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3283 - accuracy: 0.8362\n",
            "Epoch 4564: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3297 - accuracy: 0.8350 - val_loss: 0.4744 - val_accuracy: 0.7099\n",
            "Epoch 4565/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8329\n",
            "Epoch 4565: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3333 - accuracy: 0.8333 - val_loss: 0.5716 - val_accuracy: 0.6340\n",
            "Epoch 4566/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8344\n",
            "Epoch 4566: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3308 - accuracy: 0.8354 - val_loss: 0.4948 - val_accuracy: 0.6941\n",
            "Epoch 4567/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3259 - accuracy: 0.8368\n",
            "Epoch 4567: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3272 - accuracy: 0.8356 - val_loss: 0.4340 - val_accuracy: 0.7490\n",
            "Epoch 4568/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.8362\n",
            "Epoch 4568: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3293 - accuracy: 0.8367 - val_loss: 0.4635 - val_accuracy: 0.7213\n",
            "Epoch 4569/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3277 - accuracy: 0.8348\n",
            "Epoch 4569: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3275 - accuracy: 0.8354 - val_loss: 0.5036 - val_accuracy: 0.6910\n",
            "Epoch 4570/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3285 - accuracy: 0.8366\n",
            "Epoch 4570: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3293 - accuracy: 0.8357 - val_loss: 0.5945 - val_accuracy: 0.6267\n",
            "Epoch 4571/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3379 - accuracy: 0.8279\n",
            "Epoch 4571: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3392 - accuracy: 0.8266 - val_loss: 0.4665 - val_accuracy: 0.7217\n",
            "Epoch 4572/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3287 - accuracy: 0.8358\n",
            "Epoch 4572: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3282 - accuracy: 0.8360 - val_loss: 0.4157 - val_accuracy: 0.7623\n",
            "Epoch 4573/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3359 - accuracy: 0.8299\n",
            "Epoch 4573: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3346 - accuracy: 0.8313 - val_loss: 0.4677 - val_accuracy: 0.7250\n",
            "Epoch 4574/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3464 - accuracy: 0.8242\n",
            "Epoch 4574: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3491 - accuracy: 0.8244 - val_loss: 0.6624 - val_accuracy: 0.5943\n",
            "Epoch 4575/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3399 - accuracy: 0.8276\n",
            "Epoch 4575: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3407 - accuracy: 0.8277 - val_loss: 0.8811 - val_accuracy: 0.4777\n",
            "Epoch 4576/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4024 - accuracy: 0.7910\n",
            "Epoch 4576: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3976 - accuracy: 0.7939 - val_loss: 0.4813 - val_accuracy: 0.7097\n",
            "Epoch 4577/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3400 - accuracy: 0.8297\n",
            "Epoch 4577: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3428 - accuracy: 0.8274 - val_loss: 0.4375 - val_accuracy: 0.7454\n",
            "Epoch 4578/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8379\n",
            "Epoch 4578: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3389 - accuracy: 0.8337 - val_loss: 0.3813 - val_accuracy: 0.7724\n",
            "Epoch 4579/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3779 - accuracy: 0.8077\n",
            "Epoch 4579: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3740 - accuracy: 0.8103 - val_loss: 0.4352 - val_accuracy: 0.7496\n",
            "Epoch 4580/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3477 - accuracy: 0.8206\n",
            "Epoch 4580: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3503 - accuracy: 0.8194 - val_loss: 0.6565 - val_accuracy: 0.5848\n",
            "Epoch 4581/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3367 - accuracy: 0.8286\n",
            "Epoch 4581: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3385 - accuracy: 0.8279 - val_loss: 0.8293 - val_accuracy: 0.5113\n",
            "Epoch 4582/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3845 - accuracy: 0.8036\n",
            "Epoch 4582: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3794 - accuracy: 0.8056 - val_loss: 0.6136 - val_accuracy: 0.6327\n",
            "Epoch 4583/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3660 - accuracy: 0.8169\n",
            "Epoch 4583: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3689 - accuracy: 0.8149 - val_loss: 0.4354 - val_accuracy: 0.7542\n",
            "Epoch 4584/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3390 - accuracy: 0.8309\n",
            "Epoch 4584: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3439 - accuracy: 0.8274 - val_loss: 0.3605 - val_accuracy: 0.7902\n",
            "Epoch 4585/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3686 - accuracy: 0.8099\n",
            "Epoch 4585: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3653 - accuracy: 0.8119 - val_loss: 0.4108 - val_accuracy: 0.7601\n",
            "Epoch 4586/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3526 - accuracy: 0.8172\n",
            "Epoch 4586: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3547 - accuracy: 0.8168 - val_loss: 0.5649 - val_accuracy: 0.6390\n",
            "Epoch 4587/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3322 - accuracy: 0.8320\n",
            "Epoch 4587: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3342 - accuracy: 0.8309 - val_loss: 0.6481 - val_accuracy: 0.6180\n",
            "Epoch 4588/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3581 - accuracy: 0.8181\n",
            "Epoch 4588: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3558 - accuracy: 0.8196 - val_loss: 0.5010 - val_accuracy: 0.6930\n",
            "Epoch 4589/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3294 - accuracy: 0.8371\n",
            "Epoch 4589: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3295 - accuracy: 0.8365 - val_loss: 0.5452 - val_accuracy: 0.6621\n",
            "Epoch 4590/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3467 - accuracy: 0.8242\n",
            "Epoch 4590: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3494 - accuracy: 0.8226 - val_loss: 0.3858 - val_accuracy: 0.7711\n",
            "Epoch 4591/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3712 - accuracy: 0.8053\n",
            "Epoch 4591: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3675 - accuracy: 0.8082 - val_loss: 0.5724 - val_accuracy: 0.6410\n",
            "Epoch 4592/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3338 - accuracy: 0.8334\n",
            "Epoch 4592: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3337 - accuracy: 0.8329 - val_loss: 0.6198 - val_accuracy: 0.6186\n",
            "Epoch 4593/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3553 - accuracy: 0.8192\n",
            "Epoch 4593: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3552 - accuracy: 0.8185 - val_loss: 0.4122 - val_accuracy: 0.7566\n",
            "Epoch 4594/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3489 - accuracy: 0.8216\n",
            "Epoch 4594: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3478 - accuracy: 0.8222 - val_loss: 0.4338 - val_accuracy: 0.7479\n",
            "Epoch 4595/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3453 - accuracy: 0.8252\n",
            "Epoch 4595: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3451 - accuracy: 0.8252 - val_loss: 0.6520 - val_accuracy: 0.5927\n",
            "Epoch 4596/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3396 - accuracy: 0.8270\n",
            "Epoch 4596: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3390 - accuracy: 0.8279 - val_loss: 0.6435 - val_accuracy: 0.6008\n",
            "Epoch 4597/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3730 - accuracy: 0.8063\n",
            "Epoch 4597: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3750 - accuracy: 0.8053 - val_loss: 0.3750 - val_accuracy: 0.7810\n",
            "Epoch 4598/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8205\n",
            "Epoch 4598: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3506 - accuracy: 0.8215 - val_loss: 0.3728 - val_accuracy: 0.7830\n",
            "Epoch 4599/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4018 - accuracy: 0.7929\n",
            "Epoch 4599: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3972 - accuracy: 0.7952 - val_loss: 0.4654 - val_accuracy: 0.7237\n",
            "Epoch 4600/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3567 - accuracy: 0.8133\n",
            "Epoch 4600: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3585 - accuracy: 0.8130 - val_loss: 0.5851 - val_accuracy: 0.6307\n",
            "Epoch 4601/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3410 - accuracy: 0.8240\n",
            "Epoch 4601: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3441 - accuracy: 0.8228 - val_loss: 0.6644 - val_accuracy: 0.5842\n",
            "Epoch 4602/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3448 - accuracy: 0.8250\n",
            "Epoch 4602: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 0.3448 - accuracy: 0.8258 - val_loss: 0.7790 - val_accuracy: 0.5262\n",
            "Epoch 4603/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4002 - accuracy: 0.7924\n",
            "Epoch 4603: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.3969 - accuracy: 0.7945 - val_loss: 0.4958 - val_accuracy: 0.6910\n",
            "Epoch 4604/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8311\n",
            "Epoch 4604: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3434 - accuracy: 0.8282 - val_loss: 0.3857 - val_accuracy: 0.7797\n",
            "Epoch 4605/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3561 - accuracy: 0.8191\n",
            "Epoch 4605: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3541 - accuracy: 0.8205 - val_loss: 0.4448 - val_accuracy: 0.7417\n",
            "Epoch 4606/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3390 - accuracy: 0.8315\n",
            "Epoch 4606: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3385 - accuracy: 0.8323 - val_loss: 0.4546 - val_accuracy: 0.7268\n",
            "Epoch 4607/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3465 - accuracy: 0.8262\n",
            "Epoch 4607: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3468 - accuracy: 0.8263 - val_loss: 0.6241 - val_accuracy: 0.6063\n",
            "Epoch 4608/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3416 - accuracy: 0.8281\n",
            "Epoch 4608: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3394 - accuracy: 0.8305 - val_loss: 0.5692 - val_accuracy: 0.6401\n",
            "Epoch 4609/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3330 - accuracy: 0.8342\n",
            "Epoch 4609: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3328 - accuracy: 0.8346 - val_loss: 0.5624 - val_accuracy: 0.6445\n",
            "Epoch 4610/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8301\n",
            "Epoch 4610: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3330 - accuracy: 0.8318 - val_loss: 0.5459 - val_accuracy: 0.6607\n",
            "Epoch 4611/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3323 - accuracy: 0.8346\n",
            "Epoch 4611: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3336 - accuracy: 0.8338 - val_loss: 0.4501 - val_accuracy: 0.7329\n",
            "Epoch 4612/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3301 - accuracy: 0.8357\n",
            "Epoch 4612: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3304 - accuracy: 0.8353 - val_loss: 0.4204 - val_accuracy: 0.7584\n",
            "Epoch 4613/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3473 - accuracy: 0.8234\n",
            "Epoch 4613: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3438 - accuracy: 0.8260 - val_loss: 0.4626 - val_accuracy: 0.7160\n",
            "Epoch 4614/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3364 - accuracy: 0.8314\n",
            "Epoch 4614: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3339 - accuracy: 0.8328 - val_loss: 0.6243 - val_accuracy: 0.6032\n",
            "Epoch 4615/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8321\n",
            "Epoch 4615: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3363 - accuracy: 0.8321 - val_loss: 0.5668 - val_accuracy: 0.6460\n",
            "Epoch 4616/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3522 - accuracy: 0.8188\n",
            "Epoch 4616: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3545 - accuracy: 0.8163 - val_loss: 0.4364 - val_accuracy: 0.7419\n",
            "Epoch 4617/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3430 - accuracy: 0.8301\n",
            "Epoch 4617: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3429 - accuracy: 0.8288 - val_loss: 0.3457 - val_accuracy: 0.8080\n",
            "Epoch 4618/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3866 - accuracy: 0.7994\n",
            "Epoch 4618: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3850 - accuracy: 0.8008 - val_loss: 0.6086 - val_accuracy: 0.6173\n",
            "Epoch 4619/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8262\n",
            "Epoch 4619: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3470 - accuracy: 0.8242 - val_loss: 0.7527 - val_accuracy: 0.5532\n",
            "Epoch 4620/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8156\n",
            "Epoch 4620: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3653 - accuracy: 0.8159 - val_loss: 0.5647 - val_accuracy: 0.6417\n",
            "Epoch 4621/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8258\n",
            "Epoch 4621: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3425 - accuracy: 0.8262 - val_loss: 0.4803 - val_accuracy: 0.7119\n",
            "Epoch 4622/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3358 - accuracy: 0.8317\n",
            "Epoch 4622: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3360 - accuracy: 0.8317 - val_loss: 0.4970 - val_accuracy: 0.6969\n",
            "Epoch 4623/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3330 - accuracy: 0.8323\n",
            "Epoch 4623: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3336 - accuracy: 0.8324 - val_loss: 0.5289 - val_accuracy: 0.6704\n",
            "Epoch 4624/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3290 - accuracy: 0.8368\n",
            "Epoch 4624: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3283 - accuracy: 0.8375 - val_loss: 0.4992 - val_accuracy: 0.6912\n",
            "Epoch 4625/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3313 - accuracy: 0.8351\n",
            "Epoch 4625: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3320 - accuracy: 0.8337 - val_loss: 0.4286 - val_accuracy: 0.7498\n",
            "Epoch 4626/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8331\n",
            "Epoch 4626: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3339 - accuracy: 0.8330 - val_loss: 0.3857 - val_accuracy: 0.7814\n",
            "Epoch 4627/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3780 - accuracy: 0.8039\n",
            "Epoch 4627: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3786 - accuracy: 0.8048 - val_loss: 0.5429 - val_accuracy: 0.6605\n",
            "Epoch 4628/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8288\n",
            "Epoch 4628: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3367 - accuracy: 0.8278 - val_loss: 0.7444 - val_accuracy: 0.5390\n",
            "Epoch 4629/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3611 - accuracy: 0.8143\n",
            "Epoch 4629: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3563 - accuracy: 0.8168 - val_loss: 0.8386 - val_accuracy: 0.5049\n",
            "Epoch 4630/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4204 - accuracy: 0.7837\n",
            "Epoch 4630: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4178 - accuracy: 0.7853 - val_loss: 0.4484 - val_accuracy: 0.7397\n",
            "Epoch 4631/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3434 - accuracy: 0.8280\n",
            "Epoch 4631: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3494 - accuracy: 0.8240 - val_loss: 0.3823 - val_accuracy: 0.7768\n",
            "Epoch 4632/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3547 - accuracy: 0.8202\n",
            "Epoch 4632: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3542 - accuracy: 0.8202 - val_loss: 0.3650 - val_accuracy: 0.7867\n",
            "Epoch 4633/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3737 - accuracy: 0.8067\n",
            "Epoch 4633: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3692 - accuracy: 0.8095 - val_loss: 0.4520 - val_accuracy: 0.7360\n",
            "Epoch 4634/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3388 - accuracy: 0.8293\n",
            "Epoch 4634: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3379 - accuracy: 0.8302 - val_loss: 0.5250 - val_accuracy: 0.6763\n",
            "Epoch 4635/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3301 - accuracy: 0.8361\n",
            "Epoch 4635: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3309 - accuracy: 0.8351 - val_loss: 0.4688 - val_accuracy: 0.7167\n",
            "Epoch 4636/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3316 - accuracy: 0.8343\n",
            "Epoch 4636: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3325 - accuracy: 0.8341 - val_loss: 0.5225 - val_accuracy: 0.6746\n",
            "Epoch 4637/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3293 - accuracy: 0.8341\n",
            "Epoch 4637: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3302 - accuracy: 0.8341 - val_loss: 0.4871 - val_accuracy: 0.7029\n",
            "Epoch 4638/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3272 - accuracy: 0.8358\n",
            "Epoch 4638: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3284 - accuracy: 0.8351 - val_loss: 0.5089 - val_accuracy: 0.6770\n",
            "Epoch 4639/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3296 - accuracy: 0.8352\n",
            "Epoch 4639: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3295 - accuracy: 0.8352 - val_loss: 0.4488 - val_accuracy: 0.7364\n",
            "Epoch 4640/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3356 - accuracy: 0.8304\n",
            "Epoch 4640: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3355 - accuracy: 0.8307 - val_loss: 0.4650 - val_accuracy: 0.7200\n",
            "Epoch 4641/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3404 - accuracy: 0.8247\n",
            "Epoch 4641: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3459 - accuracy: 0.8208 - val_loss: 0.9548 - val_accuracy: 0.4665\n",
            "Epoch 4642/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4082 - accuracy: 0.7892\n",
            "Epoch 4642: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4004 - accuracy: 0.7935 - val_loss: 0.6650 - val_accuracy: 0.5815\n",
            "Epoch 4643/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3808 - accuracy: 0.8067\n",
            "Epoch 4643: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3841 - accuracy: 0.8046 - val_loss: 0.3987 - val_accuracy: 0.7711\n",
            "Epoch 4644/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3466 - accuracy: 0.8273\n",
            "Epoch 4644: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3514 - accuracy: 0.8234 - val_loss: 0.3805 - val_accuracy: 0.7764\n",
            "Epoch 4645/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3751 - accuracy: 0.8099\n",
            "Epoch 4645: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3720 - accuracy: 0.8119 - val_loss: 0.3770 - val_accuracy: 0.7893\n",
            "Epoch 4646/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3631 - accuracy: 0.8138\n",
            "Epoch 4646: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3618 - accuracy: 0.8147 - val_loss: 0.4777 - val_accuracy: 0.7178\n",
            "Epoch 4647/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3604 - accuracy: 0.8151\n",
            "Epoch 4647: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3598 - accuracy: 0.8165 - val_loss: 0.5095 - val_accuracy: 0.6895\n",
            "Epoch 4648/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3462 - accuracy: 0.8236\n",
            "Epoch 4648: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3509 - accuracy: 0.8212 - val_loss: 0.9821 - val_accuracy: 0.4608\n",
            "Epoch 4649/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4048 - accuracy: 0.7957\n",
            "Epoch 4649: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3977 - accuracy: 0.7993 - val_loss: 0.6685 - val_accuracy: 0.5936\n",
            "Epoch 4650/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3732 - accuracy: 0.8101\n",
            "Epoch 4650: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3692 - accuracy: 0.8129 - val_loss: 0.6075 - val_accuracy: 0.6234\n",
            "Epoch 4651/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3665 - accuracy: 0.8112\n",
            "Epoch 4651: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3681 - accuracy: 0.8101 - val_loss: 0.4360 - val_accuracy: 0.7433\n",
            "Epoch 4652/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3446 - accuracy: 0.8287\n",
            "Epoch 4652: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3461 - accuracy: 0.8272 - val_loss: 0.3912 - val_accuracy: 0.7727\n",
            "Epoch 4653/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3493 - accuracy: 0.8214\n",
            "Epoch 4653: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3486 - accuracy: 0.8219 - val_loss: 0.4255 - val_accuracy: 0.7551\n",
            "Epoch 4654/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8261\n",
            "Epoch 4654: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3442 - accuracy: 0.8250 - val_loss: 0.4521 - val_accuracy: 0.7340\n",
            "Epoch 4655/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3514 - accuracy: 0.8217\n",
            "Epoch 4655: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3514 - accuracy: 0.8222 - val_loss: 0.5928 - val_accuracy: 0.6276\n",
            "Epoch 4656/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3383 - accuracy: 0.8282\n",
            "Epoch 4656: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3384 - accuracy: 0.8284 - val_loss: 0.7107 - val_accuracy: 0.5745\n",
            "Epoch 4657/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3663 - accuracy: 0.8113\n",
            "Epoch 4657: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3643 - accuracy: 0.8125 - val_loss: 0.4704 - val_accuracy: 0.7163\n",
            "Epoch 4658/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3404 - accuracy: 0.8304\n",
            "Epoch 4658: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3384 - accuracy: 0.8313 - val_loss: 0.4405 - val_accuracy: 0.7487\n",
            "Epoch 4659/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3380 - accuracy: 0.8304\n",
            "Epoch 4659: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3368 - accuracy: 0.8307 - val_loss: 0.4371 - val_accuracy: 0.7430\n",
            "Epoch 4660/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3463 - accuracy: 0.8237\n",
            "Epoch 4660: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3453 - accuracy: 0.8245 - val_loss: 0.4797 - val_accuracy: 0.7046\n",
            "Epoch 4661/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8211\n",
            "Epoch 4661: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3541 - accuracy: 0.8196 - val_loss: 0.8581 - val_accuracy: 0.5030\n",
            "Epoch 4662/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4005 - accuracy: 0.7913\n",
            "Epoch 4662: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3943 - accuracy: 0.7952 - val_loss: 0.6793 - val_accuracy: 0.5644\n",
            "Epoch 4663/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3670 - accuracy: 0.8129\n",
            "Epoch 4663: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3685 - accuracy: 0.8116 - val_loss: 0.3688 - val_accuracy: 0.7909\n",
            "Epoch 4664/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3533 - accuracy: 0.8224\n",
            "Epoch 4664: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3526 - accuracy: 0.8218 - val_loss: 0.3924 - val_accuracy: 0.7652\n",
            "Epoch 4665/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3629 - accuracy: 0.8168\n",
            "Epoch 4665: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3586 - accuracy: 0.8198 - val_loss: 0.4840 - val_accuracy: 0.7053\n",
            "Epoch 4666/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3338 - accuracy: 0.8329\n",
            "Epoch 4666: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3347 - accuracy: 0.8319 - val_loss: 0.5736 - val_accuracy: 0.6421\n",
            "Epoch 4667/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8310\n",
            "Epoch 4667: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3343 - accuracy: 0.8305 - val_loss: 0.4648 - val_accuracy: 0.7237\n",
            "Epoch 4668/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3275 - accuracy: 0.8371\n",
            "Epoch 4668: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3290 - accuracy: 0.8358 - val_loss: 0.5234 - val_accuracy: 0.6695\n",
            "Epoch 4669/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3267 - accuracy: 0.8383\n",
            "Epoch 4669: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3273 - accuracy: 0.8370 - val_loss: 0.5482 - val_accuracy: 0.6570\n",
            "Epoch 4670/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3300 - accuracy: 0.8359\n",
            "Epoch 4670: loss did not improve from 0.32687\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3291 - accuracy: 0.8364 - val_loss: 0.4884 - val_accuracy: 0.7035\n",
            "Epoch 4671/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3263 - accuracy: 0.8386\n",
            "Epoch 4671: loss improved from 0.32687 to 0.32597, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 1s 235ms/step - loss: 0.3260 - accuracy: 0.8387 - val_loss: 0.4506 - val_accuracy: 0.7303\n",
            "Epoch 4672/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8328\n",
            "Epoch 4672: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3317 - accuracy: 0.8330 - val_loss: 0.6146 - val_accuracy: 0.6074\n",
            "Epoch 4673/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8273\n",
            "Epoch 4673: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.3409 - accuracy: 0.8257 - val_loss: 0.3627 - val_accuracy: 0.7928\n",
            "Epoch 4674/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3703 - accuracy: 0.8076\n",
            "Epoch 4674: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3690 - accuracy: 0.8089 - val_loss: 0.5909 - val_accuracy: 0.6269\n",
            "Epoch 4675/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3376 - accuracy: 0.8298\n",
            "Epoch 4675: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3397 - accuracy: 0.8277 - val_loss: 0.8747 - val_accuracy: 0.4918\n",
            "Epoch 4676/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4052 - accuracy: 0.7933\n",
            "Epoch 4676: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3987 - accuracy: 0.7968 - val_loss: 0.5474 - val_accuracy: 0.6550\n",
            "Epoch 4677/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3497 - accuracy: 0.8211\n",
            "Epoch 4677: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3520 - accuracy: 0.8194 - val_loss: 0.4158 - val_accuracy: 0.7566\n",
            "Epoch 4678/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3496 - accuracy: 0.8245\n",
            "Epoch 4678: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3474 - accuracy: 0.8252 - val_loss: 0.4623 - val_accuracy: 0.7228\n",
            "Epoch 4679/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8324\n",
            "Epoch 4679: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3337 - accuracy: 0.8331 - val_loss: 0.4828 - val_accuracy: 0.7125\n",
            "Epoch 4680/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3283 - accuracy: 0.8389\n",
            "Epoch 4680: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3281 - accuracy: 0.8386 - val_loss: 0.4816 - val_accuracy: 0.7123\n",
            "Epoch 4681/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3276 - accuracy: 0.8392\n",
            "Epoch 4681: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3289 - accuracy: 0.8377 - val_loss: 0.4486 - val_accuracy: 0.7373\n",
            "Epoch 4682/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3329 - accuracy: 0.8330\n",
            "Epoch 4682: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3318 - accuracy: 0.8337 - val_loss: 0.4926 - val_accuracy: 0.6983\n",
            "Epoch 4683/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8284\n",
            "Epoch 4683: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3351 - accuracy: 0.8283 - val_loss: 0.6816 - val_accuracy: 0.5782\n",
            "Epoch 4684/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3548 - accuracy: 0.8180\n",
            "Epoch 4684: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3537 - accuracy: 0.8174 - val_loss: 0.5059 - val_accuracy: 0.6930\n",
            "Epoch 4685/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3294 - accuracy: 0.8369\n",
            "Epoch 4685: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3300 - accuracy: 0.8367 - val_loss: 0.4209 - val_accuracy: 0.7573\n",
            "Epoch 4686/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3280 - accuracy: 0.8372\n",
            "Epoch 4686: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3285 - accuracy: 0.8366 - val_loss: 0.5177 - val_accuracy: 0.6858\n",
            "Epoch 4687/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8353\n",
            "Epoch 4687: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3312 - accuracy: 0.8353 - val_loss: 0.4080 - val_accuracy: 0.7711\n",
            "Epoch 4688/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3439 - accuracy: 0.8251\n",
            "Epoch 4688: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3435 - accuracy: 0.8257 - val_loss: 0.5462 - val_accuracy: 0.6605\n",
            "Epoch 4689/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3329 - accuracy: 0.8344\n",
            "Epoch 4689: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3333 - accuracy: 0.8347 - val_loss: 0.4586 - val_accuracy: 0.7316\n",
            "Epoch 4690/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8262\n",
            "Epoch 4690: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3456 - accuracy: 0.8237 - val_loss: 0.6397 - val_accuracy: 0.6028\n",
            "Epoch 4691/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8266\n",
            "Epoch 4691: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3433 - accuracy: 0.8273 - val_loss: 0.5930 - val_accuracy: 0.6342\n",
            "Epoch 4692/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3540 - accuracy: 0.8211\n",
            "Epoch 4692: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3555 - accuracy: 0.8198 - val_loss: 0.4220 - val_accuracy: 0.7558\n",
            "Epoch 4693/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3485 - accuracy: 0.8276\n",
            "Epoch 4693: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3483 - accuracy: 0.8273 - val_loss: 0.4385 - val_accuracy: 0.7496\n",
            "Epoch 4694/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3426 - accuracy: 0.8257\n",
            "Epoch 4694: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3401 - accuracy: 0.8273 - val_loss: 0.5969 - val_accuracy: 0.6296\n",
            "Epoch 4695/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8312\n",
            "Epoch 4695: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3367 - accuracy: 0.8306 - val_loss: 0.4742 - val_accuracy: 0.7198\n",
            "Epoch 4696/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3349 - accuracy: 0.8330\n",
            "Epoch 4696: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3359 - accuracy: 0.8331 - val_loss: 0.5776 - val_accuracy: 0.6412\n",
            "Epoch 4697/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3348 - accuracy: 0.8309\n",
            "Epoch 4697: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3372 - accuracy: 0.8286 - val_loss: 0.8372 - val_accuracy: 0.5049\n",
            "Epoch 4698/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4193 - accuracy: 0.7858\n",
            "Epoch 4698: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.4145 - accuracy: 0.7871 - val_loss: 0.5021 - val_accuracy: 0.6961\n",
            "Epoch 4699/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8210\n",
            "Epoch 4699: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3554 - accuracy: 0.8181 - val_loss: 0.4111 - val_accuracy: 0.7639\n",
            "Epoch 4700/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3421 - accuracy: 0.8301\n",
            "Epoch 4700: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3457 - accuracy: 0.8281 - val_loss: 0.3755 - val_accuracy: 0.7876\n",
            "Epoch 4701/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3510 - accuracy: 0.8219\n",
            "Epoch 4701: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3487 - accuracy: 0.8222 - val_loss: 0.4069 - val_accuracy: 0.7601\n",
            "Epoch 4702/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3629 - accuracy: 0.8131\n",
            "Epoch 4702: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3606 - accuracy: 0.8149 - val_loss: 0.4254 - val_accuracy: 0.7551\n",
            "Epoch 4703/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3622 - accuracy: 0.8151\n",
            "Epoch 4703: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3685 - accuracy: 0.8115 - val_loss: 0.7658 - val_accuracy: 0.5425\n",
            "Epoch 4704/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3559 - accuracy: 0.8174\n",
            "Epoch 4704: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3558 - accuracy: 0.8178 - val_loss: 0.9947 - val_accuracy: 0.4494\n",
            "Epoch 4705/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4323 - accuracy: 0.7767\n",
            "Epoch 4705: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.4233 - accuracy: 0.7805 - val_loss: 0.5924 - val_accuracy: 0.6370\n",
            "Epoch 4706/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3728 - accuracy: 0.8106\n",
            "Epoch 4706: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3751 - accuracy: 0.8084 - val_loss: 0.4631 - val_accuracy: 0.7226\n",
            "Epoch 4707/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3449 - accuracy: 0.8273\n",
            "Epoch 4707: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3447 - accuracy: 0.8262 - val_loss: 0.3870 - val_accuracy: 0.7812\n",
            "Epoch 4708/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3459 - accuracy: 0.8246\n",
            "Epoch 4708: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3446 - accuracy: 0.8254 - val_loss: 0.4597 - val_accuracy: 0.7248\n",
            "Epoch 4709/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3482 - accuracy: 0.8234\n",
            "Epoch 4709: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3484 - accuracy: 0.8239 - val_loss: 0.4538 - val_accuracy: 0.7299\n",
            "Epoch 4710/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3469 - accuracy: 0.8243\n",
            "Epoch 4710: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3519 - accuracy: 0.8216 - val_loss: 0.7214 - val_accuracy: 0.5523\n",
            "Epoch 4711/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3466 - accuracy: 0.8236\n",
            "Epoch 4711: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3479 - accuracy: 0.8233 - val_loss: 0.7953 - val_accuracy: 0.5354\n",
            "Epoch 4712/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3697 - accuracy: 0.8109\n",
            "Epoch 4712: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3660 - accuracy: 0.8134 - val_loss: 0.6979 - val_accuracy: 0.5627\n",
            "Epoch 4713/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3682 - accuracy: 0.8084\n",
            "Epoch 4713: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3660 - accuracy: 0.8094 - val_loss: 0.4777 - val_accuracy: 0.7143\n",
            "Epoch 4714/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3356 - accuracy: 0.8321\n",
            "Epoch 4714: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3374 - accuracy: 0.8312 - val_loss: 0.4436 - val_accuracy: 0.7415\n",
            "Epoch 4715/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8324\n",
            "Epoch 4715: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3360 - accuracy: 0.8320 - val_loss: 0.4311 - val_accuracy: 0.7527\n",
            "Epoch 4716/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3399 - accuracy: 0.8299\n",
            "Epoch 4716: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3405 - accuracy: 0.8297 - val_loss: 0.3441 - val_accuracy: 0.8025\n",
            "Epoch 4717/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4226 - accuracy: 0.7803\n",
            "Epoch 4717: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4183 - accuracy: 0.7829 - val_loss: 0.5727 - val_accuracy: 0.6432\n",
            "Epoch 4718/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3393 - accuracy: 0.8276\n",
            "Epoch 4718: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3446 - accuracy: 0.8238 - val_loss: 0.8541 - val_accuracy: 0.5087\n",
            "Epoch 4719/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3988 - accuracy: 0.7960\n",
            "Epoch 4719: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3938 - accuracy: 0.7986 - val_loss: 0.5359 - val_accuracy: 0.6605\n",
            "Epoch 4720/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8355\n",
            "Epoch 4720: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3339 - accuracy: 0.8357 - val_loss: 0.4549 - val_accuracy: 0.7340\n",
            "Epoch 4721/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3330 - accuracy: 0.8352\n",
            "Epoch 4721: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3382 - accuracy: 0.8309 - val_loss: 0.3511 - val_accuracy: 0.8027\n",
            "Epoch 4722/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3796 - accuracy: 0.8052\n",
            "Epoch 4722: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3748 - accuracy: 0.8091 - val_loss: 0.4646 - val_accuracy: 0.7141\n",
            "Epoch 4723/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3462 - accuracy: 0.8270\n",
            "Epoch 4723: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3467 - accuracy: 0.8271 - val_loss: 0.4446 - val_accuracy: 0.7378\n",
            "Epoch 4724/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3517 - accuracy: 0.8169\n",
            "Epoch 4724: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3547 - accuracy: 0.8162 - val_loss: 0.6530 - val_accuracy: 0.5839\n",
            "Epoch 4725/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8264\n",
            "Epoch 4725: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3436 - accuracy: 0.8263 - val_loss: 0.6528 - val_accuracy: 0.6083\n",
            "Epoch 4726/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3542 - accuracy: 0.8184\n",
            "Epoch 4726: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3523 - accuracy: 0.8204 - val_loss: 0.5447 - val_accuracy: 0.6583\n",
            "Epoch 4727/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8334\n",
            "Epoch 4727: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3334 - accuracy: 0.8337 - val_loss: 0.4818 - val_accuracy: 0.7070\n",
            "Epoch 4728/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8344\n",
            "Epoch 4728: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3324 - accuracy: 0.8342 - val_loss: 0.4803 - val_accuracy: 0.7149\n",
            "Epoch 4729/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8328\n",
            "Epoch 4729: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3330 - accuracy: 0.8333 - val_loss: 0.4732 - val_accuracy: 0.7180\n",
            "Epoch 4730/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3317 - accuracy: 0.8344\n",
            "Epoch 4730: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3329 - accuracy: 0.8340 - val_loss: 0.4675 - val_accuracy: 0.7226\n",
            "Epoch 4731/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8334\n",
            "Epoch 4731: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3336 - accuracy: 0.8330 - val_loss: 0.6231 - val_accuracy: 0.6043\n",
            "Epoch 4732/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3376 - accuracy: 0.8304\n",
            "Epoch 4732: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3367 - accuracy: 0.8313 - val_loss: 0.5686 - val_accuracy: 0.6395\n",
            "Epoch 4733/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3364 - accuracy: 0.8281\n",
            "Epoch 4733: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3362 - accuracy: 0.8293 - val_loss: 0.4667 - val_accuracy: 0.7244\n",
            "Epoch 4734/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3271 - accuracy: 0.8369\n",
            "Epoch 4734: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3267 - accuracy: 0.8371 - val_loss: 0.4858 - val_accuracy: 0.7073\n",
            "Epoch 4735/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3285 - accuracy: 0.8369\n",
            "Epoch 4735: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3287 - accuracy: 0.8375 - val_loss: 0.4905 - val_accuracy: 0.7059\n",
            "Epoch 4736/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8318\n",
            "Epoch 4736: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3328 - accuracy: 0.8323 - val_loss: 0.4150 - val_accuracy: 0.7654\n",
            "Epoch 4737/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3369 - accuracy: 0.8320\n",
            "Epoch 4737: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3373 - accuracy: 0.8318 - val_loss: 0.4454 - val_accuracy: 0.7371\n",
            "Epoch 4738/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3397 - accuracy: 0.8289\n",
            "Epoch 4738: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3405 - accuracy: 0.8290 - val_loss: 0.4629 - val_accuracy: 0.7242\n",
            "Epoch 4739/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8174\n",
            "Epoch 4739: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3666 - accuracy: 0.8129 - val_loss: 0.7747 - val_accuracy: 0.5229\n",
            "Epoch 4740/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3512 - accuracy: 0.8196\n",
            "Epoch 4740: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3562 - accuracy: 0.8177 - val_loss: 0.8561 - val_accuracy: 0.4922\n",
            "Epoch 4741/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3779 - accuracy: 0.8063\n",
            "Epoch 4741: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3733 - accuracy: 0.8088 - val_loss: 0.7497 - val_accuracy: 0.5374\n",
            "Epoch 4742/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3770 - accuracy: 0.8037\n",
            "Epoch 4742: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3781 - accuracy: 0.8032 - val_loss: 0.4058 - val_accuracy: 0.7661\n",
            "Epoch 4743/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8317\n",
            "Epoch 4743: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3372 - accuracy: 0.8324 - val_loss: 0.4415 - val_accuracy: 0.7415\n",
            "Epoch 4744/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8355\n",
            "Epoch 4744: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3345 - accuracy: 0.8342 - val_loss: 0.4141 - val_accuracy: 0.7562\n",
            "Epoch 4745/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3570 - accuracy: 0.8171\n",
            "Epoch 4745: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3551 - accuracy: 0.8195 - val_loss: 0.5913 - val_accuracy: 0.6215\n",
            "Epoch 4746/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8300\n",
            "Epoch 4746: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 125ms/step - loss: 0.3333 - accuracy: 0.8322 - val_loss: 0.4800 - val_accuracy: 0.7095\n",
            "Epoch 4747/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3293 - accuracy: 0.8363\n",
            "Epoch 4747: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3277 - accuracy: 0.8379 - val_loss: 0.4167 - val_accuracy: 0.7601\n",
            "Epoch 4748/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3405 - accuracy: 0.8279\n",
            "Epoch 4748: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3420 - accuracy: 0.8269 - val_loss: 0.5034 - val_accuracy: 0.6862\n",
            "Epoch 4749/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3275 - accuracy: 0.8365\n",
            "Epoch 4749: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3275 - accuracy: 0.8362 - val_loss: 0.5577 - val_accuracy: 0.6397\n",
            "Epoch 4750/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3297 - accuracy: 0.8358\n",
            "Epoch 4750: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3292 - accuracy: 0.8362 - val_loss: 0.5573 - val_accuracy: 0.6452\n",
            "Epoch 4751/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3284 - accuracy: 0.8373\n",
            "Epoch 4751: loss did not improve from 0.32597\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3292 - accuracy: 0.8370 - val_loss: 0.5163 - val_accuracy: 0.6728\n",
            "Epoch 4752/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3262 - accuracy: 0.8390\n",
            "Epoch 4752: loss improved from 0.32597 to 0.32576, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "3/3 [==============================] - 0s 214ms/step - loss: 0.3258 - accuracy: 0.8392 - val_loss: 0.5040 - val_accuracy: 0.6901\n",
            "Epoch 4753/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3291 - accuracy: 0.8367\n",
            "Epoch 4753: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3281 - accuracy: 0.8372 - val_loss: 0.3958 - val_accuracy: 0.7749\n",
            "Epoch 4754/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3483 - accuracy: 0.8169\n",
            "Epoch 4754: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3491 - accuracy: 0.8177 - val_loss: 0.7265 - val_accuracy: 0.5486\n",
            "Epoch 4755/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3538 - accuracy: 0.8199\n",
            "Epoch 4755: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3528 - accuracy: 0.8211 - val_loss: 0.5020 - val_accuracy: 0.6873\n",
            "Epoch 4756/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3271 - accuracy: 0.8370\n",
            "Epoch 4756: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3290 - accuracy: 0.8353 - val_loss: 0.4001 - val_accuracy: 0.7691\n",
            "Epoch 4757/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3476 - accuracy: 0.8215\n",
            "Epoch 4757: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3486 - accuracy: 0.8227 - val_loss: 0.4721 - val_accuracy: 0.7176\n",
            "Epoch 4758/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3357 - accuracy: 0.8289\n",
            "Epoch 4758: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3416 - accuracy: 0.8267 - val_loss: 0.7272 - val_accuracy: 0.5567\n",
            "Epoch 4759/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3524 - accuracy: 0.8205\n",
            "Epoch 4759: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3494 - accuracy: 0.8233 - val_loss: 0.7095 - val_accuracy: 0.5587\n",
            "Epoch 4760/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3610 - accuracy: 0.8150\n",
            "Epoch 4760: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3590 - accuracy: 0.8160 - val_loss: 0.4141 - val_accuracy: 0.7617\n",
            "Epoch 4761/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3395 - accuracy: 0.8281\n",
            "Epoch 4761: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3397 - accuracy: 0.8285 - val_loss: 0.5071 - val_accuracy: 0.6904\n",
            "Epoch 4762/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3303 - accuracy: 0.8349\n",
            "Epoch 4762: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3320 - accuracy: 0.8339 - val_loss: 0.6299 - val_accuracy: 0.6006\n",
            "Epoch 4763/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3467 - accuracy: 0.8249\n",
            "Epoch 4763: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3460 - accuracy: 0.8246 - val_loss: 0.5209 - val_accuracy: 0.6787\n",
            "Epoch 4764/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3349 - accuracy: 0.8313\n",
            "Epoch 4764: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3384 - accuracy: 0.8297 - val_loss: 0.4667 - val_accuracy: 0.7244\n",
            "Epoch 4765/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3324 - accuracy: 0.8373\n",
            "Epoch 4765: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3317 - accuracy: 0.8369 - val_loss: 0.5152 - val_accuracy: 0.6752\n",
            "Epoch 4766/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8279\n",
            "Epoch 4766: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3453 - accuracy: 0.8249 - val_loss: 0.4209 - val_accuracy: 0.7599\n",
            "Epoch 4767/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3419 - accuracy: 0.8299\n",
            "Epoch 4767: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3412 - accuracy: 0.8294 - val_loss: 0.4086 - val_accuracy: 0.7632\n",
            "Epoch 4768/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3615 - accuracy: 0.8147\n",
            "Epoch 4768: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3600 - accuracy: 0.8160 - val_loss: 0.5712 - val_accuracy: 0.6419\n",
            "Epoch 4769/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.8349\n",
            "Epoch 4769: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3315 - accuracy: 0.8349 - val_loss: 0.6447 - val_accuracy: 0.5958\n",
            "Epoch 4770/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3421 - accuracy: 0.8262\n",
            "Epoch 4770: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3405 - accuracy: 0.8277 - val_loss: 0.6416 - val_accuracy: 0.5956\n",
            "Epoch 4771/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3611 - accuracy: 0.8147\n",
            "Epoch 4771: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3627 - accuracy: 0.8129 - val_loss: 0.3809 - val_accuracy: 0.7694\n",
            "Epoch 4772/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3612 - accuracy: 0.8185\n",
            "Epoch 4772: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3586 - accuracy: 0.8202 - val_loss: 0.4827 - val_accuracy: 0.7127\n",
            "Epoch 4773/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8295\n",
            "Epoch 4773: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3365 - accuracy: 0.8302 - val_loss: 0.5575 - val_accuracy: 0.6561\n",
            "Epoch 4774/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3295 - accuracy: 0.8363\n",
            "Epoch 4774: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3310 - accuracy: 0.8345 - val_loss: 0.5107 - val_accuracy: 0.6888\n",
            "Epoch 4775/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3316 - accuracy: 0.8330\n",
            "Epoch 4775: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3297 - accuracy: 0.8351 - val_loss: 0.5942 - val_accuracy: 0.6309\n",
            "Epoch 4776/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8169\n",
            "Epoch 4776: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3539 - accuracy: 0.8178 - val_loss: 0.5121 - val_accuracy: 0.6895\n",
            "Epoch 4777/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3438 - accuracy: 0.8288\n",
            "Epoch 4777: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3509 - accuracy: 0.8243 - val_loss: 0.3320 - val_accuracy: 0.8062\n",
            "Epoch 4778/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4010 - accuracy: 0.7913\n",
            "Epoch 4778: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3959 - accuracy: 0.7942 - val_loss: 0.4276 - val_accuracy: 0.7542\n",
            "Epoch 4779/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8152\n",
            "Epoch 4779: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3645 - accuracy: 0.8140 - val_loss: 0.5607 - val_accuracy: 0.6500\n",
            "Epoch 4780/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3363 - accuracy: 0.8292\n",
            "Epoch 4780: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3393 - accuracy: 0.8272 - val_loss: 0.6625 - val_accuracy: 0.5888\n",
            "Epoch 4781/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3449 - accuracy: 0.8238\n",
            "Epoch 4781: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3440 - accuracy: 0.8232 - val_loss: 0.4816 - val_accuracy: 0.7125\n",
            "Epoch 4782/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3270 - accuracy: 0.8379\n",
            "Epoch 4782: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3268 - accuracy: 0.8372 - val_loss: 0.4668 - val_accuracy: 0.7244\n",
            "Epoch 4783/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3296 - accuracy: 0.8371\n",
            "Epoch 4783: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3296 - accuracy: 0.8376 - val_loss: 0.3569 - val_accuracy: 0.7915\n",
            "Epoch 4784/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3864 - accuracy: 0.7979\n",
            "Epoch 4784: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3854 - accuracy: 0.7995 - val_loss: 0.6022 - val_accuracy: 0.6182\n",
            "Epoch 4785/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3366 - accuracy: 0.8302\n",
            "Epoch 4785: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3383 - accuracy: 0.8288 - val_loss: 0.7146 - val_accuracy: 0.5607\n",
            "Epoch 4786/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3640 - accuracy: 0.8135\n",
            "Epoch 4786: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3610 - accuracy: 0.8153 - val_loss: 0.5909 - val_accuracy: 0.6335\n",
            "Epoch 4787/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3445 - accuracy: 0.8263\n",
            "Epoch 4787: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3439 - accuracy: 0.8271 - val_loss: 0.5163 - val_accuracy: 0.6853\n",
            "Epoch 4788/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8299\n",
            "Epoch 4788: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3410 - accuracy: 0.8287 - val_loss: 0.4317 - val_accuracy: 0.7465\n",
            "Epoch 4789/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8336\n",
            "Epoch 4789: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3375 - accuracy: 0.8321 - val_loss: 0.4044 - val_accuracy: 0.7648\n",
            "Epoch 4790/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8198\n",
            "Epoch 4790: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3554 - accuracy: 0.8201 - val_loss: 0.5359 - val_accuracy: 0.6654\n",
            "Epoch 4791/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3336 - accuracy: 0.8314\n",
            "Epoch 4791: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3350 - accuracy: 0.8308 - val_loss: 0.5756 - val_accuracy: 0.6456\n",
            "Epoch 4792/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3316 - accuracy: 0.8329\n",
            "Epoch 4792: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3321 - accuracy: 0.8328 - val_loss: 0.5459 - val_accuracy: 0.6607\n",
            "Epoch 4793/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8353\n",
            "Epoch 4793: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.3296 - accuracy: 0.8357 - val_loss: 0.6455 - val_accuracy: 0.5980\n",
            "Epoch 4794/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3613 - accuracy: 0.8151\n",
            "Epoch 4794: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3657 - accuracy: 0.8134 - val_loss: 0.3559 - val_accuracy: 0.7920\n",
            "Epoch 4795/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3738 - accuracy: 0.8072\n",
            "Epoch 4795: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3703 - accuracy: 0.8093 - val_loss: 0.4140 - val_accuracy: 0.7623\n",
            "Epoch 4796/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3593 - accuracy: 0.8132\n",
            "Epoch 4796: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3598 - accuracy: 0.8135 - val_loss: 0.6129 - val_accuracy: 0.6169\n",
            "Epoch 4797/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3370 - accuracy: 0.8299\n",
            "Epoch 4797: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3369 - accuracy: 0.8298 - val_loss: 0.6229 - val_accuracy: 0.6057\n",
            "Epoch 4798/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3433 - accuracy: 0.8265\n",
            "Epoch 4798: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3419 - accuracy: 0.8274 - val_loss: 0.4502 - val_accuracy: 0.7356\n",
            "Epoch 4799/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3319 - accuracy: 0.8340\n",
            "Epoch 4799: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3311 - accuracy: 0.8343 - val_loss: 0.5529 - val_accuracy: 0.6544\n",
            "Epoch 4800/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3310 - accuracy: 0.8362\n",
            "Epoch 4800: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3322 - accuracy: 0.8362 - val_loss: 0.4546 - val_accuracy: 0.7354\n",
            "Epoch 4801/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3287 - accuracy: 0.8359\n",
            "Epoch 4801: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3289 - accuracy: 0.8367 - val_loss: 0.5402 - val_accuracy: 0.6596\n",
            "Epoch 4802/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3317 - accuracy: 0.8351\n",
            "Epoch 4802: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3313 - accuracy: 0.8347 - val_loss: 0.5573 - val_accuracy: 0.6588\n",
            "Epoch 4803/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3430 - accuracy: 0.8296\n",
            "Epoch 4803: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3436 - accuracy: 0.8289 - val_loss: 0.4208 - val_accuracy: 0.7538\n",
            "Epoch 4804/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8274\n",
            "Epoch 4804: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3409 - accuracy: 0.8287 - val_loss: 0.4915 - val_accuracy: 0.7024\n",
            "Epoch 4805/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3267 - accuracy: 0.8385\n",
            "Epoch 4805: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3289 - accuracy: 0.8361 - val_loss: 0.5361 - val_accuracy: 0.6680\n",
            "Epoch 4806/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3380 - accuracy: 0.8296\n",
            "Epoch 4806: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3377 - accuracy: 0.8299 - val_loss: 0.4677 - val_accuracy: 0.7198\n",
            "Epoch 4807/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3294 - accuracy: 0.8354\n",
            "Epoch 4807: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3303 - accuracy: 0.8353 - val_loss: 0.4731 - val_accuracy: 0.7259\n",
            "Epoch 4808/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3314 - accuracy: 0.8370\n",
            "Epoch 4808: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3321 - accuracy: 0.8359 - val_loss: 0.5314 - val_accuracy: 0.6730\n",
            "Epoch 4809/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3293 - accuracy: 0.8359\n",
            "Epoch 4809: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3293 - accuracy: 0.8351 - val_loss: 0.4711 - val_accuracy: 0.7303\n",
            "Epoch 4810/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3393 - accuracy: 0.8293\n",
            "Epoch 4810: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3423 - accuracy: 0.8274 - val_loss: 0.6988 - val_accuracy: 0.5708\n",
            "Epoch 4811/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8144\n",
            "Epoch 4811: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3566 - accuracy: 0.8156 - val_loss: 0.4669 - val_accuracy: 0.7160\n",
            "Epoch 4812/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3347 - accuracy: 0.8334\n",
            "Epoch 4812: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3352 - accuracy: 0.8329 - val_loss: 0.4837 - val_accuracy: 0.7027\n",
            "Epoch 4813/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3366 - accuracy: 0.8300\n",
            "Epoch 4813: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3383 - accuracy: 0.8295 - val_loss: 0.7006 - val_accuracy: 0.5695\n",
            "Epoch 4814/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3755 - accuracy: 0.8044\n",
            "Epoch 4814: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3765 - accuracy: 0.8041 - val_loss: 0.4195 - val_accuracy: 0.7571\n",
            "Epoch 4815/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3401 - accuracy: 0.8310\n",
            "Epoch 4815: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.3396 - accuracy: 0.8316 - val_loss: 0.3818 - val_accuracy: 0.7720\n",
            "Epoch 4816/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3912 - accuracy: 0.7994\n",
            "Epoch 4816: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3905 - accuracy: 0.8005 - val_loss: 0.5387 - val_accuracy: 0.6719\n",
            "Epoch 4817/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3379 - accuracy: 0.8287\n",
            "Epoch 4817: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.3388 - accuracy: 0.8285 - val_loss: 0.7059 - val_accuracy: 0.5627\n",
            "Epoch 4818/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3583 - accuracy: 0.8196\n",
            "Epoch 4818: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3569 - accuracy: 0.8192 - val_loss: 0.4694 - val_accuracy: 0.7136\n",
            "Epoch 4819/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8334\n",
            "Epoch 4819: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3343 - accuracy: 0.8319 - val_loss: 0.4549 - val_accuracy: 0.7294\n",
            "Epoch 4820/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3465 - accuracy: 0.8236\n",
            "Epoch 4820: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3463 - accuracy: 0.8236 - val_loss: 0.5388 - val_accuracy: 0.6732\n",
            "Epoch 4821/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3394 - accuracy: 0.8270\n",
            "Epoch 4821: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3387 - accuracy: 0.8274 - val_loss: 0.6304 - val_accuracy: 0.5993\n",
            "Epoch 4822/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3415 - accuracy: 0.8293\n",
            "Epoch 4822: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3414 - accuracy: 0.8290 - val_loss: 0.6582 - val_accuracy: 0.6054\n",
            "Epoch 4823/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4000 - accuracy: 0.7978\n",
            "Epoch 4823: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3997 - accuracy: 0.7973 - val_loss: 0.4898 - val_accuracy: 0.7077\n",
            "Epoch 4824/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3493 - accuracy: 0.8259\n",
            "Epoch 4824: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3582 - accuracy: 0.8192 - val_loss: 0.3497 - val_accuracy: 0.7885\n",
            "Epoch 4825/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3968 - accuracy: 0.7998\n",
            "Epoch 4825: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3918 - accuracy: 0.8019 - val_loss: 0.3767 - val_accuracy: 0.7817\n",
            "Epoch 4826/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4169 - accuracy: 0.7896\n",
            "Epoch 4826: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4104 - accuracy: 0.7931 - val_loss: 0.4577 - val_accuracy: 0.7325\n",
            "Epoch 4827/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3740 - accuracy: 0.8102\n",
            "Epoch 4827: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3769 - accuracy: 0.8081 - val_loss: 0.6600 - val_accuracy: 0.5883\n",
            "Epoch 4828/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3438 - accuracy: 0.8241\n",
            "Epoch 4828: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3440 - accuracy: 0.8241 - val_loss: 0.6723 - val_accuracy: 0.5765\n",
            "Epoch 4829/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8255\n",
            "Epoch 4829: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3412 - accuracy: 0.8268 - val_loss: 0.5822 - val_accuracy: 0.6489\n",
            "Epoch 4830/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3405 - accuracy: 0.8299\n",
            "Epoch 4830: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3401 - accuracy: 0.8297 - val_loss: 0.7076 - val_accuracy: 0.5550\n",
            "Epoch 4831/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3915 - accuracy: 0.7949\n",
            "Epoch 4831: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3926 - accuracy: 0.7950 - val_loss: 0.4562 - val_accuracy: 0.7285\n",
            "Epoch 4832/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3464 - accuracy: 0.8268\n",
            "Epoch 4832: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3531 - accuracy: 0.8221 - val_loss: 0.3468 - val_accuracy: 0.7931\n",
            "Epoch 4833/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3862 - accuracy: 0.8039\n",
            "Epoch 4833: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3819 - accuracy: 0.8071 - val_loss: 0.3771 - val_accuracy: 0.7854\n",
            "Epoch 4834/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3813 - accuracy: 0.8036\n",
            "Epoch 4834: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3785 - accuracy: 0.8068 - val_loss: 0.4304 - val_accuracy: 0.7470\n",
            "Epoch 4835/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3542 - accuracy: 0.8189\n",
            "Epoch 4835: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3542 - accuracy: 0.8196 - val_loss: 0.7921 - val_accuracy: 0.5201\n",
            "Epoch 4836/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3731 - accuracy: 0.8091\n",
            "Epoch 4836: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3686 - accuracy: 0.8115 - val_loss: 0.6744 - val_accuracy: 0.5785\n",
            "Epoch 4837/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3785 - accuracy: 0.8082\n",
            "Epoch 4837: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3791 - accuracy: 0.8070 - val_loss: 0.4965 - val_accuracy: 0.6998\n",
            "Epoch 4838/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3498 - accuracy: 0.8247\n",
            "Epoch 4838: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3538 - accuracy: 0.8222 - val_loss: 0.4192 - val_accuracy: 0.7571\n",
            "Epoch 4839/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3439 - accuracy: 0.8285\n",
            "Epoch 4839: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3512 - accuracy: 0.8230 - val_loss: 0.3547 - val_accuracy: 0.7913\n",
            "Epoch 4840/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3873 - accuracy: 0.8026\n",
            "Epoch 4840: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3817 - accuracy: 0.8058 - val_loss: 0.3887 - val_accuracy: 0.7744\n",
            "Epoch 4841/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3626 - accuracy: 0.8154\n",
            "Epoch 4841: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3611 - accuracy: 0.8166 - val_loss: 0.4319 - val_accuracy: 0.7446\n",
            "Epoch 4842/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3460 - accuracy: 0.8217\n",
            "Epoch 4842: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.3447 - accuracy: 0.8228 - val_loss: 0.4921 - val_accuracy: 0.6985\n",
            "Epoch 4843/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3414 - accuracy: 0.8295\n",
            "Epoch 4843: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3435 - accuracy: 0.8279 - val_loss: 0.6318 - val_accuracy: 0.5986\n",
            "Epoch 4844/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3380 - accuracy: 0.8303\n",
            "Epoch 4844: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3397 - accuracy: 0.8291 - val_loss: 0.5819 - val_accuracy: 0.6353\n",
            "Epoch 4845/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8310\n",
            "Epoch 4845: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3350 - accuracy: 0.8309 - val_loss: 0.4157 - val_accuracy: 0.7623\n",
            "Epoch 4846/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3361 - accuracy: 0.8301\n",
            "Epoch 4846: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3367 - accuracy: 0.8298 - val_loss: 0.4649 - val_accuracy: 0.7202\n",
            "Epoch 4847/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3359 - accuracy: 0.8313\n",
            "Epoch 4847: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3360 - accuracy: 0.8323 - val_loss: 0.4619 - val_accuracy: 0.7228\n",
            "Epoch 4848/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3406 - accuracy: 0.8278\n",
            "Epoch 4848: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3425 - accuracy: 0.8269 - val_loss: 0.6089 - val_accuracy: 0.6151\n",
            "Epoch 4849/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3340 - accuracy: 0.8340\n",
            "Epoch 4849: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3347 - accuracy: 0.8330 - val_loss: 0.5446 - val_accuracy: 0.6561\n",
            "Epoch 4850/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3303 - accuracy: 0.8331\n",
            "Epoch 4850: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3310 - accuracy: 0.8337 - val_loss: 0.5015 - val_accuracy: 0.6930\n",
            "Epoch 4851/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3269 - accuracy: 0.8375\n",
            "Epoch 4851: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3280 - accuracy: 0.8375 - val_loss: 0.4656 - val_accuracy: 0.7211\n",
            "Epoch 4852/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3392 - accuracy: 0.8267\n",
            "Epoch 4852: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3398 - accuracy: 0.8267 - val_loss: 0.5969 - val_accuracy: 0.6283\n",
            "Epoch 4853/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3323 - accuracy: 0.8320\n",
            "Epoch 4853: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3328 - accuracy: 0.8323 - val_loss: 0.6017 - val_accuracy: 0.6212\n",
            "Epoch 4854/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8306\n",
            "Epoch 4854: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.3358 - accuracy: 0.8312 - val_loss: 0.6753 - val_accuracy: 0.5868\n",
            "Epoch 4855/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3780 - accuracy: 0.8032\n",
            "Epoch 4855: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3777 - accuracy: 0.8043 - val_loss: 0.4996 - val_accuracy: 0.6912\n",
            "Epoch 4856/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3382 - accuracy: 0.8317\n",
            "Epoch 4856: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3436 - accuracy: 0.8272 - val_loss: 0.3286 - val_accuracy: 0.8146\n",
            "Epoch 4857/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3758 - accuracy: 0.8068\n",
            "Epoch 4857: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3729 - accuracy: 0.8076 - val_loss: 0.3971 - val_accuracy: 0.7606\n",
            "Epoch 4858/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4139 - accuracy: 0.7890\n",
            "Epoch 4858: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4093 - accuracy: 0.7913 - val_loss: 0.4900 - val_accuracy: 0.7112\n",
            "Epoch 4859/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8208\n",
            "Epoch 4859: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3570 - accuracy: 0.8188 - val_loss: 0.9638 - val_accuracy: 0.4624\n",
            "Epoch 4860/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4060 - accuracy: 0.7947\n",
            "Epoch 4860: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3988 - accuracy: 0.7983 - val_loss: 0.6828 - val_accuracy: 0.5811\n",
            "Epoch 4861/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8161\n",
            "Epoch 4861: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3582 - accuracy: 0.8175 - val_loss: 0.5803 - val_accuracy: 0.6406\n",
            "Epoch 4862/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3538 - accuracy: 0.8199\n",
            "Epoch 4862: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.3540 - accuracy: 0.8189 - val_loss: 0.5052 - val_accuracy: 0.6998\n",
            "Epoch 4863/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3424 - accuracy: 0.8284\n",
            "Epoch 4863: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3445 - accuracy: 0.8267 - val_loss: 0.4270 - val_accuracy: 0.7496\n",
            "Epoch 4864/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3400 - accuracy: 0.8300\n",
            "Epoch 4864: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.3407 - accuracy: 0.8284 - val_loss: 0.3789 - val_accuracy: 0.7784\n",
            "Epoch 4865/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3818 - accuracy: 0.8021\n",
            "Epoch 4865: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3781 - accuracy: 0.8054 - val_loss: 0.4168 - val_accuracy: 0.7544\n",
            "Epoch 4866/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8155\n",
            "Epoch 4866: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3654 - accuracy: 0.8125 - val_loss: 0.6589 - val_accuracy: 0.5901\n",
            "Epoch 4867/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3450 - accuracy: 0.8248\n",
            "Epoch 4867: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3449 - accuracy: 0.8255 - val_loss: 0.5503 - val_accuracy: 0.6566\n",
            "Epoch 4868/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3324 - accuracy: 0.8338\n",
            "Epoch 4868: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.3329 - accuracy: 0.8334 - val_loss: 0.5434 - val_accuracy: 0.6649\n",
            "Epoch 4869/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3313 - accuracy: 0.8337\n",
            "Epoch 4869: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3305 - accuracy: 0.8345 - val_loss: 0.5280 - val_accuracy: 0.6772\n",
            "Epoch 4870/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8355\n",
            "Epoch 4870: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3322 - accuracy: 0.8347 - val_loss: 0.5106 - val_accuracy: 0.6899\n",
            "Epoch 4871/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8357\n",
            "Epoch 4871: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3296 - accuracy: 0.8371 - val_loss: 0.4801 - val_accuracy: 0.7035\n",
            "Epoch 4872/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3274 - accuracy: 0.8357\n",
            "Epoch 4872: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3288 - accuracy: 0.8355 - val_loss: 0.5864 - val_accuracy: 0.6182\n",
            "Epoch 4873/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8334\n",
            "Epoch 4873: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3324 - accuracy: 0.8334 - val_loss: 0.5807 - val_accuracy: 0.6344\n",
            "Epoch 4874/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8286\n",
            "Epoch 4874: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3413 - accuracy: 0.8296 - val_loss: 0.5435 - val_accuracy: 0.6651\n",
            "Epoch 4875/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8339\n",
            "Epoch 4875: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3359 - accuracy: 0.8327 - val_loss: 0.3737 - val_accuracy: 0.7891\n",
            "Epoch 4876/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3498 - accuracy: 0.8230\n",
            "Epoch 4876: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3491 - accuracy: 0.8235 - val_loss: 0.4104 - val_accuracy: 0.7615\n",
            "Epoch 4877/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3705 - accuracy: 0.8089\n",
            "Epoch 4877: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3711 - accuracy: 0.8096 - val_loss: 0.5264 - val_accuracy: 0.6711\n",
            "Epoch 4878/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3408 - accuracy: 0.8261\n",
            "Epoch 4878: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3483 - accuracy: 0.8218 - val_loss: 0.9327 - val_accuracy: 0.4689\n",
            "Epoch 4879/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3881 - accuracy: 0.8023\n",
            "Epoch 4879: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3828 - accuracy: 0.8057 - val_loss: 0.7955 - val_accuracy: 0.5216\n",
            "Epoch 4880/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3838 - accuracy: 0.8008\n",
            "Epoch 4880: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3778 - accuracy: 0.8040 - val_loss: 0.8257 - val_accuracy: 0.5124\n",
            "Epoch 4881/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4518 - accuracy: 0.7678\n",
            "Epoch 4881: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.4501 - accuracy: 0.7682 - val_loss: 0.4682 - val_accuracy: 0.7248\n",
            "Epoch 4882/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3576 - accuracy: 0.8223\n",
            "Epoch 4882: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3584 - accuracy: 0.8216 - val_loss: 0.5131 - val_accuracy: 0.6860\n",
            "Epoch 4883/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3514 - accuracy: 0.8229\n",
            "Epoch 4883: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.3563 - accuracy: 0.8194 - val_loss: 0.3802 - val_accuracy: 0.7814\n",
            "Epoch 4884/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3500 - accuracy: 0.8264\n",
            "Epoch 4884: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3527 - accuracy: 0.8240 - val_loss: 0.3659 - val_accuracy: 0.7849\n",
            "Epoch 4885/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3758 - accuracy: 0.8074\n",
            "Epoch 4885: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3698 - accuracy: 0.8108 - val_loss: 0.3834 - val_accuracy: 0.7784\n",
            "Epoch 4886/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3815 - accuracy: 0.8057\n",
            "Epoch 4886: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3781 - accuracy: 0.8077 - val_loss: 0.4265 - val_accuracy: 0.7564\n",
            "Epoch 4887/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3588 - accuracy: 0.8177\n",
            "Epoch 4887: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3590 - accuracy: 0.8178 - val_loss: 0.5171 - val_accuracy: 0.6847\n",
            "Epoch 4888/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3342 - accuracy: 0.8329\n",
            "Epoch 4888: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3347 - accuracy: 0.8331 - val_loss: 0.5960 - val_accuracy: 0.6243\n",
            "Epoch 4889/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8340\n",
            "Epoch 4889: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3339 - accuracy: 0.8333 - val_loss: 0.5203 - val_accuracy: 0.6689\n",
            "Epoch 4890/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3289 - accuracy: 0.8335\n",
            "Epoch 4890: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3285 - accuracy: 0.8338 - val_loss: 0.5208 - val_accuracy: 0.6741\n",
            "Epoch 4891/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3268 - accuracy: 0.8373\n",
            "Epoch 4891: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3281 - accuracy: 0.8370 - val_loss: 0.4622 - val_accuracy: 0.7182\n",
            "Epoch 4892/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3281 - accuracy: 0.8355\n",
            "Epoch 4892: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3271 - accuracy: 0.8364 - val_loss: 0.4898 - val_accuracy: 0.6998\n",
            "Epoch 4893/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3261 - accuracy: 0.8374\n",
            "Epoch 4893: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3275 - accuracy: 0.8363 - val_loss: 0.5364 - val_accuracy: 0.6542\n",
            "Epoch 4894/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3260 - accuracy: 0.8410\n",
            "Epoch 4894: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3274 - accuracy: 0.8390 - val_loss: 0.5066 - val_accuracy: 0.6831\n",
            "Epoch 4895/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3329 - accuracy: 0.8341\n",
            "Epoch 4895: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3340 - accuracy: 0.8326 - val_loss: 0.4111 - val_accuracy: 0.7678\n",
            "Epoch 4896/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3348 - accuracy: 0.8315\n",
            "Epoch 4896: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3345 - accuracy: 0.8318 - val_loss: 0.4274 - val_accuracy: 0.7494\n",
            "Epoch 4897/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8299\n",
            "Epoch 4897: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3334 - accuracy: 0.8313 - val_loss: 0.4598 - val_accuracy: 0.7233\n",
            "Epoch 4898/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3301 - accuracy: 0.8344\n",
            "Epoch 4898: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3317 - accuracy: 0.8337 - val_loss: 0.5536 - val_accuracy: 0.6493\n",
            "Epoch 4899/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3290 - accuracy: 0.8354\n",
            "Epoch 4899: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3294 - accuracy: 0.8354 - val_loss: 0.6186 - val_accuracy: 0.6147\n",
            "Epoch 4900/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3381 - accuracy: 0.8310\n",
            "Epoch 4900: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3373 - accuracy: 0.8326 - val_loss: 0.6062 - val_accuracy: 0.6149\n",
            "Epoch 4901/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3455 - accuracy: 0.8249\n",
            "Epoch 4901: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3465 - accuracy: 0.8234 - val_loss: 0.4398 - val_accuracy: 0.7413\n",
            "Epoch 4902/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3282 - accuracy: 0.8368\n",
            "Epoch 4902: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3289 - accuracy: 0.8365 - val_loss: 0.4646 - val_accuracy: 0.7294\n",
            "Epoch 4903/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3273 - accuracy: 0.8392\n",
            "Epoch 4903: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3263 - accuracy: 0.8392 - val_loss: 0.4241 - val_accuracy: 0.7582\n",
            "Epoch 4904/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3317 - accuracy: 0.8339\n",
            "Epoch 4904: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3324 - accuracy: 0.8330 - val_loss: 0.5288 - val_accuracy: 0.6741\n",
            "Epoch 4905/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3264 - accuracy: 0.8372\n",
            "Epoch 4905: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3267 - accuracy: 0.8369 - val_loss: 0.5923 - val_accuracy: 0.6300\n",
            "Epoch 4906/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3436 - accuracy: 0.8254\n",
            "Epoch 4906: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3456 - accuracy: 0.8245 - val_loss: 0.4127 - val_accuracy: 0.7604\n",
            "Epoch 4907/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3323 - accuracy: 0.8337\n",
            "Epoch 4907: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3310 - accuracy: 0.8344 - val_loss: 0.4115 - val_accuracy: 0.7619\n",
            "Epoch 4908/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3398 - accuracy: 0.8295\n",
            "Epoch 4908: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3384 - accuracy: 0.8312 - val_loss: 0.5402 - val_accuracy: 0.6629\n",
            "Epoch 4909/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3267 - accuracy: 0.8379\n",
            "Epoch 4909: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3282 - accuracy: 0.8358 - val_loss: 0.5371 - val_accuracy: 0.6638\n",
            "Epoch 4910/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3352 - accuracy: 0.8311\n",
            "Epoch 4910: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3341 - accuracy: 0.8312 - val_loss: 0.4379 - val_accuracy: 0.7419\n",
            "Epoch 4911/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3268 - accuracy: 0.8391\n",
            "Epoch 4911: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3277 - accuracy: 0.8369 - val_loss: 0.4396 - val_accuracy: 0.7465\n",
            "Epoch 4912/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3332 - accuracy: 0.8349\n",
            "Epoch 4912: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3329 - accuracy: 0.8353 - val_loss: 0.4243 - val_accuracy: 0.7569\n",
            "Epoch 4913/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8286\n",
            "Epoch 4913: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3363 - accuracy: 0.8280 - val_loss: 0.6091 - val_accuracy: 0.6050\n",
            "Epoch 4914/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8282\n",
            "Epoch 4914: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3369 - accuracy: 0.8279 - val_loss: 0.4900 - val_accuracy: 0.6996\n",
            "Epoch 4915/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3273 - accuracy: 0.8383\n",
            "Epoch 4915: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3274 - accuracy: 0.8376 - val_loss: 0.4230 - val_accuracy: 0.7485\n",
            "Epoch 4916/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8313\n",
            "Epoch 4916: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3325 - accuracy: 0.8303 - val_loss: 0.5420 - val_accuracy: 0.6627\n",
            "Epoch 4917/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3250 - accuracy: 0.8362\n",
            "Epoch 4917: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3272 - accuracy: 0.8354 - val_loss: 0.7089 - val_accuracy: 0.5567\n",
            "Epoch 4918/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3608 - accuracy: 0.8123\n",
            "Epoch 4918: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3583 - accuracy: 0.8140 - val_loss: 0.5268 - val_accuracy: 0.6801\n",
            "Epoch 4919/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3368 - accuracy: 0.8311\n",
            "Epoch 4919: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3386 - accuracy: 0.8297 - val_loss: 0.3955 - val_accuracy: 0.7740\n",
            "Epoch 4920/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3463 - accuracy: 0.8252\n",
            "Epoch 4920: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3445 - accuracy: 0.8267 - val_loss: 0.5065 - val_accuracy: 0.6895\n",
            "Epoch 4921/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3284 - accuracy: 0.8372\n",
            "Epoch 4921: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3304 - accuracy: 0.8356 - val_loss: 0.5924 - val_accuracy: 0.6278\n",
            "Epoch 4922/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3344 - accuracy: 0.8308\n",
            "Epoch 4922: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3337 - accuracy: 0.8311 - val_loss: 0.3902 - val_accuracy: 0.7759\n",
            "Epoch 4923/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3606 - accuracy: 0.8146\n",
            "Epoch 4923: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3614 - accuracy: 0.8155 - val_loss: 0.5153 - val_accuracy: 0.6820\n",
            "Epoch 4924/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3317 - accuracy: 0.8324\n",
            "Epoch 4924: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3370 - accuracy: 0.8300 - val_loss: 0.7439 - val_accuracy: 0.5460\n",
            "Epoch 4925/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3562 - accuracy: 0.8175\n",
            "Epoch 4925: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3539 - accuracy: 0.8197 - val_loss: 0.6129 - val_accuracy: 0.6177\n",
            "Epoch 4926/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3434 - accuracy: 0.8269\n",
            "Epoch 4926: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3452 - accuracy: 0.8249 - val_loss: 0.3774 - val_accuracy: 0.7801\n",
            "Epoch 4927/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3595 - accuracy: 0.8157\n",
            "Epoch 4927: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3565 - accuracy: 0.8187 - val_loss: 0.4874 - val_accuracy: 0.7051\n",
            "Epoch 4928/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3301 - accuracy: 0.8320\n",
            "Epoch 4928: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3302 - accuracy: 0.8323 - val_loss: 0.5249 - val_accuracy: 0.6785\n",
            "Epoch 4929/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3288 - accuracy: 0.8355\n",
            "Epoch 4929: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3308 - accuracy: 0.8341 - val_loss: 0.5218 - val_accuracy: 0.6822\n",
            "Epoch 4930/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3269 - accuracy: 0.8391\n",
            "Epoch 4930: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3276 - accuracy: 0.8383 - val_loss: 0.4729 - val_accuracy: 0.7101\n",
            "Epoch 4931/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3275 - accuracy: 0.8360\n",
            "Epoch 4931: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3268 - accuracy: 0.8371 - val_loss: 0.7223 - val_accuracy: 0.5466\n",
            "Epoch 4932/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3770 - accuracy: 0.8067\n",
            "Epoch 4932: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.3804 - accuracy: 0.8049 - val_loss: 0.3602 - val_accuracy: 0.7900\n",
            "Epoch 4933/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3602 - accuracy: 0.8166\n",
            "Epoch 4933: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3575 - accuracy: 0.8179 - val_loss: 0.3980 - val_accuracy: 0.7672\n",
            "Epoch 4934/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3627 - accuracy: 0.8147\n",
            "Epoch 4934: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3621 - accuracy: 0.8154 - val_loss: 0.6480 - val_accuracy: 0.5951\n",
            "Epoch 4935/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3423 - accuracy: 0.8259\n",
            "Epoch 4935: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3428 - accuracy: 0.8246 - val_loss: 0.4747 - val_accuracy: 0.7156\n",
            "Epoch 4936/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3265 - accuracy: 0.8395\n",
            "Epoch 4936: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.3274 - accuracy: 0.8389 - val_loss: 0.4603 - val_accuracy: 0.7266\n",
            "Epoch 4937/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8360\n",
            "Epoch 4937: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.3310 - accuracy: 0.8355 - val_loss: 0.3766 - val_accuracy: 0.7876\n",
            "Epoch 4938/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3768 - accuracy: 0.8056\n",
            "Epoch 4938: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 0.3763 - accuracy: 0.8061 - val_loss: 0.5306 - val_accuracy: 0.6675\n",
            "Epoch 4939/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3315 - accuracy: 0.8309\n",
            "Epoch 4939: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3355 - accuracy: 0.8291 - val_loss: 0.7673 - val_accuracy: 0.5363\n",
            "Epoch 4940/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3603 - accuracy: 0.8148\n",
            "Epoch 4940: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3586 - accuracy: 0.8165 - val_loss: 0.5221 - val_accuracy: 0.6792\n",
            "Epoch 4941/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3320 - accuracy: 0.8330\n",
            "Epoch 4941: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3324 - accuracy: 0.8334 - val_loss: 0.5090 - val_accuracy: 0.6833\n",
            "Epoch 4942/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8362\n",
            "Epoch 4942: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3330 - accuracy: 0.8342 - val_loss: 0.3898 - val_accuracy: 0.7773\n",
            "Epoch 4943/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3431 - accuracy: 0.8261\n",
            "Epoch 4943: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3410 - accuracy: 0.8273 - val_loss: 0.5156 - val_accuracy: 0.6781\n",
            "Epoch 4944/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3291 - accuracy: 0.8374\n",
            "Epoch 4944: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3285 - accuracy: 0.8373 - val_loss: 0.4609 - val_accuracy: 0.7244\n",
            "Epoch 4945/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3303 - accuracy: 0.8355\n",
            "Epoch 4945: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.3298 - accuracy: 0.8360 - val_loss: 0.4996 - val_accuracy: 0.6941\n",
            "Epoch 4946/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3308 - accuracy: 0.8339\n",
            "Epoch 4946: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.3305 - accuracy: 0.8338 - val_loss: 0.5470 - val_accuracy: 0.6623\n",
            "Epoch 4947/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3327 - accuracy: 0.8355\n",
            "Epoch 4947: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3334 - accuracy: 0.8340 - val_loss: 0.3855 - val_accuracy: 0.7792\n",
            "Epoch 4948/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3434 - accuracy: 0.8239\n",
            "Epoch 4948: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.3417 - accuracy: 0.8263 - val_loss: 0.5654 - val_accuracy: 0.6476\n",
            "Epoch 4949/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3271 - accuracy: 0.8388\n",
            "Epoch 4949: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3267 - accuracy: 0.8386 - val_loss: 0.4899 - val_accuracy: 0.7046\n",
            "Epoch 4950/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3260 - accuracy: 0.8360\n",
            "Epoch 4950: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.3276 - accuracy: 0.8345 - val_loss: 0.4080 - val_accuracy: 0.7661\n",
            "Epoch 4951/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3407 - accuracy: 0.8275\n",
            "Epoch 4951: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.3381 - accuracy: 0.8300 - val_loss: 0.4739 - val_accuracy: 0.7130\n",
            "Epoch 4952/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3333 - accuracy: 0.8331\n",
            "Epoch 4952: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.3344 - accuracy: 0.8326 - val_loss: 0.6260 - val_accuracy: 0.6072\n",
            "Epoch 4953/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3324 - accuracy: 0.8327\n",
            "Epoch 4953: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.3315 - accuracy: 0.8334 - val_loss: 0.6132 - val_accuracy: 0.6195\n",
            "Epoch 4954/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8242\n",
            "Epoch 4954: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3429 - accuracy: 0.8254 - val_loss: 0.4992 - val_accuracy: 0.6919\n",
            "Epoch 4955/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3321 - accuracy: 0.8361\n",
            "Epoch 4955: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3326 - accuracy: 0.8349 - val_loss: 0.3521 - val_accuracy: 0.8045\n",
            "Epoch 4956/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3656 - accuracy: 0.8084\n",
            "Epoch 4956: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.3649 - accuracy: 0.8093 - val_loss: 0.7991 - val_accuracy: 0.5128\n",
            "Epoch 4957/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3799 - accuracy: 0.8017\n",
            "Epoch 4957: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3756 - accuracy: 0.8050 - val_loss: 0.4541 - val_accuracy: 0.7285\n",
            "Epoch 4958/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3313 - accuracy: 0.8370\n",
            "Epoch 4958: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3337 - accuracy: 0.8348 - val_loss: 0.4071 - val_accuracy: 0.7577\n",
            "Epoch 4959/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3537 - accuracy: 0.8197\n",
            "Epoch 4959: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3523 - accuracy: 0.8202 - val_loss: 0.4837 - val_accuracy: 0.7090\n",
            "Epoch 4960/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8328\n",
            "Epoch 4960: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.3347 - accuracy: 0.8322 - val_loss: 0.7565 - val_accuracy: 0.5403\n",
            "Epoch 4961/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3685 - accuracy: 0.8113\n",
            "Epoch 4961: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3660 - accuracy: 0.8129 - val_loss: 0.4510 - val_accuracy: 0.7299\n",
            "Epoch 4962/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3306 - accuracy: 0.8341\n",
            "Epoch 4962: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3292 - accuracy: 0.8343 - val_loss: 0.5438 - val_accuracy: 0.6570\n",
            "Epoch 4963/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3292 - accuracy: 0.8378\n",
            "Epoch 4963: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.3306 - accuracy: 0.8357 - val_loss: 0.4177 - val_accuracy: 0.7553\n",
            "Epoch 4964/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3362 - accuracy: 0.8296\n",
            "Epoch 4964: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3366 - accuracy: 0.8278 - val_loss: 0.4641 - val_accuracy: 0.7242\n",
            "Epoch 4965/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3312 - accuracy: 0.8323\n",
            "Epoch 4965: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3327 - accuracy: 0.8317 - val_loss: 0.5847 - val_accuracy: 0.6373\n",
            "Epoch 4966/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8313\n",
            "Epoch 4966: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3323 - accuracy: 0.8324 - val_loss: 0.4705 - val_accuracy: 0.7171\n",
            "Epoch 4967/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3328 - accuracy: 0.8320\n",
            "Epoch 4967: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3330 - accuracy: 0.8321 - val_loss: 0.4963 - val_accuracy: 0.6996\n",
            "Epoch 4968/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3343 - accuracy: 0.8317\n",
            "Epoch 4968: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3349 - accuracy: 0.8318 - val_loss: 0.5115 - val_accuracy: 0.6862\n",
            "Epoch 4969/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3335 - accuracy: 0.8310\n",
            "Epoch 4969: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.3330 - accuracy: 0.8314 - val_loss: 0.5846 - val_accuracy: 0.6294\n",
            "Epoch 4970/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3319 - accuracy: 0.8340\n",
            "Epoch 4970: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3316 - accuracy: 0.8345 - val_loss: 0.4556 - val_accuracy: 0.7288\n",
            "Epoch 4971/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3260 - accuracy: 0.8363\n",
            "Epoch 4971: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3273 - accuracy: 0.8360 - val_loss: 0.5549 - val_accuracy: 0.6599\n",
            "Epoch 4972/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3359 - accuracy: 0.8310\n",
            "Epoch 4972: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3356 - accuracy: 0.8303 - val_loss: 0.4049 - val_accuracy: 0.7608\n",
            "Epoch 4973/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3433 - accuracy: 0.8264\n",
            "Epoch 4973: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.3438 - accuracy: 0.8258 - val_loss: 0.4574 - val_accuracy: 0.7268\n",
            "Epoch 4974/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3458 - accuracy: 0.8224\n",
            "Epoch 4974: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3501 - accuracy: 0.8206 - val_loss: 0.8530 - val_accuracy: 0.5003\n",
            "Epoch 4975/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3959 - accuracy: 0.7948\n",
            "Epoch 4975: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3935 - accuracy: 0.7969 - val_loss: 0.4793 - val_accuracy: 0.7077\n",
            "Epoch 4976/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3426 - accuracy: 0.8278\n",
            "Epoch 4976: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3466 - accuracy: 0.8249 - val_loss: 0.4210 - val_accuracy: 0.7384\n",
            "Epoch 4977/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8273\n",
            "Epoch 4977: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3503 - accuracy: 0.8282 - val_loss: 0.4664 - val_accuracy: 0.7211\n",
            "Epoch 4978/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3337 - accuracy: 0.8320\n",
            "Epoch 4978: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.3322 - accuracy: 0.8328 - val_loss: 0.5276 - val_accuracy: 0.6814\n",
            "Epoch 4979/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3285 - accuracy: 0.8371\n",
            "Epoch 4979: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3276 - accuracy: 0.8373 - val_loss: 0.4960 - val_accuracy: 0.6930\n",
            "Epoch 4980/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3256 - accuracy: 0.8371\n",
            "Epoch 4980: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3278 - accuracy: 0.8351 - val_loss: 0.4150 - val_accuracy: 0.7623\n",
            "Epoch 4981/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3418 - accuracy: 0.8244\n",
            "Epoch 4981: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3418 - accuracy: 0.8242 - val_loss: 0.7046 - val_accuracy: 0.5607\n",
            "Epoch 4982/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3707 - accuracy: 0.8071\n",
            "Epoch 4982: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.3760 - accuracy: 0.8046 - val_loss: 0.3526 - val_accuracy: 0.7983\n",
            "Epoch 4983/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3841 - accuracy: 0.8038\n",
            "Epoch 4983: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3815 - accuracy: 0.8049 - val_loss: 0.5846 - val_accuracy: 0.6329\n",
            "Epoch 4984/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3351 - accuracy: 0.8312\n",
            "Epoch 4984: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3357 - accuracy: 0.8301 - val_loss: 0.5006 - val_accuracy: 0.6928\n",
            "Epoch 4985/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3322 - accuracy: 0.8333\n",
            "Epoch 4985: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3328 - accuracy: 0.8320 - val_loss: 0.4106 - val_accuracy: 0.7566\n",
            "Epoch 4986/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3503 - accuracy: 0.8242\n",
            "Epoch 4986: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.3503 - accuracy: 0.8238 - val_loss: 0.6829 - val_accuracy: 0.5848\n",
            "Epoch 4987/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3591 - accuracy: 0.8167\n",
            "Epoch 4987: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3585 - accuracy: 0.8157 - val_loss: 0.4688 - val_accuracy: 0.7250\n",
            "Epoch 4988/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3361 - accuracy: 0.8324\n",
            "Epoch 4988: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3362 - accuracy: 0.8322 - val_loss: 0.3925 - val_accuracy: 0.7740\n",
            "Epoch 4989/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3523 - accuracy: 0.8200\n",
            "Epoch 4989: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3536 - accuracy: 0.8200 - val_loss: 0.6346 - val_accuracy: 0.6046\n",
            "Epoch 4990/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3402 - accuracy: 0.8276\n",
            "Epoch 4990: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3386 - accuracy: 0.8285 - val_loss: 0.5255 - val_accuracy: 0.6803\n",
            "Epoch 4991/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3365 - accuracy: 0.8320\n",
            "Epoch 4991: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3387 - accuracy: 0.8301 - val_loss: 0.3953 - val_accuracy: 0.7676\n",
            "Epoch 4992/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3441 - accuracy: 0.8270\n",
            "Epoch 4992: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3414 - accuracy: 0.8284 - val_loss: 0.4536 - val_accuracy: 0.7364\n",
            "Epoch 4993/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3387 - accuracy: 0.8262\n",
            "Epoch 4993: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3378 - accuracy: 0.8286 - val_loss: 0.5846 - val_accuracy: 0.6370\n",
            "Epoch 4994/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3302 - accuracy: 0.8346\n",
            "Epoch 4994: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.3292 - accuracy: 0.8357 - val_loss: 0.5909 - val_accuracy: 0.6199\n",
            "Epoch 4995/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8309\n",
            "Epoch 4995: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3357 - accuracy: 0.8301 - val_loss: 0.4490 - val_accuracy: 0.7327\n",
            "Epoch 4996/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3267 - accuracy: 0.8386\n",
            "Epoch 4996: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.3288 - accuracy: 0.8375 - val_loss: 0.3962 - val_accuracy: 0.7724\n",
            "Epoch 4997/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3455 - accuracy: 0.8253\n",
            "Epoch 4997: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3439 - accuracy: 0.8273 - val_loss: 0.4410 - val_accuracy: 0.7479\n",
            "Epoch 4998/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3641 - accuracy: 0.8147\n",
            "Epoch 4998: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.3740 - accuracy: 0.8092 - val_loss: 1.0719 - val_accuracy: 0.4409\n",
            "Epoch 4999/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4239 - accuracy: 0.7892\n",
            "Epoch 4999: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.4157 - accuracy: 0.7940 - val_loss: 0.7655 - val_accuracy: 0.5488\n",
            "Epoch 5000/5000\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3947 - accuracy: 0.7982\n",
            "Epoch 5000: loss did not improve from 0.32576\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.3947 - accuracy: 0.7969 - val_loss: 0.4449 - val_accuracy: 0.7354\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 2.2428 - accuracy: 0.4989\n",
            "[2.242753505706787, 0.4989221692085266]\n",
            "131/131 [==============================] - 0s 3ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.64      0.62      0.63      2848\n",
            "     class 1       0.23      0.24      0.23      1327\n",
            "\n",
            "    accuracy                           0.50      4175\n",
            "   macro avg       0.43      0.43      0.43      4175\n",
            "weighted avg       0.51      0.50      0.50      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "712/712 [==============================] - 2s 3ms/step - loss: 0.3718 - accuracy: 0.8074\n",
            "[0.3717667758464813, 0.8074356913566589]\n",
            "712/712 [==============================] - 1s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.77      0.87      0.82     11391\n",
            "     class 1       0.85      0.75      0.80     11391\n",
            "\n",
            "    accuracy                           0.81     22782\n",
            "   macro avg       0.81      0.81      0.81     22782\n",
            "weighted avg       0.81      0.81      0.81     22782\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.best"
      ],
      "metadata": {
        "id": "PioVDan-fUtJ",
        "outputId": "85137219-b0cf-4e43-a756-aba6f86ee740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3257596492767334"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Best Model Test Only***#\n",
        "#--------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_2', 'class 1']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E7RFuvZPfVAO",
        "outputId": "ffa1f1d0-38e0-48d1-99c1-9c739aea5f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 2.3065 - accuracy: 0.5092\n",
            "[2.306478261947632, 0.5092215538024902]\n",
            "131/131 [==============================] - 0s 1ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.64      0.65      0.64      2848\n",
            "     class 1       0.22      0.21      0.21      1327\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.43      0.43      0.43      4175\n",
            "weighted avg       0.50      0.51      0.51      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "712/712 [==============================] - 1s 2ms/step - loss: 0.3603 - accuracy: 0.8089\n",
            "[0.36029955744743347, 0.8088842034339905]\n",
            "712/712 [==============================] - 1s 1ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.76      0.90      0.83     11391\n",
            "     class 1       0.88      0.71      0.79     11391\n",
            "\n",
            "    accuracy                           0.81     22782\n",
            "   macro avg       0.82      0.81      0.81     22782\n",
            "weighted avg       0.82      0.81      0.81     22782\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/new_df/best_model_by_class1\")"
      ],
      "metadata": {
        "id": "VWrKfxRiRWiB",
        "outputId": "cb0526fa-d54c-4726-a591-062631d5fec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}
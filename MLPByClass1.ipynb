{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpOjXKdWmiEo7smVva6evV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vajihe-ameri/predict-software-bugs-in-java-classes/blob/main/MLPByClass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7jHTbeotRQLV",
        "outputId": "efa315ec-5b3d-4a38-dee4-04d4b323697a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (0.0.post7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn pandas\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_1.csv\")\n",
        "train_features = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_1.csv\")\n",
        "test_target = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_NB_1.csv\")\n",
        "train_target = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_NB_1.csv\")"
      ],
      "metadata": {
        "id": "k-NJh-s4RVqK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons\n",
        "f_measure = tensorflow_addons.metrics.F1Score(num_classes=2, average='macro', threshold=0.5)"
      ],
      "metadata": {
        "id": "-MTCXh0lRWCF",
        "outputId": "8f2461dc-30c3-4516-8d61-11161fd428e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "file_path = \"/content/drive/MyDrive/new_df/best_model_by_class1.hdf5\""
      ],
      "metadata": {
        "id": "ttn14_jcRWZh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu', input_dim = train_features.shape[1]))\n",
        "model.add(Dense(80, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(60, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(40, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(20, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1,save_best_only=True, mode='auto', period=1)"
      ],
      "metadata": {
        "id": "feauoHLTRWcM",
        "outputId": "345aa617-67aa-4b26-d7e7-2756312bb2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 600, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_2', 'class 1']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFZoAhH6RWfE",
        "outputId": "3209c700-a0f0-467d-819c-e63ea5ecc268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3155 - accuracy: 0.8403\n",
            "Epoch 3752: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3153 - accuracy: 0.8402 - val_loss: 0.4238 - val_accuracy: 0.7492\n",
            "Epoch 3753/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3225 - accuracy: 0.8349\n",
            "Epoch 3753: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3214 - accuracy: 0.8353 - val_loss: 0.3796 - val_accuracy: 0.7790\n",
            "Epoch 3754/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8386\n",
            "Epoch 3754: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3121 - accuracy: 0.8386 - val_loss: 0.4536 - val_accuracy: 0.7174\n",
            "Epoch 3755/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3062 - accuracy: 0.8427\n",
            "Epoch 3755: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3067 - accuracy: 0.8424 - val_loss: 0.4431 - val_accuracy: 0.7437\n",
            "Epoch 3756/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3086 - accuracy: 0.8401\n",
            "Epoch 3756: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3099 - accuracy: 0.8387 - val_loss: 0.3610 - val_accuracy: 0.7830\n",
            "Epoch 3757/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.8327\n",
            "Epoch 3757: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3271 - accuracy: 0.8327 - val_loss: 0.5707 - val_accuracy: 0.6568\n",
            "Epoch 3758/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8466\n",
            "Epoch 3758: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3046 - accuracy: 0.8452 - val_loss: 0.4316 - val_accuracy: 0.7323\n",
            "Epoch 3759/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3356 - accuracy: 0.8297\n",
            "Epoch 3759: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3341 - accuracy: 0.8301 - val_loss: 0.3914 - val_accuracy: 0.7691\n",
            "Epoch 3760/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.8363\n",
            "Epoch 3760: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3189 - accuracy: 0.8363 - val_loss: 0.4090 - val_accuracy: 0.7612\n",
            "Epoch 3761/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.8411\n",
            "Epoch 3761: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3097 - accuracy: 0.8409 - val_loss: 0.3995 - val_accuracy: 0.7615\n",
            "Epoch 3762/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3136 - accuracy: 0.8379\n",
            "Epoch 3762: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3137 - accuracy: 0.8376 - val_loss: 0.4461 - val_accuracy: 0.7285\n",
            "Epoch 3763/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8436\n",
            "Epoch 3763: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3052 - accuracy: 0.8433 - val_loss: 0.4719 - val_accuracy: 0.7132\n",
            "Epoch 3764/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3127 - accuracy: 0.8401\n",
            "Epoch 3764: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3137 - accuracy: 0.8393 - val_loss: 0.4526 - val_accuracy: 0.7321\n",
            "Epoch 3765/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3033 - accuracy: 0.8455\n",
            "Epoch 3765: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3044 - accuracy: 0.8445 - val_loss: 0.4239 - val_accuracy: 0.7536\n",
            "Epoch 3766/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3134 - accuracy: 0.8409\n",
            "Epoch 3766: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3142 - accuracy: 0.8401 - val_loss: 0.4118 - val_accuracy: 0.7615\n",
            "Epoch 3767/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3067 - accuracy: 0.8432\n",
            "Epoch 3767: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3063 - accuracy: 0.8438 - val_loss: 0.3869 - val_accuracy: 0.7670\n",
            "Epoch 3768/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8396\n",
            "Epoch 3768: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3151 - accuracy: 0.8396 - val_loss: 0.4972 - val_accuracy: 0.7141\n",
            "Epoch 3769/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8376\n",
            "Epoch 3769: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 28ms/step - loss: 0.3198 - accuracy: 0.8376 - val_loss: 0.4237 - val_accuracy: 0.7509\n",
            "Epoch 3770/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3047 - accuracy: 0.8453\n",
            "Epoch 3770: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3062 - accuracy: 0.8441 - val_loss: 0.4240 - val_accuracy: 0.7443\n",
            "Epoch 3771/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3058 - accuracy: 0.8448\n",
            "Epoch 3771: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3051 - accuracy: 0.8446 - val_loss: 0.4737 - val_accuracy: 0.7165\n",
            "Epoch 3772/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3077 - accuracy: 0.8442\n",
            "Epoch 3772: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3091 - accuracy: 0.8430 - val_loss: 0.3757 - val_accuracy: 0.7830\n",
            "Epoch 3773/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3173 - accuracy: 0.8383\n",
            "Epoch 3773: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 27ms/step - loss: 0.3163 - accuracy: 0.8392 - val_loss: 0.4368 - val_accuracy: 0.7345\n",
            "Epoch 3774/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8389\n",
            "Epoch 3774: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3102 - accuracy: 0.8390 - val_loss: 0.4011 - val_accuracy: 0.7648\n",
            "Epoch 3775/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8429\n",
            "Epoch 3775: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3060 - accuracy: 0.8424 - val_loss: 0.4279 - val_accuracy: 0.7435\n",
            "Epoch 3776/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3046 - accuracy: 0.8455\n",
            "Epoch 3776: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3061 - accuracy: 0.8447 - val_loss: 0.4184 - val_accuracy: 0.7419\n",
            "Epoch 3777/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3168 - accuracy: 0.8380\n",
            "Epoch 3777: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3163 - accuracy: 0.8382 - val_loss: 0.6189 - val_accuracy: 0.6458\n",
            "Epoch 3778/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.8359\n",
            "Epoch 3778: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3170 - accuracy: 0.8359 - val_loss: 0.3551 - val_accuracy: 0.7966\n",
            "Epoch 3779/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3093 - accuracy: 0.8399\n",
            "Epoch 3779: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3086 - accuracy: 0.8408 - val_loss: 0.4692 - val_accuracy: 0.7132\n",
            "Epoch 3780/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8434\n",
            "Epoch 3780: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3037 - accuracy: 0.8434 - val_loss: 0.5329 - val_accuracy: 0.6748\n",
            "Epoch 3781/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.8449\n",
            "Epoch 3781: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3044 - accuracy: 0.8449 - val_loss: 0.4601 - val_accuracy: 0.7202\n",
            "Epoch 3782/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3052 - accuracy: 0.8440\n",
            "Epoch 3782: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3053 - accuracy: 0.8433 - val_loss: 0.4252 - val_accuracy: 0.7553\n",
            "Epoch 3783/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8434\n",
            "Epoch 3783: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3057 - accuracy: 0.8432 - val_loss: 0.3933 - val_accuracy: 0.7632\n",
            "Epoch 3784/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3119 - accuracy: 0.8397\n",
            "Epoch 3784: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3120 - accuracy: 0.8390 - val_loss: 0.4503 - val_accuracy: 0.7323\n",
            "Epoch 3785/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3066 - accuracy: 0.8444\n",
            "Epoch 3785: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3064 - accuracy: 0.8448 - val_loss: 0.4216 - val_accuracy: 0.7411\n",
            "Epoch 3786/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3047 - accuracy: 0.8434\n",
            "Epoch 3786: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3041 - accuracy: 0.8441 - val_loss: 0.4144 - val_accuracy: 0.7527\n",
            "Epoch 3787/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8424\n",
            "Epoch 3787: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3065 - accuracy: 0.8422 - val_loss: 0.4096 - val_accuracy: 0.7606\n",
            "Epoch 3788/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.8460\n",
            "Epoch 3788: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3021 - accuracy: 0.8443 - val_loss: 0.3770 - val_accuracy: 0.7795\n",
            "Epoch 3789/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3219 - accuracy: 0.8368\n",
            "Epoch 3789: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3217 - accuracy: 0.8364 - val_loss: 0.3922 - val_accuracy: 0.7683\n",
            "Epoch 3790/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3261 - accuracy: 0.8328\n",
            "Epoch 3790: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3267 - accuracy: 0.8324 - val_loss: 0.6020 - val_accuracy: 0.6456\n",
            "Epoch 3791/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3142 - accuracy: 0.8373\n",
            "Epoch 3791: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3134 - accuracy: 0.8379 - val_loss: 0.4928 - val_accuracy: 0.6869\n",
            "Epoch 3792/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3065 - accuracy: 0.8442\n",
            "Epoch 3792: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3081 - accuracy: 0.8433 - val_loss: 0.3981 - val_accuracy: 0.7652\n",
            "Epoch 3793/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3108 - accuracy: 0.8411\n",
            "Epoch 3793: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3108 - accuracy: 0.8408 - val_loss: 0.4584 - val_accuracy: 0.7200\n",
            "Epoch 3794/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8382\n",
            "Epoch 3794: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3129 - accuracy: 0.8381 - val_loss: 0.4072 - val_accuracy: 0.7623\n",
            "Epoch 3795/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3090 - accuracy: 0.8414\n",
            "Epoch 3795: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3093 - accuracy: 0.8405 - val_loss: 0.4047 - val_accuracy: 0.7606\n",
            "Epoch 3796/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.8403\n",
            "Epoch 3796: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3137 - accuracy: 0.8403 - val_loss: 0.5232 - val_accuracy: 0.6831\n",
            "Epoch 3797/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.8390\n",
            "Epoch 3797: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3113 - accuracy: 0.8384 - val_loss: 0.3999 - val_accuracy: 0.7665\n",
            "Epoch 3798/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3172 - accuracy: 0.8379\n",
            "Epoch 3798: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3167 - accuracy: 0.8380 - val_loss: 0.5867 - val_accuracy: 0.6575\n",
            "Epoch 3799/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.8407\n",
            "Epoch 3799: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3113 - accuracy: 0.8406 - val_loss: 0.4233 - val_accuracy: 0.7501\n",
            "Epoch 3800/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.8399\n",
            "Epoch 3800: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3089 - accuracy: 0.8401 - val_loss: 0.4684 - val_accuracy: 0.7171\n",
            "Epoch 3801/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.8370\n",
            "Epoch 3801: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3144 - accuracy: 0.8370 - val_loss: 0.3866 - val_accuracy: 0.7759\n",
            "Epoch 3802/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8407\n",
            "Epoch 3802: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3127 - accuracy: 0.8408 - val_loss: 0.3652 - val_accuracy: 0.7935\n",
            "Epoch 3803/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.8410\n",
            "Epoch 3803: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3090 - accuracy: 0.8410 - val_loss: 0.4353 - val_accuracy: 0.7364\n",
            "Epoch 3804/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3088 - accuracy: 0.8401\n",
            "Epoch 3804: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3102 - accuracy: 0.8388 - val_loss: 0.4425 - val_accuracy: 0.7358\n",
            "Epoch 3805/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3066 - accuracy: 0.8429\n",
            "Epoch 3805: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3082 - accuracy: 0.8413 - val_loss: 0.4595 - val_accuracy: 0.7176\n",
            "Epoch 3806/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8409\n",
            "Epoch 3806: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3082 - accuracy: 0.8408 - val_loss: 0.4154 - val_accuracy: 0.7533\n",
            "Epoch 3807/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3081 - accuracy: 0.8426\n",
            "Epoch 3807: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3084 - accuracy: 0.8420 - val_loss: 0.5454 - val_accuracy: 0.6739\n",
            "Epoch 3808/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3083 - accuracy: 0.8411\n",
            "Epoch 3808: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3094 - accuracy: 0.8403 - val_loss: 0.4671 - val_accuracy: 0.7158\n",
            "Epoch 3809/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3065 - accuracy: 0.8433\n",
            "Epoch 3809: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3077 - accuracy: 0.8425 - val_loss: 0.4111 - val_accuracy: 0.7551\n",
            "Epoch 3810/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3083 - accuracy: 0.8443\n",
            "Epoch 3810: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3103 - accuracy: 0.8435 - val_loss: 0.3845 - val_accuracy: 0.7727\n",
            "Epoch 3811/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8409\n",
            "Epoch 3811: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3132 - accuracy: 0.8410 - val_loss: 0.3983 - val_accuracy: 0.7687\n",
            "Epoch 3812/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3070 - accuracy: 0.8413\n",
            "Epoch 3812: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3078 - accuracy: 0.8416 - val_loss: 0.4047 - val_accuracy: 0.7632\n",
            "Epoch 3813/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3090 - accuracy: 0.8422\n",
            "Epoch 3813: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3096 - accuracy: 0.8422 - val_loss: 0.4735 - val_accuracy: 0.7204\n",
            "Epoch 3814/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3134 - accuracy: 0.8382\n",
            "Epoch 3814: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3138 - accuracy: 0.8378 - val_loss: 0.3590 - val_accuracy: 0.7909\n",
            "Epoch 3815/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8378\n",
            "Epoch 3815: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3198 - accuracy: 0.8378 - val_loss: 0.5930 - val_accuracy: 0.6456\n",
            "Epoch 3816/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8371\n",
            "Epoch 3816: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3195 - accuracy: 0.8371 - val_loss: 0.4003 - val_accuracy: 0.7612\n",
            "Epoch 3817/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3076 - accuracy: 0.8412\n",
            "Epoch 3817: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3085 - accuracy: 0.8414 - val_loss: 0.4720 - val_accuracy: 0.7143\n",
            "Epoch 3818/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3081 - accuracy: 0.8420\n",
            "Epoch 3818: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3088 - accuracy: 0.8417 - val_loss: 0.3902 - val_accuracy: 0.7720\n",
            "Epoch 3819/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3117 - accuracy: 0.8405\n",
            "Epoch 3819: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3119 - accuracy: 0.8405 - val_loss: 0.4451 - val_accuracy: 0.7318\n",
            "Epoch 3820/5000\n",
            "26/31 [========================>.....] - ETA: 0s - loss: 0.3161 - accuracy: 0.8426\n",
            "Epoch 3820: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3207 - accuracy: 0.8388 - val_loss: 0.3944 - val_accuracy: 0.7623\n",
            "Epoch 3821/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8437\n",
            "Epoch 3821: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3135 - accuracy: 0.8440 - val_loss: 0.4395 - val_accuracy: 0.7389\n",
            "Epoch 3822/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.8390\n",
            "Epoch 3822: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3138 - accuracy: 0.8390 - val_loss: 0.4122 - val_accuracy: 0.7503\n",
            "Epoch 3823/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3073 - accuracy: 0.8418\n",
            "Epoch 3823: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3082 - accuracy: 0.8413 - val_loss: 0.4774 - val_accuracy: 0.7149\n",
            "Epoch 3824/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8421\n",
            "Epoch 3824: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3077 - accuracy: 0.8421 - val_loss: 0.4538 - val_accuracy: 0.7134\n",
            "Epoch 3825/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.8408\n",
            "Epoch 3825: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3097 - accuracy: 0.8407 - val_loss: 0.4390 - val_accuracy: 0.7386\n",
            "Epoch 3826/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3131 - accuracy: 0.8399\n",
            "Epoch 3826: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3140 - accuracy: 0.8392 - val_loss: 0.4329 - val_accuracy: 0.7261\n",
            "Epoch 3827/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8389\n",
            "Epoch 3827: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3142 - accuracy: 0.8384 - val_loss: 0.3815 - val_accuracy: 0.7784\n",
            "Epoch 3828/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.8382\n",
            "Epoch 3828: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3131 - accuracy: 0.8385 - val_loss: 0.4128 - val_accuracy: 0.7533\n",
            "Epoch 3829/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3114 - accuracy: 0.8392\n",
            "Epoch 3829: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3117 - accuracy: 0.8392 - val_loss: 0.4762 - val_accuracy: 0.7281\n",
            "Epoch 3830/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8330\n",
            "Epoch 3830: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3222 - accuracy: 0.8333 - val_loss: 0.4451 - val_accuracy: 0.7255\n",
            "Epoch 3831/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8417\n",
            "Epoch 3831: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3138 - accuracy: 0.8408 - val_loss: 0.3729 - val_accuracy: 0.7854\n",
            "Epoch 3832/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3088 - accuracy: 0.8421\n",
            "Epoch 3832: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3085 - accuracy: 0.8426 - val_loss: 0.3893 - val_accuracy: 0.7764\n",
            "Epoch 3833/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3251 - accuracy: 0.8355\n",
            "Epoch 3833: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3238 - accuracy: 0.8362 - val_loss: 0.4543 - val_accuracy: 0.7382\n",
            "Epoch 3834/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3077 - accuracy: 0.8420\n",
            "Epoch 3834: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3086 - accuracy: 0.8408 - val_loss: 0.4999 - val_accuracy: 0.6847\n",
            "Epoch 3835/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8419\n",
            "Epoch 3835: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3071 - accuracy: 0.8422 - val_loss: 0.4192 - val_accuracy: 0.7531\n",
            "Epoch 3836/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3019 - accuracy: 0.8463\n",
            "Epoch 3836: loss did not improve from 0.30181\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3044 - accuracy: 0.8441 - val_loss: 0.4258 - val_accuracy: 0.7373\n",
            "Epoch 3837/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2986 - accuracy: 0.8460\n",
            "Epoch 3837: loss improved from 0.30181 to 0.30062, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3006 - accuracy: 0.8443 - val_loss: 0.4678 - val_accuracy: 0.7088\n",
            "Epoch 3838/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3051 - accuracy: 0.8435\n",
            "Epoch 3838: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3063 - accuracy: 0.8420 - val_loss: 0.4453 - val_accuracy: 0.7288\n",
            "Epoch 3839/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8392\n",
            "Epoch 3839: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3125 - accuracy: 0.8398 - val_loss: 0.4681 - val_accuracy: 0.7158\n",
            "Epoch 3840/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3124 - accuracy: 0.8405\n",
            "Epoch 3840: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3130 - accuracy: 0.8404 - val_loss: 0.4962 - val_accuracy: 0.6976\n",
            "Epoch 3841/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8409\n",
            "Epoch 3841: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3079 - accuracy: 0.8408 - val_loss: 0.4379 - val_accuracy: 0.7406\n",
            "Epoch 3842/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8341\n",
            "Epoch 3842: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3246 - accuracy: 0.8346 - val_loss: 0.3941 - val_accuracy: 0.7544\n",
            "Epoch 3843/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3153 - accuracy: 0.8386\n",
            "Epoch 3843: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3167 - accuracy: 0.8382 - val_loss: 0.5527 - val_accuracy: 0.6564\n",
            "Epoch 3844/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3204 - accuracy: 0.8334\n",
            "Epoch 3844: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3209 - accuracy: 0.8333 - val_loss: 0.3467 - val_accuracy: 0.8049\n",
            "Epoch 3845/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.8392\n",
            "Epoch 3845: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3155 - accuracy: 0.8392 - val_loss: 0.4652 - val_accuracy: 0.7016\n",
            "Epoch 3846/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3118 - accuracy: 0.8426\n",
            "Epoch 3846: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3111 - accuracy: 0.8425 - val_loss: 0.4508 - val_accuracy: 0.7327\n",
            "Epoch 3847/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.8337\n",
            "Epoch 3847: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3238 - accuracy: 0.8334 - val_loss: 0.4251 - val_accuracy: 0.7492\n",
            "Epoch 3848/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8444\n",
            "Epoch 3848: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3112 - accuracy: 0.8435 - val_loss: 0.4136 - val_accuracy: 0.7483\n",
            "Epoch 3849/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3020 - accuracy: 0.8433\n",
            "Epoch 3849: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3028 - accuracy: 0.8429 - val_loss: 0.4566 - val_accuracy: 0.7272\n",
            "Epoch 3850/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3041 - accuracy: 0.8446\n",
            "Epoch 3850: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3066 - accuracy: 0.8432 - val_loss: 0.4677 - val_accuracy: 0.7143\n",
            "Epoch 3851/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3200 - accuracy: 0.8361\n",
            "Epoch 3851: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3189 - accuracy: 0.8370 - val_loss: 0.3816 - val_accuracy: 0.7779\n",
            "Epoch 3852/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3103 - accuracy: 0.8423\n",
            "Epoch 3852: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3117 - accuracy: 0.8413 - val_loss: 0.4365 - val_accuracy: 0.7380\n",
            "Epoch 3853/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8408\n",
            "Epoch 3853: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3128 - accuracy: 0.8404 - val_loss: 0.4552 - val_accuracy: 0.7165\n",
            "Epoch 3854/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3105 - accuracy: 0.8384\n",
            "Epoch 3854: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3108 - accuracy: 0.8380 - val_loss: 0.3833 - val_accuracy: 0.7817\n",
            "Epoch 3855/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3084 - accuracy: 0.8408\n",
            "Epoch 3855: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3083 - accuracy: 0.8411 - val_loss: 0.4748 - val_accuracy: 0.7228\n",
            "Epoch 3856/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8428\n",
            "Epoch 3856: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3058 - accuracy: 0.8427 - val_loss: 0.4309 - val_accuracy: 0.7367\n",
            "Epoch 3857/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3099 - accuracy: 0.8413\n",
            "Epoch 3857: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3114 - accuracy: 0.8405 - val_loss: 0.5057 - val_accuracy: 0.6897\n",
            "Epoch 3858/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8395\n",
            "Epoch 3858: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3143 - accuracy: 0.8393 - val_loss: 0.4936 - val_accuracy: 0.6965\n",
            "Epoch 3859/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8420\n",
            "Epoch 3859: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3087 - accuracy: 0.8420 - val_loss: 0.4466 - val_accuracy: 0.7336\n",
            "Epoch 3860/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8428\n",
            "Epoch 3860: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3080 - accuracy: 0.8429 - val_loss: 0.4493 - val_accuracy: 0.7360\n",
            "Epoch 3861/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.8407\n",
            "Epoch 3861: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3138 - accuracy: 0.8407 - val_loss: 0.4038 - val_accuracy: 0.7645\n",
            "Epoch 3862/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.8396\n",
            "Epoch 3862: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3134 - accuracy: 0.8396 - val_loss: 0.3956 - val_accuracy: 0.7659\n",
            "Epoch 3863/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3096 - accuracy: 0.8415\n",
            "Epoch 3863: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3090 - accuracy: 0.8424 - val_loss: 0.4764 - val_accuracy: 0.7156\n",
            "Epoch 3864/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3144 - accuracy: 0.8394\n",
            "Epoch 3864: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3140 - accuracy: 0.8395 - val_loss: 0.3530 - val_accuracy: 0.7968\n",
            "Epoch 3865/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8422\n",
            "Epoch 3865: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3120 - accuracy: 0.8422 - val_loss: 0.4372 - val_accuracy: 0.7448\n",
            "Epoch 3866/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8412\n",
            "Epoch 3866: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3112 - accuracy: 0.8412 - val_loss: 0.5313 - val_accuracy: 0.7117\n",
            "Epoch 3867/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8397\n",
            "Epoch 3867: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3192 - accuracy: 0.8397 - val_loss: 0.3779 - val_accuracy: 0.7801\n",
            "Epoch 3868/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.8452\n",
            "Epoch 3868: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3062 - accuracy: 0.8448 - val_loss: 0.4078 - val_accuracy: 0.7577\n",
            "Epoch 3869/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3051 - accuracy: 0.8433\n",
            "Epoch 3869: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3056 - accuracy: 0.8430 - val_loss: 0.4694 - val_accuracy: 0.7106\n",
            "Epoch 3870/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3028 - accuracy: 0.8427\n",
            "Epoch 3870: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3048 - accuracy: 0.8414 - val_loss: 0.4024 - val_accuracy: 0.7619\n",
            "Epoch 3871/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.8384\n",
            "Epoch 3871: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3126 - accuracy: 0.8385 - val_loss: 0.5753 - val_accuracy: 0.6559\n",
            "Epoch 3872/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3173 - accuracy: 0.8365\n",
            "Epoch 3872: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3179 - accuracy: 0.8370 - val_loss: 0.4460 - val_accuracy: 0.7340\n",
            "Epoch 3873/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3180 - accuracy: 0.8382\n",
            "Epoch 3873: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3184 - accuracy: 0.8382 - val_loss: 0.4029 - val_accuracy: 0.7612\n",
            "Epoch 3874/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3178 - accuracy: 0.8376\n",
            "Epoch 3874: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3181 - accuracy: 0.8374 - val_loss: 0.4443 - val_accuracy: 0.7428\n",
            "Epoch 3875/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3149 - accuracy: 0.8383\n",
            "Epoch 3875: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3145 - accuracy: 0.8385 - val_loss: 0.4085 - val_accuracy: 0.7562\n",
            "Epoch 3876/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3144 - accuracy: 0.8364\n",
            "Epoch 3876: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3146 - accuracy: 0.8363 - val_loss: 0.4514 - val_accuracy: 0.7138\n",
            "Epoch 3877/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8436\n",
            "Epoch 3877: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3055 - accuracy: 0.8436 - val_loss: 0.3538 - val_accuracy: 0.8001\n",
            "Epoch 3878/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3073 - accuracy: 0.8447\n",
            "Epoch 3878: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3073 - accuracy: 0.8442 - val_loss: 0.3623 - val_accuracy: 0.7913\n",
            "Epoch 3879/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3022 - accuracy: 0.8445\n",
            "Epoch 3879: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3029 - accuracy: 0.8443 - val_loss: 0.4004 - val_accuracy: 0.7661\n",
            "Epoch 3880/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2990 - accuracy: 0.8475\n",
            "Epoch 3880: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3010 - accuracy: 0.8454 - val_loss: 0.4261 - val_accuracy: 0.7389\n",
            "Epoch 3881/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8463\n",
            "Epoch 3881: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3048 - accuracy: 0.8455 - val_loss: 0.5756 - val_accuracy: 0.6504\n",
            "Epoch 3882/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3145 - accuracy: 0.8360\n",
            "Epoch 3882: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3147 - accuracy: 0.8358 - val_loss: 0.3752 - val_accuracy: 0.7852\n",
            "Epoch 3883/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8408\n",
            "Epoch 3883: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3102 - accuracy: 0.8408 - val_loss: 0.4710 - val_accuracy: 0.7268\n",
            "Epoch 3884/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.8425\n",
            "Epoch 3884: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3098 - accuracy: 0.8422 - val_loss: 0.4194 - val_accuracy: 0.7492\n",
            "Epoch 3885/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3077 - accuracy: 0.8419\n",
            "Epoch 3885: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3080 - accuracy: 0.8414 - val_loss: 0.5657 - val_accuracy: 0.6535\n",
            "Epoch 3886/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8357\n",
            "Epoch 3886: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3215 - accuracy: 0.8355 - val_loss: 0.3856 - val_accuracy: 0.7702\n",
            "Epoch 3887/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3251 - accuracy: 0.8345\n",
            "Epoch 3887: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3247 - accuracy: 0.8343 - val_loss: 0.5457 - val_accuracy: 0.6855\n",
            "Epoch 3888/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8380\n",
            "Epoch 3888: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3175 - accuracy: 0.8380 - val_loss: 0.4173 - val_accuracy: 0.7538\n",
            "Epoch 3889/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3045 - accuracy: 0.8433\n",
            "Epoch 3889: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3065 - accuracy: 0.8419 - val_loss: 0.4519 - val_accuracy: 0.7231\n",
            "Epoch 3890/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3033 - accuracy: 0.8455\n",
            "Epoch 3890: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3039 - accuracy: 0.8452 - val_loss: 0.4227 - val_accuracy: 0.7509\n",
            "Epoch 3891/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3045 - accuracy: 0.8431\n",
            "Epoch 3891: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3046 - accuracy: 0.8429 - val_loss: 0.3581 - val_accuracy: 0.7972\n",
            "Epoch 3892/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3102 - accuracy: 0.8413\n",
            "Epoch 3892: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3110 - accuracy: 0.8410 - val_loss: 0.5237 - val_accuracy: 0.6853\n",
            "Epoch 3893/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.8413\n",
            "Epoch 3893: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3100 - accuracy: 0.8416 - val_loss: 0.5240 - val_accuracy: 0.7020\n",
            "Epoch 3894/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.8391\n",
            "Epoch 3894: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3124 - accuracy: 0.8391 - val_loss: 0.3964 - val_accuracy: 0.7702\n",
            "Epoch 3895/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.8445\n",
            "Epoch 3895: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3051 - accuracy: 0.8432 - val_loss: 0.4286 - val_accuracy: 0.7426\n",
            "Epoch 3896/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3216 - accuracy: 0.8348\n",
            "Epoch 3896: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3221 - accuracy: 0.8345 - val_loss: 0.4882 - val_accuracy: 0.6897\n",
            "Epoch 3897/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.8358\n",
            "Epoch 3897: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3214 - accuracy: 0.8358 - val_loss: 0.4805 - val_accuracy: 0.7193\n",
            "Epoch 3898/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3353 - accuracy: 0.8296\n",
            "Epoch 3898: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3391 - accuracy: 0.8280 - val_loss: 0.4183 - val_accuracy: 0.7454\n",
            "Epoch 3899/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.8386\n",
            "Epoch 3899: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3174 - accuracy: 0.8384 - val_loss: 0.5236 - val_accuracy: 0.6783\n",
            "Epoch 3900/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3079 - accuracy: 0.8410\n",
            "Epoch 3900: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3076 - accuracy: 0.8408 - val_loss: 0.3804 - val_accuracy: 0.7768\n",
            "Epoch 3901/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8437\n",
            "Epoch 3901: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3041 - accuracy: 0.8429 - val_loss: 0.4342 - val_accuracy: 0.7391\n",
            "Epoch 3902/5000\n",
            "26/31 [========================>.....] - ETA: 0s - loss: 0.3037 - accuracy: 0.8458\n",
            "Epoch 3902: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.3053 - accuracy: 0.8446 - val_loss: 0.4988 - val_accuracy: 0.7040\n",
            "Epoch 3903/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3069 - accuracy: 0.8427\n",
            "Epoch 3903: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3058 - accuracy: 0.8435 - val_loss: 0.5397 - val_accuracy: 0.6711\n",
            "Epoch 3904/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3092 - accuracy: 0.8402\n",
            "Epoch 3904: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3091 - accuracy: 0.8400 - val_loss: 0.4329 - val_accuracy: 0.7419\n",
            "Epoch 3905/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3292 - accuracy: 0.8326\n",
            "Epoch 3905: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3301 - accuracy: 0.8320 - val_loss: 0.4415 - val_accuracy: 0.7239\n",
            "Epoch 3906/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3215 - accuracy: 0.8367\n",
            "Epoch 3906: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3219 - accuracy: 0.8367 - val_loss: 0.4451 - val_accuracy: 0.7411\n",
            "Epoch 3907/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8348\n",
            "Epoch 3907: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3229 - accuracy: 0.8346 - val_loss: 0.5229 - val_accuracy: 0.6669\n",
            "Epoch 3908/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3114 - accuracy: 0.8391\n",
            "Epoch 3908: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3114 - accuracy: 0.8391 - val_loss: 0.4449 - val_accuracy: 0.7259\n",
            "Epoch 3909/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8343\n",
            "Epoch 3909: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3155 - accuracy: 0.8342 - val_loss: 0.3977 - val_accuracy: 0.7648\n",
            "Epoch 3910/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3175 - accuracy: 0.8352\n",
            "Epoch 3910: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3173 - accuracy: 0.8357 - val_loss: 0.4944 - val_accuracy: 0.7022\n",
            "Epoch 3911/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3109 - accuracy: 0.8405\n",
            "Epoch 3911: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3129 - accuracy: 0.8391 - val_loss: 0.6029 - val_accuracy: 0.6649\n",
            "Epoch 3912/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3254 - accuracy: 0.8330\n",
            "Epoch 3912: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3220 - accuracy: 0.8347 - val_loss: 0.5342 - val_accuracy: 0.6952\n",
            "Epoch 3913/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3076 - accuracy: 0.8424\n",
            "Epoch 3913: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3086 - accuracy: 0.8415 - val_loss: 0.5368 - val_accuracy: 0.6748\n",
            "Epoch 3914/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8354\n",
            "Epoch 3914: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3199 - accuracy: 0.8354 - val_loss: 0.3645 - val_accuracy: 0.7867\n",
            "Epoch 3915/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8323\n",
            "Epoch 3915: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 30ms/step - loss: 0.3213 - accuracy: 0.8315 - val_loss: 0.3983 - val_accuracy: 0.7610\n",
            "Epoch 3916/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3096 - accuracy: 0.8397\n",
            "Epoch 3916: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3094 - accuracy: 0.8398 - val_loss: 0.4313 - val_accuracy: 0.7459\n",
            "Epoch 3917/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.8441\n",
            "Epoch 3917: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3119 - accuracy: 0.8443 - val_loss: 0.4638 - val_accuracy: 0.7167\n",
            "Epoch 3918/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3039 - accuracy: 0.8437\n",
            "Epoch 3918: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3049 - accuracy: 0.8432 - val_loss: 0.5052 - val_accuracy: 0.6880\n",
            "Epoch 3919/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3042 - accuracy: 0.8446\n",
            "Epoch 3919: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3046 - accuracy: 0.8440 - val_loss: 0.3802 - val_accuracy: 0.7865\n",
            "Epoch 3920/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3056 - accuracy: 0.8429\n",
            "Epoch 3920: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3060 - accuracy: 0.8425 - val_loss: 0.3914 - val_accuracy: 0.7691\n",
            "Epoch 3921/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3069 - accuracy: 0.8439\n",
            "Epoch 3921: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3065 - accuracy: 0.8440 - val_loss: 0.5154 - val_accuracy: 0.6838\n",
            "Epoch 3922/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.8436\n",
            "Epoch 3922: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3040 - accuracy: 0.8435 - val_loss: 0.4814 - val_accuracy: 0.7206\n",
            "Epoch 3923/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3028 - accuracy: 0.8471\n",
            "Epoch 3923: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3032 - accuracy: 0.8461 - val_loss: 0.5197 - val_accuracy: 0.6847\n",
            "Epoch 3924/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3165 - accuracy: 0.8373\n",
            "Epoch 3924: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3186 - accuracy: 0.8359 - val_loss: 0.4906 - val_accuracy: 0.6882\n",
            "Epoch 3925/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8421\n",
            "Epoch 3925: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3128 - accuracy: 0.8416 - val_loss: 0.4457 - val_accuracy: 0.7389\n",
            "Epoch 3926/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3103 - accuracy: 0.8393\n",
            "Epoch 3926: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3094 - accuracy: 0.8402 - val_loss: 0.3991 - val_accuracy: 0.7634\n",
            "Epoch 3927/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.8457\n",
            "Epoch 3927: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3024 - accuracy: 0.8457 - val_loss: 0.4511 - val_accuracy: 0.7290\n",
            "Epoch 3928/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8464\n",
            "Epoch 3928: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3046 - accuracy: 0.8464 - val_loss: 0.5581 - val_accuracy: 0.6763\n",
            "Epoch 3929/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3082 - accuracy: 0.8431\n",
            "Epoch 3929: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3078 - accuracy: 0.8426 - val_loss: 0.5492 - val_accuracy: 0.6654\n",
            "Epoch 3930/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8370\n",
            "Epoch 3930: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3135 - accuracy: 0.8369 - val_loss: 0.4799 - val_accuracy: 0.7185\n",
            "Epoch 3931/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3084 - accuracy: 0.8434\n",
            "Epoch 3931: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3090 - accuracy: 0.8425 - val_loss: 0.4745 - val_accuracy: 0.7125\n",
            "Epoch 3932/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3134 - accuracy: 0.8400\n",
            "Epoch 3932: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3117 - accuracy: 0.8410 - val_loss: 0.4394 - val_accuracy: 0.7261\n",
            "Epoch 3933/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3086 - accuracy: 0.8419\n",
            "Epoch 3933: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3104 - accuracy: 0.8408 - val_loss: 0.4472 - val_accuracy: 0.7369\n",
            "Epoch 3934/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3077 - accuracy: 0.8409\n",
            "Epoch 3934: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3082 - accuracy: 0.8409 - val_loss: 0.4416 - val_accuracy: 0.7391\n",
            "Epoch 3935/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3064 - accuracy: 0.8449\n",
            "Epoch 3935: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3082 - accuracy: 0.8437 - val_loss: 0.5631 - val_accuracy: 0.6717\n",
            "Epoch 3936/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3204 - accuracy: 0.8355\n",
            "Epoch 3936: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3205 - accuracy: 0.8350 - val_loss: 0.5079 - val_accuracy: 0.7158\n",
            "Epoch 3937/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3441 - accuracy: 0.8239\n",
            "Epoch 3937: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3430 - accuracy: 0.8244 - val_loss: 0.4055 - val_accuracy: 0.7544\n",
            "Epoch 3938/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3153 - accuracy: 0.8373\n",
            "Epoch 3938: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3156 - accuracy: 0.8379 - val_loss: 0.4195 - val_accuracy: 0.7490\n",
            "Epoch 3939/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3092 - accuracy: 0.8416\n",
            "Epoch 3939: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3087 - accuracy: 0.8416 - val_loss: 0.4757 - val_accuracy: 0.6875\n",
            "Epoch 3940/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3090 - accuracy: 0.8430\n",
            "Epoch 3940: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3089 - accuracy: 0.8429 - val_loss: 0.3672 - val_accuracy: 0.8005\n",
            "Epoch 3941/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3018 - accuracy: 0.8451\n",
            "Epoch 3941: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3026 - accuracy: 0.8451 - val_loss: 0.4514 - val_accuracy: 0.7191\n",
            "Epoch 3942/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8401\n",
            "Epoch 3942: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3095 - accuracy: 0.8402 - val_loss: 0.4657 - val_accuracy: 0.7182\n",
            "Epoch 3943/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.8401\n",
            "Epoch 3943: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3113 - accuracy: 0.8404 - val_loss: 0.4575 - val_accuracy: 0.7277\n",
            "Epoch 3944/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8474\n",
            "Epoch 3944: loss did not improve from 0.30062\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3011 - accuracy: 0.8464 - val_loss: 0.4059 - val_accuracy: 0.7689\n",
            "Epoch 3945/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3004 - accuracy: 0.8461\n",
            "Epoch 3945: loss improved from 0.30062 to 0.30061, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 28ms/step - loss: 0.3006 - accuracy: 0.8458 - val_loss: 0.4066 - val_accuracy: 0.7680\n",
            "Epoch 3946/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8405\n",
            "Epoch 3946: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3124 - accuracy: 0.8405 - val_loss: 0.3748 - val_accuracy: 0.7836\n",
            "Epoch 3947/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3314 - accuracy: 0.8318\n",
            "Epoch 3947: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3318 - accuracy: 0.8317 - val_loss: 0.4179 - val_accuracy: 0.7457\n",
            "Epoch 3948/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3144 - accuracy: 0.8398\n",
            "Epoch 3948: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3177 - accuracy: 0.8371 - val_loss: 0.5292 - val_accuracy: 0.6702\n",
            "Epoch 3949/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3071 - accuracy: 0.8417\n",
            "Epoch 3949: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3097 - accuracy: 0.8401 - val_loss: 0.4922 - val_accuracy: 0.6959\n",
            "Epoch 3950/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3028 - accuracy: 0.8476\n",
            "Epoch 3950: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3041 - accuracy: 0.8466 - val_loss: 0.4769 - val_accuracy: 0.7119\n",
            "Epoch 3951/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3110 - accuracy: 0.8409\n",
            "Epoch 3951: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3114 - accuracy: 0.8405 - val_loss: 0.3927 - val_accuracy: 0.7707\n",
            "Epoch 3952/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3107 - accuracy: 0.8405\n",
            "Epoch 3952: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3105 - accuracy: 0.8404 - val_loss: 0.4053 - val_accuracy: 0.7623\n",
            "Epoch 3953/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8416\n",
            "Epoch 3953: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3071 - accuracy: 0.8413 - val_loss: 0.5032 - val_accuracy: 0.7009\n",
            "Epoch 3954/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3134 - accuracy: 0.8376\n",
            "Epoch 3954: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3133 - accuracy: 0.8381 - val_loss: 0.4167 - val_accuracy: 0.7516\n",
            "Epoch 3955/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3007 - accuracy: 0.8471\n",
            "Epoch 3955: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3017 - accuracy: 0.8465 - val_loss: 0.4869 - val_accuracy: 0.7178\n",
            "Epoch 3956/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8429\n",
            "Epoch 3956: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3100 - accuracy: 0.8426 - val_loss: 0.4297 - val_accuracy: 0.7487\n",
            "Epoch 3957/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3036 - accuracy: 0.8436\n",
            "Epoch 3957: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3039 - accuracy: 0.8432 - val_loss: 0.4994 - val_accuracy: 0.7053\n",
            "Epoch 3958/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3180 - accuracy: 0.8356\n",
            "Epoch 3958: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3182 - accuracy: 0.8355 - val_loss: 0.4056 - val_accuracy: 0.7643\n",
            "Epoch 3959/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3135 - accuracy: 0.8361\n",
            "Epoch 3959: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3143 - accuracy: 0.8357 - val_loss: 0.4640 - val_accuracy: 0.7152\n",
            "Epoch 3960/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3064 - accuracy: 0.8428\n",
            "Epoch 3960: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3102 - accuracy: 0.8405 - val_loss: 0.5017 - val_accuracy: 0.7158\n",
            "Epoch 3961/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8408\n",
            "Epoch 3961: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3142 - accuracy: 0.8407 - val_loss: 0.5124 - val_accuracy: 0.6926\n",
            "Epoch 3962/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8429\n",
            "Epoch 3962: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3108 - accuracy: 0.8420 - val_loss: 0.3648 - val_accuracy: 0.7880\n",
            "Epoch 3963/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3084 - accuracy: 0.8426\n",
            "Epoch 3963: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3096 - accuracy: 0.8427 - val_loss: 0.4344 - val_accuracy: 0.7435\n",
            "Epoch 3964/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3065 - accuracy: 0.8423\n",
            "Epoch 3964: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3065 - accuracy: 0.8417 - val_loss: 0.3771 - val_accuracy: 0.7909\n",
            "Epoch 3965/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3076 - accuracy: 0.8419\n",
            "Epoch 3965: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3089 - accuracy: 0.8405 - val_loss: 0.4334 - val_accuracy: 0.7375\n",
            "Epoch 3966/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.8388\n",
            "Epoch 3966: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3092 - accuracy: 0.8390 - val_loss: 0.3575 - val_accuracy: 0.7942\n",
            "Epoch 3967/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.8388\n",
            "Epoch 3967: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3135 - accuracy: 0.8388 - val_loss: 0.5045 - val_accuracy: 0.7007\n",
            "Epoch 3968/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8438\n",
            "Epoch 3968: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3038 - accuracy: 0.8436 - val_loss: 0.3786 - val_accuracy: 0.7810\n",
            "Epoch 3969/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3032 - accuracy: 0.8468\n",
            "Epoch 3969: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3047 - accuracy: 0.8457 - val_loss: 0.3823 - val_accuracy: 0.7869\n",
            "Epoch 3970/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8439\n",
            "Epoch 3970: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3039 - accuracy: 0.8442 - val_loss: 0.3931 - val_accuracy: 0.7665\n",
            "Epoch 3971/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.8391\n",
            "Epoch 3971: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3085 - accuracy: 0.8391 - val_loss: 0.4619 - val_accuracy: 0.7196\n",
            "Epoch 3972/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8443\n",
            "Epoch 3972: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3047 - accuracy: 0.8443 - val_loss: 0.3998 - val_accuracy: 0.7637\n",
            "Epoch 3973/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.8389\n",
            "Epoch 3973: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3128 - accuracy: 0.8386 - val_loss: 0.4271 - val_accuracy: 0.7536\n",
            "Epoch 3974/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3092 - accuracy: 0.8430\n",
            "Epoch 3974: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3109 - accuracy: 0.8414 - val_loss: 0.3580 - val_accuracy: 0.7942\n",
            "Epoch 3975/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3172 - accuracy: 0.8375\n",
            "Epoch 3975: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3174 - accuracy: 0.8365 - val_loss: 0.5394 - val_accuracy: 0.6671\n",
            "Epoch 3976/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3054 - accuracy: 0.8441\n",
            "Epoch 3976: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3073 - accuracy: 0.8427 - val_loss: 0.4115 - val_accuracy: 0.7538\n",
            "Epoch 3977/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3129 - accuracy: 0.8434\n",
            "Epoch 3977: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3138 - accuracy: 0.8422 - val_loss: 0.4209 - val_accuracy: 0.7485\n",
            "Epoch 3978/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3011 - accuracy: 0.8439\n",
            "Epoch 3978: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3012 - accuracy: 0.8437 - val_loss: 0.4888 - val_accuracy: 0.7090\n",
            "Epoch 3979/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8405\n",
            "Epoch 3979: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3082 - accuracy: 0.8405 - val_loss: 0.4638 - val_accuracy: 0.7125\n",
            "Epoch 3980/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3110 - accuracy: 0.8405\n",
            "Epoch 3980: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3117 - accuracy: 0.8397 - val_loss: 0.4296 - val_accuracy: 0.7437\n",
            "Epoch 3981/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3165 - accuracy: 0.8378\n",
            "Epoch 3981: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3172 - accuracy: 0.8376 - val_loss: 0.4196 - val_accuracy: 0.7457\n",
            "Epoch 3982/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.8443\n",
            "Epoch 3982: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3068 - accuracy: 0.8443 - val_loss: 0.4419 - val_accuracy: 0.7349\n",
            "Epoch 3983/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3069 - accuracy: 0.8426\n",
            "Epoch 3983: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3088 - accuracy: 0.8415 - val_loss: 0.4150 - val_accuracy: 0.7555\n",
            "Epoch 3984/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.8393\n",
            "Epoch 3984: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3150 - accuracy: 0.8391 - val_loss: 0.4333 - val_accuracy: 0.7507\n",
            "Epoch 3985/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3103 - accuracy: 0.8405\n",
            "Epoch 3985: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3108 - accuracy: 0.8402 - val_loss: 0.4610 - val_accuracy: 0.7187\n",
            "Epoch 3986/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.8424\n",
            "Epoch 3986: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3093 - accuracy: 0.8424 - val_loss: 0.5378 - val_accuracy: 0.6798\n",
            "Epoch 3987/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.8414\n",
            "Epoch 3987: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3067 - accuracy: 0.8414 - val_loss: 0.4064 - val_accuracy: 0.7558\n",
            "Epoch 3988/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8453\n",
            "Epoch 3988: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3028 - accuracy: 0.8453 - val_loss: 0.5276 - val_accuracy: 0.6820\n",
            "Epoch 3989/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3140 - accuracy: 0.8395\n",
            "Epoch 3989: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3138 - accuracy: 0.8395 - val_loss: 0.4937 - val_accuracy: 0.7193\n",
            "Epoch 3990/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3195 - accuracy: 0.8364\n",
            "Epoch 3990: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3193 - accuracy: 0.8367 - val_loss: 0.4147 - val_accuracy: 0.7619\n",
            "Epoch 3991/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3114 - accuracy: 0.8391\n",
            "Epoch 3991: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3134 - accuracy: 0.8372 - val_loss: 0.4592 - val_accuracy: 0.7204\n",
            "Epoch 3992/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3119 - accuracy: 0.8413\n",
            "Epoch 3992: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3110 - accuracy: 0.8420 - val_loss: 0.4509 - val_accuracy: 0.7288\n",
            "Epoch 3993/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3067 - accuracy: 0.8437\n",
            "Epoch 3993: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3079 - accuracy: 0.8432 - val_loss: 0.5008 - val_accuracy: 0.6937\n",
            "Epoch 3994/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8456\n",
            "Epoch 3994: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3023 - accuracy: 0.8458 - val_loss: 0.4095 - val_accuracy: 0.7656\n",
            "Epoch 3995/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8432\n",
            "Epoch 3995: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3053 - accuracy: 0.8433 - val_loss: 0.4631 - val_accuracy: 0.7187\n",
            "Epoch 3996/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3104 - accuracy: 0.8396\n",
            "Epoch 3996: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3107 - accuracy: 0.8393 - val_loss: 0.4641 - val_accuracy: 0.7130\n",
            "Epoch 3997/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8397\n",
            "Epoch 3997: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3143 - accuracy: 0.8390 - val_loss: 0.3825 - val_accuracy: 0.7759\n",
            "Epoch 3998/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3197 - accuracy: 0.8391\n",
            "Epoch 3998: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3205 - accuracy: 0.8386 - val_loss: 0.4413 - val_accuracy: 0.7356\n",
            "Epoch 3999/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3087 - accuracy: 0.8421\n",
            "Epoch 3999: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3091 - accuracy: 0.8420 - val_loss: 0.6506 - val_accuracy: 0.6026\n",
            "Epoch 4000/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3203 - accuracy: 0.8343\n",
            "Epoch 4000: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3218 - accuracy: 0.8327 - val_loss: 0.4560 - val_accuracy: 0.7220\n",
            "Epoch 4001/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3086 - accuracy: 0.8416\n",
            "Epoch 4001: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3089 - accuracy: 0.8412 - val_loss: 0.3869 - val_accuracy: 0.7773\n",
            "Epoch 4002/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.8397\n",
            "Epoch 4002: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3099 - accuracy: 0.8397 - val_loss: 0.4319 - val_accuracy: 0.7334\n",
            "Epoch 4003/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3097 - accuracy: 0.8460\n",
            "Epoch 4003: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3102 - accuracy: 0.8442 - val_loss: 0.4764 - val_accuracy: 0.7174\n",
            "Epoch 4004/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3191 - accuracy: 0.8390\n",
            "Epoch 4004: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3198 - accuracy: 0.8370 - val_loss: 0.4531 - val_accuracy: 0.7233\n",
            "Epoch 4005/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3132 - accuracy: 0.8393\n",
            "Epoch 4005: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3145 - accuracy: 0.8379 - val_loss: 0.3657 - val_accuracy: 0.7898\n",
            "Epoch 4006/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8356\n",
            "Epoch 4006: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3200 - accuracy: 0.8356 - val_loss: 0.3894 - val_accuracy: 0.7667\n",
            "Epoch 4007/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3166 - accuracy: 0.8374\n",
            "Epoch 4007: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3165 - accuracy: 0.8378 - val_loss: 0.4174 - val_accuracy: 0.7498\n",
            "Epoch 4008/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8446\n",
            "Epoch 4008: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3067 - accuracy: 0.8446 - val_loss: 0.3909 - val_accuracy: 0.7799\n",
            "Epoch 4009/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3133 - accuracy: 0.8405\n",
            "Epoch 4009: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3154 - accuracy: 0.8392 - val_loss: 0.4590 - val_accuracy: 0.7206\n",
            "Epoch 4010/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3070 - accuracy: 0.8415\n",
            "Epoch 4010: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3090 - accuracy: 0.8400 - val_loss: 0.3383 - val_accuracy: 0.8108\n",
            "Epoch 4011/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3195 - accuracy: 0.8338\n",
            "Epoch 4011: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3189 - accuracy: 0.8340 - val_loss: 0.4669 - val_accuracy: 0.7185\n",
            "Epoch 4012/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.8387\n",
            "Epoch 4012: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3133 - accuracy: 0.8387 - val_loss: 0.3767 - val_accuracy: 0.7803\n",
            "Epoch 4013/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.8448\n",
            "Epoch 4013: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3021 - accuracy: 0.8448 - val_loss: 0.4371 - val_accuracy: 0.7323\n",
            "Epoch 4014/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8481\n",
            "Epoch 4014: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3011 - accuracy: 0.8480 - val_loss: 0.4476 - val_accuracy: 0.7351\n",
            "Epoch 4015/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8406\n",
            "Epoch 4015: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3080 - accuracy: 0.8412 - val_loss: 0.3820 - val_accuracy: 0.7729\n",
            "Epoch 4016/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8417\n",
            "Epoch 4016: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 30ms/step - loss: 0.3064 - accuracy: 0.8418 - val_loss: 0.4235 - val_accuracy: 0.7505\n",
            "Epoch 4017/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8451\n",
            "Epoch 4017: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3056 - accuracy: 0.8445 - val_loss: 0.4903 - val_accuracy: 0.7057\n",
            "Epoch 4018/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8422\n",
            "Epoch 4018: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3072 - accuracy: 0.8417 - val_loss: 0.3835 - val_accuracy: 0.7709\n",
            "Epoch 4019/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3068 - accuracy: 0.8438\n",
            "Epoch 4019: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3060 - accuracy: 0.8437 - val_loss: 0.5595 - val_accuracy: 0.6500\n",
            "Epoch 4020/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3135 - accuracy: 0.8383\n",
            "Epoch 4020: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3138 - accuracy: 0.8373 - val_loss: 0.3657 - val_accuracy: 0.7915\n",
            "Epoch 4021/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3094 - accuracy: 0.8388\n",
            "Epoch 4021: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3115 - accuracy: 0.8378 - val_loss: 0.4107 - val_accuracy: 0.7419\n",
            "Epoch 4022/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3201 - accuracy: 0.8371\n",
            "Epoch 4022: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3211 - accuracy: 0.8369 - val_loss: 0.4249 - val_accuracy: 0.7448\n",
            "Epoch 4023/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3085 - accuracy: 0.8418\n",
            "Epoch 4023: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3093 - accuracy: 0.8408 - val_loss: 0.5072 - val_accuracy: 0.6932\n",
            "Epoch 4024/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3092 - accuracy: 0.8403\n",
            "Epoch 4024: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3089 - accuracy: 0.8401 - val_loss: 0.4853 - val_accuracy: 0.6976\n",
            "Epoch 4025/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3064 - accuracy: 0.8421\n",
            "Epoch 4025: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3078 - accuracy: 0.8405 - val_loss: 0.5251 - val_accuracy: 0.6796\n",
            "Epoch 4026/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8402\n",
            "Epoch 4026: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3087 - accuracy: 0.8402 - val_loss: 0.6029 - val_accuracy: 0.6471\n",
            "Epoch 4027/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3109 - accuracy: 0.8394\n",
            "Epoch 4027: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3109 - accuracy: 0.8394 - val_loss: 0.3991 - val_accuracy: 0.7659\n",
            "Epoch 4028/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3032 - accuracy: 0.8460\n",
            "Epoch 4028: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3030 - accuracy: 0.8456 - val_loss: 0.4626 - val_accuracy: 0.7167\n",
            "Epoch 4029/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3022 - accuracy: 0.8432\n",
            "Epoch 4029: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3026 - accuracy: 0.8433 - val_loss: 0.4231 - val_accuracy: 0.7413\n",
            "Epoch 4030/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8371\n",
            "Epoch 4030: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3200 - accuracy: 0.8373 - val_loss: 0.4186 - val_accuracy: 0.7463\n",
            "Epoch 4031/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3171 - accuracy: 0.8378\n",
            "Epoch 4031: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3178 - accuracy: 0.8374 - val_loss: 0.4555 - val_accuracy: 0.7239\n",
            "Epoch 4032/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3152 - accuracy: 0.8376\n",
            "Epoch 4032: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3155 - accuracy: 0.8374 - val_loss: 0.4103 - val_accuracy: 0.7672\n",
            "Epoch 4033/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3052 - accuracy: 0.8437\n",
            "Epoch 4033: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3054 - accuracy: 0.8433 - val_loss: 0.4028 - val_accuracy: 0.7687\n",
            "Epoch 4034/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8459\n",
            "Epoch 4034: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3029 - accuracy: 0.8459 - val_loss: 0.3864 - val_accuracy: 0.7670\n",
            "Epoch 4035/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3222 - accuracy: 0.8344\n",
            "Epoch 4035: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3229 - accuracy: 0.8340 - val_loss: 0.5223 - val_accuracy: 0.6829\n",
            "Epoch 4036/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8429\n",
            "Epoch 4036: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3065 - accuracy: 0.8420 - val_loss: 0.4231 - val_accuracy: 0.7422\n",
            "Epoch 4037/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8429\n",
            "Epoch 4037: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3055 - accuracy: 0.8432 - val_loss: 0.4084 - val_accuracy: 0.7670\n",
            "Epoch 4038/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.8413\n",
            "Epoch 4038: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3133 - accuracy: 0.8413 - val_loss: 0.3444 - val_accuracy: 0.8051\n",
            "Epoch 4039/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8384\n",
            "Epoch 4039: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3163 - accuracy: 0.8382 - val_loss: 0.4645 - val_accuracy: 0.7114\n",
            "Epoch 4040/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3069 - accuracy: 0.8456\n",
            "Epoch 4040: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3078 - accuracy: 0.8440 - val_loss: 0.4453 - val_accuracy: 0.7310\n",
            "Epoch 4041/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8485\n",
            "Epoch 4041: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3014 - accuracy: 0.8484 - val_loss: 0.3878 - val_accuracy: 0.7722\n",
            "Epoch 4042/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.8415\n",
            "Epoch 4042: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3081 - accuracy: 0.8415 - val_loss: 0.4905 - val_accuracy: 0.7053\n",
            "Epoch 4043/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3042 - accuracy: 0.8439\n",
            "Epoch 4043: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3049 - accuracy: 0.8431 - val_loss: 0.4131 - val_accuracy: 0.7536\n",
            "Epoch 4044/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8434\n",
            "Epoch 4044: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3078 - accuracy: 0.8435 - val_loss: 0.4450 - val_accuracy: 0.7296\n",
            "Epoch 4045/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3118 - accuracy: 0.8413\n",
            "Epoch 4045: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3120 - accuracy: 0.8410 - val_loss: 0.5053 - val_accuracy: 0.7048\n",
            "Epoch 4046/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3077 - accuracy: 0.8418\n",
            "Epoch 4046: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3091 - accuracy: 0.8399 - val_loss: 0.4144 - val_accuracy: 0.7549\n",
            "Epoch 4047/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8444\n",
            "Epoch 4047: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3012 - accuracy: 0.8444 - val_loss: 0.4058 - val_accuracy: 0.7544\n",
            "Epoch 4048/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3053 - accuracy: 0.8450\n",
            "Epoch 4048: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3090 - accuracy: 0.8427 - val_loss: 0.4900 - val_accuracy: 0.7154\n",
            "Epoch 4049/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3084 - accuracy: 0.8422\n",
            "Epoch 4049: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3097 - accuracy: 0.8416 - val_loss: 0.4107 - val_accuracy: 0.7547\n",
            "Epoch 4050/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3193 - accuracy: 0.8381\n",
            "Epoch 4050: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3192 - accuracy: 0.8379 - val_loss: 0.4062 - val_accuracy: 0.7608\n",
            "Epoch 4051/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.8414\n",
            "Epoch 4051: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3116 - accuracy: 0.8407 - val_loss: 0.3842 - val_accuracy: 0.7808\n",
            "Epoch 4052/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3113 - accuracy: 0.8397\n",
            "Epoch 4052: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3117 - accuracy: 0.8392 - val_loss: 0.4261 - val_accuracy: 0.7450\n",
            "Epoch 4053/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.8417\n",
            "Epoch 4053: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3083 - accuracy: 0.8417 - val_loss: 0.4027 - val_accuracy: 0.7547\n",
            "Epoch 4054/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3147 - accuracy: 0.8392\n",
            "Epoch 4054: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3151 - accuracy: 0.8383 - val_loss: 0.4470 - val_accuracy: 0.7338\n",
            "Epoch 4055/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3083 - accuracy: 0.8423\n",
            "Epoch 4055: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3080 - accuracy: 0.8414 - val_loss: 0.4458 - val_accuracy: 0.7303\n",
            "Epoch 4056/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3060 - accuracy: 0.8449\n",
            "Epoch 4056: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3067 - accuracy: 0.8443 - val_loss: 0.3742 - val_accuracy: 0.7874\n",
            "Epoch 4057/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3139 - accuracy: 0.8381\n",
            "Epoch 4057: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 27ms/step - loss: 0.3152 - accuracy: 0.8378 - val_loss: 0.6679 - val_accuracy: 0.6340\n",
            "Epoch 4058/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.8349\n",
            "Epoch 4058: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3238 - accuracy: 0.8350 - val_loss: 0.4181 - val_accuracy: 0.7496\n",
            "Epoch 4059/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3056 - accuracy: 0.8405\n",
            "Epoch 4059: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3074 - accuracy: 0.8395 - val_loss: 0.4175 - val_accuracy: 0.7505\n",
            "Epoch 4060/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8383\n",
            "Epoch 4060: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3112 - accuracy: 0.8385 - val_loss: 0.4190 - val_accuracy: 0.7408\n",
            "Epoch 4061/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3216 - accuracy: 0.8351\n",
            "Epoch 4061: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3216 - accuracy: 0.8346 - val_loss: 0.5246 - val_accuracy: 0.6884\n",
            "Epoch 4062/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.8387\n",
            "Epoch 4062: loss did not improve from 0.30061\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3091 - accuracy: 0.8387 - val_loss: 0.4555 - val_accuracy: 0.7277\n",
            "Epoch 4063/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8443\n",
            "Epoch 4063: loss improved from 0.30061 to 0.30060, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 2s 50ms/step - loss: 0.3006 - accuracy: 0.8443 - val_loss: 0.3909 - val_accuracy: 0.7670\n",
            "Epoch 4064/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8424\n",
            "Epoch 4064: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3044 - accuracy: 0.8425 - val_loss: 0.3769 - val_accuracy: 0.7797\n",
            "Epoch 4065/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8416\n",
            "Epoch 4065: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3100 - accuracy: 0.8416 - val_loss: 0.4143 - val_accuracy: 0.7558\n",
            "Epoch 4066/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8434\n",
            "Epoch 4066: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3072 - accuracy: 0.8429 - val_loss: 0.4380 - val_accuracy: 0.7356\n",
            "Epoch 4067/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3048 - accuracy: 0.8453\n",
            "Epoch 4067: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3062 - accuracy: 0.8444 - val_loss: 0.4070 - val_accuracy: 0.7540\n",
            "Epoch 4068/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8447\n",
            "Epoch 4068: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3026 - accuracy: 0.8444 - val_loss: 0.4606 - val_accuracy: 0.7132\n",
            "Epoch 4069/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3062 - accuracy: 0.8435\n",
            "Epoch 4069: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3077 - accuracy: 0.8421 - val_loss: 0.4198 - val_accuracy: 0.7527\n",
            "Epoch 4070/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3065 - accuracy: 0.8426\n",
            "Epoch 4070: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3064 - accuracy: 0.8431 - val_loss: 0.4131 - val_accuracy: 0.7522\n",
            "Epoch 4071/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3107 - accuracy: 0.8410\n",
            "Epoch 4071: loss did not improve from 0.30060\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3114 - accuracy: 0.8403 - val_loss: 0.4364 - val_accuracy: 0.7360\n",
            "Epoch 4072/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2988 - accuracy: 0.8457\n",
            "Epoch 4072: loss improved from 0.30060 to 0.29939, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2994 - accuracy: 0.8455 - val_loss: 0.4299 - val_accuracy: 0.7391\n",
            "Epoch 4073/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2946 - accuracy: 0.8505\n",
            "Epoch 4073: loss improved from 0.29939 to 0.29665, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2966 - accuracy: 0.8491 - val_loss: 0.4882 - val_accuracy: 0.7068\n",
            "Epoch 4074/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3054 - accuracy: 0.8450\n",
            "Epoch 4074: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3058 - accuracy: 0.8444 - val_loss: 0.4283 - val_accuracy: 0.7354\n",
            "Epoch 4075/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3112 - accuracy: 0.8411\n",
            "Epoch 4075: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3110 - accuracy: 0.8402 - val_loss: 0.4322 - val_accuracy: 0.7397\n",
            "Epoch 4076/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3073 - accuracy: 0.8440\n",
            "Epoch 4076: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3092 - accuracy: 0.8438 - val_loss: 0.4709 - val_accuracy: 0.7220\n",
            "Epoch 4077/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3319 - accuracy: 0.8316\n",
            "Epoch 4077: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3342 - accuracy: 0.8302 - val_loss: 0.4966 - val_accuracy: 0.7027\n",
            "Epoch 4078/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3242 - accuracy: 0.8334\n",
            "Epoch 4078: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3240 - accuracy: 0.8333 - val_loss: 0.4368 - val_accuracy: 0.7283\n",
            "Epoch 4079/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3021 - accuracy: 0.8455\n",
            "Epoch 4079: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3036 - accuracy: 0.8443 - val_loss: 0.4301 - val_accuracy: 0.7406\n",
            "Epoch 4080/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.8421\n",
            "Epoch 4080: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3083 - accuracy: 0.8419 - val_loss: 0.4194 - val_accuracy: 0.7542\n",
            "Epoch 4081/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8410\n",
            "Epoch 4081: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3051 - accuracy: 0.8407 - val_loss: 0.4279 - val_accuracy: 0.7422\n",
            "Epoch 4082/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3029 - accuracy: 0.8453\n",
            "Epoch 4082: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3033 - accuracy: 0.8452 - val_loss: 0.4689 - val_accuracy: 0.7084\n",
            "Epoch 4083/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3182 - accuracy: 0.8367\n",
            "Epoch 4083: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3187 - accuracy: 0.8373 - val_loss: 0.4029 - val_accuracy: 0.7648\n",
            "Epoch 4084/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8412\n",
            "Epoch 4084: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3093 - accuracy: 0.8405 - val_loss: 0.4112 - val_accuracy: 0.7544\n",
            "Epoch 4085/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3094 - accuracy: 0.8399\n",
            "Epoch 4085: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3095 - accuracy: 0.8401 - val_loss: 0.4430 - val_accuracy: 0.7351\n",
            "Epoch 4086/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3106 - accuracy: 0.8413\n",
            "Epoch 4086: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3115 - accuracy: 0.8404 - val_loss: 0.4820 - val_accuracy: 0.7125\n",
            "Epoch 4087/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3084 - accuracy: 0.8409\n",
            "Epoch 4087: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3070 - accuracy: 0.8417 - val_loss: 0.3480 - val_accuracy: 0.8051\n",
            "Epoch 4088/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3038 - accuracy: 0.8443\n",
            "Epoch 4088: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3052 - accuracy: 0.8436 - val_loss: 0.3769 - val_accuracy: 0.7808\n",
            "Epoch 4089/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3055 - accuracy: 0.8443\n",
            "Epoch 4089: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3073 - accuracy: 0.8429 - val_loss: 0.4579 - val_accuracy: 0.7134\n",
            "Epoch 4090/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8438\n",
            "Epoch 4090: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3069 - accuracy: 0.8433 - val_loss: 0.4883 - val_accuracy: 0.7053\n",
            "Epoch 4091/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3089 - accuracy: 0.8441\n",
            "Epoch 4091: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3104 - accuracy: 0.8429 - val_loss: 0.3991 - val_accuracy: 0.7628\n",
            "Epoch 4092/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8411\n",
            "Epoch 4092: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3102 - accuracy: 0.8413 - val_loss: 0.4398 - val_accuracy: 0.7393\n",
            "Epoch 4093/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3098 - accuracy: 0.8417\n",
            "Epoch 4093: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3102 - accuracy: 0.8414 - val_loss: 0.3857 - val_accuracy: 0.7749\n",
            "Epoch 4094/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8434\n",
            "Epoch 4094: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3067 - accuracy: 0.8435 - val_loss: 0.5589 - val_accuracy: 0.6592\n",
            "Epoch 4095/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8339\n",
            "Epoch 4095: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3237 - accuracy: 0.8339 - val_loss: 0.3834 - val_accuracy: 0.7814\n",
            "Epoch 4096/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3173 - accuracy: 0.8394\n",
            "Epoch 4096: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3174 - accuracy: 0.8395 - val_loss: 0.4432 - val_accuracy: 0.7364\n",
            "Epoch 4097/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3171 - accuracy: 0.8371\n",
            "Epoch 4097: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3163 - accuracy: 0.8377 - val_loss: 0.4058 - val_accuracy: 0.7606\n",
            "Epoch 4098/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3087 - accuracy: 0.8406\n",
            "Epoch 4098: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3089 - accuracy: 0.8399 - val_loss: 0.4106 - val_accuracy: 0.7516\n",
            "Epoch 4099/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3022 - accuracy: 0.8443\n",
            "Epoch 4099: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3018 - accuracy: 0.8452 - val_loss: 0.3904 - val_accuracy: 0.7749\n",
            "Epoch 4100/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2999 - accuracy: 0.8460\n",
            "Epoch 4100: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3018 - accuracy: 0.8449 - val_loss: 0.4298 - val_accuracy: 0.7430\n",
            "Epoch 4101/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3113 - accuracy: 0.8427\n",
            "Epoch 4101: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3125 - accuracy: 0.8415 - val_loss: 0.4329 - val_accuracy: 0.7571\n",
            "Epoch 4102/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.8429\n",
            "Epoch 4102: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3060 - accuracy: 0.8429 - val_loss: 0.4580 - val_accuracy: 0.7167\n",
            "Epoch 4103/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3052 - accuracy: 0.8437\n",
            "Epoch 4103: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3054 - accuracy: 0.8432 - val_loss: 0.4372 - val_accuracy: 0.7343\n",
            "Epoch 4104/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8402\n",
            "Epoch 4104: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3119 - accuracy: 0.8403 - val_loss: 0.4718 - val_accuracy: 0.7329\n",
            "Epoch 4105/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3115 - accuracy: 0.8392\n",
            "Epoch 4105: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3112 - accuracy: 0.8395 - val_loss: 0.4266 - val_accuracy: 0.7428\n",
            "Epoch 4106/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8421\n",
            "Epoch 4106: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3089 - accuracy: 0.8421 - val_loss: 0.4080 - val_accuracy: 0.7628\n",
            "Epoch 4107/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3269 - accuracy: 0.8343\n",
            "Epoch 4107: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3301 - accuracy: 0.8321 - val_loss: 0.4616 - val_accuracy: 0.7101\n",
            "Epoch 4108/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3200 - accuracy: 0.8361\n",
            "Epoch 4108: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3207 - accuracy: 0.8356 - val_loss: 0.4273 - val_accuracy: 0.7536\n",
            "Epoch 4109/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3084 - accuracy: 0.8417\n",
            "Epoch 4109: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3098 - accuracy: 0.8404 - val_loss: 0.4226 - val_accuracy: 0.7490\n",
            "Epoch 4110/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3129 - accuracy: 0.8388\n",
            "Epoch 4110: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3130 - accuracy: 0.8384 - val_loss: 0.4303 - val_accuracy: 0.7347\n",
            "Epoch 4111/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8398\n",
            "Epoch 4111: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3142 - accuracy: 0.8399 - val_loss: 0.5244 - val_accuracy: 0.6866\n",
            "Epoch 4112/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3049 - accuracy: 0.8411\n",
            "Epoch 4112: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3052 - accuracy: 0.8408 - val_loss: 0.3713 - val_accuracy: 0.7882\n",
            "Epoch 4113/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3084 - accuracy: 0.8407\n",
            "Epoch 4113: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3085 - accuracy: 0.8407 - val_loss: 0.4634 - val_accuracy: 0.7134\n",
            "Epoch 4114/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3111 - accuracy: 0.8436\n",
            "Epoch 4114: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3126 - accuracy: 0.8423 - val_loss: 0.5865 - val_accuracy: 0.6647\n",
            "Epoch 4115/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8374\n",
            "Epoch 4115: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3148 - accuracy: 0.8374 - val_loss: 0.4384 - val_accuracy: 0.7382\n",
            "Epoch 4116/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3093 - accuracy: 0.8371\n",
            "Epoch 4116: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3095 - accuracy: 0.8368 - val_loss: 0.4198 - val_accuracy: 0.7582\n",
            "Epoch 4117/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3083 - accuracy: 0.8389\n",
            "Epoch 4117: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3084 - accuracy: 0.8387 - val_loss: 0.4174 - val_accuracy: 0.7426\n",
            "Epoch 4118/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3041 - accuracy: 0.8449\n",
            "Epoch 4118: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3045 - accuracy: 0.8447 - val_loss: 0.4010 - val_accuracy: 0.7586\n",
            "Epoch 4119/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3034 - accuracy: 0.8452\n",
            "Epoch 4119: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3039 - accuracy: 0.8449 - val_loss: 0.4769 - val_accuracy: 0.7020\n",
            "Epoch 4120/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3099 - accuracy: 0.8416\n",
            "Epoch 4120: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3099 - accuracy: 0.8416 - val_loss: 0.5155 - val_accuracy: 0.6785\n",
            "Epoch 4121/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8436\n",
            "Epoch 4121: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3038 - accuracy: 0.8432 - val_loss: 0.4461 - val_accuracy: 0.7215\n",
            "Epoch 4122/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3013 - accuracy: 0.8455\n",
            "Epoch 4122: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3025 - accuracy: 0.8447 - val_loss: 0.3832 - val_accuracy: 0.7759\n",
            "Epoch 4123/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3087 - accuracy: 0.8401\n",
            "Epoch 4123: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3112 - accuracy: 0.8386 - val_loss: 0.3817 - val_accuracy: 0.7716\n",
            "Epoch 4124/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3166 - accuracy: 0.8398\n",
            "Epoch 4124: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3173 - accuracy: 0.8392 - val_loss: 0.5738 - val_accuracy: 0.6658\n",
            "Epoch 4125/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3044 - accuracy: 0.8444\n",
            "Epoch 4125: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3052 - accuracy: 0.8442 - val_loss: 0.4399 - val_accuracy: 0.7411\n",
            "Epoch 4126/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.8437\n",
            "Epoch 4126: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3095 - accuracy: 0.8435 - val_loss: 0.4793 - val_accuracy: 0.7134\n",
            "Epoch 4127/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3056 - accuracy: 0.8427\n",
            "Epoch 4127: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3091 - accuracy: 0.8402 - val_loss: 0.4403 - val_accuracy: 0.7266\n",
            "Epoch 4128/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8346\n",
            "Epoch 4128: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3217 - accuracy: 0.8343 - val_loss: 0.4714 - val_accuracy: 0.7160\n",
            "Epoch 4129/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8402\n",
            "Epoch 4129: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3121 - accuracy: 0.8396 - val_loss: 0.4338 - val_accuracy: 0.7277\n",
            "Epoch 4130/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3102 - accuracy: 0.8411\n",
            "Epoch 4130: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3097 - accuracy: 0.8413 - val_loss: 0.4318 - val_accuracy: 0.7360\n",
            "Epoch 4131/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3033 - accuracy: 0.8441\n",
            "Epoch 4131: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3041 - accuracy: 0.8441 - val_loss: 0.4134 - val_accuracy: 0.7507\n",
            "Epoch 4132/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.8403\n",
            "Epoch 4132: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3144 - accuracy: 0.8404 - val_loss: 0.4206 - val_accuracy: 0.7536\n",
            "Epoch 4133/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8434\n",
            "Epoch 4133: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3056 - accuracy: 0.8433 - val_loss: 0.5026 - val_accuracy: 0.6886\n",
            "Epoch 4134/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8463\n",
            "Epoch 4134: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3026 - accuracy: 0.8465 - val_loss: 0.4932 - val_accuracy: 0.7044\n",
            "Epoch 4135/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.8400\n",
            "Epoch 4135: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3077 - accuracy: 0.8400 - val_loss: 0.4230 - val_accuracy: 0.7547\n",
            "Epoch 4136/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.8383\n",
            "Epoch 4136: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3123 - accuracy: 0.8384 - val_loss: 0.3953 - val_accuracy: 0.7654\n",
            "Epoch 4137/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3109 - accuracy: 0.8395\n",
            "Epoch 4137: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3128 - accuracy: 0.8388 - val_loss: 0.4976 - val_accuracy: 0.6991\n",
            "Epoch 4138/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3076 - accuracy: 0.8434\n",
            "Epoch 4138: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3088 - accuracy: 0.8425 - val_loss: 0.4293 - val_accuracy: 0.7470\n",
            "Epoch 4139/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3018 - accuracy: 0.8417\n",
            "Epoch 4139: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3031 - accuracy: 0.8420 - val_loss: 0.4301 - val_accuracy: 0.7415\n",
            "Epoch 4140/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3057 - accuracy: 0.8461\n",
            "Epoch 4140: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3056 - accuracy: 0.8457 - val_loss: 0.4197 - val_accuracy: 0.7527\n",
            "Epoch 4141/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3022 - accuracy: 0.8435\n",
            "Epoch 4141: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3028 - accuracy: 0.8439 - val_loss: 0.4013 - val_accuracy: 0.7558\n",
            "Epoch 4142/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3016 - accuracy: 0.8445\n",
            "Epoch 4142: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3029 - accuracy: 0.8435 - val_loss: 0.3901 - val_accuracy: 0.7753\n",
            "Epoch 4143/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.8429\n",
            "Epoch 4143: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3060 - accuracy: 0.8431 - val_loss: 0.4915 - val_accuracy: 0.7022\n",
            "Epoch 4144/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3070 - accuracy: 0.8438\n",
            "Epoch 4144: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3083 - accuracy: 0.8426 - val_loss: 0.4648 - val_accuracy: 0.7231\n",
            "Epoch 4145/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3146 - accuracy: 0.8363\n",
            "Epoch 4145: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3161 - accuracy: 0.8351 - val_loss: 0.4670 - val_accuracy: 0.7152\n",
            "Epoch 4146/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3123 - accuracy: 0.8370\n",
            "Epoch 4146: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3115 - accuracy: 0.8376 - val_loss: 0.3739 - val_accuracy: 0.7832\n",
            "Epoch 4147/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8427\n",
            "Epoch 4147: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3064 - accuracy: 0.8421 - val_loss: 0.4752 - val_accuracy: 0.7101\n",
            "Epoch 4148/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3077 - accuracy: 0.8428\n",
            "Epoch 4148: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3109 - accuracy: 0.8407 - val_loss: 0.4387 - val_accuracy: 0.7336\n",
            "Epoch 4149/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3142 - accuracy: 0.8411\n",
            "Epoch 4149: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3162 - accuracy: 0.8394 - val_loss: 0.4958 - val_accuracy: 0.6998\n",
            "Epoch 4150/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3087 - accuracy: 0.8400\n",
            "Epoch 4150: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3081 - accuracy: 0.8404 - val_loss: 0.4502 - val_accuracy: 0.7261\n",
            "Epoch 4151/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3040 - accuracy: 0.8445\n",
            "Epoch 4151: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3046 - accuracy: 0.8436 - val_loss: 0.4744 - val_accuracy: 0.7018\n",
            "Epoch 4152/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3151 - accuracy: 0.8398\n",
            "Epoch 4152: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3143 - accuracy: 0.8407 - val_loss: 0.3944 - val_accuracy: 0.7742\n",
            "Epoch 4153/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3080 - accuracy: 0.8407\n",
            "Epoch 4153: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3076 - accuracy: 0.8409 - val_loss: 0.4494 - val_accuracy: 0.7362\n",
            "Epoch 4154/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8439\n",
            "Epoch 4154: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3052 - accuracy: 0.8440 - val_loss: 0.3927 - val_accuracy: 0.7707\n",
            "Epoch 4155/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3133 - accuracy: 0.8397\n",
            "Epoch 4155: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3128 - accuracy: 0.8401 - val_loss: 0.4422 - val_accuracy: 0.7411\n",
            "Epoch 4156/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8466\n",
            "Epoch 4156: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3026 - accuracy: 0.8460 - val_loss: 0.4687 - val_accuracy: 0.7206\n",
            "Epoch 4157/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3057 - accuracy: 0.8421\n",
            "Epoch 4157: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3080 - accuracy: 0.8403 - val_loss: 0.3693 - val_accuracy: 0.7799\n",
            "Epoch 4158/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8346\n",
            "Epoch 4158: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3206 - accuracy: 0.8346 - val_loss: 0.4143 - val_accuracy: 0.7512\n",
            "Epoch 4159/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3025 - accuracy: 0.8415\n",
            "Epoch 4159: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3027 - accuracy: 0.8422 - val_loss: 0.5376 - val_accuracy: 0.6840\n",
            "Epoch 4160/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3151 - accuracy: 0.8382\n",
            "Epoch 4160: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3166 - accuracy: 0.8367 - val_loss: 0.4006 - val_accuracy: 0.7494\n",
            "Epoch 4161/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3095 - accuracy: 0.8407\n",
            "Epoch 4161: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3110 - accuracy: 0.8393 - val_loss: 0.4635 - val_accuracy: 0.7180\n",
            "Epoch 4162/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8401\n",
            "Epoch 4162: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3080 - accuracy: 0.8396 - val_loss: 0.4489 - val_accuracy: 0.7288\n",
            "Epoch 4163/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3084 - accuracy: 0.8435\n",
            "Epoch 4163: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3090 - accuracy: 0.8426 - val_loss: 0.4205 - val_accuracy: 0.7472\n",
            "Epoch 4164/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.8416\n",
            "Epoch 4164: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3107 - accuracy: 0.8418 - val_loss: 0.4239 - val_accuracy: 0.7483\n",
            "Epoch 4165/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8395\n",
            "Epoch 4165: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3106 - accuracy: 0.8398 - val_loss: 0.3664 - val_accuracy: 0.7885\n",
            "Epoch 4166/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.8404\n",
            "Epoch 4166: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3129 - accuracy: 0.8403 - val_loss: 0.4251 - val_accuracy: 0.7406\n",
            "Epoch 4167/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8456\n",
            "Epoch 4167: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3022 - accuracy: 0.8451 - val_loss: 0.4192 - val_accuracy: 0.7509\n",
            "Epoch 4168/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3067 - accuracy: 0.8440\n",
            "Epoch 4168: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3079 - accuracy: 0.8432 - val_loss: 0.4344 - val_accuracy: 0.7402\n",
            "Epoch 4169/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8363\n",
            "Epoch 4169: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3220 - accuracy: 0.8363 - val_loss: 0.4645 - val_accuracy: 0.7160\n",
            "Epoch 4170/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8373\n",
            "Epoch 4170: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3217 - accuracy: 0.8369 - val_loss: 0.4619 - val_accuracy: 0.7103\n",
            "Epoch 4171/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.8387\n",
            "Epoch 4171: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3185 - accuracy: 0.8387 - val_loss: 0.5879 - val_accuracy: 0.6474\n",
            "Epoch 4172/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.8359\n",
            "Epoch 4172: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3232 - accuracy: 0.8359 - val_loss: 0.4385 - val_accuracy: 0.7501\n",
            "Epoch 4173/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3103 - accuracy: 0.8404\n",
            "Epoch 4173: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3100 - accuracy: 0.8403 - val_loss: 0.4587 - val_accuracy: 0.7141\n",
            "Epoch 4174/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3037 - accuracy: 0.8444\n",
            "Epoch 4174: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3037 - accuracy: 0.8447 - val_loss: 0.4002 - val_accuracy: 0.7580\n",
            "Epoch 4175/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3059 - accuracy: 0.8422\n",
            "Epoch 4175: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3053 - accuracy: 0.8432 - val_loss: 0.5487 - val_accuracy: 0.6658\n",
            "Epoch 4176/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8449\n",
            "Epoch 4176: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3038 - accuracy: 0.8449 - val_loss: 0.4517 - val_accuracy: 0.7211\n",
            "Epoch 4177/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8443\n",
            "Epoch 4177: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3017 - accuracy: 0.8447 - val_loss: 0.4359 - val_accuracy: 0.7305\n",
            "Epoch 4178/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.8446\n",
            "Epoch 4178: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3032 - accuracy: 0.8446 - val_loss: 0.4904 - val_accuracy: 0.7055\n",
            "Epoch 4179/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8451\n",
            "Epoch 4179: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3044 - accuracy: 0.8449 - val_loss: 0.4049 - val_accuracy: 0.7483\n",
            "Epoch 4180/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8416\n",
            "Epoch 4180: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3084 - accuracy: 0.8417 - val_loss: 0.4470 - val_accuracy: 0.7250\n",
            "Epoch 4181/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8458\n",
            "Epoch 4181: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3024 - accuracy: 0.8454 - val_loss: 0.4296 - val_accuracy: 0.7325\n",
            "Epoch 4182/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3016 - accuracy: 0.8468\n",
            "Epoch 4182: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3032 - accuracy: 0.8456 - val_loss: 0.5907 - val_accuracy: 0.6629\n",
            "Epoch 4183/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3236 - accuracy: 0.8341\n",
            "Epoch 4183: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3246 - accuracy: 0.8330 - val_loss: 0.4039 - val_accuracy: 0.7665\n",
            "Epoch 4184/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3121 - accuracy: 0.8388\n",
            "Epoch 4184: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3134 - accuracy: 0.8378 - val_loss: 0.3929 - val_accuracy: 0.7665\n",
            "Epoch 4185/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3109 - accuracy: 0.8408\n",
            "Epoch 4185: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3102 - accuracy: 0.8407 - val_loss: 0.4574 - val_accuracy: 0.7101\n",
            "Epoch 4186/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8427\n",
            "Epoch 4186: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3091 - accuracy: 0.8425 - val_loss: 0.4100 - val_accuracy: 0.7507\n",
            "Epoch 4187/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8459\n",
            "Epoch 4187: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3045 - accuracy: 0.8456 - val_loss: 0.5040 - val_accuracy: 0.6987\n",
            "Epoch 4188/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8404\n",
            "Epoch 4188: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3146 - accuracy: 0.8405 - val_loss: 0.4212 - val_accuracy: 0.7437\n",
            "Epoch 4189/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3057 - accuracy: 0.8450\n",
            "Epoch 4189: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3059 - accuracy: 0.8449 - val_loss: 0.3982 - val_accuracy: 0.7547\n",
            "Epoch 4190/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3159 - accuracy: 0.8368\n",
            "Epoch 4190: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3159 - accuracy: 0.8368 - val_loss: 0.5756 - val_accuracy: 0.6588\n",
            "Epoch 4191/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3187 - accuracy: 0.8355\n",
            "Epoch 4191: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3174 - accuracy: 0.8367 - val_loss: 0.3956 - val_accuracy: 0.7652\n",
            "Epoch 4192/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8441\n",
            "Epoch 4192: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3066 - accuracy: 0.8439 - val_loss: 0.3981 - val_accuracy: 0.7746\n",
            "Epoch 4193/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3034 - accuracy: 0.8434\n",
            "Epoch 4193: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3034 - accuracy: 0.8434 - val_loss: 0.4237 - val_accuracy: 0.7465\n",
            "Epoch 4194/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.8494\n",
            "Epoch 4194: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2999 - accuracy: 0.8486 - val_loss: 0.3585 - val_accuracy: 0.7955\n",
            "Epoch 4195/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8421\n",
            "Epoch 4195: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3104 - accuracy: 0.8421 - val_loss: 0.5331 - val_accuracy: 0.6860\n",
            "Epoch 4196/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8389\n",
            "Epoch 4196: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3101 - accuracy: 0.8394 - val_loss: 0.4073 - val_accuracy: 0.7538\n",
            "Epoch 4197/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2986 - accuracy: 0.8474\n",
            "Epoch 4197: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2991 - accuracy: 0.8468 - val_loss: 0.4278 - val_accuracy: 0.7378\n",
            "Epoch 4198/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3091 - accuracy: 0.8386\n",
            "Epoch 4198: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3110 - accuracy: 0.8366 - val_loss: 0.4610 - val_accuracy: 0.7152\n",
            "Epoch 4199/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.8418\n",
            "Epoch 4199: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3032 - accuracy: 0.8418 - val_loss: 0.4053 - val_accuracy: 0.7582\n",
            "Epoch 4200/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3045 - accuracy: 0.8443\n",
            "Epoch 4200: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3040 - accuracy: 0.8447 - val_loss: 0.4594 - val_accuracy: 0.7202\n",
            "Epoch 4201/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3047 - accuracy: 0.8448\n",
            "Epoch 4201: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3058 - accuracy: 0.8436 - val_loss: 0.4650 - val_accuracy: 0.7062\n",
            "Epoch 4202/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3082 - accuracy: 0.8446\n",
            "Epoch 4202: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3084 - accuracy: 0.8437 - val_loss: 0.4405 - val_accuracy: 0.7481\n",
            "Epoch 4203/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3060 - accuracy: 0.8429\n",
            "Epoch 4203: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3047 - accuracy: 0.8437 - val_loss: 0.4369 - val_accuracy: 0.7343\n",
            "Epoch 4204/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8445\n",
            "Epoch 4204: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3030 - accuracy: 0.8442 - val_loss: 0.4484 - val_accuracy: 0.7371\n",
            "Epoch 4205/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8459\n",
            "Epoch 4205: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3001 - accuracy: 0.8458 - val_loss: 0.5030 - val_accuracy: 0.6939\n",
            "Epoch 4206/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8436\n",
            "Epoch 4206: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3068 - accuracy: 0.8438 - val_loss: 0.3959 - val_accuracy: 0.7645\n",
            "Epoch 4207/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3265 - accuracy: 0.8340\n",
            "Epoch 4207: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3245 - accuracy: 0.8353 - val_loss: 0.4168 - val_accuracy: 0.7670\n",
            "Epoch 4208/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3091 - accuracy: 0.8385\n",
            "Epoch 4208: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3087 - accuracy: 0.8386 - val_loss: 0.3989 - val_accuracy: 0.7656\n",
            "Epoch 4209/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3051 - accuracy: 0.8431\n",
            "Epoch 4209: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3064 - accuracy: 0.8418 - val_loss: 0.4682 - val_accuracy: 0.7261\n",
            "Epoch 4210/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3010 - accuracy: 0.8465\n",
            "Epoch 4210: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3013 - accuracy: 0.8462 - val_loss: 0.5098 - val_accuracy: 0.7051\n",
            "Epoch 4211/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3077 - accuracy: 0.8432\n",
            "Epoch 4211: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3071 - accuracy: 0.8434 - val_loss: 0.3674 - val_accuracy: 0.7889\n",
            "Epoch 4212/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3097 - accuracy: 0.8444\n",
            "Epoch 4212: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3102 - accuracy: 0.8439 - val_loss: 0.4413 - val_accuracy: 0.7301\n",
            "Epoch 4213/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3087 - accuracy: 0.8433\n",
            "Epoch 4213: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3097 - accuracy: 0.8427 - val_loss: 0.4011 - val_accuracy: 0.7551\n",
            "Epoch 4214/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3088 - accuracy: 0.8423\n",
            "Epoch 4214: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3096 - accuracy: 0.8415 - val_loss: 0.5535 - val_accuracy: 0.6689\n",
            "Epoch 4215/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3212 - accuracy: 0.8353\n",
            "Epoch 4215: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3227 - accuracy: 0.8349 - val_loss: 0.3604 - val_accuracy: 0.7867\n",
            "Epoch 4216/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8388\n",
            "Epoch 4216: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3148 - accuracy: 0.8388 - val_loss: 0.4357 - val_accuracy: 0.7358\n",
            "Epoch 4217/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3083 - accuracy: 0.8413\n",
            "Epoch 4217: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3082 - accuracy: 0.8414 - val_loss: 0.5283 - val_accuracy: 0.6901\n",
            "Epoch 4218/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3078 - accuracy: 0.8408\n",
            "Epoch 4218: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3088 - accuracy: 0.8398 - val_loss: 0.4631 - val_accuracy: 0.7117\n",
            "Epoch 4219/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.8433\n",
            "Epoch 4219: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3039 - accuracy: 0.8433 - val_loss: 0.4166 - val_accuracy: 0.7476\n",
            "Epoch 4220/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3083 - accuracy: 0.8432\n",
            "Epoch 4220: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3079 - accuracy: 0.8431 - val_loss: 0.4151 - val_accuracy: 0.7544\n",
            "Epoch 4221/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.8414\n",
            "Epoch 4221: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3116 - accuracy: 0.8411 - val_loss: 0.4173 - val_accuracy: 0.7476\n",
            "Epoch 4222/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3065 - accuracy: 0.8434\n",
            "Epoch 4222: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3073 - accuracy: 0.8421 - val_loss: 0.4245 - val_accuracy: 0.7454\n",
            "Epoch 4223/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8424\n",
            "Epoch 4223: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3128 - accuracy: 0.8424 - val_loss: 0.3635 - val_accuracy: 0.7865\n",
            "Epoch 4224/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3107 - accuracy: 0.8429\n",
            "Epoch 4224: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3107 - accuracy: 0.8425 - val_loss: 0.3962 - val_accuracy: 0.7617\n",
            "Epoch 4225/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3175 - accuracy: 0.8348\n",
            "Epoch 4225: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3151 - accuracy: 0.8361 - val_loss: 0.4749 - val_accuracy: 0.7213\n",
            "Epoch 4226/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8443\n",
            "Epoch 4226: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3038 - accuracy: 0.8440 - val_loss: 0.3860 - val_accuracy: 0.7731\n",
            "Epoch 4227/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.8408\n",
            "Epoch 4227: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3089 - accuracy: 0.8408 - val_loss: 0.4841 - val_accuracy: 0.7123\n",
            "Epoch 4228/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3016 - accuracy: 0.8440\n",
            "Epoch 4228: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3027 - accuracy: 0.8430 - val_loss: 0.6161 - val_accuracy: 0.6577\n",
            "Epoch 4229/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3140 - accuracy: 0.8395\n",
            "Epoch 4229: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3129 - accuracy: 0.8400 - val_loss: 0.5240 - val_accuracy: 0.6838\n",
            "Epoch 4230/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8412\n",
            "Epoch 4230: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3121 - accuracy: 0.8412 - val_loss: 0.4623 - val_accuracy: 0.7127\n",
            "Epoch 4231/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.8412\n",
            "Epoch 4231: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3140 - accuracy: 0.8411 - val_loss: 0.4274 - val_accuracy: 0.7553\n",
            "Epoch 4232/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8419\n",
            "Epoch 4232: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3107 - accuracy: 0.8408 - val_loss: 0.4810 - val_accuracy: 0.7055\n",
            "Epoch 4233/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8422\n",
            "Epoch 4233: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3012 - accuracy: 0.8425 - val_loss: 0.4536 - val_accuracy: 0.7235\n",
            "Epoch 4234/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3084 - accuracy: 0.8423\n",
            "Epoch 4234: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3097 - accuracy: 0.8415 - val_loss: 0.3883 - val_accuracy: 0.7617\n",
            "Epoch 4235/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3067 - accuracy: 0.8424\n",
            "Epoch 4235: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3084 - accuracy: 0.8411 - val_loss: 0.4821 - val_accuracy: 0.7211\n",
            "Epoch 4236/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3068 - accuracy: 0.8449\n",
            "Epoch 4236: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3072 - accuracy: 0.8443 - val_loss: 0.5192 - val_accuracy: 0.6912\n",
            "Epoch 4237/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8409\n",
            "Epoch 4237: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3089 - accuracy: 0.8409 - val_loss: 0.4464 - val_accuracy: 0.7202\n",
            "Epoch 4238/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3036 - accuracy: 0.8459\n",
            "Epoch 4238: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3042 - accuracy: 0.8454 - val_loss: 0.3626 - val_accuracy: 0.7966\n",
            "Epoch 4239/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3122 - accuracy: 0.8424\n",
            "Epoch 4239: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3132 - accuracy: 0.8415 - val_loss: 0.4736 - val_accuracy: 0.7209\n",
            "Epoch 4240/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3120 - accuracy: 0.8418\n",
            "Epoch 4240: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3119 - accuracy: 0.8416 - val_loss: 0.3400 - val_accuracy: 0.8069\n",
            "Epoch 4241/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.8410\n",
            "Epoch 4241: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3129 - accuracy: 0.8410 - val_loss: 0.4338 - val_accuracy: 0.7303\n",
            "Epoch 4242/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3040 - accuracy: 0.8440\n",
            "Epoch 4242: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3045 - accuracy: 0.8434 - val_loss: 0.3902 - val_accuracy: 0.7685\n",
            "Epoch 4243/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8399\n",
            "Epoch 4243: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3108 - accuracy: 0.8399 - val_loss: 0.4261 - val_accuracy: 0.7408\n",
            "Epoch 4244/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3143 - accuracy: 0.8372\n",
            "Epoch 4244: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3154 - accuracy: 0.8368 - val_loss: 0.5760 - val_accuracy: 0.6704\n",
            "Epoch 4245/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3266 - accuracy: 0.8325\n",
            "Epoch 4245: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3256 - accuracy: 0.8329 - val_loss: 0.4333 - val_accuracy: 0.7461\n",
            "Epoch 4246/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3051 - accuracy: 0.8437\n",
            "Epoch 4246: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3067 - accuracy: 0.8420 - val_loss: 0.4587 - val_accuracy: 0.7097\n",
            "Epoch 4247/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8384\n",
            "Epoch 4247: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3185 - accuracy: 0.8371 - val_loss: 0.3882 - val_accuracy: 0.7683\n",
            "Epoch 4248/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3130 - accuracy: 0.8395\n",
            "Epoch 4248: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3128 - accuracy: 0.8400 - val_loss: 0.3983 - val_accuracy: 0.7733\n",
            "Epoch 4249/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3017 - accuracy: 0.8473\n",
            "Epoch 4249: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3039 - accuracy: 0.8453 - val_loss: 0.4859 - val_accuracy: 0.6961\n",
            "Epoch 4250/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3024 - accuracy: 0.8462\n",
            "Epoch 4250: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3023 - accuracy: 0.8463 - val_loss: 0.4242 - val_accuracy: 0.7505\n",
            "Epoch 4251/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2989 - accuracy: 0.8472\n",
            "Epoch 4251: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3002 - accuracy: 0.8464 - val_loss: 0.4883 - val_accuracy: 0.7114\n",
            "Epoch 4252/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.8459\n",
            "Epoch 4252: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.2998 - accuracy: 0.8458 - val_loss: 0.4150 - val_accuracy: 0.7428\n",
            "Epoch 4253/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3062 - accuracy: 0.8436\n",
            "Epoch 4253: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3054 - accuracy: 0.8440 - val_loss: 0.4905 - val_accuracy: 0.7084\n",
            "Epoch 4254/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3089 - accuracy: 0.8410\n",
            "Epoch 4254: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3091 - accuracy: 0.8403 - val_loss: 0.4766 - val_accuracy: 0.7257\n",
            "Epoch 4255/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2988 - accuracy: 0.8467\n",
            "Epoch 4255: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2994 - accuracy: 0.8457 - val_loss: 0.5334 - val_accuracy: 0.6805\n",
            "Epoch 4256/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2999 - accuracy: 0.8444\n",
            "Epoch 4256: loss did not improve from 0.29665\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2993 - accuracy: 0.8444 - val_loss: 0.4289 - val_accuracy: 0.7395\n",
            "Epoch 4257/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.8490\n",
            "Epoch 4257: loss improved from 0.29665 to 0.29404, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.2940 - accuracy: 0.8487 - val_loss: 0.4495 - val_accuracy: 0.7257\n",
            "Epoch 4258/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8445\n",
            "Epoch 4258: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3009 - accuracy: 0.8440 - val_loss: 0.5045 - val_accuracy: 0.6959\n",
            "Epoch 4259/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.8474\n",
            "Epoch 4259: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3016 - accuracy: 0.8474 - val_loss: 0.4622 - val_accuracy: 0.7259\n",
            "Epoch 4260/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3149 - accuracy: 0.8382\n",
            "Epoch 4260: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3150 - accuracy: 0.8378 - val_loss: 0.4567 - val_accuracy: 0.7220\n",
            "Epoch 4261/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3051 - accuracy: 0.8439\n",
            "Epoch 4261: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3062 - accuracy: 0.8430 - val_loss: 0.4503 - val_accuracy: 0.7253\n",
            "Epoch 4262/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8468\n",
            "Epoch 4262: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3018 - accuracy: 0.8467 - val_loss: 0.4705 - val_accuracy: 0.7147\n",
            "Epoch 4263/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8437\n",
            "Epoch 4263: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3014 - accuracy: 0.8435 - val_loss: 0.4557 - val_accuracy: 0.7198\n",
            "Epoch 4264/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.8457\n",
            "Epoch 4264: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3033 - accuracy: 0.8462 - val_loss: 0.3640 - val_accuracy: 0.7933\n",
            "Epoch 4265/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2971 - accuracy: 0.8463\n",
            "Epoch 4265: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2972 - accuracy: 0.8464 - val_loss: 0.3821 - val_accuracy: 0.7731\n",
            "Epoch 4266/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2987 - accuracy: 0.8496\n",
            "Epoch 4266: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3001 - accuracy: 0.8482 - val_loss: 0.4069 - val_accuracy: 0.7571\n",
            "Epoch 4267/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3131 - accuracy: 0.8384\n",
            "Epoch 4267: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3127 - accuracy: 0.8384 - val_loss: 0.4614 - val_accuracy: 0.7178\n",
            "Epoch 4268/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3070 - accuracy: 0.8416\n",
            "Epoch 4268: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3066 - accuracy: 0.8417 - val_loss: 0.4968 - val_accuracy: 0.7020\n",
            "Epoch 4269/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8476\n",
            "Epoch 4269: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2995 - accuracy: 0.8471 - val_loss: 0.3802 - val_accuracy: 0.7746\n",
            "Epoch 4270/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3041 - accuracy: 0.8452\n",
            "Epoch 4270: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3031 - accuracy: 0.8454 - val_loss: 0.4359 - val_accuracy: 0.7397\n",
            "Epoch 4271/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3080 - accuracy: 0.8456\n",
            "Epoch 4271: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3102 - accuracy: 0.8449 - val_loss: 0.3833 - val_accuracy: 0.7762\n",
            "Epoch 4272/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3115 - accuracy: 0.8414\n",
            "Epoch 4272: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3153 - accuracy: 0.8390 - val_loss: 0.6021 - val_accuracy: 0.6605\n",
            "Epoch 4273/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8397\n",
            "Epoch 4273: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3120 - accuracy: 0.8402 - val_loss: 0.4652 - val_accuracy: 0.7336\n",
            "Epoch 4274/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3093 - accuracy: 0.8430\n",
            "Epoch 4274: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3112 - accuracy: 0.8420 - val_loss: 0.5726 - val_accuracy: 0.6684\n",
            "Epoch 4275/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3211 - accuracy: 0.8357\n",
            "Epoch 4275: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3217 - accuracy: 0.8355 - val_loss: 0.4797 - val_accuracy: 0.7145\n",
            "Epoch 4276/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.8400\n",
            "Epoch 4276: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3089 - accuracy: 0.8399 - val_loss: 0.4247 - val_accuracy: 0.7389\n",
            "Epoch 4277/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8467\n",
            "Epoch 4277: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3020 - accuracy: 0.8470 - val_loss: 0.5458 - val_accuracy: 0.6741\n",
            "Epoch 4278/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8463\n",
            "Epoch 4278: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3007 - accuracy: 0.8463 - val_loss: 0.5252 - val_accuracy: 0.6945\n",
            "Epoch 4279/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3038 - accuracy: 0.8449\n",
            "Epoch 4279: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3043 - accuracy: 0.8441 - val_loss: 0.4363 - val_accuracy: 0.7233\n",
            "Epoch 4280/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8482\n",
            "Epoch 4280: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2996 - accuracy: 0.8480 - val_loss: 0.4148 - val_accuracy: 0.7476\n",
            "Epoch 4281/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3132 - accuracy: 0.8377\n",
            "Epoch 4281: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3128 - accuracy: 0.8379 - val_loss: 0.3653 - val_accuracy: 0.7893\n",
            "Epoch 4282/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3155 - accuracy: 0.8411\n",
            "Epoch 4282: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3187 - accuracy: 0.8389 - val_loss: 0.5023 - val_accuracy: 0.6976\n",
            "Epoch 4283/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8368\n",
            "Epoch 4283: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3157 - accuracy: 0.8368 - val_loss: 0.3794 - val_accuracy: 0.7819\n",
            "Epoch 4284/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3053 - accuracy: 0.8456\n",
            "Epoch 4284: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3078 - accuracy: 0.8442 - val_loss: 0.5550 - val_accuracy: 0.6671\n",
            "Epoch 4285/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3071 - accuracy: 0.8424\n",
            "Epoch 4285: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3068 - accuracy: 0.8423 - val_loss: 0.3631 - val_accuracy: 0.7946\n",
            "Epoch 4286/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8448\n",
            "Epoch 4286: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3025 - accuracy: 0.8445 - val_loss: 0.3838 - val_accuracy: 0.7764\n",
            "Epoch 4287/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8436\n",
            "Epoch 4287: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3045 - accuracy: 0.8436 - val_loss: 0.4476 - val_accuracy: 0.7362\n",
            "Epoch 4288/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3159 - accuracy: 0.8398\n",
            "Epoch 4288: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3163 - accuracy: 0.8399 - val_loss: 0.4058 - val_accuracy: 0.7472\n",
            "Epoch 4289/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8410\n",
            "Epoch 4289: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3086 - accuracy: 0.8402 - val_loss: 0.3650 - val_accuracy: 0.7869\n",
            "Epoch 4290/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8459\n",
            "Epoch 4290: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3029 - accuracy: 0.8459 - val_loss: 0.4097 - val_accuracy: 0.7643\n",
            "Epoch 4291/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8447\n",
            "Epoch 4291: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3009 - accuracy: 0.8447 - val_loss: 0.6807 - val_accuracy: 0.6234\n",
            "Epoch 4292/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8327\n",
            "Epoch 4292: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3300 - accuracy: 0.8328 - val_loss: 0.4199 - val_accuracy: 0.7514\n",
            "Epoch 4293/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3088 - accuracy: 0.8408\n",
            "Epoch 4293: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3094 - accuracy: 0.8409 - val_loss: 0.4686 - val_accuracy: 0.7167\n",
            "Epoch 4294/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3090 - accuracy: 0.8420\n",
            "Epoch 4294: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3108 - accuracy: 0.8407 - val_loss: 0.4517 - val_accuracy: 0.7351\n",
            "Epoch 4295/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3150 - accuracy: 0.8378\n",
            "Epoch 4295: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3145 - accuracy: 0.8380 - val_loss: 0.3530 - val_accuracy: 0.7966\n",
            "Epoch 4296/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3040 - accuracy: 0.8445\n",
            "Epoch 4296: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3052 - accuracy: 0.8442 - val_loss: 0.4363 - val_accuracy: 0.7413\n",
            "Epoch 4297/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3064 - accuracy: 0.8419\n",
            "Epoch 4297: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3083 - accuracy: 0.8408 - val_loss: 0.5106 - val_accuracy: 0.6877\n",
            "Epoch 4298/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3048 - accuracy: 0.8446\n",
            "Epoch 4298: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3058 - accuracy: 0.8446 - val_loss: 0.4732 - val_accuracy: 0.7106\n",
            "Epoch 4299/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8434\n",
            "Epoch 4299: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3054 - accuracy: 0.8426 - val_loss: 0.4383 - val_accuracy: 0.7301\n",
            "Epoch 4300/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3065 - accuracy: 0.8448\n",
            "Epoch 4300: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3063 - accuracy: 0.8443 - val_loss: 0.4500 - val_accuracy: 0.7253\n",
            "Epoch 4301/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2999 - accuracy: 0.8458\n",
            "Epoch 4301: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3011 - accuracy: 0.8453 - val_loss: 0.4222 - val_accuracy: 0.7496\n",
            "Epoch 4302/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3120 - accuracy: 0.8389\n",
            "Epoch 4302: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3138 - accuracy: 0.8382 - val_loss: 0.5211 - val_accuracy: 0.6743\n",
            "Epoch 4303/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3106 - accuracy: 0.8405\n",
            "Epoch 4303: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3099 - accuracy: 0.8417 - val_loss: 0.4465 - val_accuracy: 0.7360\n",
            "Epoch 4304/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2969 - accuracy: 0.8478\n",
            "Epoch 4304: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2960 - accuracy: 0.8482 - val_loss: 0.4082 - val_accuracy: 0.7619\n",
            "Epoch 4305/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8453\n",
            "Epoch 4305: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3027 - accuracy: 0.8454 - val_loss: 0.4212 - val_accuracy: 0.7468\n",
            "Epoch 4306/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8416\n",
            "Epoch 4306: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3094 - accuracy: 0.8415 - val_loss: 0.4317 - val_accuracy: 0.7296\n",
            "Epoch 4307/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.8475\n",
            "Epoch 4307: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2982 - accuracy: 0.8475 - val_loss: 0.3776 - val_accuracy: 0.7887\n",
            "Epoch 4308/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3012 - accuracy: 0.8453\n",
            "Epoch 4308: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3019 - accuracy: 0.8449 - val_loss: 0.4435 - val_accuracy: 0.7351\n",
            "Epoch 4309/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3044 - accuracy: 0.8454\n",
            "Epoch 4309: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3054 - accuracy: 0.8446 - val_loss: 0.3629 - val_accuracy: 0.7937\n",
            "Epoch 4310/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3051 - accuracy: 0.8430\n",
            "Epoch 4310: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3083 - accuracy: 0.8410 - val_loss: 0.4724 - val_accuracy: 0.7007\n",
            "Epoch 4311/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8436\n",
            "Epoch 4311: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3062 - accuracy: 0.8425 - val_loss: 0.4022 - val_accuracy: 0.7652\n",
            "Epoch 4312/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3072 - accuracy: 0.8423\n",
            "Epoch 4312: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3073 - accuracy: 0.8423 - val_loss: 0.4879 - val_accuracy: 0.7112\n",
            "Epoch 4313/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8438\n",
            "Epoch 4313: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3071 - accuracy: 0.8430 - val_loss: 0.4093 - val_accuracy: 0.7641\n",
            "Epoch 4314/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.8437\n",
            "Epoch 4314: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3033 - accuracy: 0.8435 - val_loss: 0.4401 - val_accuracy: 0.7364\n",
            "Epoch 4315/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8431\n",
            "Epoch 4315: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3035 - accuracy: 0.8432 - val_loss: 0.3649 - val_accuracy: 0.7858\n",
            "Epoch 4316/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8441\n",
            "Epoch 4316: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3057 - accuracy: 0.8444 - val_loss: 0.4469 - val_accuracy: 0.7279\n",
            "Epoch 4317/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2976 - accuracy: 0.8471\n",
            "Epoch 4317: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2987 - accuracy: 0.8463 - val_loss: 0.4319 - val_accuracy: 0.7345\n",
            "Epoch 4318/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.8447\n",
            "Epoch 4318: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2970 - accuracy: 0.8455 - val_loss: 0.3725 - val_accuracy: 0.7849\n",
            "Epoch 4319/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3043 - accuracy: 0.8471\n",
            "Epoch 4319: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3048 - accuracy: 0.8468 - val_loss: 0.3944 - val_accuracy: 0.7586\n",
            "Epoch 4320/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8393\n",
            "Epoch 4320: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3154 - accuracy: 0.8390 - val_loss: 0.4613 - val_accuracy: 0.7130\n",
            "Epoch 4321/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3022 - accuracy: 0.8476\n",
            "Epoch 4321: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3057 - accuracy: 0.8441 - val_loss: 0.4341 - val_accuracy: 0.7446\n",
            "Epoch 4322/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8416\n",
            "Epoch 4322: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3054 - accuracy: 0.8414 - val_loss: 0.4661 - val_accuracy: 0.7281\n",
            "Epoch 4323/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3024 - accuracy: 0.8448\n",
            "Epoch 4323: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3034 - accuracy: 0.8446 - val_loss: 0.4067 - val_accuracy: 0.7531\n",
            "Epoch 4324/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3038 - accuracy: 0.8446\n",
            "Epoch 4324: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3039 - accuracy: 0.8444 - val_loss: 0.3883 - val_accuracy: 0.7713\n",
            "Epoch 4325/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8420\n",
            "Epoch 4325: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3053 - accuracy: 0.8420 - val_loss: 0.4032 - val_accuracy: 0.7617\n",
            "Epoch 4326/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3074 - accuracy: 0.8419\n",
            "Epoch 4326: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3076 - accuracy: 0.8421 - val_loss: 0.3882 - val_accuracy: 0.7722\n",
            "Epoch 4327/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3046 - accuracy: 0.8440\n",
            "Epoch 4327: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3052 - accuracy: 0.8436 - val_loss: 0.4269 - val_accuracy: 0.7375\n",
            "Epoch 4328/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3154 - accuracy: 0.8409\n",
            "Epoch 4328: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3164 - accuracy: 0.8401 - val_loss: 0.5214 - val_accuracy: 0.6737\n",
            "Epoch 4329/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3231 - accuracy: 0.8355\n",
            "Epoch 4329: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3225 - accuracy: 0.8353 - val_loss: 0.4202 - val_accuracy: 0.7452\n",
            "Epoch 4330/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8404\n",
            "Epoch 4330: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3049 - accuracy: 0.8405 - val_loss: 0.4325 - val_accuracy: 0.7393\n",
            "Epoch 4331/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.8424\n",
            "Epoch 4331: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3078 - accuracy: 0.8425 - val_loss: 0.5614 - val_accuracy: 0.6761\n",
            "Epoch 4332/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8383\n",
            "Epoch 4332: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3120 - accuracy: 0.8386 - val_loss: 0.4789 - val_accuracy: 0.7020\n",
            "Epoch 4333/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3044 - accuracy: 0.8447\n",
            "Epoch 4333: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3049 - accuracy: 0.8450 - val_loss: 0.4175 - val_accuracy: 0.7461\n",
            "Epoch 4334/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8462\n",
            "Epoch 4334: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3022 - accuracy: 0.8455 - val_loss: 0.4129 - val_accuracy: 0.7573\n",
            "Epoch 4335/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3040 - accuracy: 0.8452\n",
            "Epoch 4335: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3052 - accuracy: 0.8443 - val_loss: 0.4265 - val_accuracy: 0.7334\n",
            "Epoch 4336/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.8436\n",
            "Epoch 4336: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3099 - accuracy: 0.8431 - val_loss: 0.6311 - val_accuracy: 0.6572\n",
            "Epoch 4337/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8398\n",
            "Epoch 4337: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3133 - accuracy: 0.8404 - val_loss: 0.4627 - val_accuracy: 0.7283\n",
            "Epoch 4338/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8423\n",
            "Epoch 4338: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3052 - accuracy: 0.8423 - val_loss: 0.4030 - val_accuracy: 0.7650\n",
            "Epoch 4339/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3136 - accuracy: 0.8417\n",
            "Epoch 4339: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3160 - accuracy: 0.8393 - val_loss: 0.5385 - val_accuracy: 0.6768\n",
            "Epoch 4340/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3119 - accuracy: 0.8427\n",
            "Epoch 4340: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3129 - accuracy: 0.8412 - val_loss: 0.4998 - val_accuracy: 0.6849\n",
            "Epoch 4341/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8437\n",
            "Epoch 4341: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3101 - accuracy: 0.8437 - val_loss: 0.4846 - val_accuracy: 0.7095\n",
            "Epoch 4342/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3013 - accuracy: 0.8477\n",
            "Epoch 4342: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3035 - accuracy: 0.8457 - val_loss: 0.3664 - val_accuracy: 0.7876\n",
            "Epoch 4343/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8475\n",
            "Epoch 4343: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 27ms/step - loss: 0.2986 - accuracy: 0.8475 - val_loss: 0.4367 - val_accuracy: 0.7406\n",
            "Epoch 4344/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3168 - accuracy: 0.8386\n",
            "Epoch 4344: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3182 - accuracy: 0.8376 - val_loss: 0.4162 - val_accuracy: 0.7443\n",
            "Epoch 4345/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3088 - accuracy: 0.8417\n",
            "Epoch 4345: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3084 - accuracy: 0.8419 - val_loss: 0.5024 - val_accuracy: 0.7073\n",
            "Epoch 4346/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8465\n",
            "Epoch 4346: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3029 - accuracy: 0.8465 - val_loss: 0.4223 - val_accuracy: 0.7487\n",
            "Epoch 4347/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8450\n",
            "Epoch 4347: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3032 - accuracy: 0.8446 - val_loss: 0.4400 - val_accuracy: 0.7419\n",
            "Epoch 4348/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2994 - accuracy: 0.8473\n",
            "Epoch 4348: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2997 - accuracy: 0.8469 - val_loss: 0.4400 - val_accuracy: 0.7336\n",
            "Epoch 4349/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8499\n",
            "Epoch 4349: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2952 - accuracy: 0.8495 - val_loss: 0.4596 - val_accuracy: 0.7237\n",
            "Epoch 4350/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2967 - accuracy: 0.8477\n",
            "Epoch 4350: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2987 - accuracy: 0.8471 - val_loss: 0.4163 - val_accuracy: 0.7454\n",
            "Epoch 4351/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8430\n",
            "Epoch 4351: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3041 - accuracy: 0.8430 - val_loss: 0.3970 - val_accuracy: 0.7698\n",
            "Epoch 4352/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3152 - accuracy: 0.8383\n",
            "Epoch 4352: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3141 - accuracy: 0.8392 - val_loss: 0.4639 - val_accuracy: 0.7301\n",
            "Epoch 4353/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3110 - accuracy: 0.8397\n",
            "Epoch 4353: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3103 - accuracy: 0.8401 - val_loss: 0.4357 - val_accuracy: 0.7338\n",
            "Epoch 4354/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2986 - accuracy: 0.8465\n",
            "Epoch 4354: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2987 - accuracy: 0.8465 - val_loss: 0.3877 - val_accuracy: 0.7722\n",
            "Epoch 4355/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3110 - accuracy: 0.8427\n",
            "Epoch 4355: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3104 - accuracy: 0.8434 - val_loss: 0.4487 - val_accuracy: 0.7382\n",
            "Epoch 4356/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3100 - accuracy: 0.8416\n",
            "Epoch 4356: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3098 - accuracy: 0.8419 - val_loss: 0.4559 - val_accuracy: 0.7294\n",
            "Epoch 4357/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.8464\n",
            "Epoch 4357: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3054 - accuracy: 0.8454 - val_loss: 0.5255 - val_accuracy: 0.6803\n",
            "Epoch 4358/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3042 - accuracy: 0.8425\n",
            "Epoch 4358: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3053 - accuracy: 0.8417 - val_loss: 0.4485 - val_accuracy: 0.7147\n",
            "Epoch 4359/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3043 - accuracy: 0.8431\n",
            "Epoch 4359: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3041 - accuracy: 0.8425 - val_loss: 0.3622 - val_accuracy: 0.7896\n",
            "Epoch 4360/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3011 - accuracy: 0.8456\n",
            "Epoch 4360: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3003 - accuracy: 0.8466 - val_loss: 0.4250 - val_accuracy: 0.7520\n",
            "Epoch 4361/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8430\n",
            "Epoch 4361: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3023 - accuracy: 0.8431 - val_loss: 0.4381 - val_accuracy: 0.7389\n",
            "Epoch 4362/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3020 - accuracy: 0.8443\n",
            "Epoch 4362: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3028 - accuracy: 0.8444 - val_loss: 0.4308 - val_accuracy: 0.7373\n",
            "Epoch 4363/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8410\n",
            "Epoch 4363: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3102 - accuracy: 0.8410 - val_loss: 0.4367 - val_accuracy: 0.7283\n",
            "Epoch 4364/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3094 - accuracy: 0.8414\n",
            "Epoch 4364: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3106 - accuracy: 0.8407 - val_loss: 0.4696 - val_accuracy: 0.7057\n",
            "Epoch 4365/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8442\n",
            "Epoch 4365: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3071 - accuracy: 0.8429 - val_loss: 0.4222 - val_accuracy: 0.7400\n",
            "Epoch 4366/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3077 - accuracy: 0.8403\n",
            "Epoch 4366: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3086 - accuracy: 0.8398 - val_loss: 0.5247 - val_accuracy: 0.6522\n",
            "Epoch 4367/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3108 - accuracy: 0.8415\n",
            "Epoch 4367: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3110 - accuracy: 0.8414 - val_loss: 0.4318 - val_accuracy: 0.7433\n",
            "Epoch 4368/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3056 - accuracy: 0.8436\n",
            "Epoch 4368: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3059 - accuracy: 0.8429 - val_loss: 0.4564 - val_accuracy: 0.7187\n",
            "Epoch 4369/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.8463\n",
            "Epoch 4369: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3031 - accuracy: 0.8463 - val_loss: 0.3881 - val_accuracy: 0.7709\n",
            "Epoch 4370/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3039 - accuracy: 0.8439\n",
            "Epoch 4370: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3047 - accuracy: 0.8429 - val_loss: 0.4587 - val_accuracy: 0.7279\n",
            "Epoch 4371/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3005 - accuracy: 0.8441\n",
            "Epoch 4371: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3012 - accuracy: 0.8434 - val_loss: 0.4280 - val_accuracy: 0.7325\n",
            "Epoch 4372/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3025 - accuracy: 0.8457\n",
            "Epoch 4372: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3036 - accuracy: 0.8444 - val_loss: 0.3825 - val_accuracy: 0.7700\n",
            "Epoch 4373/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3039 - accuracy: 0.8426\n",
            "Epoch 4373: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3036 - accuracy: 0.8429 - val_loss: 0.4513 - val_accuracy: 0.7277\n",
            "Epoch 4374/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8386\n",
            "Epoch 4374: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3123 - accuracy: 0.8387 - val_loss: 0.4590 - val_accuracy: 0.7255\n",
            "Epoch 4375/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8409\n",
            "Epoch 4375: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.3099 - accuracy: 0.8410 - val_loss: 0.3816 - val_accuracy: 0.7683\n",
            "Epoch 4376/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3066 - accuracy: 0.8450\n",
            "Epoch 4376: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3066 - accuracy: 0.8452 - val_loss: 0.4755 - val_accuracy: 0.7086\n",
            "Epoch 4377/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8395\n",
            "Epoch 4377: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3132 - accuracy: 0.8390 - val_loss: 0.5883 - val_accuracy: 0.6550\n",
            "Epoch 4378/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3130 - accuracy: 0.8429\n",
            "Epoch 4378: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3124 - accuracy: 0.8431 - val_loss: 0.4631 - val_accuracy: 0.7141\n",
            "Epoch 4379/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8467\n",
            "Epoch 4379: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2986 - accuracy: 0.8467 - val_loss: 0.4303 - val_accuracy: 0.7386\n",
            "Epoch 4380/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3007 - accuracy: 0.8452\n",
            "Epoch 4380: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3009 - accuracy: 0.8451 - val_loss: 0.3995 - val_accuracy: 0.7645\n",
            "Epoch 4381/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8470\n",
            "Epoch 4381: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2990 - accuracy: 0.8469 - val_loss: 0.4476 - val_accuracy: 0.7294\n",
            "Epoch 4382/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8463\n",
            "Epoch 4382: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3030 - accuracy: 0.8461 - val_loss: 0.4191 - val_accuracy: 0.7428\n",
            "Epoch 4383/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3013 - accuracy: 0.8472\n",
            "Epoch 4383: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3030 - accuracy: 0.8457 - val_loss: 0.4422 - val_accuracy: 0.7316\n",
            "Epoch 4384/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2979 - accuracy: 0.8490\n",
            "Epoch 4384: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3003 - accuracy: 0.8475 - val_loss: 0.4238 - val_accuracy: 0.7426\n",
            "Epoch 4385/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8431\n",
            "Epoch 4385: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3043 - accuracy: 0.8435 - val_loss: 0.4862 - val_accuracy: 0.6928\n",
            "Epoch 4386/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3025 - accuracy: 0.8470\n",
            "Epoch 4386: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3048 - accuracy: 0.8451 - val_loss: 0.4582 - val_accuracy: 0.7316\n",
            "Epoch 4387/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3178 - accuracy: 0.8378\n",
            "Epoch 4387: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3189 - accuracy: 0.8368 - val_loss: 0.4314 - val_accuracy: 0.7318\n",
            "Epoch 4388/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3113 - accuracy: 0.8397\n",
            "Epoch 4388: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3128 - accuracy: 0.8390 - val_loss: 0.4801 - val_accuracy: 0.6899\n",
            "Epoch 4389/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3178 - accuracy: 0.8380\n",
            "Epoch 4389: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3183 - accuracy: 0.8382 - val_loss: 0.5587 - val_accuracy: 0.6785\n",
            "Epoch 4390/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8419\n",
            "Epoch 4390: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3102 - accuracy: 0.8419 - val_loss: 0.4310 - val_accuracy: 0.7479\n",
            "Epoch 4391/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3102 - accuracy: 0.8396\n",
            "Epoch 4391: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3115 - accuracy: 0.8391 - val_loss: 0.3979 - val_accuracy: 0.7617\n",
            "Epoch 4392/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3056 - accuracy: 0.8436\n",
            "Epoch 4392: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3078 - accuracy: 0.8418 - val_loss: 0.4116 - val_accuracy: 0.7533\n",
            "Epoch 4393/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3131 - accuracy: 0.8400\n",
            "Epoch 4393: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3140 - accuracy: 0.8396 - val_loss: 0.4388 - val_accuracy: 0.7272\n",
            "Epoch 4394/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3077 - accuracy: 0.8413\n",
            "Epoch 4394: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3078 - accuracy: 0.8415 - val_loss: 0.3860 - val_accuracy: 0.7742\n",
            "Epoch 4395/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3045 - accuracy: 0.8442\n",
            "Epoch 4395: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3039 - accuracy: 0.8445 - val_loss: 0.4270 - val_accuracy: 0.7443\n",
            "Epoch 4396/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3011 - accuracy: 0.8477\n",
            "Epoch 4396: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3012 - accuracy: 0.8471 - val_loss: 0.3840 - val_accuracy: 0.7744\n",
            "Epoch 4397/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2992 - accuracy: 0.8477\n",
            "Epoch 4397: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3011 - accuracy: 0.8455 - val_loss: 0.5723 - val_accuracy: 0.6478\n",
            "Epoch 4398/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8428\n",
            "Epoch 4398: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3049 - accuracy: 0.8426 - val_loss: 0.3686 - val_accuracy: 0.7911\n",
            "Epoch 4399/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.8449\n",
            "Epoch 4399: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3022 - accuracy: 0.8449 - val_loss: 0.5263 - val_accuracy: 0.6684\n",
            "Epoch 4400/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3024 - accuracy: 0.8426\n",
            "Epoch 4400: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3022 - accuracy: 0.8424 - val_loss: 0.3558 - val_accuracy: 0.8018\n",
            "Epoch 4401/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8452\n",
            "Epoch 4401: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3029 - accuracy: 0.8453 - val_loss: 0.4127 - val_accuracy: 0.7542\n",
            "Epoch 4402/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3170 - accuracy: 0.8368\n",
            "Epoch 4402: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3164 - accuracy: 0.8372 - val_loss: 0.4742 - val_accuracy: 0.7119\n",
            "Epoch 4403/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3005 - accuracy: 0.8475\n",
            "Epoch 4403: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3012 - accuracy: 0.8470 - val_loss: 0.4196 - val_accuracy: 0.7474\n",
            "Epoch 4404/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3057 - accuracy: 0.8439\n",
            "Epoch 4404: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3058 - accuracy: 0.8437 - val_loss: 0.4539 - val_accuracy: 0.7281\n",
            "Epoch 4405/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2980 - accuracy: 0.8483\n",
            "Epoch 4405: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2983 - accuracy: 0.8477 - val_loss: 0.4687 - val_accuracy: 0.7178\n",
            "Epoch 4406/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3019 - accuracy: 0.8475\n",
            "Epoch 4406: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3024 - accuracy: 0.8466 - val_loss: 0.4879 - val_accuracy: 0.7097\n",
            "Epoch 4407/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3262 - accuracy: 0.8346\n",
            "Epoch 4407: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3258 - accuracy: 0.8348 - val_loss: 0.3937 - val_accuracy: 0.7696\n",
            "Epoch 4408/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8442\n",
            "Epoch 4408: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3042 - accuracy: 0.8437 - val_loss: 0.4240 - val_accuracy: 0.7452\n",
            "Epoch 4409/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3053 - accuracy: 0.8439\n",
            "Epoch 4409: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3051 - accuracy: 0.8443 - val_loss: 0.4435 - val_accuracy: 0.7428\n",
            "Epoch 4410/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8417\n",
            "Epoch 4410: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3084 - accuracy: 0.8421 - val_loss: 0.4157 - val_accuracy: 0.7533\n",
            "Epoch 4411/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8431\n",
            "Epoch 4411: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3046 - accuracy: 0.8431 - val_loss: 0.3849 - val_accuracy: 0.7711\n",
            "Epoch 4412/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8396\n",
            "Epoch 4412: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3136 - accuracy: 0.8397 - val_loss: 0.4990 - val_accuracy: 0.6875\n",
            "Epoch 4413/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3042 - accuracy: 0.8442\n",
            "Epoch 4413: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3044 - accuracy: 0.8441 - val_loss: 0.4150 - val_accuracy: 0.7531\n",
            "Epoch 4414/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3071 - accuracy: 0.8431\n",
            "Epoch 4414: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3080 - accuracy: 0.8422 - val_loss: 0.5009 - val_accuracy: 0.6899\n",
            "Epoch 4415/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.8391\n",
            "Epoch 4415: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3092 - accuracy: 0.8391 - val_loss: 0.6306 - val_accuracy: 0.6491\n",
            "Epoch 4416/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8388\n",
            "Epoch 4416: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3127 - accuracy: 0.8388 - val_loss: 0.3827 - val_accuracy: 0.7670\n",
            "Epoch 4417/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.8475\n",
            "Epoch 4417: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3000 - accuracy: 0.8467 - val_loss: 0.4940 - val_accuracy: 0.6945\n",
            "Epoch 4418/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2998 - accuracy: 0.8460\n",
            "Epoch 4418: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3015 - accuracy: 0.8444 - val_loss: 0.4586 - val_accuracy: 0.7248\n",
            "Epoch 4419/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3101 - accuracy: 0.8425\n",
            "Epoch 4419: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3102 - accuracy: 0.8421 - val_loss: 0.3934 - val_accuracy: 0.7670\n",
            "Epoch 4420/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2946 - accuracy: 0.8483\n",
            "Epoch 4420: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2956 - accuracy: 0.8472 - val_loss: 0.4400 - val_accuracy: 0.7327\n",
            "Epoch 4421/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.8465\n",
            "Epoch 4421: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3013 - accuracy: 0.8465 - val_loss: 0.4843 - val_accuracy: 0.6983\n",
            "Epoch 4422/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3015 - accuracy: 0.8483\n",
            "Epoch 4422: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3027 - accuracy: 0.8470 - val_loss: 0.4385 - val_accuracy: 0.7358\n",
            "Epoch 4423/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2984 - accuracy: 0.8457\n",
            "Epoch 4423: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2993 - accuracy: 0.8456 - val_loss: 0.3859 - val_accuracy: 0.7711\n",
            "Epoch 4424/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8436\n",
            "Epoch 4424: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3087 - accuracy: 0.8435 - val_loss: 0.4232 - val_accuracy: 0.7468\n",
            "Epoch 4425/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8441\n",
            "Epoch 4425: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3047 - accuracy: 0.8437 - val_loss: 0.3963 - val_accuracy: 0.7663\n",
            "Epoch 4426/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3077 - accuracy: 0.8427\n",
            "Epoch 4426: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3105 - accuracy: 0.8409 - val_loss: 0.4576 - val_accuracy: 0.7092\n",
            "Epoch 4427/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3180 - accuracy: 0.8369\n",
            "Epoch 4427: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3174 - accuracy: 0.8368 - val_loss: 0.5019 - val_accuracy: 0.6961\n",
            "Epoch 4428/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8419\n",
            "Epoch 4428: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3106 - accuracy: 0.8418 - val_loss: 0.4580 - val_accuracy: 0.7253\n",
            "Epoch 4429/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3023 - accuracy: 0.8456\n",
            "Epoch 4429: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3026 - accuracy: 0.8449 - val_loss: 0.3967 - val_accuracy: 0.7595\n",
            "Epoch 4430/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8446\n",
            "Epoch 4430: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2996 - accuracy: 0.8447 - val_loss: 0.4286 - val_accuracy: 0.7382\n",
            "Epoch 4431/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2982 - accuracy: 0.8460\n",
            "Epoch 4431: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2999 - accuracy: 0.8444 - val_loss: 0.4089 - val_accuracy: 0.7525\n",
            "Epoch 4432/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8473\n",
            "Epoch 4432: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3006 - accuracy: 0.8469 - val_loss: 0.3925 - val_accuracy: 0.7731\n",
            "Epoch 4433/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.8441\n",
            "Epoch 4433: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3029 - accuracy: 0.8428 - val_loss: 0.4271 - val_accuracy: 0.7496\n",
            "Epoch 4434/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3103 - accuracy: 0.8394\n",
            "Epoch 4434: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3118 - accuracy: 0.8386 - val_loss: 0.5756 - val_accuracy: 0.6544\n",
            "Epoch 4435/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3159 - accuracy: 0.8389\n",
            "Epoch 4435: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3168 - accuracy: 0.8382 - val_loss: 0.5443 - val_accuracy: 0.6557\n",
            "Epoch 4436/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3120 - accuracy: 0.8411\n",
            "Epoch 4436: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3128 - accuracy: 0.8404 - val_loss: 0.4992 - val_accuracy: 0.6985\n",
            "Epoch 4437/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8405\n",
            "Epoch 4437: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3062 - accuracy: 0.8405 - val_loss: 0.4169 - val_accuracy: 0.7479\n",
            "Epoch 4438/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3053 - accuracy: 0.8433\n",
            "Epoch 4438: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3041 - accuracy: 0.8435 - val_loss: 0.3691 - val_accuracy: 0.7825\n",
            "Epoch 4439/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3028 - accuracy: 0.8433\n",
            "Epoch 4439: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3029 - accuracy: 0.8432 - val_loss: 0.4371 - val_accuracy: 0.7441\n",
            "Epoch 4440/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.8411\n",
            "Epoch 4440: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3081 - accuracy: 0.8411 - val_loss: 0.3867 - val_accuracy: 0.7663\n",
            "Epoch 4441/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3131 - accuracy: 0.8431\n",
            "Epoch 4441: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3136 - accuracy: 0.8415 - val_loss: 0.4249 - val_accuracy: 0.7496\n",
            "Epoch 4442/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.8421\n",
            "Epoch 4442: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3080 - accuracy: 0.8421 - val_loss: 0.5066 - val_accuracy: 0.6787\n",
            "Epoch 4443/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8441\n",
            "Epoch 4443: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3020 - accuracy: 0.8438 - val_loss: 0.4609 - val_accuracy: 0.7095\n",
            "Epoch 4444/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3005 - accuracy: 0.8473\n",
            "Epoch 4444: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3032 - accuracy: 0.8450 - val_loss: 0.4077 - val_accuracy: 0.7487\n",
            "Epoch 4445/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3072 - accuracy: 0.8413\n",
            "Epoch 4445: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3078 - accuracy: 0.8412 - val_loss: 0.5099 - val_accuracy: 0.7002\n",
            "Epoch 4446/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2975 - accuracy: 0.8482\n",
            "Epoch 4446: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2986 - accuracy: 0.8464 - val_loss: 0.4156 - val_accuracy: 0.7474\n",
            "Epoch 4447/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3049 - accuracy: 0.8429\n",
            "Epoch 4447: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3051 - accuracy: 0.8424 - val_loss: 0.3938 - val_accuracy: 0.7577\n",
            "Epoch 4448/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3166 - accuracy: 0.8364\n",
            "Epoch 4448: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3159 - accuracy: 0.8368 - val_loss: 0.4646 - val_accuracy: 0.7239\n",
            "Epoch 4449/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8493\n",
            "Epoch 4449: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3014 - accuracy: 0.8488 - val_loss: 0.3885 - val_accuracy: 0.7773\n",
            "Epoch 4450/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3032 - accuracy: 0.8449\n",
            "Epoch 4450: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3058 - accuracy: 0.8431 - val_loss: 0.4338 - val_accuracy: 0.7329\n",
            "Epoch 4451/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2954 - accuracy: 0.8493\n",
            "Epoch 4451: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2948 - accuracy: 0.8500 - val_loss: 0.4726 - val_accuracy: 0.7114\n",
            "Epoch 4452/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3000 - accuracy: 0.8469\n",
            "Epoch 4452: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3026 - accuracy: 0.8448 - val_loss: 0.3925 - val_accuracy: 0.7694\n",
            "Epoch 4453/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8423\n",
            "Epoch 4453: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3057 - accuracy: 0.8423 - val_loss: 0.4083 - val_accuracy: 0.7501\n",
            "Epoch 4454/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.8451\n",
            "Epoch 4454: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2996 - accuracy: 0.8449 - val_loss: 0.3820 - val_accuracy: 0.7814\n",
            "Epoch 4455/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8465\n",
            "Epoch 4455: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3011 - accuracy: 0.8465 - val_loss: 0.4738 - val_accuracy: 0.7191\n",
            "Epoch 4456/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8457\n",
            "Epoch 4456: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3033 - accuracy: 0.8456 - val_loss: 0.3616 - val_accuracy: 0.8007\n",
            "Epoch 4457/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2978 - accuracy: 0.8486\n",
            "Epoch 4457: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3014 - accuracy: 0.8452 - val_loss: 0.4713 - val_accuracy: 0.7064\n",
            "Epoch 4458/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3095 - accuracy: 0.8433\n",
            "Epoch 4458: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3137 - accuracy: 0.8410 - val_loss: 0.5219 - val_accuracy: 0.6926\n",
            "Epoch 4459/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3189 - accuracy: 0.8370\n",
            "Epoch 4459: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3194 - accuracy: 0.8362 - val_loss: 0.4169 - val_accuracy: 0.7542\n",
            "Epoch 4460/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8409\n",
            "Epoch 4460: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3097 - accuracy: 0.8409 - val_loss: 0.5306 - val_accuracy: 0.6697\n",
            "Epoch 4461/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8426\n",
            "Epoch 4461: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3107 - accuracy: 0.8418 - val_loss: 0.4771 - val_accuracy: 0.7132\n",
            "Epoch 4462/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3121 - accuracy: 0.8414\n",
            "Epoch 4462: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3143 - accuracy: 0.8396 - val_loss: 0.3666 - val_accuracy: 0.7891\n",
            "Epoch 4463/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3134 - accuracy: 0.8395\n",
            "Epoch 4463: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3138 - accuracy: 0.8390 - val_loss: 0.4771 - val_accuracy: 0.7101\n",
            "Epoch 4464/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3045 - accuracy: 0.8464\n",
            "Epoch 4464: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3042 - accuracy: 0.8463 - val_loss: 0.3901 - val_accuracy: 0.7724\n",
            "Epoch 4465/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3121 - accuracy: 0.8395\n",
            "Epoch 4465: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3112 - accuracy: 0.8403 - val_loss: 0.5430 - val_accuracy: 0.6829\n",
            "Epoch 4466/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3093 - accuracy: 0.8398\n",
            "Epoch 4466: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3097 - accuracy: 0.8402 - val_loss: 0.4776 - val_accuracy: 0.7013\n",
            "Epoch 4467/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3078 - accuracy: 0.8455\n",
            "Epoch 4467: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3095 - accuracy: 0.8443 - val_loss: 0.3760 - val_accuracy: 0.7770\n",
            "Epoch 4468/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8459\n",
            "Epoch 4468: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3042 - accuracy: 0.8458 - val_loss: 0.7051 - val_accuracy: 0.6155\n",
            "Epoch 4469/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8349\n",
            "Epoch 4469: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3271 - accuracy: 0.8346 - val_loss: 0.6481 - val_accuracy: 0.6357\n",
            "Epoch 4470/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.8387\n",
            "Epoch 4470: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3125 - accuracy: 0.8387 - val_loss: 0.4812 - val_accuracy: 0.7005\n",
            "Epoch 4471/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8402\n",
            "Epoch 4471: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3087 - accuracy: 0.8402 - val_loss: 0.3555 - val_accuracy: 0.7983\n",
            "Epoch 4472/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3180 - accuracy: 0.8385\n",
            "Epoch 4472: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3180 - accuracy: 0.8384 - val_loss: 0.4670 - val_accuracy: 0.7196\n",
            "Epoch 4473/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3082 - accuracy: 0.8433\n",
            "Epoch 4473: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3087 - accuracy: 0.8427 - val_loss: 0.4930 - val_accuracy: 0.7002\n",
            "Epoch 4474/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8425\n",
            "Epoch 4474: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3045 - accuracy: 0.8425 - val_loss: 0.3937 - val_accuracy: 0.7663\n",
            "Epoch 4475/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2942 - accuracy: 0.8507\n",
            "Epoch 4475: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2949 - accuracy: 0.8501 - val_loss: 0.4172 - val_accuracy: 0.7382\n",
            "Epoch 4476/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2971 - accuracy: 0.8470\n",
            "Epoch 4476: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2996 - accuracy: 0.8457 - val_loss: 0.5191 - val_accuracy: 0.6783\n",
            "Epoch 4477/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3044 - accuracy: 0.8444\n",
            "Epoch 4477: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3044 - accuracy: 0.8442 - val_loss: 0.5111 - val_accuracy: 0.6829\n",
            "Epoch 4478/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2993 - accuracy: 0.8451\n",
            "Epoch 4478: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2983 - accuracy: 0.8458 - val_loss: 0.4507 - val_accuracy: 0.7185\n",
            "Epoch 4479/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.8467\n",
            "Epoch 4479: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2995 - accuracy: 0.8464 - val_loss: 0.4043 - val_accuracy: 0.7615\n",
            "Epoch 4480/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8443\n",
            "Epoch 4480: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3014 - accuracy: 0.8443 - val_loss: 0.4899 - val_accuracy: 0.7095\n",
            "Epoch 4481/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.8416\n",
            "Epoch 4481: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3079 - accuracy: 0.8415 - val_loss: 0.3753 - val_accuracy: 0.7797\n",
            "Epoch 4482/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3047 - accuracy: 0.8449\n",
            "Epoch 4482: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3046 - accuracy: 0.8449 - val_loss: 0.4245 - val_accuracy: 0.7443\n",
            "Epoch 4483/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2996 - accuracy: 0.8455\n",
            "Epoch 4483: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3010 - accuracy: 0.8449 - val_loss: 0.5385 - val_accuracy: 0.6941\n",
            "Epoch 4484/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.8348\n",
            "Epoch 4484: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3199 - accuracy: 0.8348 - val_loss: 0.4242 - val_accuracy: 0.7470\n",
            "Epoch 4485/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3064 - accuracy: 0.8443\n",
            "Epoch 4485: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3084 - accuracy: 0.8427 - val_loss: 0.4012 - val_accuracy: 0.7667\n",
            "Epoch 4486/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8452\n",
            "Epoch 4486: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3033 - accuracy: 0.8452 - val_loss: 0.3270 - val_accuracy: 0.8192\n",
            "Epoch 4487/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8430\n",
            "Epoch 4487: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3081 - accuracy: 0.8431 - val_loss: 0.4322 - val_accuracy: 0.7461\n",
            "Epoch 4488/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8477\n",
            "Epoch 4488: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2984 - accuracy: 0.8478 - val_loss: 0.5609 - val_accuracy: 0.6616\n",
            "Epoch 4489/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.8398\n",
            "Epoch 4489: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3160 - accuracy: 0.8392 - val_loss: 0.4658 - val_accuracy: 0.7147\n",
            "Epoch 4490/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8467\n",
            "Epoch 4490: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3005 - accuracy: 0.8466 - val_loss: 0.4367 - val_accuracy: 0.7266\n",
            "Epoch 4491/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2990 - accuracy: 0.8483\n",
            "Epoch 4491: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3013 - accuracy: 0.8463 - val_loss: 0.4073 - val_accuracy: 0.7569\n",
            "Epoch 4492/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3010 - accuracy: 0.8472\n",
            "Epoch 4492: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3023 - accuracy: 0.8464 - val_loss: 0.4142 - val_accuracy: 0.7501\n",
            "Epoch 4493/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3076 - accuracy: 0.8427\n",
            "Epoch 4493: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3075 - accuracy: 0.8426 - val_loss: 0.5143 - val_accuracy: 0.6930\n",
            "Epoch 4494/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3162 - accuracy: 0.8388\n",
            "Epoch 4494: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3174 - accuracy: 0.8381 - val_loss: 0.4367 - val_accuracy: 0.7349\n",
            "Epoch 4495/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3053 - accuracy: 0.8445\n",
            "Epoch 4495: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3055 - accuracy: 0.8441 - val_loss: 0.4588 - val_accuracy: 0.7185\n",
            "Epoch 4496/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8414\n",
            "Epoch 4496: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3042 - accuracy: 0.8413 - val_loss: 0.3823 - val_accuracy: 0.7845\n",
            "Epoch 4497/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8434\n",
            "Epoch 4497: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3051 - accuracy: 0.8437 - val_loss: 0.4547 - val_accuracy: 0.7182\n",
            "Epoch 4498/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3004 - accuracy: 0.8487\n",
            "Epoch 4498: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3015 - accuracy: 0.8477 - val_loss: 0.4128 - val_accuracy: 0.7531\n",
            "Epoch 4499/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3105 - accuracy: 0.8427\n",
            "Epoch 4499: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3126 - accuracy: 0.8407 - val_loss: 0.3667 - val_accuracy: 0.7852\n",
            "Epoch 4500/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.8479\n",
            "Epoch 4500: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2994 - accuracy: 0.8467 - val_loss: 0.5371 - val_accuracy: 0.6787\n",
            "Epoch 4501/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.8347\n",
            "Epoch 4501: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3219 - accuracy: 0.8347 - val_loss: 0.5034 - val_accuracy: 0.6959\n",
            "Epoch 4502/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8381\n",
            "Epoch 4502: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3168 - accuracy: 0.8379 - val_loss: 0.3947 - val_accuracy: 0.7729\n",
            "Epoch 4503/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8440\n",
            "Epoch 4503: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3059 - accuracy: 0.8440 - val_loss: 0.4581 - val_accuracy: 0.7160\n",
            "Epoch 4504/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3011 - accuracy: 0.8455\n",
            "Epoch 4504: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3023 - accuracy: 0.8448 - val_loss: 0.4996 - val_accuracy: 0.7237\n",
            "Epoch 4505/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8436\n",
            "Epoch 4505: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3104 - accuracy: 0.8436 - val_loss: 0.4530 - val_accuracy: 0.7217\n",
            "Epoch 4506/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8446\n",
            "Epoch 4506: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3052 - accuracy: 0.8437 - val_loss: 0.4748 - val_accuracy: 0.6976\n",
            "Epoch 4507/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2985 - accuracy: 0.8483\n",
            "Epoch 4507: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2996 - accuracy: 0.8470 - val_loss: 0.4631 - val_accuracy: 0.7217\n",
            "Epoch 4508/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8473\n",
            "Epoch 4508: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2978 - accuracy: 0.8471 - val_loss: 0.4330 - val_accuracy: 0.7450\n",
            "Epoch 4509/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3034 - accuracy: 0.8444\n",
            "Epoch 4509: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3042 - accuracy: 0.8445 - val_loss: 0.4169 - val_accuracy: 0.7386\n",
            "Epoch 4510/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3028 - accuracy: 0.8424\n",
            "Epoch 4510: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3040 - accuracy: 0.8414 - val_loss: 0.3725 - val_accuracy: 0.7803\n",
            "Epoch 4511/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3014 - accuracy: 0.8461\n",
            "Epoch 4511: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3011 - accuracy: 0.8466 - val_loss: 0.3959 - val_accuracy: 0.7639\n",
            "Epoch 4512/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.8463\n",
            "Epoch 4512: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2970 - accuracy: 0.8463 - val_loss: 0.5452 - val_accuracy: 0.6689\n",
            "Epoch 4513/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8411\n",
            "Epoch 4513: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3041 - accuracy: 0.8414 - val_loss: 0.3479 - val_accuracy: 0.8027\n",
            "Epoch 4514/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3166 - accuracy: 0.8367\n",
            "Epoch 4514: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3166 - accuracy: 0.8367 - val_loss: 0.4592 - val_accuracy: 0.7325\n",
            "Epoch 4515/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3230 - accuracy: 0.8364\n",
            "Epoch 4515: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3225 - accuracy: 0.8366 - val_loss: 0.4965 - val_accuracy: 0.7005\n",
            "Epoch 4516/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3043 - accuracy: 0.8452\n",
            "Epoch 4516: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3043 - accuracy: 0.8455 - val_loss: 0.3557 - val_accuracy: 0.8034\n",
            "Epoch 4517/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8445\n",
            "Epoch 4517: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3042 - accuracy: 0.8439 - val_loss: 0.4433 - val_accuracy: 0.7345\n",
            "Epoch 4518/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.8443\n",
            "Epoch 4518: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3038 - accuracy: 0.8443 - val_loss: 0.4346 - val_accuracy: 0.7430\n",
            "Epoch 4519/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8456\n",
            "Epoch 4519: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2998 - accuracy: 0.8452 - val_loss: 0.4730 - val_accuracy: 0.7132\n",
            "Epoch 4520/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8413\n",
            "Epoch 4520: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3077 - accuracy: 0.8411 - val_loss: 0.4474 - val_accuracy: 0.7424\n",
            "Epoch 4521/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8468\n",
            "Epoch 4521: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3058 - accuracy: 0.8449 - val_loss: 0.6023 - val_accuracy: 0.6500\n",
            "Epoch 4522/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3066 - accuracy: 0.8430\n",
            "Epoch 4522: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3076 - accuracy: 0.8427 - val_loss: 0.4838 - val_accuracy: 0.7114\n",
            "Epoch 4523/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3097 - accuracy: 0.8404\n",
            "Epoch 4523: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3099 - accuracy: 0.8397 - val_loss: 0.4671 - val_accuracy: 0.7222\n",
            "Epoch 4524/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3167 - accuracy: 0.8386\n",
            "Epoch 4524: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3145 - accuracy: 0.8397 - val_loss: 0.4932 - val_accuracy: 0.7011\n",
            "Epoch 4525/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8389\n",
            "Epoch 4525: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3087 - accuracy: 0.8391 - val_loss: 0.3571 - val_accuracy: 0.7887\n",
            "Epoch 4526/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8408\n",
            "Epoch 4526: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3123 - accuracy: 0.8409 - val_loss: 0.4202 - val_accuracy: 0.7468\n",
            "Epoch 4527/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8431\n",
            "Epoch 4527: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3068 - accuracy: 0.8421 - val_loss: 0.3983 - val_accuracy: 0.7634\n",
            "Epoch 4528/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3019 - accuracy: 0.8449\n",
            "Epoch 4528: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3026 - accuracy: 0.8444 - val_loss: 0.5058 - val_accuracy: 0.7016\n",
            "Epoch 4529/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8425\n",
            "Epoch 4529: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3054 - accuracy: 0.8427 - val_loss: 0.4652 - val_accuracy: 0.7275\n",
            "Epoch 4530/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3002 - accuracy: 0.8473\n",
            "Epoch 4530: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3013 - accuracy: 0.8463 - val_loss: 0.4729 - val_accuracy: 0.7125\n",
            "Epoch 4531/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2983 - accuracy: 0.8483\n",
            "Epoch 4531: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2990 - accuracy: 0.8478 - val_loss: 0.4206 - val_accuracy: 0.7529\n",
            "Epoch 4532/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3082 - accuracy: 0.8389\n",
            "Epoch 4532: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3071 - accuracy: 0.8401 - val_loss: 0.4163 - val_accuracy: 0.7459\n",
            "Epoch 4533/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3032 - accuracy: 0.8438\n",
            "Epoch 4533: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3039 - accuracy: 0.8431 - val_loss: 0.4439 - val_accuracy: 0.7323\n",
            "Epoch 4534/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3062 - accuracy: 0.8436\n",
            "Epoch 4534: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3059 - accuracy: 0.8437 - val_loss: 0.4397 - val_accuracy: 0.7318\n",
            "Epoch 4535/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8449\n",
            "Epoch 4535: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3059 - accuracy: 0.8455 - val_loss: 0.4988 - val_accuracy: 0.6941\n",
            "Epoch 4536/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.8432\n",
            "Epoch 4536: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3046 - accuracy: 0.8431 - val_loss: 0.3843 - val_accuracy: 0.7680\n",
            "Epoch 4537/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3113 - accuracy: 0.8420\n",
            "Epoch 4537: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3137 - accuracy: 0.8402 - val_loss: 0.5049 - val_accuracy: 0.6754\n",
            "Epoch 4538/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3090 - accuracy: 0.8443\n",
            "Epoch 4538: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3082 - accuracy: 0.8440 - val_loss: 0.5035 - val_accuracy: 0.7005\n",
            "Epoch 4539/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3013 - accuracy: 0.8491\n",
            "Epoch 4539: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3022 - accuracy: 0.8478 - val_loss: 0.4046 - val_accuracy: 0.7555\n",
            "Epoch 4540/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2980 - accuracy: 0.8491\n",
            "Epoch 4540: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2986 - accuracy: 0.8489 - val_loss: 0.4553 - val_accuracy: 0.7288\n",
            "Epoch 4541/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3044 - accuracy: 0.8451\n",
            "Epoch 4541: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 27ms/step - loss: 0.3055 - accuracy: 0.8443 - val_loss: 0.4016 - val_accuracy: 0.7617\n",
            "Epoch 4542/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.8398\n",
            "Epoch 4542: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3103 - accuracy: 0.8398 - val_loss: 0.5429 - val_accuracy: 0.6765\n",
            "Epoch 4543/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2999 - accuracy: 0.8455\n",
            "Epoch 4543: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3006 - accuracy: 0.8450 - val_loss: 0.4604 - val_accuracy: 0.7204\n",
            "Epoch 4544/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8469\n",
            "Epoch 4544: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2996 - accuracy: 0.8470 - val_loss: 0.3982 - val_accuracy: 0.7634\n",
            "Epoch 4545/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8470\n",
            "Epoch 4545: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3011 - accuracy: 0.8470 - val_loss: 0.4162 - val_accuracy: 0.7610\n",
            "Epoch 4546/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3004 - accuracy: 0.8457\n",
            "Epoch 4546: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2993 - accuracy: 0.8452 - val_loss: 0.4078 - val_accuracy: 0.7457\n",
            "Epoch 4547/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2994 - accuracy: 0.8492\n",
            "Epoch 4547: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2996 - accuracy: 0.8487 - val_loss: 0.4207 - val_accuracy: 0.7437\n",
            "Epoch 4548/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2935 - accuracy: 0.8484\n",
            "Epoch 4548: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2945 - accuracy: 0.8481 - val_loss: 0.4157 - val_accuracy: 0.7562\n",
            "Epoch 4549/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8456\n",
            "Epoch 4549: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3014 - accuracy: 0.8452 - val_loss: 0.4921 - val_accuracy: 0.6884\n",
            "Epoch 4550/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8484\n",
            "Epoch 4550: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2964 - accuracy: 0.8477 - val_loss: 0.3946 - val_accuracy: 0.7711\n",
            "Epoch 4551/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3053 - accuracy: 0.8424\n",
            "Epoch 4551: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3066 - accuracy: 0.8424 - val_loss: 0.4518 - val_accuracy: 0.7329\n",
            "Epoch 4552/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8430\n",
            "Epoch 4552: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3087 - accuracy: 0.8430 - val_loss: 0.3917 - val_accuracy: 0.7775\n",
            "Epoch 4553/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2960 - accuracy: 0.8490\n",
            "Epoch 4553: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2975 - accuracy: 0.8476 - val_loss: 0.5054 - val_accuracy: 0.6941\n",
            "Epoch 4554/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8475\n",
            "Epoch 4554: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2971 - accuracy: 0.8475 - val_loss: 0.4457 - val_accuracy: 0.7191\n",
            "Epoch 4555/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3069 - accuracy: 0.8444\n",
            "Epoch 4555: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3113 - accuracy: 0.8411 - val_loss: 0.3565 - val_accuracy: 0.7979\n",
            "Epoch 4556/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3149 - accuracy: 0.8407\n",
            "Epoch 4556: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3162 - accuracy: 0.8401 - val_loss: 0.5164 - val_accuracy: 0.7064\n",
            "Epoch 4557/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3201 - accuracy: 0.8372\n",
            "Epoch 4557: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3189 - accuracy: 0.8369 - val_loss: 0.3992 - val_accuracy: 0.7588\n",
            "Epoch 4558/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3092 - accuracy: 0.8388\n",
            "Epoch 4558: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3111 - accuracy: 0.8376 - val_loss: 0.4537 - val_accuracy: 0.7246\n",
            "Epoch 4559/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8434\n",
            "Epoch 4559: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3061 - accuracy: 0.8435 - val_loss: 0.3595 - val_accuracy: 0.7904\n",
            "Epoch 4560/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8465\n",
            "Epoch 4560: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3018 - accuracy: 0.8466 - val_loss: 0.4044 - val_accuracy: 0.7720\n",
            "Epoch 4561/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3006 - accuracy: 0.8451\n",
            "Epoch 4561: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3019 - accuracy: 0.8443 - val_loss: 0.5574 - val_accuracy: 0.6638\n",
            "Epoch 4562/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3036 - accuracy: 0.8470\n",
            "Epoch 4562: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3033 - accuracy: 0.8466 - val_loss: 0.4619 - val_accuracy: 0.7156\n",
            "Epoch 4563/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2987 - accuracy: 0.8464\n",
            "Epoch 4563: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2993 - accuracy: 0.8458 - val_loss: 0.4332 - val_accuracy: 0.7443\n",
            "Epoch 4564/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8481\n",
            "Epoch 4564: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3001 - accuracy: 0.8479 - val_loss: 0.3943 - val_accuracy: 0.7694\n",
            "Epoch 4565/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2973 - accuracy: 0.8498\n",
            "Epoch 4565: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2979 - accuracy: 0.8495 - val_loss: 0.5595 - val_accuracy: 0.6612\n",
            "Epoch 4566/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8437\n",
            "Epoch 4566: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3049 - accuracy: 0.8438 - val_loss: 0.3761 - val_accuracy: 0.7749\n",
            "Epoch 4567/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3081 - accuracy: 0.8430\n",
            "Epoch 4567: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3088 - accuracy: 0.8418 - val_loss: 0.5259 - val_accuracy: 0.7011\n",
            "Epoch 4568/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3086 - accuracy: 0.8446\n",
            "Epoch 4568: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3085 - accuracy: 0.8450 - val_loss: 0.4250 - val_accuracy: 0.7516\n",
            "Epoch 4569/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.8453\n",
            "Epoch 4569: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3041 - accuracy: 0.8448 - val_loss: 0.4620 - val_accuracy: 0.7220\n",
            "Epoch 4570/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2986 - accuracy: 0.8491\n",
            "Epoch 4570: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2988 - accuracy: 0.8487 - val_loss: 0.4682 - val_accuracy: 0.7345\n",
            "Epoch 4571/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8426\n",
            "Epoch 4571: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3059 - accuracy: 0.8427 - val_loss: 0.4649 - val_accuracy: 0.7327\n",
            "Epoch 4572/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3034 - accuracy: 0.8435\n",
            "Epoch 4572: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3034 - accuracy: 0.8435 - val_loss: 0.4737 - val_accuracy: 0.7169\n",
            "Epoch 4573/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8493\n",
            "Epoch 4573: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2981 - accuracy: 0.8491 - val_loss: 0.4439 - val_accuracy: 0.7294\n",
            "Epoch 4574/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2994 - accuracy: 0.8463\n",
            "Epoch 4574: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3005 - accuracy: 0.8453 - val_loss: 0.3651 - val_accuracy: 0.7847\n",
            "Epoch 4575/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8474\n",
            "Epoch 4575: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3039 - accuracy: 0.8465 - val_loss: 0.4711 - val_accuracy: 0.6908\n",
            "Epoch 4576/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3092 - accuracy: 0.8387\n",
            "Epoch 4576: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3083 - accuracy: 0.8395 - val_loss: 0.3909 - val_accuracy: 0.7685\n",
            "Epoch 4577/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8442\n",
            "Epoch 4577: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3079 - accuracy: 0.8442 - val_loss: 0.4026 - val_accuracy: 0.7623\n",
            "Epoch 4578/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3185 - accuracy: 0.8352\n",
            "Epoch 4578: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3187 - accuracy: 0.8357 - val_loss: 0.4380 - val_accuracy: 0.7206\n",
            "Epoch 4579/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3021 - accuracy: 0.8483\n",
            "Epoch 4579: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3025 - accuracy: 0.8474 - val_loss: 0.4506 - val_accuracy: 0.7259\n",
            "Epoch 4580/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2985 - accuracy: 0.8485\n",
            "Epoch 4580: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3020 - accuracy: 0.8472 - val_loss: 0.5461 - val_accuracy: 0.6664\n",
            "Epoch 4581/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8411\n",
            "Epoch 4581: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3108 - accuracy: 0.8414 - val_loss: 0.4665 - val_accuracy: 0.7158\n",
            "Epoch 4582/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3037 - accuracy: 0.8438\n",
            "Epoch 4582: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3040 - accuracy: 0.8433 - val_loss: 0.5018 - val_accuracy: 0.7027\n",
            "Epoch 4583/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.8439\n",
            "Epoch 4583: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3087 - accuracy: 0.8439 - val_loss: 0.4817 - val_accuracy: 0.7119\n",
            "Epoch 4584/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3123 - accuracy: 0.8418\n",
            "Epoch 4584: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3127 - accuracy: 0.8413 - val_loss: 0.5287 - val_accuracy: 0.6684\n",
            "Epoch 4585/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3004 - accuracy: 0.8465\n",
            "Epoch 4585: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3021 - accuracy: 0.8454 - val_loss: 0.3938 - val_accuracy: 0.7683\n",
            "Epoch 4586/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8487\n",
            "Epoch 4586: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2972 - accuracy: 0.8487 - val_loss: 0.4205 - val_accuracy: 0.7424\n",
            "Epoch 4587/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8438\n",
            "Epoch 4587: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3065 - accuracy: 0.8438 - val_loss: 0.3937 - val_accuracy: 0.7746\n",
            "Epoch 4588/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3020 - accuracy: 0.8484\n",
            "Epoch 4588: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3029 - accuracy: 0.8472 - val_loss: 0.4603 - val_accuracy: 0.7119\n",
            "Epoch 4589/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3019 - accuracy: 0.8443\n",
            "Epoch 4589: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3024 - accuracy: 0.8441 - val_loss: 0.4400 - val_accuracy: 0.7356\n",
            "Epoch 4590/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2983 - accuracy: 0.8457\n",
            "Epoch 4590: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2974 - accuracy: 0.8459 - val_loss: 0.4232 - val_accuracy: 0.7424\n",
            "Epoch 4591/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2997 - accuracy: 0.8455\n",
            "Epoch 4591: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3005 - accuracy: 0.8454 - val_loss: 0.4674 - val_accuracy: 0.7156\n",
            "Epoch 4592/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3013 - accuracy: 0.8470\n",
            "Epoch 4592: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3015 - accuracy: 0.8475 - val_loss: 0.4123 - val_accuracy: 0.7582\n",
            "Epoch 4593/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2992 - accuracy: 0.8476\n",
            "Epoch 4593: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2986 - accuracy: 0.8478 - val_loss: 0.3809 - val_accuracy: 0.7845\n",
            "Epoch 4594/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3037 - accuracy: 0.8435\n",
            "Epoch 4594: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3069 - accuracy: 0.8420 - val_loss: 0.4369 - val_accuracy: 0.7310\n",
            "Epoch 4595/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8415\n",
            "Epoch 4595: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3093 - accuracy: 0.8415 - val_loss: 0.4637 - val_accuracy: 0.7294\n",
            "Epoch 4596/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3013 - accuracy: 0.8465\n",
            "Epoch 4596: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3027 - accuracy: 0.8449 - val_loss: 0.4613 - val_accuracy: 0.7213\n",
            "Epoch 4597/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3004 - accuracy: 0.8472\n",
            "Epoch 4597: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3041 - accuracy: 0.8454 - val_loss: 0.3661 - val_accuracy: 0.7931\n",
            "Epoch 4598/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8403\n",
            "Epoch 4598: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3118 - accuracy: 0.8403 - val_loss: 0.3937 - val_accuracy: 0.7626\n",
            "Epoch 4599/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3074 - accuracy: 0.8428\n",
            "Epoch 4599: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3074 - accuracy: 0.8426 - val_loss: 0.4542 - val_accuracy: 0.7264\n",
            "Epoch 4600/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8427\n",
            "Epoch 4600: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3059 - accuracy: 0.8430 - val_loss: 0.3607 - val_accuracy: 0.7990\n",
            "Epoch 4601/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2976 - accuracy: 0.8447\n",
            "Epoch 4601: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2986 - accuracy: 0.8444 - val_loss: 0.4126 - val_accuracy: 0.7479\n",
            "Epoch 4602/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8451\n",
            "Epoch 4602: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3075 - accuracy: 0.8443 - val_loss: 0.3723 - val_accuracy: 0.7814\n",
            "Epoch 4603/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3151 - accuracy: 0.8405\n",
            "Epoch 4603: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3162 - accuracy: 0.8396 - val_loss: 0.5535 - val_accuracy: 0.6678\n",
            "Epoch 4604/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3112 - accuracy: 0.8393\n",
            "Epoch 4604: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3113 - accuracy: 0.8392 - val_loss: 0.4556 - val_accuracy: 0.7213\n",
            "Epoch 4605/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8419\n",
            "Epoch 4605: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3085 - accuracy: 0.8419 - val_loss: 0.4504 - val_accuracy: 0.7275\n",
            "Epoch 4606/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3013 - accuracy: 0.8446\n",
            "Epoch 4606: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3021 - accuracy: 0.8444 - val_loss: 0.3994 - val_accuracy: 0.7632\n",
            "Epoch 4607/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3047 - accuracy: 0.8454\n",
            "Epoch 4607: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3048 - accuracy: 0.8457 - val_loss: 0.4906 - val_accuracy: 0.6923\n",
            "Epoch 4608/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8427\n",
            "Epoch 4608: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3062 - accuracy: 0.8426 - val_loss: 0.4206 - val_accuracy: 0.7476\n",
            "Epoch 4609/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3078 - accuracy: 0.8399\n",
            "Epoch 4609: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3067 - accuracy: 0.8407 - val_loss: 0.4293 - val_accuracy: 0.7325\n",
            "Epoch 4610/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3001 - accuracy: 0.8470\n",
            "Epoch 4610: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3012 - accuracy: 0.8463 - val_loss: 0.4662 - val_accuracy: 0.7149\n",
            "Epoch 4611/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3055 - accuracy: 0.8448\n",
            "Epoch 4611: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3067 - accuracy: 0.8433 - val_loss: 0.3466 - val_accuracy: 0.7944\n",
            "Epoch 4612/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.8414\n",
            "Epoch 4612: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3114 - accuracy: 0.8407 - val_loss: 0.4068 - val_accuracy: 0.7538\n",
            "Epoch 4613/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.8471\n",
            "Epoch 4613: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2970 - accuracy: 0.8471 - val_loss: 0.4856 - val_accuracy: 0.7033\n",
            "Epoch 4614/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3121 - accuracy: 0.8384\n",
            "Epoch 4614: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3139 - accuracy: 0.8370 - val_loss: 0.4215 - val_accuracy: 0.7433\n",
            "Epoch 4615/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3018 - accuracy: 0.8485\n",
            "Epoch 4615: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3044 - accuracy: 0.8471 - val_loss: 0.4432 - val_accuracy: 0.7329\n",
            "Epoch 4616/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8463\n",
            "Epoch 4616: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3015 - accuracy: 0.8460 - val_loss: 0.5311 - val_accuracy: 0.6754\n",
            "Epoch 4617/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8452\n",
            "Epoch 4617: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3033 - accuracy: 0.8451 - val_loss: 0.4091 - val_accuracy: 0.7501\n",
            "Epoch 4618/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2987 - accuracy: 0.8501\n",
            "Epoch 4618: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2993 - accuracy: 0.8499 - val_loss: 0.4294 - val_accuracy: 0.7439\n",
            "Epoch 4619/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8437\n",
            "Epoch 4619: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3021 - accuracy: 0.8437 - val_loss: 0.4875 - val_accuracy: 0.6952\n",
            "Epoch 4620/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3011 - accuracy: 0.8453\n",
            "Epoch 4620: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3019 - accuracy: 0.8447 - val_loss: 0.5408 - val_accuracy: 0.6783\n",
            "Epoch 4621/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3175 - accuracy: 0.8379\n",
            "Epoch 4621: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3169 - accuracy: 0.8382 - val_loss: 0.6013 - val_accuracy: 0.6533\n",
            "Epoch 4622/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8371\n",
            "Epoch 4622: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3127 - accuracy: 0.8371 - val_loss: 0.6064 - val_accuracy: 0.6493\n",
            "Epoch 4623/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3176 - accuracy: 0.8406\n",
            "Epoch 4623: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3190 - accuracy: 0.8395 - val_loss: 0.3927 - val_accuracy: 0.7498\n",
            "Epoch 4624/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3385 - accuracy: 0.8257\n",
            "Epoch 4624: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3386 - accuracy: 0.8255 - val_loss: 0.5270 - val_accuracy: 0.6895\n",
            "Epoch 4625/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3135 - accuracy: 0.8382\n",
            "Epoch 4625: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3137 - accuracy: 0.8376 - val_loss: 0.4748 - val_accuracy: 0.7011\n",
            "Epoch 4626/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3036 - accuracy: 0.8455\n",
            "Epoch 4626: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3039 - accuracy: 0.8449 - val_loss: 0.4172 - val_accuracy: 0.7588\n",
            "Epoch 4627/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.8456\n",
            "Epoch 4627: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3002 - accuracy: 0.8461 - val_loss: 0.4363 - val_accuracy: 0.7404\n",
            "Epoch 4628/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2975 - accuracy: 0.8482\n",
            "Epoch 4628: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2985 - accuracy: 0.8472 - val_loss: 0.4243 - val_accuracy: 0.7384\n",
            "Epoch 4629/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2997 - accuracy: 0.8460\n",
            "Epoch 4629: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3030 - accuracy: 0.8433 - val_loss: 0.4412 - val_accuracy: 0.7182\n",
            "Epoch 4630/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3143 - accuracy: 0.8384\n",
            "Epoch 4630: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3131 - accuracy: 0.8394 - val_loss: 0.3879 - val_accuracy: 0.7665\n",
            "Epoch 4631/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2977 - accuracy: 0.8493\n",
            "Epoch 4631: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2985 - accuracy: 0.8486 - val_loss: 0.4662 - val_accuracy: 0.7075\n",
            "Epoch 4632/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8483\n",
            "Epoch 4632: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2964 - accuracy: 0.8483 - val_loss: 0.4350 - val_accuracy: 0.7360\n",
            "Epoch 4633/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8498\n",
            "Epoch 4633: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.2984 - accuracy: 0.8499 - val_loss: 0.4691 - val_accuracy: 0.7051\n",
            "Epoch 4634/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8471\n",
            "Epoch 4634: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3009 - accuracy: 0.8471 - val_loss: 0.3986 - val_accuracy: 0.7702\n",
            "Epoch 4635/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3022 - accuracy: 0.8476\n",
            "Epoch 4635: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3025 - accuracy: 0.8472 - val_loss: 0.5130 - val_accuracy: 0.6978\n",
            "Epoch 4636/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3009 - accuracy: 0.8446\n",
            "Epoch 4636: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3006 - accuracy: 0.8443 - val_loss: 0.3964 - val_accuracy: 0.7678\n",
            "Epoch 4637/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3038 - accuracy: 0.8441\n",
            "Epoch 4637: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3035 - accuracy: 0.8439 - val_loss: 0.4089 - val_accuracy: 0.7527\n",
            "Epoch 4638/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3013 - accuracy: 0.8469\n",
            "Epoch 4638: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3012 - accuracy: 0.8472 - val_loss: 0.4147 - val_accuracy: 0.7494\n",
            "Epoch 4639/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8432\n",
            "Epoch 4639: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3029 - accuracy: 0.8430 - val_loss: 0.4059 - val_accuracy: 0.7564\n",
            "Epoch 4640/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.8460\n",
            "Epoch 4640: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3033 - accuracy: 0.8459 - val_loss: 0.4381 - val_accuracy: 0.7237\n",
            "Epoch 4641/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8420\n",
            "Epoch 4641: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3107 - accuracy: 0.8422 - val_loss: 0.4299 - val_accuracy: 0.7375\n",
            "Epoch 4642/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8387\n",
            "Epoch 4642: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3200 - accuracy: 0.8388 - val_loss: 0.4563 - val_accuracy: 0.7277\n",
            "Epoch 4643/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3029 - accuracy: 0.8464\n",
            "Epoch 4643: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3034 - accuracy: 0.8459 - val_loss: 0.3894 - val_accuracy: 0.7654\n",
            "Epoch 4644/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8461\n",
            "Epoch 4644: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3061 - accuracy: 0.8462 - val_loss: 0.5165 - val_accuracy: 0.6956\n",
            "Epoch 4645/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8439\n",
            "Epoch 4645: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3004 - accuracy: 0.8437 - val_loss: 0.3976 - val_accuracy: 0.7591\n",
            "Epoch 4646/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3058 - accuracy: 0.8431\n",
            "Epoch 4646: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3077 - accuracy: 0.8422 - val_loss: 0.4720 - val_accuracy: 0.7084\n",
            "Epoch 4647/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3009 - accuracy: 0.8467\n",
            "Epoch 4647: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3027 - accuracy: 0.8457 - val_loss: 0.3836 - val_accuracy: 0.7757\n",
            "Epoch 4648/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8417\n",
            "Epoch 4648: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3049 - accuracy: 0.8417 - val_loss: 0.4124 - val_accuracy: 0.7522\n",
            "Epoch 4649/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8431\n",
            "Epoch 4649: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3065 - accuracy: 0.8431 - val_loss: 0.3936 - val_accuracy: 0.7667\n",
            "Epoch 4650/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3024 - accuracy: 0.8448\n",
            "Epoch 4650: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3023 - accuracy: 0.8454 - val_loss: 0.3714 - val_accuracy: 0.7869\n",
            "Epoch 4651/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3047 - accuracy: 0.8441\n",
            "Epoch 4651: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3039 - accuracy: 0.8450 - val_loss: 0.4588 - val_accuracy: 0.7246\n",
            "Epoch 4652/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3018 - accuracy: 0.8458\n",
            "Epoch 4652: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3032 - accuracy: 0.8442 - val_loss: 0.4297 - val_accuracy: 0.7277\n",
            "Epoch 4653/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.8473\n",
            "Epoch 4653: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2974 - accuracy: 0.8473 - val_loss: 0.3595 - val_accuracy: 0.7933\n",
            "Epoch 4654/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8425\n",
            "Epoch 4654: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3059 - accuracy: 0.8425 - val_loss: 0.5462 - val_accuracy: 0.6623\n",
            "Epoch 4655/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8439\n",
            "Epoch 4655: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3083 - accuracy: 0.8438 - val_loss: 0.3745 - val_accuracy: 0.7806\n",
            "Epoch 4656/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8467\n",
            "Epoch 4656: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3031 - accuracy: 0.8466 - val_loss: 0.4081 - val_accuracy: 0.7569\n",
            "Epoch 4657/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3022 - accuracy: 0.8470\n",
            "Epoch 4657: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3010 - accuracy: 0.8467 - val_loss: 0.4481 - val_accuracy: 0.7187\n",
            "Epoch 4658/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2954 - accuracy: 0.8464\n",
            "Epoch 4658: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2977 - accuracy: 0.8453 - val_loss: 0.3777 - val_accuracy: 0.7753\n",
            "Epoch 4659/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3092 - accuracy: 0.8403\n",
            "Epoch 4659: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3093 - accuracy: 0.8398 - val_loss: 0.4372 - val_accuracy: 0.7349\n",
            "Epoch 4660/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3007 - accuracy: 0.8451\n",
            "Epoch 4660: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3019 - accuracy: 0.8450 - val_loss: 0.3936 - val_accuracy: 0.7713\n",
            "Epoch 4661/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3011 - accuracy: 0.8474\n",
            "Epoch 4661: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3017 - accuracy: 0.8463 - val_loss: 0.3506 - val_accuracy: 0.8054\n",
            "Epoch 4662/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3053 - accuracy: 0.8446\n",
            "Epoch 4662: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3073 - accuracy: 0.8433 - val_loss: 0.4441 - val_accuracy: 0.7127\n",
            "Epoch 4663/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3096 - accuracy: 0.8421\n",
            "Epoch 4663: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3096 - accuracy: 0.8419 - val_loss: 0.4135 - val_accuracy: 0.7422\n",
            "Epoch 4664/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3190 - accuracy: 0.8356\n",
            "Epoch 4664: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3178 - accuracy: 0.8360 - val_loss: 0.5235 - val_accuracy: 0.6860\n",
            "Epoch 4665/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3038 - accuracy: 0.8457\n",
            "Epoch 4665: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3054 - accuracy: 0.8447 - val_loss: 0.3677 - val_accuracy: 0.7828\n",
            "Epoch 4666/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3135 - accuracy: 0.8402\n",
            "Epoch 4666: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3134 - accuracy: 0.8412 - val_loss: 0.4040 - val_accuracy: 0.7529\n",
            "Epoch 4667/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3076 - accuracy: 0.8442\n",
            "Epoch 4667: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3076 - accuracy: 0.8446 - val_loss: 0.4210 - val_accuracy: 0.7474\n",
            "Epoch 4668/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3003 - accuracy: 0.8482\n",
            "Epoch 4668: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3009 - accuracy: 0.8476 - val_loss: 0.4157 - val_accuracy: 0.7577\n",
            "Epoch 4669/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3147 - accuracy: 0.8411\n",
            "Epoch 4669: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3151 - accuracy: 0.8404 - val_loss: 0.4382 - val_accuracy: 0.7206\n",
            "Epoch 4670/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3104 - accuracy: 0.8391\n",
            "Epoch 4670: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3114 - accuracy: 0.8398 - val_loss: 0.3950 - val_accuracy: 0.7680\n",
            "Epoch 4671/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.8436\n",
            "Epoch 4671: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3011 - accuracy: 0.8434 - val_loss: 0.4232 - val_accuracy: 0.7408\n",
            "Epoch 4672/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3092 - accuracy: 0.8420\n",
            "Epoch 4672: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3102 - accuracy: 0.8410 - val_loss: 0.4378 - val_accuracy: 0.7411\n",
            "Epoch 4673/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8428\n",
            "Epoch 4673: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3053 - accuracy: 0.8430 - val_loss: 0.3632 - val_accuracy: 0.7865\n",
            "Epoch 4674/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3017 - accuracy: 0.8462\n",
            "Epoch 4674: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3021 - accuracy: 0.8458 - val_loss: 0.4256 - val_accuracy: 0.7411\n",
            "Epoch 4675/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.8393\n",
            "Epoch 4675: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3140 - accuracy: 0.8391 - val_loss: 0.4599 - val_accuracy: 0.7226\n",
            "Epoch 4676/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3003 - accuracy: 0.8448\n",
            "Epoch 4676: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3002 - accuracy: 0.8444 - val_loss: 0.3824 - val_accuracy: 0.7700\n",
            "Epoch 4677/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8422\n",
            "Epoch 4677: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3039 - accuracy: 0.8421 - val_loss: 0.3574 - val_accuracy: 0.7994\n",
            "Epoch 4678/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3041 - accuracy: 0.8461\n",
            "Epoch 4678: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3056 - accuracy: 0.8450 - val_loss: 0.4541 - val_accuracy: 0.7367\n",
            "Epoch 4679/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8439\n",
            "Epoch 4679: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3040 - accuracy: 0.8438 - val_loss: 0.4362 - val_accuracy: 0.7375\n",
            "Epoch 4680/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2990 - accuracy: 0.8440\n",
            "Epoch 4680: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2991 - accuracy: 0.8442 - val_loss: 0.4998 - val_accuracy: 0.6987\n",
            "Epoch 4681/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8427\n",
            "Epoch 4681: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3089 - accuracy: 0.8424 - val_loss: 0.4031 - val_accuracy: 0.7509\n",
            "Epoch 4682/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.8445\n",
            "Epoch 4682: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3037 - accuracy: 0.8446 - val_loss: 0.4512 - val_accuracy: 0.7246\n",
            "Epoch 4683/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2938 - accuracy: 0.8492\n",
            "Epoch 4683: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2950 - accuracy: 0.8486 - val_loss: 0.3809 - val_accuracy: 0.7847\n",
            "Epoch 4684/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2992 - accuracy: 0.8463\n",
            "Epoch 4684: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3002 - accuracy: 0.8452 - val_loss: 0.4341 - val_accuracy: 0.7312\n",
            "Epoch 4685/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8457\n",
            "Epoch 4685: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3014 - accuracy: 0.8457 - val_loss: 0.3637 - val_accuracy: 0.7920\n",
            "Epoch 4686/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3048 - accuracy: 0.8422\n",
            "Epoch 4686: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3050 - accuracy: 0.8421 - val_loss: 0.4211 - val_accuracy: 0.7364\n",
            "Epoch 4687/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2969 - accuracy: 0.8496\n",
            "Epoch 4687: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2980 - accuracy: 0.8486 - val_loss: 0.5005 - val_accuracy: 0.6974\n",
            "Epoch 4688/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8438\n",
            "Epoch 4688: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3080 - accuracy: 0.8437 - val_loss: 0.4326 - val_accuracy: 0.7354\n",
            "Epoch 4689/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8418\n",
            "Epoch 4689: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3055 - accuracy: 0.8416 - val_loss: 0.4585 - val_accuracy: 0.7211\n",
            "Epoch 4690/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2989 - accuracy: 0.8478\n",
            "Epoch 4690: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3002 - accuracy: 0.8461 - val_loss: 0.3769 - val_accuracy: 0.7711\n",
            "Epoch 4691/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3013 - accuracy: 0.8476\n",
            "Epoch 4691: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3026 - accuracy: 0.8460 - val_loss: 0.4071 - val_accuracy: 0.7424\n",
            "Epoch 4692/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8466\n",
            "Epoch 4692: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3023 - accuracy: 0.8448 - val_loss: 0.4426 - val_accuracy: 0.7382\n",
            "Epoch 4693/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2947 - accuracy: 0.8489\n",
            "Epoch 4693: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2956 - accuracy: 0.8484 - val_loss: 0.4562 - val_accuracy: 0.7209\n",
            "Epoch 4694/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8502\n",
            "Epoch 4694: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2950 - accuracy: 0.8498 - val_loss: 0.3776 - val_accuracy: 0.7762\n",
            "Epoch 4695/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8449\n",
            "Epoch 4695: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3000 - accuracy: 0.8447 - val_loss: 0.5727 - val_accuracy: 0.6739\n",
            "Epoch 4696/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8416\n",
            "Epoch 4696: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3132 - accuracy: 0.8416 - val_loss: 0.4168 - val_accuracy: 0.7454\n",
            "Epoch 4697/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.8424\n",
            "Epoch 4697: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3064 - accuracy: 0.8424 - val_loss: 0.4586 - val_accuracy: 0.7040\n",
            "Epoch 4698/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3113 - accuracy: 0.8410\n",
            "Epoch 4698: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3123 - accuracy: 0.8397 - val_loss: 0.4469 - val_accuracy: 0.7332\n",
            "Epoch 4699/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.8418\n",
            "Epoch 4699: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3084 - accuracy: 0.8416 - val_loss: 0.4987 - val_accuracy: 0.7009\n",
            "Epoch 4700/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3004 - accuracy: 0.8472\n",
            "Epoch 4700: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3011 - accuracy: 0.8459 - val_loss: 0.3937 - val_accuracy: 0.7702\n",
            "Epoch 4701/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2945 - accuracy: 0.8480\n",
            "Epoch 4701: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.2977 - accuracy: 0.8454 - val_loss: 0.4278 - val_accuracy: 0.7439\n",
            "Epoch 4702/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8483\n",
            "Epoch 4702: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3006 - accuracy: 0.8480 - val_loss: 0.4798 - val_accuracy: 0.7038\n",
            "Epoch 4703/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8480\n",
            "Epoch 4703: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2990 - accuracy: 0.8475 - val_loss: 0.4049 - val_accuracy: 0.7547\n",
            "Epoch 4704/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.8476\n",
            "Epoch 4704: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2983 - accuracy: 0.8476 - val_loss: 0.4721 - val_accuracy: 0.7255\n",
            "Epoch 4705/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3330 - accuracy: 0.8314\n",
            "Epoch 4705: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3322 - accuracy: 0.8311 - val_loss: 0.5089 - val_accuracy: 0.6980\n",
            "Epoch 4706/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8398\n",
            "Epoch 4706: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3127 - accuracy: 0.8398 - val_loss: 0.4394 - val_accuracy: 0.7261\n",
            "Epoch 4707/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3039 - accuracy: 0.8456\n",
            "Epoch 4707: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3044 - accuracy: 0.8449 - val_loss: 0.3811 - val_accuracy: 0.7803\n",
            "Epoch 4708/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3019 - accuracy: 0.8471\n",
            "Epoch 4708: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3015 - accuracy: 0.8472 - val_loss: 0.3407 - val_accuracy: 0.8071\n",
            "Epoch 4709/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3054 - accuracy: 0.8453\n",
            "Epoch 4709: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3063 - accuracy: 0.8448 - val_loss: 0.4711 - val_accuracy: 0.7134\n",
            "Epoch 4710/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2982 - accuracy: 0.8461\n",
            "Epoch 4710: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2971 - accuracy: 0.8465 - val_loss: 0.4282 - val_accuracy: 0.7483\n",
            "Epoch 4711/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8402\n",
            "Epoch 4711: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3043 - accuracy: 0.8400 - val_loss: 0.4666 - val_accuracy: 0.7237\n",
            "Epoch 4712/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8460\n",
            "Epoch 4712: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3006 - accuracy: 0.8460 - val_loss: 0.5443 - val_accuracy: 0.6695\n",
            "Epoch 4713/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2961 - accuracy: 0.8471\n",
            "Epoch 4713: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2970 - accuracy: 0.8470 - val_loss: 0.3911 - val_accuracy: 0.7722\n",
            "Epoch 4714/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.8481\n",
            "Epoch 4714: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2981 - accuracy: 0.8481 - val_loss: 0.4846 - val_accuracy: 0.6989\n",
            "Epoch 4715/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.8453\n",
            "Epoch 4715: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3019 - accuracy: 0.8453 - val_loss: 0.4106 - val_accuracy: 0.7542\n",
            "Epoch 4716/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8438\n",
            "Epoch 4716: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3051 - accuracy: 0.8436 - val_loss: 0.4217 - val_accuracy: 0.7487\n",
            "Epoch 4717/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3076 - accuracy: 0.8407\n",
            "Epoch 4717: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3080 - accuracy: 0.8406 - val_loss: 0.4596 - val_accuracy: 0.7235\n",
            "Epoch 4718/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2966 - accuracy: 0.8452\n",
            "Epoch 4718: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2978 - accuracy: 0.8447 - val_loss: 0.3786 - val_accuracy: 0.7821\n",
            "Epoch 4719/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3085 - accuracy: 0.8403\n",
            "Epoch 4719: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3083 - accuracy: 0.8400 - val_loss: 0.3865 - val_accuracy: 0.7742\n",
            "Epoch 4720/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3010 - accuracy: 0.8454\n",
            "Epoch 4720: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3027 - accuracy: 0.8449 - val_loss: 0.5999 - val_accuracy: 0.6570\n",
            "Epoch 4721/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3075 - accuracy: 0.8410\n",
            "Epoch 4721: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3105 - accuracy: 0.8391 - val_loss: 0.3696 - val_accuracy: 0.7858\n",
            "Epoch 4722/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3106 - accuracy: 0.8414\n",
            "Epoch 4722: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3109 - accuracy: 0.8408 - val_loss: 0.3890 - val_accuracy: 0.7599\n",
            "Epoch 4723/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3252 - accuracy: 0.8338\n",
            "Epoch 4723: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3257 - accuracy: 0.8334 - val_loss: 0.4977 - val_accuracy: 0.7016\n",
            "Epoch 4724/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8431\n",
            "Epoch 4724: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3045 - accuracy: 0.8429 - val_loss: 0.3896 - val_accuracy: 0.7753\n",
            "Epoch 4725/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8453\n",
            "Epoch 4725: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3050 - accuracy: 0.8450 - val_loss: 0.4606 - val_accuracy: 0.7200\n",
            "Epoch 4726/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3021 - accuracy: 0.8454\n",
            "Epoch 4726: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 27ms/step - loss: 0.3026 - accuracy: 0.8450 - val_loss: 0.4443 - val_accuracy: 0.7305\n",
            "Epoch 4727/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3028 - accuracy: 0.8461\n",
            "Epoch 4727: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3039 - accuracy: 0.8452 - val_loss: 0.3799 - val_accuracy: 0.7803\n",
            "Epoch 4728/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3034 - accuracy: 0.8468\n",
            "Epoch 4728: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3034 - accuracy: 0.8468 - val_loss: 0.4695 - val_accuracy: 0.7117\n",
            "Epoch 4729/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8447\n",
            "Epoch 4729: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3054 - accuracy: 0.8441 - val_loss: 0.4458 - val_accuracy: 0.7264\n",
            "Epoch 4730/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8423\n",
            "Epoch 4730: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3081 - accuracy: 0.8420 - val_loss: 0.4840 - val_accuracy: 0.7088\n",
            "Epoch 4731/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8471\n",
            "Epoch 4731: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2995 - accuracy: 0.8471 - val_loss: 0.4148 - val_accuracy: 0.7542\n",
            "Epoch 4732/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2970 - accuracy: 0.8472\n",
            "Epoch 4732: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2984 - accuracy: 0.8466 - val_loss: 0.4070 - val_accuracy: 0.7562\n",
            "Epoch 4733/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2982 - accuracy: 0.8473\n",
            "Epoch 4733: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2985 - accuracy: 0.8476 - val_loss: 0.5243 - val_accuracy: 0.6866\n",
            "Epoch 4734/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3200 - accuracy: 0.8377\n",
            "Epoch 4734: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3201 - accuracy: 0.8376 - val_loss: 0.5464 - val_accuracy: 0.6851\n",
            "Epoch 4735/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3033 - accuracy: 0.8474\n",
            "Epoch 4735: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3033 - accuracy: 0.8469 - val_loss: 0.4465 - val_accuracy: 0.7257\n",
            "Epoch 4736/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3031 - accuracy: 0.8440\n",
            "Epoch 4736: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3037 - accuracy: 0.8433 - val_loss: 0.4354 - val_accuracy: 0.7371\n",
            "Epoch 4737/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2960 - accuracy: 0.8480\n",
            "Epoch 4737: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2959 - accuracy: 0.8472 - val_loss: 0.4719 - val_accuracy: 0.7079\n",
            "Epoch 4738/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2974 - accuracy: 0.8471\n",
            "Epoch 4738: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2989 - accuracy: 0.8464 - val_loss: 0.5663 - val_accuracy: 0.6684\n",
            "Epoch 4739/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3156 - accuracy: 0.8388\n",
            "Epoch 4739: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3155 - accuracy: 0.8387 - val_loss: 0.3785 - val_accuracy: 0.7742\n",
            "Epoch 4740/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3148 - accuracy: 0.8384\n",
            "Epoch 4740: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3140 - accuracy: 0.8389 - val_loss: 0.4316 - val_accuracy: 0.7404\n",
            "Epoch 4741/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.8463\n",
            "Epoch 4741: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3032 - accuracy: 0.8463 - val_loss: 0.4816 - val_accuracy: 0.7053\n",
            "Epoch 4742/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8494\n",
            "Epoch 4742: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2967 - accuracy: 0.8495 - val_loss: 0.4912 - val_accuracy: 0.7101\n",
            "Epoch 4743/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3006 - accuracy: 0.8467\n",
            "Epoch 4743: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3006 - accuracy: 0.8460 - val_loss: 0.5146 - val_accuracy: 0.6921\n",
            "Epoch 4744/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3016 - accuracy: 0.8425\n",
            "Epoch 4744: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3020 - accuracy: 0.8422 - val_loss: 0.4413 - val_accuracy: 0.7347\n",
            "Epoch 4745/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2979 - accuracy: 0.8463\n",
            "Epoch 4745: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2994 - accuracy: 0.8447 - val_loss: 0.5179 - val_accuracy: 0.6919\n",
            "Epoch 4746/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3071 - accuracy: 0.8430\n",
            "Epoch 4746: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3057 - accuracy: 0.8435 - val_loss: 0.4503 - val_accuracy: 0.7272\n",
            "Epoch 4747/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3097 - accuracy: 0.8411\n",
            "Epoch 4747: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3110 - accuracy: 0.8405 - val_loss: 0.4906 - val_accuracy: 0.6937\n",
            "Epoch 4748/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8433\n",
            "Epoch 4748: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3031 - accuracy: 0.8436 - val_loss: 0.4239 - val_accuracy: 0.7446\n",
            "Epoch 4749/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2971 - accuracy: 0.8474\n",
            "Epoch 4749: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2988 - accuracy: 0.8465 - val_loss: 0.4902 - val_accuracy: 0.7081\n",
            "Epoch 4750/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8438\n",
            "Epoch 4750: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3049 - accuracy: 0.8432 - val_loss: 0.4686 - val_accuracy: 0.7092\n",
            "Epoch 4751/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8448\n",
            "Epoch 4751: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3028 - accuracy: 0.8447 - val_loss: 0.4105 - val_accuracy: 0.7496\n",
            "Epoch 4752/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2972 - accuracy: 0.8468\n",
            "Epoch 4752: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2976 - accuracy: 0.8463 - val_loss: 0.4547 - val_accuracy: 0.7389\n",
            "Epoch 4753/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8437\n",
            "Epoch 4753: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3020 - accuracy: 0.8438 - val_loss: 0.4659 - val_accuracy: 0.7042\n",
            "Epoch 4754/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2984 - accuracy: 0.8457\n",
            "Epoch 4754: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2984 - accuracy: 0.8459 - val_loss: 0.3696 - val_accuracy: 0.7911\n",
            "Epoch 4755/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2987 - accuracy: 0.8481\n",
            "Epoch 4755: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2991 - accuracy: 0.8475 - val_loss: 0.4601 - val_accuracy: 0.7224\n",
            "Epoch 4756/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3000 - accuracy: 0.8490\n",
            "Epoch 4756: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3021 - accuracy: 0.8470 - val_loss: 0.3901 - val_accuracy: 0.7705\n",
            "Epoch 4757/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2960 - accuracy: 0.8494\n",
            "Epoch 4757: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2983 - accuracy: 0.8476 - val_loss: 0.3908 - val_accuracy: 0.7571\n",
            "Epoch 4758/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.8405\n",
            "Epoch 4758: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3103 - accuracy: 0.8410 - val_loss: 0.3630 - val_accuracy: 0.7871\n",
            "Epoch 4759/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3139 - accuracy: 0.8395\n",
            "Epoch 4759: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3138 - accuracy: 0.8399 - val_loss: 0.4725 - val_accuracy: 0.7125\n",
            "Epoch 4760/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3051 - accuracy: 0.8420\n",
            "Epoch 4760: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3059 - accuracy: 0.8415 - val_loss: 0.4311 - val_accuracy: 0.7373\n",
            "Epoch 4761/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8448\n",
            "Epoch 4761: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3002 - accuracy: 0.8448 - val_loss: 0.3413 - val_accuracy: 0.8056\n",
            "Epoch 4762/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8473\n",
            "Epoch 4762: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3011 - accuracy: 0.8476 - val_loss: 0.4832 - val_accuracy: 0.6987\n",
            "Epoch 4763/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8466\n",
            "Epoch 4763: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3023 - accuracy: 0.8465 - val_loss: 0.4779 - val_accuracy: 0.7145\n",
            "Epoch 4764/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2997 - accuracy: 0.8461\n",
            "Epoch 4764: loss did not improve from 0.29404\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3003 - accuracy: 0.8451 - val_loss: 0.4153 - val_accuracy: 0.7494\n",
            "Epoch 4765/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2940 - accuracy: 0.8505\n",
            "Epoch 4765: loss improved from 0.29404 to 0.29291, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.2929 - accuracy: 0.8513 - val_loss: 0.4816 - val_accuracy: 0.7156\n",
            "Epoch 4766/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3048 - accuracy: 0.8432\n",
            "Epoch 4766: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3047 - accuracy: 0.8427 - val_loss: 0.4060 - val_accuracy: 0.7626\n",
            "Epoch 4767/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3074 - accuracy: 0.8417\n",
            "Epoch 4767: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3076 - accuracy: 0.8415 - val_loss: 0.4198 - val_accuracy: 0.7538\n",
            "Epoch 4768/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3069 - accuracy: 0.8436\n",
            "Epoch 4768: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3076 - accuracy: 0.8436 - val_loss: 0.3356 - val_accuracy: 0.8163\n",
            "Epoch 4769/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3019 - accuracy: 0.8438\n",
            "Epoch 4769: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3028 - accuracy: 0.8432 - val_loss: 0.4290 - val_accuracy: 0.7443\n",
            "Epoch 4770/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2956 - accuracy: 0.8495\n",
            "Epoch 4770: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2959 - accuracy: 0.8487 - val_loss: 0.4165 - val_accuracy: 0.7505\n",
            "Epoch 4771/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8488\n",
            "Epoch 4771: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2965 - accuracy: 0.8491 - val_loss: 0.4449 - val_accuracy: 0.7389\n",
            "Epoch 4772/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2999 - accuracy: 0.8459\n",
            "Epoch 4772: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3009 - accuracy: 0.8453 - val_loss: 0.5846 - val_accuracy: 0.6557\n",
            "Epoch 4773/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3039 - accuracy: 0.8451\n",
            "Epoch 4773: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3033 - accuracy: 0.8452 - val_loss: 0.3989 - val_accuracy: 0.7744\n",
            "Epoch 4774/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3003 - accuracy: 0.8448\n",
            "Epoch 4774: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3016 - accuracy: 0.8434 - val_loss: 0.4081 - val_accuracy: 0.7597\n",
            "Epoch 4775/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3029 - accuracy: 0.8465\n",
            "Epoch 4775: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3035 - accuracy: 0.8461 - val_loss: 0.4584 - val_accuracy: 0.7213\n",
            "Epoch 4776/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2966 - accuracy: 0.8497\n",
            "Epoch 4776: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2974 - accuracy: 0.8487 - val_loss: 0.4089 - val_accuracy: 0.7514\n",
            "Epoch 4777/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3020 - accuracy: 0.8479\n",
            "Epoch 4777: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3023 - accuracy: 0.8471 - val_loss: 0.4469 - val_accuracy: 0.7356\n",
            "Epoch 4778/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3052 - accuracy: 0.8454\n",
            "Epoch 4778: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3064 - accuracy: 0.8441 - val_loss: 0.5250 - val_accuracy: 0.6825\n",
            "Epoch 4779/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2976 - accuracy: 0.8477\n",
            "Epoch 4779: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2981 - accuracy: 0.8475 - val_loss: 0.3448 - val_accuracy: 0.8144\n",
            "Epoch 4780/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.8460\n",
            "Epoch 4780: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3023 - accuracy: 0.8464 - val_loss: 0.5941 - val_accuracy: 0.6410\n",
            "Epoch 4781/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8455\n",
            "Epoch 4781: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3008 - accuracy: 0.8455 - val_loss: 0.3530 - val_accuracy: 0.7981\n",
            "Epoch 4782/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8405\n",
            "Epoch 4782: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3174 - accuracy: 0.8405 - val_loss: 0.4134 - val_accuracy: 0.7378\n",
            "Epoch 4783/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3160 - accuracy: 0.8383\n",
            "Epoch 4783: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3161 - accuracy: 0.8381 - val_loss: 0.4096 - val_accuracy: 0.7591\n",
            "Epoch 4784/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3101 - accuracy: 0.8411\n",
            "Epoch 4784: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3121 - accuracy: 0.8393 - val_loss: 0.4193 - val_accuracy: 0.7457\n",
            "Epoch 4785/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.8410\n",
            "Epoch 4785: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3116 - accuracy: 0.8410 - val_loss: 0.4691 - val_accuracy: 0.7204\n",
            "Epoch 4786/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3042 - accuracy: 0.8447\n",
            "Epoch 4786: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3044 - accuracy: 0.8442 - val_loss: 0.4152 - val_accuracy: 0.7547\n",
            "Epoch 4787/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8451\n",
            "Epoch 4787: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3027 - accuracy: 0.8449 - val_loss: 0.3920 - val_accuracy: 0.7654\n",
            "Epoch 4788/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2963 - accuracy: 0.8493\n",
            "Epoch 4788: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2977 - accuracy: 0.8488 - val_loss: 0.4589 - val_accuracy: 0.7279\n",
            "Epoch 4789/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.8436\n",
            "Epoch 4789: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3057 - accuracy: 0.8436 - val_loss: 0.4150 - val_accuracy: 0.7586\n",
            "Epoch 4790/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2998 - accuracy: 0.8466\n",
            "Epoch 4790: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3025 - accuracy: 0.8453 - val_loss: 0.4816 - val_accuracy: 0.7275\n",
            "Epoch 4791/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3017 - accuracy: 0.8449\n",
            "Epoch 4791: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3004 - accuracy: 0.8456 - val_loss: 0.4219 - val_accuracy: 0.7452\n",
            "Epoch 4792/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8433\n",
            "Epoch 4792: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3054 - accuracy: 0.8435 - val_loss: 0.4719 - val_accuracy: 0.7088\n",
            "Epoch 4793/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3100 - accuracy: 0.8411\n",
            "Epoch 4793: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3099 - accuracy: 0.8409 - val_loss: 0.3687 - val_accuracy: 0.7924\n",
            "Epoch 4794/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2974 - accuracy: 0.8498\n",
            "Epoch 4794: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2998 - accuracy: 0.8490 - val_loss: 0.4146 - val_accuracy: 0.7527\n",
            "Epoch 4795/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8461\n",
            "Epoch 4795: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3012 - accuracy: 0.8461 - val_loss: 0.5373 - val_accuracy: 0.6818\n",
            "Epoch 4796/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3002 - accuracy: 0.8485\n",
            "Epoch 4796: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3012 - accuracy: 0.8480 - val_loss: 0.4370 - val_accuracy: 0.7354\n",
            "Epoch 4797/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2994 - accuracy: 0.8475\n",
            "Epoch 4797: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3011 - accuracy: 0.8463 - val_loss: 0.5051 - val_accuracy: 0.6906\n",
            "Epoch 4798/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2966 - accuracy: 0.8499\n",
            "Epoch 4798: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2979 - accuracy: 0.8483 - val_loss: 0.4151 - val_accuracy: 0.7525\n",
            "Epoch 4799/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3015 - accuracy: 0.8449\n",
            "Epoch 4799: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3015 - accuracy: 0.8447 - val_loss: 0.4447 - val_accuracy: 0.7242\n",
            "Epoch 4800/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3019 - accuracy: 0.8473\n",
            "Epoch 4800: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3024 - accuracy: 0.8471 - val_loss: 0.4443 - val_accuracy: 0.7215\n",
            "Epoch 4801/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8431\n",
            "Epoch 4801: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3075 - accuracy: 0.8431 - val_loss: 0.4886 - val_accuracy: 0.7024\n",
            "Epoch 4802/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8433\n",
            "Epoch 4802: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3064 - accuracy: 0.8428 - val_loss: 0.4874 - val_accuracy: 0.7064\n",
            "Epoch 4803/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3050 - accuracy: 0.8418\n",
            "Epoch 4803: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3046 - accuracy: 0.8421 - val_loss: 0.4872 - val_accuracy: 0.6853\n",
            "Epoch 4804/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3005 - accuracy: 0.8465\n",
            "Epoch 4804: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3011 - accuracy: 0.8459 - val_loss: 0.3814 - val_accuracy: 0.7817\n",
            "Epoch 4805/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2992 - accuracy: 0.8468\n",
            "Epoch 4805: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2998 - accuracy: 0.8462 - val_loss: 0.4951 - val_accuracy: 0.6798\n",
            "Epoch 4806/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.8443\n",
            "Epoch 4806: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3012 - accuracy: 0.8442 - val_loss: 0.4328 - val_accuracy: 0.7321\n",
            "Epoch 4807/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2965 - accuracy: 0.8471\n",
            "Epoch 4807: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2968 - accuracy: 0.8467 - val_loss: 0.3903 - val_accuracy: 0.7648\n",
            "Epoch 4808/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3112 - accuracy: 0.8393\n",
            "Epoch 4808: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3115 - accuracy: 0.8390 - val_loss: 0.7608 - val_accuracy: 0.5749\n",
            "Epoch 4809/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8362\n",
            "Epoch 4809: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3203 - accuracy: 0.8356 - val_loss: 0.3750 - val_accuracy: 0.7777\n",
            "Epoch 4810/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3107 - accuracy: 0.8422\n",
            "Epoch 4810: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3099 - accuracy: 0.8435 - val_loss: 0.3752 - val_accuracy: 0.7722\n",
            "Epoch 4811/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3127 - accuracy: 0.8403\n",
            "Epoch 4811: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3126 - accuracy: 0.8405 - val_loss: 0.4508 - val_accuracy: 0.7340\n",
            "Epoch 4812/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8470\n",
            "Epoch 4812: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3022 - accuracy: 0.8469 - val_loss: 0.3949 - val_accuracy: 0.7617\n",
            "Epoch 4813/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8445\n",
            "Epoch 4813: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 29ms/step - loss: 0.2990 - accuracy: 0.8445 - val_loss: 0.4238 - val_accuracy: 0.7531\n",
            "Epoch 4814/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8439\n",
            "Epoch 4814: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3056 - accuracy: 0.8436 - val_loss: 0.4471 - val_accuracy: 0.7323\n",
            "Epoch 4815/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.8430\n",
            "Epoch 4815: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3059 - accuracy: 0.8430 - val_loss: 0.4623 - val_accuracy: 0.7202\n",
            "Epoch 4816/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2964 - accuracy: 0.8481\n",
            "Epoch 4816: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2970 - accuracy: 0.8482 - val_loss: 0.5105 - val_accuracy: 0.6869\n",
            "Epoch 4817/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2988 - accuracy: 0.8483\n",
            "Epoch 4817: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2981 - accuracy: 0.8486 - val_loss: 0.3914 - val_accuracy: 0.7648\n",
            "Epoch 4818/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8466\n",
            "Epoch 4818: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2980 - accuracy: 0.8463 - val_loss: 0.4407 - val_accuracy: 0.7487\n",
            "Epoch 4819/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2971 - accuracy: 0.8480\n",
            "Epoch 4819: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2980 - accuracy: 0.8475 - val_loss: 0.4220 - val_accuracy: 0.7375\n",
            "Epoch 4820/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2963 - accuracy: 0.8480\n",
            "Epoch 4820: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2972 - accuracy: 0.8478 - val_loss: 0.4925 - val_accuracy: 0.6978\n",
            "Epoch 4821/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2989 - accuracy: 0.8486\n",
            "Epoch 4821: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2993 - accuracy: 0.8483 - val_loss: 0.3899 - val_accuracy: 0.7742\n",
            "Epoch 4822/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2945 - accuracy: 0.8504\n",
            "Epoch 4822: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2954 - accuracy: 0.8495 - val_loss: 0.3413 - val_accuracy: 0.8106\n",
            "Epoch 4823/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3023 - accuracy: 0.8442\n",
            "Epoch 4823: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3026 - accuracy: 0.8439 - val_loss: 0.4487 - val_accuracy: 0.7185\n",
            "Epoch 4824/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8472\n",
            "Epoch 4824: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2983 - accuracy: 0.8470 - val_loss: 0.4812 - val_accuracy: 0.7040\n",
            "Epoch 4825/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.8403\n",
            "Epoch 4825: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3136 - accuracy: 0.8410 - val_loss: 0.3517 - val_accuracy: 0.8001\n",
            "Epoch 4826/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8429\n",
            "Epoch 4826: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3049 - accuracy: 0.8430 - val_loss: 0.4615 - val_accuracy: 0.7233\n",
            "Epoch 4827/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3003 - accuracy: 0.8467\n",
            "Epoch 4827: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3005 - accuracy: 0.8466 - val_loss: 0.5378 - val_accuracy: 0.6713\n",
            "Epoch 4828/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8445\n",
            "Epoch 4828: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3005 - accuracy: 0.8444 - val_loss: 0.3983 - val_accuracy: 0.7713\n",
            "Epoch 4829/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8456\n",
            "Epoch 4829: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2976 - accuracy: 0.8460 - val_loss: 0.4561 - val_accuracy: 0.7224\n",
            "Epoch 4830/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3040 - accuracy: 0.8437\n",
            "Epoch 4830: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3045 - accuracy: 0.8433 - val_loss: 0.3970 - val_accuracy: 0.7663\n",
            "Epoch 4831/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2983 - accuracy: 0.8479\n",
            "Epoch 4831: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2989 - accuracy: 0.8474 - val_loss: 0.3837 - val_accuracy: 0.7779\n",
            "Epoch 4832/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3081 - accuracy: 0.8417\n",
            "Epoch 4832: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3071 - accuracy: 0.8421 - val_loss: 0.3698 - val_accuracy: 0.7955\n",
            "Epoch 4833/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.8463\n",
            "Epoch 4833: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3006 - accuracy: 0.8463 - val_loss: 0.4260 - val_accuracy: 0.7389\n",
            "Epoch 4834/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2966 - accuracy: 0.8496\n",
            "Epoch 4834: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.2965 - accuracy: 0.8493 - val_loss: 0.3546 - val_accuracy: 0.8007\n",
            "Epoch 4835/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3105 - accuracy: 0.8435\n",
            "Epoch 4835: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3083 - accuracy: 0.8447 - val_loss: 0.4331 - val_accuracy: 0.7316\n",
            "Epoch 4836/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3015 - accuracy: 0.8465\n",
            "Epoch 4836: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3019 - accuracy: 0.8458 - val_loss: 0.4520 - val_accuracy: 0.7393\n",
            "Epoch 4837/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8462\n",
            "Epoch 4837: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3014 - accuracy: 0.8459 - val_loss: 0.4105 - val_accuracy: 0.7443\n",
            "Epoch 4838/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3037 - accuracy: 0.8459\n",
            "Epoch 4838: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3021 - accuracy: 0.8463 - val_loss: 0.5295 - val_accuracy: 0.6770\n",
            "Epoch 4839/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3032 - accuracy: 0.8457\n",
            "Epoch 4839: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3056 - accuracy: 0.8436 - val_loss: 0.4188 - val_accuracy: 0.7468\n",
            "Epoch 4840/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2987 - accuracy: 0.8464\n",
            "Epoch 4840: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2992 - accuracy: 0.8464 - val_loss: 0.4527 - val_accuracy: 0.7239\n",
            "Epoch 4841/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2990 - accuracy: 0.8457\n",
            "Epoch 4841: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.2981 - accuracy: 0.8465 - val_loss: 0.5032 - val_accuracy: 0.6987\n",
            "Epoch 4842/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2983 - accuracy: 0.8483\n",
            "Epoch 4842: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2989 - accuracy: 0.8472 - val_loss: 0.4745 - val_accuracy: 0.7064\n",
            "Epoch 4843/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.8468\n",
            "Epoch 4843: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3058 - accuracy: 0.8468 - val_loss: 0.3586 - val_accuracy: 0.7924\n",
            "Epoch 4844/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2964 - accuracy: 0.8476\n",
            "Epoch 4844: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2958 - accuracy: 0.8478 - val_loss: 0.4588 - val_accuracy: 0.7185\n",
            "Epoch 4845/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8424\n",
            "Epoch 4845: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3112 - accuracy: 0.8418 - val_loss: 0.4433 - val_accuracy: 0.7476\n",
            "Epoch 4846/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3056 - accuracy: 0.8445\n",
            "Epoch 4846: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3064 - accuracy: 0.8441 - val_loss: 0.4888 - val_accuracy: 0.6803\n",
            "Epoch 4847/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3063 - accuracy: 0.8435\n",
            "Epoch 4847: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3069 - accuracy: 0.8429 - val_loss: 0.3814 - val_accuracy: 0.7707\n",
            "Epoch 4848/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3002 - accuracy: 0.8465\n",
            "Epoch 4848: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3009 - accuracy: 0.8453 - val_loss: 0.4904 - val_accuracy: 0.6978\n",
            "Epoch 4849/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2957 - accuracy: 0.8489\n",
            "Epoch 4849: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2981 - accuracy: 0.8471 - val_loss: 0.3960 - val_accuracy: 0.7577\n",
            "Epoch 4850/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2989 - accuracy: 0.8467\n",
            "Epoch 4850: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2991 - accuracy: 0.8464 - val_loss: 0.4358 - val_accuracy: 0.7351\n",
            "Epoch 4851/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3010 - accuracy: 0.8470\n",
            "Epoch 4851: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3029 - accuracy: 0.8460 - val_loss: 0.3936 - val_accuracy: 0.7551\n",
            "Epoch 4852/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3021 - accuracy: 0.8456\n",
            "Epoch 4852: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3026 - accuracy: 0.8448 - val_loss: 0.4231 - val_accuracy: 0.7494\n",
            "Epoch 4853/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3014 - accuracy: 0.8454\n",
            "Epoch 4853: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3012 - accuracy: 0.8459 - val_loss: 0.4349 - val_accuracy: 0.7248\n",
            "Epoch 4854/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3024 - accuracy: 0.8468\n",
            "Epoch 4854: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3031 - accuracy: 0.8468 - val_loss: 0.3944 - val_accuracy: 0.7601\n",
            "Epoch 4855/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3078 - accuracy: 0.8430\n",
            "Epoch 4855: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3077 - accuracy: 0.8434 - val_loss: 0.5356 - val_accuracy: 0.7057\n",
            "Epoch 4856/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3181 - accuracy: 0.8343\n",
            "Epoch 4856: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3167 - accuracy: 0.8355 - val_loss: 0.4100 - val_accuracy: 0.7443\n",
            "Epoch 4857/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3133 - accuracy: 0.8406\n",
            "Epoch 4857: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3139 - accuracy: 0.8397 - val_loss: 0.5163 - val_accuracy: 0.7000\n",
            "Epoch 4858/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.8418\n",
            "Epoch 4858: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3098 - accuracy: 0.8418 - val_loss: 0.3719 - val_accuracy: 0.7880\n",
            "Epoch 4859/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8396\n",
            "Epoch 4859: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3069 - accuracy: 0.8394 - val_loss: 0.6132 - val_accuracy: 0.6493\n",
            "Epoch 4860/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.8430\n",
            "Epoch 4860: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3126 - accuracy: 0.8426 - val_loss: 0.4078 - val_accuracy: 0.7612\n",
            "Epoch 4861/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2972 - accuracy: 0.8495\n",
            "Epoch 4861: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2977 - accuracy: 0.8488 - val_loss: 0.4073 - val_accuracy: 0.7507\n",
            "Epoch 4862/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2994 - accuracy: 0.8474\n",
            "Epoch 4862: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2986 - accuracy: 0.8478 - val_loss: 0.5218 - val_accuracy: 0.6743\n",
            "Epoch 4863/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3118 - accuracy: 0.8411\n",
            "Epoch 4863: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3113 - accuracy: 0.8413 - val_loss: 0.4474 - val_accuracy: 0.7323\n",
            "Epoch 4864/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3122 - accuracy: 0.8391\n",
            "Epoch 4864: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3134 - accuracy: 0.8378 - val_loss: 0.5101 - val_accuracy: 0.6772\n",
            "Epoch 4865/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2994 - accuracy: 0.8469\n",
            "Epoch 4865: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2996 - accuracy: 0.8468 - val_loss: 0.3812 - val_accuracy: 0.7685\n",
            "Epoch 4866/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2987 - accuracy: 0.8462\n",
            "Epoch 4866: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2982 - accuracy: 0.8463 - val_loss: 0.4657 - val_accuracy: 0.7099\n",
            "Epoch 4867/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3063 - accuracy: 0.8455\n",
            "Epoch 4867: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3079 - accuracy: 0.8440 - val_loss: 0.3852 - val_accuracy: 0.7731\n",
            "Epoch 4868/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8463\n",
            "Epoch 4868: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3055 - accuracy: 0.8465 - val_loss: 0.4199 - val_accuracy: 0.7441\n",
            "Epoch 4869/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8435\n",
            "Epoch 4869: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3035 - accuracy: 0.8436 - val_loss: 0.5510 - val_accuracy: 0.6781\n",
            "Epoch 4870/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8467\n",
            "Epoch 4870: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2984 - accuracy: 0.8462 - val_loss: 0.4179 - val_accuracy: 0.7505\n",
            "Epoch 4871/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3019 - accuracy: 0.8471\n",
            "Epoch 4871: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3035 - accuracy: 0.8458 - val_loss: 0.3756 - val_accuracy: 0.7854\n",
            "Epoch 4872/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8466\n",
            "Epoch 4872: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3032 - accuracy: 0.8460 - val_loss: 0.5900 - val_accuracy: 0.6634\n",
            "Epoch 4873/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.8457\n",
            "Epoch 4873: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2994 - accuracy: 0.8458 - val_loss: 0.3885 - val_accuracy: 0.7665\n",
            "Epoch 4874/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2987 - accuracy: 0.8478\n",
            "Epoch 4874: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2986 - accuracy: 0.8472 - val_loss: 0.3571 - val_accuracy: 0.7933\n",
            "Epoch 4875/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2988 - accuracy: 0.8501\n",
            "Epoch 4875: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2995 - accuracy: 0.8490 - val_loss: 0.4451 - val_accuracy: 0.7336\n",
            "Epoch 4876/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3004 - accuracy: 0.8470\n",
            "Epoch 4876: loss did not improve from 0.29291\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3012 - accuracy: 0.8465 - val_loss: 0.4488 - val_accuracy: 0.7246\n",
            "Epoch 4877/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8524\n",
            "Epoch 4877: loss improved from 0.29291 to 0.29276, saving model to /content/drive/MyDrive/new_df/best_model_by_class1.hdf5\n",
            "31/31 [==============================] - 1s 29ms/step - loss: 0.2928 - accuracy: 0.8522 - val_loss: 0.4625 - val_accuracy: 0.7145\n",
            "Epoch 4878/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2958 - accuracy: 0.8499\n",
            "Epoch 4878: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.2966 - accuracy: 0.8489 - val_loss: 0.4373 - val_accuracy: 0.7474\n",
            "Epoch 4879/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3019 - accuracy: 0.8436\n",
            "Epoch 4879: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3036 - accuracy: 0.8427 - val_loss: 0.5004 - val_accuracy: 0.6880\n",
            "Epoch 4880/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8477\n",
            "Epoch 4880: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2969 - accuracy: 0.8474 - val_loss: 0.3855 - val_accuracy: 0.7691\n",
            "Epoch 4881/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2982 - accuracy: 0.8478\n",
            "Epoch 4881: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2974 - accuracy: 0.8489 - val_loss: 0.4042 - val_accuracy: 0.7472\n",
            "Epoch 4882/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8463\n",
            "Epoch 4882: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3019 - accuracy: 0.8459 - val_loss: 0.3745 - val_accuracy: 0.7950\n",
            "Epoch 4883/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2993 - accuracy: 0.8490\n",
            "Epoch 4883: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2988 - accuracy: 0.8494 - val_loss: 0.4473 - val_accuracy: 0.7340\n",
            "Epoch 4884/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3025 - accuracy: 0.8443\n",
            "Epoch 4884: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3019 - accuracy: 0.8448 - val_loss: 0.3849 - val_accuracy: 0.7803\n",
            "Epoch 4885/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2957 - accuracy: 0.8453\n",
            "Epoch 4885: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2966 - accuracy: 0.8444 - val_loss: 0.3718 - val_accuracy: 0.7845\n",
            "Epoch 4886/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2992 - accuracy: 0.8448\n",
            "Epoch 4886: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3008 - accuracy: 0.8433 - val_loss: 0.3952 - val_accuracy: 0.7632\n",
            "Epoch 4887/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2982 - accuracy: 0.8470\n",
            "Epoch 4887: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2981 - accuracy: 0.8475 - val_loss: 0.4012 - val_accuracy: 0.7608\n",
            "Epoch 4888/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2988 - accuracy: 0.8474\n",
            "Epoch 4888: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2996 - accuracy: 0.8476 - val_loss: 0.4188 - val_accuracy: 0.7577\n",
            "Epoch 4889/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8477\n",
            "Epoch 4889: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2999 - accuracy: 0.8479 - val_loss: 0.4020 - val_accuracy: 0.7551\n",
            "Epoch 4890/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2967 - accuracy: 0.8453\n",
            "Epoch 4890: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2972 - accuracy: 0.8454 - val_loss: 0.3588 - val_accuracy: 0.7863\n",
            "Epoch 4891/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3004 - accuracy: 0.8473\n",
            "Epoch 4891: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3018 - accuracy: 0.8463 - val_loss: 0.3931 - val_accuracy: 0.7608\n",
            "Epoch 4892/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8479\n",
            "Epoch 4892: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2989 - accuracy: 0.8478 - val_loss: 0.4102 - val_accuracy: 0.7373\n",
            "Epoch 4893/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2986 - accuracy: 0.8483\n",
            "Epoch 4893: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3025 - accuracy: 0.8453 - val_loss: 0.3931 - val_accuracy: 0.7696\n",
            "Epoch 4894/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8472\n",
            "Epoch 4894: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3029 - accuracy: 0.8472 - val_loss: 0.4468 - val_accuracy: 0.7299\n",
            "Epoch 4895/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3053 - accuracy: 0.8436\n",
            "Epoch 4895: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3064 - accuracy: 0.8436 - val_loss: 0.3858 - val_accuracy: 0.7731\n",
            "Epoch 4896/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2983 - accuracy: 0.8464\n",
            "Epoch 4896: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3004 - accuracy: 0.8453 - val_loss: 0.3667 - val_accuracy: 0.7900\n",
            "Epoch 4897/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3020 - accuracy: 0.8435\n",
            "Epoch 4897: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3025 - accuracy: 0.8432 - val_loss: 0.4084 - val_accuracy: 0.7659\n",
            "Epoch 4898/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8422\n",
            "Epoch 4898: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3082 - accuracy: 0.8420 - val_loss: 0.4425 - val_accuracy: 0.7323\n",
            "Epoch 4899/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3028 - accuracy: 0.8459\n",
            "Epoch 4899: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3029 - accuracy: 0.8457 - val_loss: 0.3741 - val_accuracy: 0.7738\n",
            "Epoch 4900/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8457\n",
            "Epoch 4900: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3011 - accuracy: 0.8457 - val_loss: 0.3988 - val_accuracy: 0.7665\n",
            "Epoch 4901/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8504\n",
            "Epoch 4901: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2948 - accuracy: 0.8499 - val_loss: 0.4046 - val_accuracy: 0.7580\n",
            "Epoch 4902/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2966 - accuracy: 0.8457\n",
            "Epoch 4902: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2966 - accuracy: 0.8457 - val_loss: 0.3942 - val_accuracy: 0.7648\n",
            "Epoch 4903/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8453\n",
            "Epoch 4903: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3019 - accuracy: 0.8452 - val_loss: 0.4431 - val_accuracy: 0.7443\n",
            "Epoch 4904/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.8460\n",
            "Epoch 4904: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3020 - accuracy: 0.8458 - val_loss: 0.5123 - val_accuracy: 0.6972\n",
            "Epoch 4905/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.8408\n",
            "Epoch 4905: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3061 - accuracy: 0.8408 - val_loss: 0.4441 - val_accuracy: 0.7288\n",
            "Epoch 4906/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3050 - accuracy: 0.8427\n",
            "Epoch 4906: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3050 - accuracy: 0.8427 - val_loss: 0.4607 - val_accuracy: 0.7196\n",
            "Epoch 4907/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8435\n",
            "Epoch 4907: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3041 - accuracy: 0.8435 - val_loss: 0.4787 - val_accuracy: 0.7121\n",
            "Epoch 4908/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3059 - accuracy: 0.8431\n",
            "Epoch 4908: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3058 - accuracy: 0.8432 - val_loss: 0.5451 - val_accuracy: 0.6772\n",
            "Epoch 4909/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3113 - accuracy: 0.8403\n",
            "Epoch 4909: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3126 - accuracy: 0.8394 - val_loss: 0.4594 - val_accuracy: 0.7125\n",
            "Epoch 4910/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.8471\n",
            "Epoch 4910: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2978 - accuracy: 0.8471 - val_loss: 0.4048 - val_accuracy: 0.7628\n",
            "Epoch 4911/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2954 - accuracy: 0.8486\n",
            "Epoch 4911: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2954 - accuracy: 0.8486 - val_loss: 0.4642 - val_accuracy: 0.7178\n",
            "Epoch 4912/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3017 - accuracy: 0.8437\n",
            "Epoch 4912: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3017 - accuracy: 0.8437 - val_loss: 0.4342 - val_accuracy: 0.7406\n",
            "Epoch 4913/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2996 - accuracy: 0.8477\n",
            "Epoch 4913: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3009 - accuracy: 0.8471 - val_loss: 0.5361 - val_accuracy: 0.6711\n",
            "Epoch 4914/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2964 - accuracy: 0.8476\n",
            "Epoch 4914: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.2975 - accuracy: 0.8472 - val_loss: 0.4067 - val_accuracy: 0.7553\n",
            "Epoch 4915/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8465\n",
            "Epoch 4915: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2999 - accuracy: 0.8459 - val_loss: 0.6283 - val_accuracy: 0.6432\n",
            "Epoch 4916/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.8397\n",
            "Epoch 4916: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3161 - accuracy: 0.8398 - val_loss: 0.3678 - val_accuracy: 0.7727\n",
            "Epoch 4917/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8389\n",
            "Epoch 4917: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3136 - accuracy: 0.8393 - val_loss: 0.5535 - val_accuracy: 0.6796\n",
            "Epoch 4918/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.8384\n",
            "Epoch 4918: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3157 - accuracy: 0.8384 - val_loss: 0.4120 - val_accuracy: 0.7443\n",
            "Epoch 4919/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3050 - accuracy: 0.8441\n",
            "Epoch 4919: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3055 - accuracy: 0.8431 - val_loss: 0.4622 - val_accuracy: 0.7345\n",
            "Epoch 4920/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3028 - accuracy: 0.8432\n",
            "Epoch 4920: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3025 - accuracy: 0.8439 - val_loss: 0.3753 - val_accuracy: 0.7790\n",
            "Epoch 4921/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8452\n",
            "Epoch 4921: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3005 - accuracy: 0.8453 - val_loss: 0.4086 - val_accuracy: 0.7610\n",
            "Epoch 4922/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8465\n",
            "Epoch 4922: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3021 - accuracy: 0.8461 - val_loss: 0.4531 - val_accuracy: 0.7299\n",
            "Epoch 4923/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2984 - accuracy: 0.8475\n",
            "Epoch 4923: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2987 - accuracy: 0.8476 - val_loss: 0.4319 - val_accuracy: 0.7417\n",
            "Epoch 4924/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8453\n",
            "Epoch 4924: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3039 - accuracy: 0.8451 - val_loss: 0.5344 - val_accuracy: 0.6787\n",
            "Epoch 4925/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3119 - accuracy: 0.8422\n",
            "Epoch 4925: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3129 - accuracy: 0.8409 - val_loss: 0.4177 - val_accuracy: 0.7542\n",
            "Epoch 4926/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3027 - accuracy: 0.8455\n",
            "Epoch 4926: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3025 - accuracy: 0.8459 - val_loss: 0.4838 - val_accuracy: 0.7136\n",
            "Epoch 4927/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8465\n",
            "Epoch 4927: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3013 - accuracy: 0.8459 - val_loss: 0.3896 - val_accuracy: 0.7722\n",
            "Epoch 4928/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2958 - accuracy: 0.8484\n",
            "Epoch 4928: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2977 - accuracy: 0.8475 - val_loss: 0.4011 - val_accuracy: 0.7656\n",
            "Epoch 4929/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2964 - accuracy: 0.8496\n",
            "Epoch 4929: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2981 - accuracy: 0.8491 - val_loss: 0.4132 - val_accuracy: 0.7408\n",
            "Epoch 4930/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3032 - accuracy: 0.8475\n",
            "Epoch 4930: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3032 - accuracy: 0.8472 - val_loss: 0.4064 - val_accuracy: 0.7595\n",
            "Epoch 4931/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2990 - accuracy: 0.8504\n",
            "Epoch 4931: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3019 - accuracy: 0.8482 - val_loss: 0.5535 - val_accuracy: 0.6768\n",
            "Epoch 4932/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3014 - accuracy: 0.8480\n",
            "Epoch 4932: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3041 - accuracy: 0.8460 - val_loss: 0.3860 - val_accuracy: 0.7564\n",
            "Epoch 4933/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3104 - accuracy: 0.8414\n",
            "Epoch 4933: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3093 - accuracy: 0.8412 - val_loss: 0.4561 - val_accuracy: 0.7215\n",
            "Epoch 4934/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2962 - accuracy: 0.8478\n",
            "Epoch 4934: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2972 - accuracy: 0.8472 - val_loss: 0.5068 - val_accuracy: 0.7042\n",
            "Epoch 4935/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3180 - accuracy: 0.8363\n",
            "Epoch 4935: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3167 - accuracy: 0.8366 - val_loss: 0.3831 - val_accuracy: 0.7849\n",
            "Epoch 4936/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3011 - accuracy: 0.8452\n",
            "Epoch 4936: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3024 - accuracy: 0.8437 - val_loss: 0.4545 - val_accuracy: 0.7255\n",
            "Epoch 4937/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2941 - accuracy: 0.8514\n",
            "Epoch 4937: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2968 - accuracy: 0.8492 - val_loss: 0.4624 - val_accuracy: 0.7189\n",
            "Epoch 4938/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3107 - accuracy: 0.8407\n",
            "Epoch 4938: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3108 - accuracy: 0.8403 - val_loss: 0.5332 - val_accuracy: 0.6899\n",
            "Epoch 4939/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3052 - accuracy: 0.8439\n",
            "Epoch 4939: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3050 - accuracy: 0.8440 - val_loss: 0.6171 - val_accuracy: 0.6423\n",
            "Epoch 4940/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3083 - accuracy: 0.8427\n",
            "Epoch 4940: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3084 - accuracy: 0.8424 - val_loss: 0.4303 - val_accuracy: 0.7448\n",
            "Epoch 4941/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3053 - accuracy: 0.8413\n",
            "Epoch 4941: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3067 - accuracy: 0.8407 - val_loss: 0.3304 - val_accuracy: 0.8150\n",
            "Epoch 4942/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8392\n",
            "Epoch 4942: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3132 - accuracy: 0.8392 - val_loss: 0.4318 - val_accuracy: 0.7522\n",
            "Epoch 4943/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.2950 - accuracy: 0.8487\n",
            "Epoch 4943: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2970 - accuracy: 0.8473 - val_loss: 0.5089 - val_accuracy: 0.6816\n",
            "Epoch 4944/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3000 - accuracy: 0.8474\n",
            "Epoch 4944: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2988 - accuracy: 0.8480 - val_loss: 0.3488 - val_accuracy: 0.8049\n",
            "Epoch 4945/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3055 - accuracy: 0.8447\n",
            "Epoch 4945: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3054 - accuracy: 0.8442 - val_loss: 0.4562 - val_accuracy: 0.7237\n",
            "Epoch 4946/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8458\n",
            "Epoch 4946: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2988 - accuracy: 0.8453 - val_loss: 0.4199 - val_accuracy: 0.7492\n",
            "Epoch 4947/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3007 - accuracy: 0.8460\n",
            "Epoch 4947: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3005 - accuracy: 0.8459 - val_loss: 0.3934 - val_accuracy: 0.7707\n",
            "Epoch 4948/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8503\n",
            "Epoch 4948: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2955 - accuracy: 0.8500 - val_loss: 0.3803 - val_accuracy: 0.7716\n",
            "Epoch 4949/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.8418\n",
            "Epoch 4949: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.3074 - accuracy: 0.8418 - val_loss: 0.5079 - val_accuracy: 0.6937\n",
            "Epoch 4950/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8496\n",
            "Epoch 4950: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2948 - accuracy: 0.8497 - val_loss: 0.4381 - val_accuracy: 0.7272\n",
            "Epoch 4951/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8416\n",
            "Epoch 4951: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3085 - accuracy: 0.8416 - val_loss: 0.4652 - val_accuracy: 0.7242\n",
            "Epoch 4952/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8464\n",
            "Epoch 4952: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3030 - accuracy: 0.8465 - val_loss: 0.5313 - val_accuracy: 0.6798\n",
            "Epoch 4953/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.8462\n",
            "Epoch 4953: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3004 - accuracy: 0.8462 - val_loss: 0.4827 - val_accuracy: 0.7312\n",
            "Epoch 4954/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3001 - accuracy: 0.8481\n",
            "Epoch 4954: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3013 - accuracy: 0.8482 - val_loss: 0.4782 - val_accuracy: 0.7211\n",
            "Epoch 4955/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2956 - accuracy: 0.8497\n",
            "Epoch 4955: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2956 - accuracy: 0.8497 - val_loss: 0.4263 - val_accuracy: 0.7378\n",
            "Epoch 4956/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8488\n",
            "Epoch 4956: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2945 - accuracy: 0.8495 - val_loss: 0.4787 - val_accuracy: 0.7051\n",
            "Epoch 4957/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8485\n",
            "Epoch 4957: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2963 - accuracy: 0.8485 - val_loss: 0.4119 - val_accuracy: 0.7498\n",
            "Epoch 4958/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8473\n",
            "Epoch 4958: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2985 - accuracy: 0.8467 - val_loss: 0.3679 - val_accuracy: 0.7808\n",
            "Epoch 4959/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8439\n",
            "Epoch 4959: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3023 - accuracy: 0.8435 - val_loss: 0.4300 - val_accuracy: 0.7310\n",
            "Epoch 4960/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8388\n",
            "Epoch 4960: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.3139 - accuracy: 0.8385 - val_loss: 0.4625 - val_accuracy: 0.7202\n",
            "Epoch 4961/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8488\n",
            "Epoch 4961: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2960 - accuracy: 0.8486 - val_loss: 0.4293 - val_accuracy: 0.7408\n",
            "Epoch 4962/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8446\n",
            "Epoch 4962: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2981 - accuracy: 0.8450 - val_loss: 0.3761 - val_accuracy: 0.7799\n",
            "Epoch 4963/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.8469\n",
            "Epoch 4963: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.2972 - accuracy: 0.8469 - val_loss: 0.4416 - val_accuracy: 0.7264\n",
            "Epoch 4964/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3081 - accuracy: 0.8435\n",
            "Epoch 4964: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3074 - accuracy: 0.8443 - val_loss: 0.4216 - val_accuracy: 0.7544\n",
            "Epoch 4965/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8427\n",
            "Epoch 4965: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3063 - accuracy: 0.8425 - val_loss: 0.4621 - val_accuracy: 0.7244\n",
            "Epoch 4966/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8440\n",
            "Epoch 4966: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.3024 - accuracy: 0.8440 - val_loss: 0.4404 - val_accuracy: 0.7345\n",
            "Epoch 4967/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8485\n",
            "Epoch 4967: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.2967 - accuracy: 0.8485 - val_loss: 0.4301 - val_accuracy: 0.7369\n",
            "Epoch 4968/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8488\n",
            "Epoch 4968: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2955 - accuracy: 0.8489 - val_loss: 0.4074 - val_accuracy: 0.7584\n",
            "Epoch 4969/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.8483\n",
            "Epoch 4969: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3002 - accuracy: 0.8466 - val_loss: 0.4235 - val_accuracy: 0.7492\n",
            "Epoch 4970/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.8463\n",
            "Epoch 4970: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3027 - accuracy: 0.8464 - val_loss: 0.4667 - val_accuracy: 0.7200\n",
            "Epoch 4971/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8454\n",
            "Epoch 4971: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.3002 - accuracy: 0.8455 - val_loss: 0.3730 - val_accuracy: 0.7893\n",
            "Epoch 4972/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3005 - accuracy: 0.8465\n",
            "Epoch 4972: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3009 - accuracy: 0.8465 - val_loss: 0.3671 - val_accuracy: 0.7953\n",
            "Epoch 4973/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8468\n",
            "Epoch 4973: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2984 - accuracy: 0.8467 - val_loss: 0.4365 - val_accuracy: 0.7318\n",
            "Epoch 4974/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3039 - accuracy: 0.8436\n",
            "Epoch 4974: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 16ms/step - loss: 0.3049 - accuracy: 0.8428 - val_loss: 0.4361 - val_accuracy: 0.7433\n",
            "Epoch 4975/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8449\n",
            "Epoch 4975: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3071 - accuracy: 0.8444 - val_loss: 0.3885 - val_accuracy: 0.7724\n",
            "Epoch 4976/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8474\n",
            "Epoch 4976: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2992 - accuracy: 0.8474 - val_loss: 0.4011 - val_accuracy: 0.7577\n",
            "Epoch 4977/5000\n",
            "27/31 [=========================>....] - ETA: 0s - loss: 0.3090 - accuracy: 0.8394\n",
            "Epoch 4977: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3083 - accuracy: 0.8396 - val_loss: 0.4070 - val_accuracy: 0.7529\n",
            "Epoch 4978/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2989 - accuracy: 0.8466\n",
            "Epoch 4978: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3005 - accuracy: 0.8454 - val_loss: 0.4676 - val_accuracy: 0.7009\n",
            "Epoch 4979/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2979 - accuracy: 0.8468\n",
            "Epoch 4979: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.2977 - accuracy: 0.8469 - val_loss: 0.3401 - val_accuracy: 0.8058\n",
            "Epoch 4980/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2955 - accuracy: 0.8478\n",
            "Epoch 4980: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2972 - accuracy: 0.8464 - val_loss: 0.4720 - val_accuracy: 0.7117\n",
            "Epoch 4981/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2956 - accuracy: 0.8492\n",
            "Epoch 4981: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2960 - accuracy: 0.8481 - val_loss: 0.4502 - val_accuracy: 0.7224\n",
            "Epoch 4982/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.8460\n",
            "Epoch 4982: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.3004 - accuracy: 0.8453 - val_loss: 0.4655 - val_accuracy: 0.7204\n",
            "Epoch 4983/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.2969 - accuracy: 0.8493\n",
            "Epoch 4983: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2983 - accuracy: 0.8482 - val_loss: 0.4422 - val_accuracy: 0.7220\n",
            "Epoch 4984/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8471\n",
            "Epoch 4984: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3019 - accuracy: 0.8475 - val_loss: 0.4342 - val_accuracy: 0.7382\n",
            "Epoch 4985/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8490\n",
            "Epoch 4985: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 21ms/step - loss: 0.2968 - accuracy: 0.8495 - val_loss: 0.3969 - val_accuracy: 0.7733\n",
            "Epoch 4986/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3010 - accuracy: 0.8445\n",
            "Epoch 4986: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.3013 - accuracy: 0.8444 - val_loss: 0.4630 - val_accuracy: 0.7198\n",
            "Epoch 4987/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3071 - accuracy: 0.8434\n",
            "Epoch 4987: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3052 - accuracy: 0.8446 - val_loss: 0.4046 - val_accuracy: 0.7709\n",
            "Epoch 4988/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3134 - accuracy: 0.8412\n",
            "Epoch 4988: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3134 - accuracy: 0.8405 - val_loss: 0.4372 - val_accuracy: 0.7430\n",
            "Epoch 4989/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3016 - accuracy: 0.8468\n",
            "Epoch 4989: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3023 - accuracy: 0.8463 - val_loss: 0.4240 - val_accuracy: 0.7349\n",
            "Epoch 4990/5000\n",
            "30/31 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8435\n",
            "Epoch 4990: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.3073 - accuracy: 0.8433 - val_loss: 0.5334 - val_accuracy: 0.6748\n",
            "Epoch 4991/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.3155 - accuracy: 0.8387\n",
            "Epoch 4991: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 22ms/step - loss: 0.3165 - accuracy: 0.8379 - val_loss: 0.3803 - val_accuracy: 0.7854\n",
            "Epoch 4992/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8444\n",
            "Epoch 4992: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.3045 - accuracy: 0.8444 - val_loss: 0.4434 - val_accuracy: 0.7422\n",
            "Epoch 4993/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3088 - accuracy: 0.8431\n",
            "Epoch 4993: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3096 - accuracy: 0.8421 - val_loss: 0.5522 - val_accuracy: 0.6561\n",
            "Epoch 4994/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.8461\n",
            "Epoch 4994: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3027 - accuracy: 0.8461 - val_loss: 0.4063 - val_accuracy: 0.7595\n",
            "Epoch 4995/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8487\n",
            "Epoch 4995: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.2971 - accuracy: 0.8487 - val_loss: 0.4320 - val_accuracy: 0.7307\n",
            "Epoch 4996/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2975 - accuracy: 0.8493\n",
            "Epoch 4996: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2980 - accuracy: 0.8490 - val_loss: 0.4088 - val_accuracy: 0.7610\n",
            "Epoch 4997/5000\n",
            "28/31 [==========================>...] - ETA: 0s - loss: 0.2949 - accuracy: 0.8478\n",
            "Epoch 4997: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2961 - accuracy: 0.8477 - val_loss: 0.3926 - val_accuracy: 0.7696\n",
            "Epoch 4998/5000\n",
            "29/31 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.8461\n",
            "Epoch 4998: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3031 - accuracy: 0.8457 - val_loss: 0.3634 - val_accuracy: 0.7836\n",
            "Epoch 4999/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.8453\n",
            "Epoch 4999: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.3027 - accuracy: 0.8453 - val_loss: 0.4707 - val_accuracy: 0.7077\n",
            "Epoch 5000/5000\n",
            "31/31 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.8482\n",
            "Epoch 5000: loss did not improve from 0.29276\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.2995 - accuracy: 0.8482 - val_loss: 0.3411 - val_accuracy: 0.7999\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACEN0lEQVR4nO3dd3wT5R8H8E+6W7qAlrbMsneZAgVUFGTKEhQBERFBFPyB4EJkuQAHIjKcLAdTQWTvvUfZlFnKakuB7tKV+/1xNE3S7FxySfp5v159Jbk899yTS5r75pkKQRAEEBEREbkIN7kLQERERCQlBjdERETkUhjcEBERkUthcENEREQuhcENERERuRQGN0RERORSGNwQERGRS2FwQ0RERC6FwQ0RERG5FAY3ROTwFAoFpkyZYvZ+cXFxUCgUWLRokcF0u3btgkKhwK5duywqHxE5FgY3RGSSRYsWQaFQQKFQYN++fcWeFwQBlSpVgkKhwPPPPy9DCYmIRAxuiMgsPj4++Ouvv4pt3717N27dugVvb28ZSkVEVITBDRGZpWvXrli5ciXy8/M1tv/1119o1qwZwsPDZSoZEZGIwQ0RmaV///64f/8+tm7dqtqWm5uLVatWYcCAATr3yczMxLhx41CpUiV4e3ujdu3a+OabbyAIgka6nJwcvPvuuwgNDUVAQAB69OiBW7du6czz9u3beP311xEWFgZvb2/Ur18fCxYskO6FAli5ciWaNWsGX19fhISE4JVXXsHt27c10iQkJGDIkCGoWLEivL29ERERgZ49eyIuLk6V5tixY+jUqRNCQkLg6+uLqlWr4vXXX5e0rERUxEPuAhCRc4mMjER0dDSWLl2KLl26AAA2btyI1NRUvPzyy5g9e7ZGekEQ0KNHD+zcuRNDhw5F48aNsXnzZrz//vu4ffs2vvvuO1XaN954A3/88QcGDBiA1q1bY8eOHejWrVuxMiQmJqJVq1ZQKBQYNWoUQkNDsXHjRgwdOhRpaWkYM2aM1a9z0aJFGDJkCJ544glMmzYNiYmJ+P7777F//36cPHkSwcHBAIA+ffrg3LlzeOeddxAZGYmkpCRs3boV8fHxqscdO3ZEaGgoPvroIwQHByMuLg7//POP1WUkIj0EIiITLFy4UAAgHD16VJgzZ44QEBAgZGVlCYIgCC+++KLwzDPPCIIgCFWqVBG6deum2m/NmjUCAOHzzz/XyK9v376CQqEQrly5IgiCIMTExAgAhLffflsj3YABAwQAwuTJk1Xbhg4dKkRERAjJyckaaV9++WUhKChIVa7r168LAISFCxcafG07d+4UAAg7d+4UBEEQcnNzhXLlygkNGjQQsrOzVenWrVsnABAmTZokCIIgPHz4UAAgfP3113rzXr16teq8EZF9sFmKiMz20ksvITs7G+vWrUN6ejrWrVunt0lqw4YNcHd3x//+9z+N7ePGjYMgCNi4caMqHYBi6bRrYQRBwN9//43u3btDEAQkJyer/jp16oTU1FScOHHCqtd37NgxJCUl4e2334aPj49qe7du3VCnTh2sX78eAODr6wsvLy/s2rULDx8+1JlXYQ3PunXrkJeXZ1W5iMg0DG6IyGyhoaHo0KED/vrrL/zzzz8oKChA3759daa9ceMGypcvj4CAAI3tdevWVT1feOvm5obq1atrpKtdu7bG43v37iElJQU///wzQkNDNf6GDBkCAEhKSrLq9RWWSfvYAFCnTh3V897e3pgxYwY2btyIsLAwPPXUU/jqq6+QkJCgSv/000+jT58+mDp1KkJCQtCzZ08sXLgQOTk5VpWRiPRjnxsissiAAQMwbNgwJCQkoEuXLqoaCltTKpUAgFdeeQWDBw/WmSYqKsouZQHEmqXu3btjzZo12Lx5MyZOnIhp06Zhx44daNKkCRQKBVatWoVDhw7hv//+w+bNm/H666/j22+/xaFDh+Dv72+3shKVFKy5ISKL9O7dG25ubjh06JDeJikAqFKlCu7cuYP09HSN7RcvXlQ9X3irVCpx9epVjXSxsbEajwtHUhUUFKBDhw46/8qVK2fVayssk/axC7cVPl+oevXqGDduHLZs2YKzZ88iNzcX3377rUaaVq1a4YsvvsCxY8fw559/4ty5c1i2bJlV5SQi3RjcEJFF/P39MX/+fEyZMgXdu3fXm65r164oKCjAnDlzNLZ/9913UCgUqhFXhbfao61mzZql8djd3R19+vTB33//jbNnzxY73r179yx5ORqaN2+OcuXK4ccff9RoPtq4cSMuXLigGsGVlZWFR48eaexbvXp1BAQEqPZ7+PBhsSHvjRs3BgA2TRHZCJuliMhi+pqF1HXv3h3PPPMMJkyYgLi4ODRq1AhbtmzBv//+izFjxqj62DRu3Bj9+/fHvHnzkJqaitatW2P79u24cuVKsTynT5+OnTt3omXLlhg2bBjq1auHBw8e4MSJE9i2bRsePHhg1evy9PTEjBkzMGTIEDz99NPo37+/aih4ZGQk3n33XQDApUuX0L59e7z00kuoV68ePDw8sHr1aiQmJuLll18GACxevBjz5s1D7969Ub16daSnp+OXX35BYGAgunbtalU5iUg3BjdEZFNubm5Yu3YtJk2ahOXLl2PhwoWIjIzE119/jXHjxmmkXbBgAUJDQ/Hnn39izZo1ePbZZ7F+/XpUqlRJI11YWBiOHDmCTz/9FP/88w/mzZuHsmXLon79+pgxY4Yk5X7ttdfg5+eH6dOn48MPP0SpUqXQu3dvzJgxQ9W/qFKlSujfvz+2b9+O33//HR4eHqhTpw5WrFiBPn36ABA7FB85cgTLli1DYmIigoKC0KJFC/z555+oWrWqJGUlIk0KQbu+lIiIiMiJsc8NERERuRQGN0RERORSGNwQERGRS2FwQ0RERC6FwQ0RERG5FAY3RERE5FJK3Dw3SqUSd+7cQUBAABQKhdzFISIiIhMIgoD09HSUL18ebm6G62ZKXHBz586dYhOCERERkXO4efMmKlasaDBNiQtuAgICAIgnJzAwUObSEBERkSnS0tJQqVIl1XXckBIX3BQ2RQUGBjK4ISIicjKmdClhh2IiIiJyKQxuiIiIyKUwuCEiIiKXUuL63BARkesoKChAXl6e3MUgiXh5eRkd5m0KBjdEROR0BEFAQkICUlJS5C4KScjNzQ1Vq1aFl5eXVfkwuCEiIqdTGNiUK1cOfn5+nJTVBRROsnv37l1UrlzZqveUwQ0RETmVgoICVWBTtmxZuYtDEgoNDcWdO3eQn58PT09Pi/Nhh2IiInIqhX1s/Pz8ZC4JSa2wOaqgoMCqfBjcEBGRU2JTlOuR6j1lcENEREQuhcENERGRE4uMjMSsWbPkLoZDYXBDRERkBwqFwuDflClTLMr36NGjGD58uLSFdXIcLWVLuVmAFzu8ERERcPfuXdX95cuXY9KkSYiNjVVt8/f3V90XBAEFBQXw8DB+mQ4NDZW2oC6ANTe2cmoZ8GUEcPQ3uUtCREQOIDw8XPUXFBQEhUKhenzx4kUEBARg48aNaNasGby9vbFv3z5cvXoVPXv2RFhYGPz9/fHEE09g27ZtGvlqN0spFAr8+uuv6N27N/z8/FCzZk2sXbvWzq9WXgxubGX1m+Lt+rHAozRg0fPA0V/lLRMRkYsSBAFZufmy/AmCINnr+OijjzB9+nRcuHABUVFRyMjIQNeuXbF9+3acPHkSnTt3Rvfu3REfH28wn6lTp+Kll17C6dOn0bVrVwwcOBAPHjyQrJyOjs1S9nBwLhC3V/x74g25S0NE5HKy8wpQb9JmWY59/tNO8POS5nL66aef4rnnnlM9LlOmDBo1aqR6/Nlnn2H16tVYu3YtRo0apTef1157Df379wcAfPnll5g9ezaOHDmCzp07S1JOR8eaG3vIzZC7BERE5ASaN2+u8TgjIwPvvfce6tati+DgYPj7++PChQtGa26ioqJU90uVKoXAwEAkJSXZpMyOiDU39iBhlSURERXn6+mO8592ku3YUilVqpTG4/feew9bt27FN998gxo1asDX1xd9+/ZFbm6uwXy0ly5QKBRQKpWSldPRMbghIiKnp1AoJGsaciT79+/Ha6+9ht69ewMQa3Li4uLkLZQTYLMUERGRg6pZsyb++ecfxMTE4NSpUxgwYECJqoGxFIMbIiIiBzVz5kyULl0arVu3Rvfu3dGpUyc0bdpU7mI5PIUg5Rg2J5CWloagoCCkpqYiMDDQdgeaElR0v9VI4NDcx9tTbXdMIqIS4NGjR7h+/TqqVq0KHx8fuYtDEjL03ppz/WbNDREREbkUBjdERETkUhjc2EWJavkjIiKSFYMbe4g/KHcJiIiISgwGN/Zw56TcJSAiIioxGNwQERGRS2FwQ0RERC6FwQ0RERG5FAY3RERE5FIY3BARETmJdu3aYcyYMarHkZGRmDVrlsF9FAoF1qxZY/WxpcrHHhjcEBER2UH37t3RuXNnnc/t3bsXCoUCp0+fNivPo0ePYvjw4VIUT2XKlClo3Lhxse13795Fly5dJD2WrTC4ISIisoOhQ4di69atuHXrVrHnFi5ciObNmyMqKsqsPENDQ+Hn5ydVEQ0KDw+Ht7e3XY5lLQY3REREdvD8888jNDQUixYt0tiekZGBlStXolevXujfvz8qVKgAPz8/NGzYEEuXLjWYp3az1OXLl/HUU0/Bx8cH9erVw9atW4vt8+GHH6JWrVrw8/NDtWrVMHHiROTl5QEAFi1ahKlTp+LUqVNQKBRQKBSq8mo3S505cwbPPvssfH19UbZsWQwfPhwZGRmq51977TX06tUL33zzDSIiIlC2bFmMHDlSdSxb8rD5EUqalHjg31Fyl4KIqGQRBCAvS55je/oBCoXRZB4eHnj11VexaNEiTJgwAYrH+6xcuRIFBQV45ZVXsHLlSnz44YcIDAzE+vXrMWjQIFSvXh0tWrQwmr9SqcQLL7yAsLAwHD58GKmpqRr9cwoFBARg0aJFKF++PM6cOYNhw4YhICAAH3zwAfr164ezZ89i06ZN2LZtGwAgKCioWB6ZmZno1KkToqOjcfToUSQlJeGNN97AqFGjNIK3nTt3IiIiAjt37sSVK1fQr18/NG7cGMOGDTP6eqzB4EZq/44Eru+RuxRERCVLXhbwZXl5jv3xHcCrlElJX3/9dXz99dfYvXs32rVrB0BskurTpw+qVKmC9957T5X2nXfewebNm7FixQqTgptt27bh4sWL2Lx5M8qXF8/Fl19+WayfzCeffKK6HxkZiffeew/Lli3DBx98AF9fX/j7+8PDwwPh4eF6j/XXX3/h0aNHWLJkCUqVEl/7nDlz0L17d8yYMQNhYWEAgNKlS2POnDlwd3dHnTp10K1bN2zfvt3mwQ2bpaSWcU/uEhARkYOqU6cOWrdujQULFgAArly5gr1792Lo0KEoKCjAZ599hoYNG6JMmTLw9/fH5s2bER8fb1LeFy5cQKVKlVSBDQBER0cXS7d8+XK0adMG4eHh8Pf3xyeffGLyMdSP1ahRI1VgAwBt2rSBUqlEbGysalv9+vXh7u6uehwREYGkpCSzjmUJ1txIzYSqSSIikpinn1iDItexzTB06FC88847mDt3LhYuXIjq1avj6aefxowZM/D9999j1qxZaNiwIUqVKoUxY8YgNzdXsqIePHgQAwcOxNSpU9GpUycEBQVh2bJl+PbbbyU7hjpPT0+NxwqFAkql0ibHUsfgRnIMboiI7E6hMLlpSG4vvfQSRo8ejb/++gtLlizBW2+9BYVCgf3796Nnz5545ZVXAIh9aC5duoR69eqZlG/dunVx8+ZN3L17FxEREQCAQ4cOaaQ5cOAAqlSpggkTJqi23bhxQyONl5cXCgoKjB5r0aJFyMzMVNXe7N+/H25ubqhdu7ZJ5bUlNktJjTU3RERkgL+/P/r164fx48fj7t27eO211wAANWvWxNatW3HgwAFcuHABb775JhITE03Ot0OHDqhVqxYGDx6MU6dOYe/evRpBTOEx4uPjsWzZMly9ehWzZ8/G6tWrNdJERkbi+vXriImJQXJyMnJycooda+DAgfDx8cHgwYNx9uxZ7Ny5E++88w4GDRqk6m8jJwY3kmNwQ0REhg0dOhQPHz5Ep06dVH1kPvnkEzRt2hSdOnVCu3btEB4ejl69epmcp5ubG1avXo3s7Gy0aNECb7zxBr744guNND169MC7776LUaNGoXHjxjhw4AAmTpyokaZPnz7o3LkznnnmGYSGhuocju7n54fNmzfjwYMHeOKJJ9C3b1+0b98ec+bMMf9k2IBCEARB7kLYU1paGoKCgpCamorAwEDpD/BjWyDhjP7np6RKf0wiohLk0aNHuH79OqpWrQofHx+5i0MSMvTemnP9Zs2N5FhzQ0REJCcGN1JjnxsiIiJZMbiRnJHgpiDfPsUgIiIqoRjcSE1h5JQe/cU+5SAiIiqhGNxIzViz1LVddikGEZGrK2HjYUoEqd5TBjdERORUCme9zcqSaaFMspnC2ZjVl2ywBGcolhw7FBMR2ZK7uzuCg4NVaxT5+fmpVtgm56VUKnHv3j34+fnBw8O68ITBjdT4D0ZEZHOFK1bbYxFGsh83NzdUrlzZ6mCVwY3kGNwQEdmaQqFAREQEypUrh7y8PLmLQxLx8vKCm5v1PWYY3EhpwwfArSNyl4KIqMRwd3e3un8GuR52KJZK9kPgyE9yl4KIiKjEY3AjFaXh5eGJiIjIPhjcSIZ9bYiIiByBrMHNnj170L17d5QvXx4KhQJr1qwxus+uXbvQtGlTeHt7o0aNGli0aJHNy0lERETOQ9bgJjMzE40aNcLcuXNNSn/9+nV069YNzzzzDGJiYjBmzBi88cYb2Lx5s41LagIOASciInIIso6W6tKlC7p06WJy+h9//BFVq1bFt99+CwCoW7cu9u3bh++++w6dOnWyVTGJiIjIiThVn5uDBw+iQ4cOGts6deqEgwcP6t0nJycHaWlpGn9ERETkupwquElISEBYWJjGtrCwMKSlpSE7O1vnPtOmTUNQUJDqr1KlSrYpHJuliIiIHIJTBTeWGD9+PFJTU1V/N2/elLtIREREZENONUNxeHg4EhMTNbYlJiYiMDAQvr6+Ovfx9vaGt7e3HUrHmhsiIiJH4FQ1N9HR0di+fbvGtq1btyI6OlqmEqlhsxQREZFDkDW4ycjIQExMDGJiYgCIQ71jYmIQHx8PQGxSevXVV1XpR4wYgWvXruGDDz7AxYsXMW/ePKxYsQLvvvuuHMUnIiIiByRrcHPs2DE0adIETZo0AQCMHTsWTZo0waRJkwAAd+/eVQU6AFC1alWsX78eW7duRaNGjfDtt9/i119/dYxh4IIgdwmIiIgIgEIQStZVOS0tDUFBQUhNTUVgYKB0GWenADOqGE9XqzMwYLl0xyUiIioBzLl+O1WfG8dmYox4ZRuwsBvw4Lpti0NERFRCMbixN2U+cGMfsHqE3CUhIiJySQxu5JJ5T+4SEBERuSQGN1IpWV2XiIiIHBaDGyIiInIpDG5kw5oeIiIiW2BwQ0RERC6FwY1c2EeHiIjIJhjcSMXSYIVBDhERkaQY3Mjp6K/A19WBhDNyl4SIiMhlMLiRjJk1MA+vA+vHAVn3gX9H2aZIREREJRCDG4fApikiIiKpMLhxCAq5C0BEROQyGNxIxZqOwQodwc2KwcCSnuxwTEREZCYGNw5BK7gpyAPOrwGu7QJm1gWOL5KhTERERM6JwY1kbFTDkn4X+G+0bfImIiJyQQxuHMGdE3KXgIiIyGUwuHEU8YeL7rOfDRERkcUY3EjF2oDkbowkxSAiIirpGNw4Il2jp4iIiMgkDG4kY2XNjXrNj6M2S904AJxZJXcpiIiIDPKQuwDkRBZ2EW/L1QPC6slbFiIiIj1Yc0PmS70pdwmIiIj0YnAjFUdtSiIiIiphGNw4CmfqRMxAjoiIHBiDG8lI2KGYiIiILMYOxY7m5J/ArSNyl4KIiMhpMbiRilQ1L/++LU0+REREJRSbpRxJ7Ea5S+AY8h4B+Tlyl4KIiJwUgxtHkXgGWPqy3KWQX0E+8FVV4KtqgLJA7tIQEZETYnAjGSubpU7+IU0xnF1WMpCXBeRmADlpcpeGiIicEIMbsgBHdhERkeNicCOVwg7F7t7yloOIiKiEY3BjC2/skLsEroFz/xARkQUY3NhCqRC5S+DEnGimZiIickgMbiTzuJZBobDNUgoX1kmfp0NibQ0REVmHwY2zWD5Q7hIQERE5BQY3UlH1D1HAZk0rSweUgH4obJYiIiLrMLhxJrHrgYwkuUtRAgIsIiJyZgxubMKGF39lvu3yJiIicgEMbiSj1qHYlhjcEBERGcTgxiZsGOAIXG+JiIjIEAY3UtHoUGxDSqVt8yciInJyDG5swoZ9blhzQ0REZBCDG6mxz411bH3+iIjI5TG4kYydhkcv7m6f4xhky5opDjMnIiLrMLixBVteoLPu2y5ve7t1HIjbJ3cpiIjIxXjIXQCXYa8Oxa5CEIBfnxXvv38NKFVW3vIQEZHLYM0NyS8rueg++9wQEZGVGNxIjRdnIiIiWTG4sQl2iiUiIpILgxsiIiJyKQxupFKSOhRzuDYRETkwBjckD1MCJAZRRERkAQY3kilcFVzeUhAREZV0DG5sISBC7hLYFkeEERGRA2NwIzkF4OENfHhD7oLYjr2aixhEERGRBRjcSEX7gu8bLEsxXAr73BARkQUY3NhSm9Fyl8AJsbaGiIisw+BGMoUditUuzs99CrT+n/SHyn4I5OdIn6/JWKNCRESOiwtn2pot+o3MiARKhQJPfwjcOgb0mge4uUt/HJvSFyAxcCIiIuswuJGcnZpVMu8BG94T79d9Hqjb3T7HJSIicnBslpKKvs6vLYbb/tg56bY/hgZbBnBaeefnAhfXi01xREREJmBwY2tBFW1/DLuPKrLj8XZNA5YNABb3sN8xiYjIqTG4kYyODsX2PrbLEYAzK8W7CaflLQoRETkNBjdERETkUhjcSK4EzNMiRTOYSXk4+LlUFgD/jgROLJG7JEREpIbBjVQ4m64NOPg5Pb8GOPkHsPYduUtCRERqGNxITY4+N/YOrLjmk4gjuIiIHBKDG8k4eC2Ds9AOnFgjRkREZmJwQ+ZjwPEYa7CIiByR7MHN3LlzERkZCR8fH7Rs2RJHjhwxmH7WrFmoXbs2fH19UalSJbz77rt49OiRnUprCl7wrKIdOLEJjIiIzCRrcLN8+XKMHTsWkydPxokTJ9CoUSN06tQJSUlJOtP/9ddf+OijjzB58mRcuHABv/32G5YvX46PP/7YziXXQdbaDNakEBERFZI1uJk5cyaGDRuGIUOGoF69evjxxx/h5+eHBQsW6Ex/4MABtGnTBgMGDEBkZCQ6duyI/v37G63tsauS0KFYkmBKTx6sqSEiIivJFtzk5ubi+PHj6NChQ1Fh3NzQoUMHHDx4UOc+rVu3xvHjx1XBzLVr17BhwwZ07dpV73FycnKQlpam8WcbLlp7kp8L3D0lT80U+/YQEZEFZFsVPDk5GQUFBQgLC9PYHhYWhosXL+rcZ8CAAUhOTkbbtm0hCALy8/MxYsQIg81S06ZNw9SpUyUte4mycjAQuwHo+IXaRjvWrjDAISIiM8neodgcu3btwpdffol58+bhxIkT+Oeff7B+/Xp89tlnevcZP348UlNTVX83b960TeFUF2EXa1aJ3SDeHppnn+M5UzDDJjQiIockW81NSEgI3N3dkZiYqLE9MTER4eHhOveZOHEiBg0ahDfeeAMA0LBhQ2RmZmL48OGYMGEC3NyKx2re3t7w9vaW/gU4FGfsc0NERGQbstXceHl5oVmzZti+fbtqm1KpxPbt2xEdHa1zn6ysrGIBjLu7OwBAcJRf/Pw1LyHBwc+nI5eNiKjkkq3mBgDGjh2LwYMHo3nz5mjRogVmzZqFzMxMDBkyBADw6quvokKFCpg2bRoAoHv37pg5cyaaNGmCli1b4sqVK5g4cSK6d++uCnLkI2Nw5SiBnTmcsczFuMJrICJyPbIGN/369cO9e/cwadIkJCQkoHHjxti0aZOqk3F8fLxGTc0nn3wChUKBTz75BLdv30ZoaCi6d++OL774Qt8hZFACfs1LHpiUgHNGRER2I2twAwCjRo3CqFGjdD63a9cujcceHh6YPHkyJk+ebIeSmcklaiLkIui+LwgOfl4ZlBEROSKnGi1F+ugIADLuAbumAyl6Roel3AS2fwakJ5h2iKwHRfcduh8MERGVdAxupOYoF/5VQ4Bd04BFeiY4XNIT2PsNsGyAafnlZ0tXNleUnyN3CYiI6DEGN5JxgOaT/Nyi+3F7xduUeN1pH1wVb28fN/84dmsqcoBzaqrfX5C7BERE9BiDG8nJVHOzaTzweSiQcEae45vNxMDFUWrCjLmxT+4SEBHRYwxupCJnx1dBKJpBeNd08/c/MEfa8phNLYCx9XnMzwFOrwAydK88bxZnCbyIiEoYBjcEbJkgdwn0kzrY2T0D+GcY8Muz0uZLREQOg8GN1GT5Na8WAFxcJ8PxraUngLFFLc7F9eJtqo3WGCMiItkxuJGME3V+1SU90Xga0sJmKcnk5wLLBgJHfpG7JETkAhjcSE6GC15upubj1Nvm57HyNfH28jbgYZy1JSohnDygdSSnl4m1jhvek7skRCXXtV3A4u5A8hW5S2I1BjdSMXSde/VfoHQk0OQV2xx76yTNx9/VMz+P+APA9T3An32A7xsZSWzLi7qe2Yoly54BiUPKSZe7BES0pKd4HVg5WO6SWE325Rdcjq6Km2rtgNGnxPtNBgELOtmzRKa7edh+x3KJIIPNUkTkgjKcv5sCa27szSUu6lJjkEBERNJhcCMZFwhaXOAlGCXlaDbOc0NE5JAY3EjO2AWvJEQQ5rLjUHDWnBERuTwGN1LhRVMaPI9ERGQlBjdSY1OFcQX5wNJ+cpeCiIhcFIMbeysdKXcJrGdt7cqlTeJ8CsYPZN1xiIioRGJwI5XyTYAP44A39xhOF1geeOl3uxTJYeU/0tpgZW2XozdlaU+ySERENsXgRiruHoBvacAnyHjaej1sXx5rOXrAUCg7Bfg+Ctj4kQwHNyEoWzcW+LI8cPOo7YujT3aKNKugExE5CQY3pEauBTitmJX4+CIgJR44PF/KAknn2G/i7e7p8pVhRhXgm5rAozT5ymAU+6oRkXQY3JBuV3fIXQITa49krGF6lCLfsS3x4KrcJSAisguLgpubN2/i1q1bqsdHjhzBmDFj8PPPP0tWMJc3zAGCB0MEpdwlcHxbPpG7BC7ESZpBicgpWBTcDBgwADt37gQAJCQk4LnnnsORI0cwYcIEfPrpp5IW0GVVaCZ3CQxz2eBGpovog2vAune54joRkR1YFNycPXsWLVq0AACsWLECDRo0wIEDB/Dnn39i0aJFUpaPpJCbZf4+BpuEpA4Q9PW3cKFf8w+uAccWAH/0lbskREQuz6LgJi8vD97e3gCAbdu2oUcPcfRPnTp1cPfuXelKR9L4uZ35+zjLaCmzO6LK3HH1/mV5j69NWQA8uC53KSD7+0JELsWi4KZ+/fr48ccfsXfvXmzduhWdO3cGANy5cwdly5aVtIAkgeRYIO0ucPZvcXZgU8jVLOU0QZWLWDkYmN0YOLVc7pIQEUnGouBmxowZ+Omnn9CuXTv0798fjRo1AgCsXbtW1VxFDmZeS2DV68ChefrTqAcWdg1u7Lhwpis1dUnhwn/i7YHZ8paDiEhCHpbs1K5dOyQnJyMtLQ2lS5dWbR8+fDj8/PwkKxxJ6FGqeHtlK9DsNePpXbZDMRERuTqLam6ys7ORk5OjCmxu3LiBWbNmITY2FuXKlZO0gCQ1BfB7b+PJBCWwS8/Ec+bUqORlA4ueB/Z9Z/o++ty7JE7Yp44LldrPqWXArCgg6YLcJSEiMsii4KZnz55YsmQJACAlJQUtW7bEt99+i169emH+fAedKZaK3D5mPI2gBHZN0/3c30OBqztNO1bMn0DcXmDbFJOLp9fcJ4BZDa3Phyyz+k0g5QaweoTcJSEiMsii4ObEiRN48sknAQCrVq1CWFgYbty4gSVLlmD2bLbdlwi/9zItXV62CYnUa1+0l2JgH5lish4A26YC92Jtf6x/RwErXtWsrSvItf1xiUoyJbsFWMui4CYrKwsBAQEAgC1btuCFF16Am5sbWrVqhRs3bkhaQJKYwWacEhZIPEoDcjLkLoX51o8D9s0E5ra07XEK8oGTvwPn/xXn6SnEEW1EtpN6G/imhvgDhixmUXBTo0YNrFmzBjdv3sTmzZvRsWNHAEBSUhICAwMlLaBLq+jiI8s2fqi5REHWAwtqG2zYp2Z6JWBaBef7laRqVrR1kCHX6DmiEmzPV0DWffEHDFnMouBm0qRJeO+99xAZGYkWLVogOjoagFiL06RJE0kL6NIGLAfK1Ze7FGokDiQO/6j5+KuqwNwWwL2LWgmtGQouQZmVedbnUaKw5oaIHJtFwU3fvn0RHx+PY8eOYfPmzart7du3x3ffSTAqpqTwKwNEvWTng9qoWSr5MrCgM3Blm/G0sRstPw4A7NTT0VmX7BTNx6YETGl3gOOLTewvZCMH5gDfNQAe6mjmZWwhD2er4SMnxRGgUrAouAGA8PBwNGnSBHfu3FGtEN6iRQvUqVNHssKVCI5U3a9+4TdliPW+74r2WTUEiD8I/NHH+H6JZ00rgz679QxR13ZwHjCjCnD0V9PSF/rpaeC//wE7PjdvP3Mc/RW4uEH/81smAKk3ga2TbFcGMl3sRrEJ89wauUtCro7TW0jCouBGqVTi008/RVBQEKpUqYIqVaogODgYn332GZT8dWMeRwpu0m4V3c/NNJ5+2xTg0ibxfmayFQe2UW3S5vHi7fpxaocy4YsjM0m8NaUWyhKJ58UyLetvPK1QYJsymKukdyJe+jKQlyUuV0FkUwxupGDRDMUTJkzAb7/9hunTp6NNmzYAgH379mHKlCl49OgRvvjiC0kL6Qyu3cvArG2XUaaUF6b0MKMfTfnGNiuTToaCEEuuX4WT6sm1VIIj/cqZEgS8cwIoW91wuodx1h3Hbi/ZyIEEQfxzs7gCmIi0OdJ3mhOz6Ftp8eLF+PXXX/HWW28hKioKUVFRePvtt/HLL79g0aJFEhfROaRk52HtqTvYfjHRvB2rtwdCatmmULoknjExoR3/we6csN+xzAnCLAnY/upnPM36sebnq85ulShGOnr/3guY0wzI57w3uHtK/CMih2BRcPPgwQOdfWvq1KmDBw8eWF0oZ+ThJgYDBQVmXnkUCqC+CcshOKzHQZA1vzZWv1m84692/tbKzdL/nJRz3dy/YjxN+l3pjiena7vE+W/unAQSzljXAduZf63mZQM/PSX+5T2SuzTk9Jz4f8GBWBTcNGrUCHPmzCm2fc6cOYiKirK6UM7I43HVfL7Skp/VDvJhzlYPTM18HdY2SxU2lx37Tbo81f3YRv9zX1eT7jgl0fl/gR/biqPlSqKc9KL7eQaCaCo5rPnucuZA34FY1Ofmq6++Qrdu3bBt2zbVHDcHDx7EzZs3sWGDgREgLszD/XHNjUXBjYOIteK9k6JjdOwmCxbYNPGLQH2GXV0EAbh/FSjjBIGOTb77rMj00Fzx9m6M5XmU9A7L5Doy7onBfsO+QCdL+p8yuJGCRTU3Tz/9NC5duoTevXsjJSUFKSkpeOGFF3Du3Dn8/vvvUpfRKbg/bpbKK3Cg0U9WMfMfrHCEkcUEYKmu/ip2uugdmif2H1n7jn2OZw2bnBIdmeoNOBiIEOl1aC6QkQAcLN66YRLW3EjCopobAChfvnyxUVGnTp3Cb7/9hp9//tnqgjkbVZ8bZ6650WDn1yH3L/fNH4u3MX9YmZGrvP+kQeFu3bD8ndPEJquOn0lXJnJRDG6kwDGcEimsubGoz03F5hKXxo74K6PksUUg6uifI2sCm/wcceLJA7PF2a/llHkfmNca2D9b3nIQ2RiDG4l4uoun0qKamxodgBcXS1wiZ8MaD5swORAxFlzw/bGYUi0wKpB52Pzeb4Gkc8DWifKWg/Rz9EDfSTC4kYh6zY1g7i9bhQKo30v6QrkEOf/RS9AFXecoHz2vX6GQdvi8vdy/qjmySRYyX7gKcuQ9vqPJuAf89bI4mMFhMLiRgll9bl544QWDz6ekpFhTFqdW2OcGEGtvCkdPOS87l19XQGgsSMzNFNdgKpT1QFyMlMz34Cqw/VOgvb61rNQ+D8mXxHWWnEnSBWBeK8ArAPj4lvH0kipBQbKz2fwxcGmj+DclVe7SiFhzIwmzgpugoCCjz7/66qtWFchZuasFN/lKAR7uMhZGEiZ+IV9cD9yWYIbhS/pWCjdQDu1h419VBSbeB9wt7idfsu39VgxulErg+EIgXH3OKie/QBeuE5Yrc82N+oXr3Brg9jGgw6f2W8JC7o77jiYjQe4SkI2YdRVYuHChrcrh9Hw93eHt4YacfCXi7meiTnig3EWyj2s7i2/LTgF8g83LZ9sU84+dquMXeH424B5gfl5U5Nw/1i8RQcYVLsJZ8QmgXk95y0KOwyFqbhyhDNZhnxuJeLi7oXX1sgCAd5e7whozVny4f20vXTHMpcyX79iOyJJf6olndWx0/i87u/jtObF5VJ2x9yDD2jmizOAQF05HwvOhm/PX8DG4kdDLLSoDAC7cTUNmTgm+yJqyvpJJLPgHO/qb/ueSLwP3L1tenBLN+b/s7OL+FWD3VwYS8GJKxvAzIgUGNxLqWC8MPp7iKd13Jdn8DNq+K962GgkElJewZJZw0otZsoHg5c8X7VcOm5LovUk8L00+pCkvU2uDkfdLYcevYfa5cXysXZMEgxsJKRQKdI8Sg5KZWy6Zn8Gzk4C3DwEdPwdCakpcOmdl7j+6gS/vh9fNzMrFLwRZFgTgVJz258TQ50bXhYsXM/tIPA/8O0p3Xz2Hws+DFDisRGIRQT4AgFxL1phycwPK1X38QOYLqyNc2AUBes9DQT4Qt1/HEwpxFtiACFuWjKS0+2tg5+dyl6JkKMmB1E9PAco8IOk8MGyHuM0Rz4cjlskJseZGYoX9bq4nZyIly4rZSB0huHBk26cCqfHFt59eBsysK/65ggv/6dio9uV39h/g8lYDGVjwObL3Z88ugY0NLxjaFyOl1lINRs+nHS9mJfl7RZkn3iaek7ccRjG4kQKDG4kV1twAwK7YezKWxMUd/tHw8+l3dWx0lS8NtQvUqiHAn33FmXfPrwVydc00XMhVXr8NSXHxL9bnRh2bpWRXkgM8kzn/Z5LBjcQUCgVeax0JANhzyYrghv+AMFjrwPOjadVQYMUgYN0YA4l4zgy6FyvW+B39FYjdKP5JwoFqbohM4vzfFQxubKBjvTAAwJbzici3pO8NAPiHFt/W9RsrSmUuKz/cDD7s6/Jm8fb0cnnL4cz+Gy3W+K0fByx9WfyTYg0t9f8FuTsUs5bI8fE9kgSDGxtoVa0sAn08kJGTjyNxD4zvoEvn6UCN5zS3eflbXzhTXd5i3f4OPyKB7CL5ivjnDHRNAJn/yA4HZp8b+TCQcFUMbmzAzU2BNjVCAAAf/3PGskwCwoFXVgFl1YeEO9EX0+oRcpdABwc8f8lXgP2zjfSVsYKcF7O8R8CcZuJfnj2CBGtJdaEzM5+S/kudARfZAIMbG6lc1g8AEHffyouW+hefM30JPLhmfR4GX689zoUdjjGnGbB1IrBrmu2PZe9fqTlpRfdzJWjesYYpAYSuNBb9zxnaR1c5bPC+JF0Aki5Kn6/U0u6K/Zx2TZfn+PYILJVKYMMHwOmVup+P3QRc22X7cpQwDG5s5I221VT3L9xNM5DSCK9SRfdL6eiH47AkCgwKrBhO70xuHi66n2+r1+xEwbEpBAG4f9W0AETWi4exGYolvsDmZgLzWgHzWhb/LDlaLdGer8R+TnYJ7mVyYS1w5CfgnzeKP5dxD1jaD1jSUwyCHIaRz0nsJiDmL/sUxUIMbmwkNMBbdb/L93stz0h9anY3dytKZGdS1TJlyjmc3ooLQX4O8M9w8/fLTgFmVLH8uGYx4/XZqtYwL9vyffd9B/zQFNj0kfG0lvYhsygY0NrHWIdiqWtush8W3c/XOr+OVvsryHFBt/M5MDQTeLZ6n8zCcjlYAKrL0n7AmreAh3Fyl0QvBjeOzt1b7YGDfTEZJEVZ5R4KbsUxji2wbORS7AYgz0b9b4pxgM+TNau4b58q3hqb88hkUjVLORCHL78TXMhtyWHfHxPLlXXftsWwAoMbGxrfpY7q/tnbqZZl0v17ILgK0OMHh7gWmSwjUe4SyMviGiepv+wl+tDI3ZyRdAHYPAHIdJAvU+1ZiK2h69zeOgbMigIurJPuOOTgSnigJzEGNzY0tG1V1f3nf9hnWSahtYAxp4Gmr0pUKifi1AGSjb6o7l8FMk1Y8FIQgNTbWhu1y+QAzVKm5juvFXBwDvDf/2xTDsD0AO6/McD0KkB6giUHMW3bny8CKTeA5QNNzzo/F3hkRf8+R3MnRhx1WexzLCVHCige/y/I/UPCXA5a+8TgxoY83KU+vY75IbKZJT3lPf59e87PYsIXWtodsY+Jdj8KXbZ8AnxXDzjyi9pGrc+PObVLt4+blm7H58APzTT7fUjp7injafKygb3firU9tnB8IZCbLs5kLAVdFzNT5tfRnj5gdmNgeiWtAMeJvzN+fho4tRT4W0dHXEdxZRtw4ncjicwMVhw0WNBpx+fArIam/eCyMwY3NtamRlnpMnOmD72tCRI2CxhTkA88srBZ0VyGfrWZcmEvdHCOeLv5Y/1pVg3R84SOz9m/b5t23D1fi0Hh4Z9NS2+LX6m7ZwDbPxVrexyBRR2KjZyXE0uALyOA44uLtqU9ruG4dcTcEspH/Xyk3QF+aA4cnKeZJjlW2mNK+T36Rx9g7SgrFuN0gu/0OzH6f9xc3QGk3gT2f2/XIpmCwY2NDXuyaEj4zQfWdhR1gn8EV6MsAD4rC0yvDGybYvp+Fl+0De1nSZ4yfWZM7Sgs1YWmQO14t45ZmIlMzQGWfFbWviPe2rKZzt52fAHcvwxsHi93ScyXpmuhXgs5RLPU4zLk54o1aL88a2QpEse7NjG4sbEnIsuo7j/51U7rMgtvaGVpyGzqq4vv+86MHc38gnqUIlbx3r9s3n5Sy06R9/iWOr7QRhlbMGKv2K94x/vit4it52EpyLFt/ro4RCDhwNSbwHMM9edyvPMoe3Azd+5cREZGwsfHBy1btsSRI4arVFNSUjBy5EhERETA29sbtWrVwoYNG+xUWvOV8vaQLrPA8tLlRaa5fULz8b8jxVE7Urt3UWzS2fO19Xld1zevkglfQDPrWn98AHa/oJvTZKfN2toj7Vqq+5eBxPN6Esu8cKalslOA7+oDa+WoKXKC82Po827q+6v6HFr4eh+liTM935OiGU/H63GybhGyBjfLly/H2LFjMXnyZJw4cQKNGjVCp06dkJSUpDN9bm4unnvuOcTFxWHVqlWIjY3FL7/8ggoVKti55OZpVClYdf9Rnh37ipD1VgzSfHzyD7E/ixyziZr6Jbn4ecuPkZcl3ZeYKfmY09RniKXzbawZKXbSzs0yf4K9wvQH5xZ/Lv5A0X1LzqfcAc/1vZqjwWL+BNLvACcW69/HInpeZ4L6mnxmnL8M3deOYnLSgYI8/ce3FykDhsJlXOa2kC5PU8+P3J9XHWQNbmbOnIlhw4ZhyJAhqFevHn788Uf4+flhwYIFOtMvWLAADx48wJo1a9CmTRtERkbi6aefRqNGjexccvN80Km26v47S0/KWBKyG5v8s1ubpx1/eZn6pX3sN2mOF6tWe2vOBSPmD3EdtIvroPv8mtAsdUbHmkEaZXCuX7y4sl0MkL+tbTytrfzybNH9rPtA3H7j++z/HvimpvHmY2UeMK0iMLeldWXUZftnZjZfm+jmEWBea+DabgNpjkp/XJMxuFHJzc3F8ePH0aFDh6LCuLmhQ4cOOHjwoM591q5di+joaIwcORJhYWFo0KABvvzySxQU6K8NycnJQVpamsafvUVXKxoxtfW8M8/dQioPrspdAgcnyPhrzt7BhDmvU1fZpD5PVuZ3zcq+gVLQXlNuUVfj+2ydJN6aWhtoi//hvd+YuYOJn9VFzwNJ54AlPazL69ouMaiLP2w0qbOTLbhJTk5GQUEBwsLCNLaHhYUhIUH35FjXrl3DqlWrUFBQgA0bNmDixIn49ttv8fnnn+s9zrRp0xAUFKT6q1SpkqSvwxRubjb4ko9w7Noql2d0sjTH+yVjMlnW+zHDyT+AYwY6EN8w4Vc+oNl5WrBhMKZei6Nr4j9bBoGWNHs4Wd8Kh2DwnJn6/hrIw5TO1qb83y7pKfbvMyVYLH4A/U+xWco6SqUS5cqVw88//4xmzZqhX79+mDBhAn78Uf/aMuPHj0dqaqrq7+bNm3YscRFvj6JTXaCU4svD8T5MJYqhf+Y8EyZgk/qYpmVgWjIp1mq6tkuc4Exq+Y/ETt3rxlif147PrNvf0Pux4T1g+SuPH6j9v//0pIkdPkvI/7c5n+m8bAdbOdtG7BEoaHeCP7EEuKpeY6coXhYnC3plC25CQkLg7u6OxETNZprExESEh4fr3CciIgK1atWCu3vR6th169ZFQkICcnNzde7j7e2NwMBAjT85DH+qaL6bdafvWJ+hA0bKJYrCwL/OF2HA7un2K4vUrFnMstDt4+KqwYXOrQZSJPhhUZBnfR6F1Fc0ViiAm2ZW1Rv7sr/wn+b8O4XOrzXvOPaUeR+4JWffDQO+CAcWd5c2T3O/R3PS9axkb+GFX8qAwdK87p4S5036vZd6Zo9vTT0/jnc9ki248fLyQrNmzbB9+3bVNqVSie3btyM6OlrnPm3atMGVK1egVIveL126hIiICHh5edm8zNYY8XR11f3Ry2IkyNHxPkwliqHgxnYHtXJ/GX95bXgPmNXA+nwepRh+3pwvePW0p5YW7+chFe0yXdsFzG+rtkHi/2VrfvjMamB+kGdPNyxco08KuZliR+QZkdbnlZ4ofU2Ipc3JqbdMS/d7b/210g74Y1vWZqmxY8fil19+weLFi3HhwgW89dZbyMzMxJAh4rTwr776KsaPL5qt8q233sKDBw8wevRoXLp0CevXr8eXX36JkSNHyvUSTKY9301eQQmoXnVlcgQ31n5/rB8nSTFc0tUdVuxs5kXqxj4gUW2os7UXBilnx82zdhZ1czneRREAELsRSFZbW05ZAHz5eJ4xXet+GQpUtN/fwz8B39YCdn5pfTk1CyFxflruXxZHFzoJWYObfv364ZtvvsGkSZPQuHFjxMTEYNOmTapOxvHx8bh7t+gft1KlSti8eTOOHj2KqKgo/O9//8Po0aPx0UcfyfUSzPJB56KhlTUnbLQuMweMlEsUWWpurFS4PsyJJUD8IfnKkfdIbP7QJ/U2sG+WZYtv2qJfgM5mCHMYK5OVE/vNrGPF2kYScrI+GXpd3wssfRmY06xo272L0uW/8QPxds9XmtvlOn+xZlyL9P4vON71SMLpcy0zatQojBo1Sudzu3btKrYtOjoahw7J+MVshX7NK+GrTVItAud4H6YSxVmDy7j9ResSyWV2Y3FZi7F6LhgLOwMp8WLfj5f/tGFBTLyY/NjWSAIjnwVjFy0pPkvnVqtnaH1+xhTkiWt4VWgGeHiJnaQXdgWeHAtEO35NehEd5+rOieLbjLI0MJFwagBLg6OTBlY11/5sOlEA64Q/P51XWX9vjcdKSUZNUckhwUXr/hXjaWytcL2ua7t0P58SL95a1FQkdT8GwYRzZuCY+ZbW+jha8KxVns0fi0Hof6PFx+vHAVnJhlehJ9uSdAoHMz9/Dvhjj8GNnb2pNmqq2seOuyYWGeFEv2A0rHtX7hKoMXIOpRwZpfPwdngPp1XU3UdDavpei2SvUSufIz+Lt6f+si5bKS6KjvC/aHDBWQnmuXF4DG5KvK4NI+QuAklChi8iSS4EDrS2mbGLktLM4CYz2fKy2NJ9YzPh2vPCYKPPra73Mv4wcOQX2wUfSqU4c+/UYGDLJ7Y5Rv7jEXTar+Hwz5qP14wA9pg7O7GWmXWBQ/OtyEDK8+zMgZaIwY2dNagQpPF45TEL5/5wwGrAEkWWX4uu9p6bcA7zHhVdYIw5t7r4+2KX98nK90XX/7K5Wer9PpDq9VvwGhd0FKcAuLzFtPTmvldxe4C4veL9Az+Yt6+pFnTSvX3j+8W3WTIp5A612fWz7gObrBgcY+j8nV4BrHrd8ryLDqJ7swNejxjc2Jm7mwJvtyua8+b9VadxJ8WCdvmQWpqPo162smRkHuf/ZSM7Uy5mM6oA39Wz/BgPrhkqgOX5mpWPBaOlzKU+N43BC43acxn3xH5N5gYVutIbOmbyZfPyN1W+CUsSGGNsJXiLOheb4dImHYe3QYfif4YBZ/+WNk8Hx+BGBh90rqPx+MUfdS8UqtMb24En3gA6fVG0bfB/QK95EpWOTGJobSMykQlfnPmPgMx7lufpCOtkrXzN9se4vkf3dkMXpx+aihOzmXvR2/N18W2FNSjWMPui7ni1BU7FYD8hA3R+phzvvWBw4wBum1NzU7E50O1bwLc00O8PoMccoOpTgJu78X1JOsd+s/8xHbDq1ypS/yrc8J442ZoGBzhnxibGU39fM5OBe5esO576ec3RXuBVx3OXNpuX/84vjKdxGg7w+ZCMmf9PM6pYNpeULg743cTgRiaNKgYZT2RM3e5A00FFjwevsz5PIruxQZW3Oc0Ijljl/nV1YO4TwKNUafLbOqno/r7vgGMLpMnXWnnZQMJZzW1Svh8LumjN/eNMLG2WsqCWsnBiT1PL4Ij/M3owuJHJ8jd1r59llapPAlXaSJ8vySvrweO5Xxzv15HZrqs1X9jii9KUPE8ts6z/gc1I/b6qnYOE00X3t02R+DhW+K0j8GMb4IIVP8gM1RbEH7CsOTA/ByiQoC+Puk3jjafRoPb+fd8IiNXRLycvG7i8TXOtJ1kDD8f7bmJwIxMfT3f0alxe9Xjqf+ekmdTv1X+B0acATz/r8yLH8FVVYFZDM/ueOKjFz6s9sMOXsa4L4Oo3xZEjJi2UaUIZnejXrE0UawrUpuP8FAZd6XeKtsndtKEsAL6qrjmCSQp5mZbv+zAOWNqv+PY5TwB/9gHWjbE8bwD6gxJB61b9KTM7lMuEwY2Mpr0Qpbq/cH8cmny2FbEJ6dZl6u4JlI4EQmpalw85nsSzxtM4E5sEBWbkKdUkgdZ+sdvjwpCRZOBJK9+H7yRY7V0u6uc+Jw3ItfL7VxImfB5SH08hcmqp2sYSHmRrYXAjI18vzU7Aqdl5+GzdeZy+lYIj1x9Yl7nRX1PkdM6skrsEErPBl/E9M9Zuu33MeJqcDMvLYjIzgxtjQaH68xn3xKaff4bpT5//CNjxBXBLX/8LI9RrX6xharB7L1ZcUTtH5kDk53byHr+QsgBY3KNoWROb0vceqX2Gk6+IEzjKTPaFM0u6no3L49+Yoi8HNzcFeszZDwA4/kmHYutRmazNaMNfaOR8Ui2c8NFR2aLmZv1YafNbq3tRX5FE5Te35sac85afDSwfaDjNhf/EP+1VqqWiXd5ru63Lb24L8TagvOF05jL383jnpLTHL2Tu5+HWMeC6hefUFrWGhaupjz4ltiLIhDU3MpvRJ0rj8Z5LRf0qktKt6NgW9RLQ0ZWGbJLLuRNj+2NY++Udb8YcVLZydadYA6Ni7aSBMlvSQ/d2c98rqWqMnJ1N5nJ6/F7oDPh09bnRkcxWkzeaiMGNzHw89c9PU2BtB+PWo4B25vbUJ7KTmD/kLoFz+L0XMLuJ3KVwfQ7YKdYkumrcbhwUOx1f3Sn98TISdWx0vHPH4MYBrP9fW53blSV9FAaR1Wz5pWvo160F+Rii3tHV0iYI2Zh4fhx6vTY7lu1hnOHntUdzXd1RPM2irkDyJTEwlpIgiIuVmkTegIfBjQOoX173hH75UgwNL1fX+jyISAeJLnjpd42MZlKzf7a4ZIIhtggSHqVIn6e9CIJ0M/HaQ8yfhp/XtfyFNpObqixYcPWBjlXuHbDWi8GNg4ib3q3YNknmvanbA+j6jfX5EJEmZb40o6nWjgK+qQkoTbggbZ1o/fHMcWUbcGIJsHuG7Y9lq0Uql78CzIi0Lo9ru6QoifPMiZR5Dzi/Fg7ff8sABjcO7J41HYoLKRRAi2HA2IvW50XkbGz5i3L/98C0CtLVaijzpcnnwGxp8gGAP/oAa9+xLg9TL+gGV3C3wkUrl6VJPA9s+USassTtkyYfe1gxCIg/ZGJi1tyQAXvef0bj8Vt/nsD+K8nSZB4YIU0+RKTJpJmOTSHRr2RbBQmW2jYZ+Ge4Y17YTQl+712Q7njFFjKVmbHXv2Kw1gYjn1H12keZ4x0GNw6kctniSyYM/FX+yZCIyA6M1XDkGlld3JGdXg4s6ibdrND2JGVTkk2GbdtQfrZp6bZPBe5fBS6stW15zMDgxsFc+aKL3EUgciGOV11usR9dYFFcewc3G9637/GMcZY+N5b4e6ieYeLyYHDjYDzci78lX22yUX+ZiMbG03SbaZtjE9mDA47i0GvLBMPPO1pzk0XsfHE/8rN9j2fM6eVyl0CLhP8f6YlawRuHgpOW69O6ajyet+sqnv12l3QHCK0LtPsYGLDCeNrmr0t3XCJ7+6MvsNdJAvSjv8pdAttztJoL9fIUSNSh2xBrOzdbwtRpBqzmWO8tgxsHpFAo0K2hZgfga/cypTtAYATQ7kMgIKxoW6P+xdM9OU785VvxCemOTWRPybFifwByEI51AUTs+qL7D6/rTmNN7d+V7ZbvK5V/htvnOA4WuDK4cVBTe9Yvtu3avQwsOxKP07dSpD9gSK3iC9EFVZL+OERUcjnYBdDm/nhB7hIYnj/I3GkMnOj946rgDirE3xtlS3nhfmbRMNNnvy2adv3vt6LRrEoZaQ/65h7gmxpFj52pvwIROQEHvjg60YVbMitelTAzxzp/rLlxYGtG6h8d0Wf+QSSmPbIsY4WOt12hAPxDgTpq64a4e4m3JfGfnoik54zfJc5YZjlonyeZfxwzuHFglcr44ezUTnqfH7filHkZdp4OBEQAnXVMpa7rH7h+YZUq/7mJSAqO/F3iyGVzBo51/hjcODh/bw9M6Kp78ct95s5e3OotYOwFIESt6anFcLGvTfMh4uPCmptS5QBPH/F+YQ0OEZE1ru+VuwRkFQMBjIPVcDG4cQLDnqomXWbaVYVdvwbGngd8S4uPG70MvLoWGKk2M7LCXbrjE1HJtWKQ3CXQz8Euzk4nKxmawQ+bpcgEX/eN0rk98qP1GPSblUs0qAc8CgVQ7WnAz4zOylEvW3d8IiLZuWpwI2GQ4USniMGNk+jTtKLe5/ZelmhxTb2MfKI5qoqIXNWZlXKXwHFItXK9HTC4cRJubgrUCQ8wmOZRXoGdSgPNUVWutH4PEZVM+pqlLm2ybzmkZu5cNobsnm56Wo6WIlN93beR3udWHL2JOhM34d+Y29IfuEzV4tu8/Ivum/MhrtXZ+vIQkWOzx1IGUuNM1i6FwY0TaVgxCCcnPqfzuQ/+Pg0AGL0sRvoDP/cZ0OQV4LUNahst7DjmHShVqYjIUe36Uu4SmO+mlX0XyaE6ZTO4cTKlS3nhj6Et7XtQvzJAz7lApNqkggFqa1+ZU/vI/jlEru/gXLlLQLLgaCmyQtuaIfjxlWbyHHzACiCqH/DUe2obzYpupC4RETkaD2+5S0AlHIMbJ9W5QTja1ymn87kXfzyAT9acQX6BUvoD1+oEvPAz4K3WuVnXcg7qBqxQS8vghsjlceLPkkmwwTXHQgxunNh3LzfWuf1o3EP8cSge/8bcsU9BjAUstfQvIUFELsjYDx4iG+Mn0IkF+ngafH7cSjPXnrIYm6WISB3/z0sk9Q7FHApO1rj4mWlDqwVb9mKv1g4YFysu2+ATbDgtm6WIXB//z0soxxkt5SF3Acg6Pp7uGNImEgv3x+l8PvKj9ar7w56signd6kl38LEXgIQzQM2O4pdZQDi/1IhKumu7gPS7cpeC5MCh4CSlyd3rY97ApqgeWspgul/2Xpf2wIHlxf40ZgU0DH6IXNqSnnKXgIjBjavo2jAC28e1M5quQClzZM3YhojINZ39W+4SqDC4cTGrRkQbfL72JxvRedYeXElKt0+BevygtYHRDRGRS0o4XXT/+GL5ygEGNy6neWQZnJrUUe/z+UoBFxPS8cqvR+xToGrPaD728LHPcYmISD5nV8l6eAY3LijIzxMHxz9rME1C2iPV/cuJ6Zi5JRbpj/JsUJrHzWCdvgTK1QOe/lB/0iptzc/et4xlxSIiIpfF0VIuKiLIF8c/6YANZ+5i4r/ndKbpOXc/oioE4fdDNwAA9zJyMO2FKNsUKHqk+GdIzzlASjzw70gg9abxPHvNF4ehz6wrSRGJiMg1sObGhZX198ag6Ei9z5+6maIKbABg6RHDAcXd1GwcvHpfquIBdZ4vuv/2YaBMVaDa04CXv2n7Nx4gjtj6QOJRYERE5NQY3JQAm8Y8aXLaM7dSARRN+nfzQRZG/H4cx288RPS0Hej/yyEcjXsgTcECyxfdL1fHePqA8rq3+5UB3tyro/MyERGVRGyWKgHqhAeanHbP5XvYfC4Bc3ZewbaxT2PcylM4dTMFm84lqNIcv/EQT0RK0ddF38gpPcPVG7wAdJgKHJoLRGoFbBFRgJfheX6IiKhkYM1NCTG7fxOT0nm5u2HOzisAgA4zd+P6vYxiadylmoU4MEL39u7fi7cdphZ/zt0DaDMaqNBUmjIQEZHLYXBTQvRoVB6vt6lqNN0XGy5oPE57lF8sjbublcHNm3uBIZuAUqG6n6/cCpiYDLQdY91xes61bn8iInJKDG5KkEnd6+HohA5Y8abhif6M8XA3Mbjx8gcCdNTOREQBVYyUwd3wiuc6CUrNxw36mp8HERE5Pfa5KWFCA7wRGuCNIF9PpGZbNq+NyTU3H1yzLEjRJ9LIPDjac94onDh2L1UOyEySuxRERE7Jib/9yRq732+Hb15sZNG+E1afxe2UbGTnFhR/svfP4m2nLwEPbytK+Jj6cPFanQ2nLVW2aMRUz7mAh5f1x5dLjfZyl4CIyGkxuCmhgv280LdZRVz7sqtF+7eZvgMNpmxGZo5Wn5xaHYFPkoxP2GcqQW3klCkdmZu+CkxJBZq8Ij4uVU6achjSf7n0eTpzrRMRkcz4DVrCubkpcOWLLhbtW6AUcPpWKvILlBi66ChmbbskPiFFjY1k7LAKem0jNUqmiGistYELjBIRWYrBDcHD3Q2Vy/hZtK9CAeyKvYftF5Mwa9tl83YONWHiPmuDE8GC/b2DrDumJbrN1HzM2IaIyGIMbggA8PvQFujVuDxWv90aS4e1Mnm/G/czkZOvNJ5Ql4rNgRcXAcN3609jSXCimUHR3aBKQMMXje/iV9rKY1pAu8mNzVJERBbjNygBAKqULYVZLzdBk8qlEV29rMn7ffj3GY0Vxm89zDLvwPV7A+UbG0hgbc2NWuD1yt+Gg6U2o8Xbbt9ad0x9KrbQ/1yxUWWsuiEishSDG9Kpf4vKJqf9bN151f1Bvx0BoHuRTaVSwM6LSbiXnmN6QbxNXzrCqNDaQEC4/uef+gCYkADU6CDdMU1VtiZQWm2SRalmgSYiKoEY3JBOX/ZugJMTn0PTysFm7Xc9ORORH61XLbL5xfqiwGfRgTgMWXQUz31noBlKW8fPgEotgd4/mVUOFe2amqc/1D+5n0IBePpadhxrKdyAYTvkOTYRkYthcEM6KRQKlC7lhUWvt0C5AMtHP/2y9zquJGVg7IoYfPq4hiclKw9ZucWXdTgZ/xAfrDqF5Ay1mp2AcGDoFqDRy5YVQDu48QkE+v6mJ7Gta0sMNIm5uT9e3XwPMPKI5YfQtR4XEVEJwxmKyaBAH0/ser8dPN3dMGvbJczdedXsPD76+zSO3Xiosa3N9B04Oamjxrbe8w4AANIf5WP+K80sL7Sl5GwKKuxAHFE4saKFZSlbXZLiEBE5M9bckFF+Xh7wdHdD32aVLNpfO7ABgIdZeXhj8VGM/OsEBv56CHkFRR1/ryQVX4nccuZ0SDYhoKjdFaj+rPnFGLbTcP7FRkupPa7bw4wD6TiG9rIUREQujsENmaxqSCnETHoO6//XFmtHtbE6v20XkrD+9F3sv3IfOy4WraN0OSkDPefuh1IpwQR82otpGmJKzU3/pWLnX3NVaGr+PoVCagEjj5qWVldn6PfMnH9IKl4B8hyXiEo8BjdklmA/L9QvH4SoisEo7ScOX/6yd0P0a25ZrU6hR3ma61SdupmCE/EPkZKVi7f+OI7tFxIty/jZT8TbZkNMSGwkuBm6zbIyWOvJcUBoLaDqU4bT1eoMePoU3+7uoXsZCh8ZJiskIrIDBjdksYPj2+PU5I4Y0LIyZvSNwmutIy3OK1FtrpxCOflKfL05FhvPJmDo4mOWZdxyBPDOieIzAPf5DfArCzw/q2iboZob3zJApSfMO7ZVQ8rVyuL1ePbovotM38cUUg6z16WcKTNQW8iUyRiJqMRicEMW8/F0R5Bv0eRzk7vXw8tPWFaD8+WGi8W25RUo8efheNXjAqWAHRcTcT8jBwVKAa/8ehgTVp9BQuojpD3KU6WLv5+F1KzHjxUKsZOtm9ZHvWFf4P2rQOVotY0GgoOhW9SSWdrx2MpmtlKmT66o4v54ZXR9Za5uw9XHAyIs26+MKZ2iOQ8QEennEMHN3LlzERkZCR8fH7Rs2RJHjpg2FHbZsmVQKBTo1auXbQtIJlEoFJjeJwr+3tIMwnttoWY/k9nbL+P1RcfQ7PNtOHj1PvZdScafh+PRatp2RE0Rg49bD7Pw1Nc70ejTLbqy1C4wTA44QtT72Uh8Ye3ydfFtXpat9QVAczj424cMJFQALy22/Di20uUr+x0rysIpBojIocke3Cxfvhxjx47F5MmTceLECTRq1AidOnVCUlKSwf3i4uLw3nvv4cknn7RTSclUu95vhxFPV8cztUMlzff77UUdY99dEaMzzXEdI7MMSUxXaw6Teii4v4HZkNW1HF58W9uxQPkmQOcZ1pXB0NBwBQDvAKDGc9YdQw5SvVfRb0uTDxE5FNmDm5kzZ2LYsGEYMmQI6tWrhx9//BF+fn5YsGCB3n0KCgowcOBATJ06FdWqVbNjackUIf7e+KhLHcx/pRn6t7Cuo7E+upZwOHs7VSO4uZyYbjSfMUtPqu5nqXdqbvZa0f1y9TV3MnVRy+c+Beo8D/RfZlp6dX5lgOG7gFYjzN9XJ13BwONtA1ealoWt++gUsmeLkxun+iJyRbIGN7m5uTh+/Dg6dCjqeOnm5oYOHTrg4MGDevf79NNPUa5cOQwdOtToMXJycpCWlqbxR/bh4+mOaS9EIW56N1z5ogv2vP+MTY/3/A/7sOTgDdXjzt/v1ZkuKe0RCh4PM3+QWRQk3bifXZSo+/fAJ/eAgX8DQzZoZtB2TNH91zfrL1CpssDLfwK1u+hPU7ur/ucsZWxUVSEvf/FWoQDePmw4rXcQ0GuedeWyxjOfaG2QKgJi3x0iVyRrcJOcnIyCggKEhYVpbA8LC0NCQoLOffbt24fffvsNv/zyi0nHmDZtGoKCglR/lSrZpiaBDPNwd0P5YB+EPl7KYd5AK+Z9MVGBjnlyjt94gBZfbseQRWJ/nvtC0XDoYjPieHgBNTsAvsGa2/3LAZNTgCmpQOVWiL9v5kro6lq+afm+2gqbaio0xeVe63Br6Bnd6V5cBITUBvr8WrTN2MimpoPEJixjfEubVFSztX6n6L5JHY6JqCSTvVnKHOnp6Rg0aBB++eUXhISEmLTP+PHjkZqaqvq7efOmjUtJ+ni4u+HAR8/i6pdd0aVBOL7s3RBvt7PthUqpFDRGUi0+INbs7Ll0D6lZeUhGEIbkvo+Xcz8ptgyVQWp9PracT8Bf+SbMWvzke+KtvoU7zdFCRz+dx26nZOO5ZWloO1ctuHFzL7pfvzcw6ggQVk93Bp5+gH+Y7ueMGXuh6L4U/WLaTwbGXdLM6wXTftgY9eq/sHoEGxE5JFmDm5CQELi7uyMxUXOCtsTERISHF++MefXqVcTFxaF79+7w8PCAh4cHlixZgrVr18LDwwNXrxZf98jb2xuBgYEafyQfT3c3uLspoFAoMKBlZXzQuQ7ipnfDwfEWLGlggmofb0DUlC1YflQcUu6mdo1sM0NchXunsgkOKeuZF9yoUSgU+Dj/Dc2NwZWLJ6zdWZwtWL3GxFwj9gNPfQC0n6Q3ic7lK15aIs7r02u+Zcc1JVCp2VFcVb2wj5IUI5GqtAYCtAItnyCgnjlLUuhRrZ3+GaxDaokr0duqJsoRNX9d7hIQSUbW4MbLywvNmjXD9u3bVduUSiW2b9+O6OjoYunr1KmDM2fOICYmRvXXo0cPPPPMM4iJiWGTkxOLCPLFuamd0KhikGRDydV9+PcZ/HPiFtbE3FFty8jRXJlcsPBXvK7LfvKL/2D29svFJyf0L6cRKNxJyYZZwhsAz04Qm4he+EVsrqnSVnyuuYE+aBWbi/P6NB5gwkEMdD42pH5v8faNbcBbB4A6XU2rAer6jYFjKXRsg9hXSXvGaJ9g48fS5uGre7vCTVyJ/nUTphRwFZZG91IIteGEj1Qiyd4sNXbsWPzyyy9YvHgxLly4gLfeeguZmZkYMkScLv/VV1/F+PHjAQA+Pj5o0KCBxl9wcDACAgLQoEEDeHl5yflSyEqlvD3w76i2ODu1E+YMaCJ5/mNXnDL4/M0H2Zi24QI2nb2LQb8dRmyC/tFWCamPcP6O2Dm9WKVG41fw1n/3MHPrpWJz9Wj7dsslk8quU9RLQMfPgVfXAKNPi/2DDDGnmehZrQ68ukaIDVqt+bhiC/HWyw8Ie1x78+Ye481IJi2NoUWhKD5j9Ec3dKc1JKQG0MrAcPDQWkCQjlo4ktZIIx3aHYW7t9wlIBPJHtz069cP33zzDSZNmoTGjRsjJiYGmzZtUnUyjo+Px927d2UuJdnb81HlseT1FqgeWgpf9G5gl2OO/OsEftpzDSP+OIG9l5Px+iL9gUmradvRdfZeXEnKwJwdVzSfVABH48Qh6RfuGh6dd1fHshNmc/cESlexPh91TV8FxpwteqxQFP91rR7wdP9eDBS0BYSLQZghpgRdUs9BpK7zNCPHtjDfl5ZYPkuztcztlxQeBfY/MoEtP4ckKdmDGwAYNWoUbty4gZycHBw+fBgtW7ZUPbdr1y4sWrRI776LFi3CmjVrbF9IsrunaoVi+7h2GNiyCq5P64pvX2xk1+PfTslGm+k70P2HfXonB/zo79O4n5mrsU17EVCnFazezKsQAxV9yunpnKytYgtgYjLwxDDN7doriJepBrh5AuENi+ehfoFRXxtMnV/Z4vMTSUm7Cay6jj5j9XrK19Sj3Veo8UBggJH5jORslnIaThDcDNkodwkcgkMEN0TGKBQKPFmraIRcoI8H/LzcDewhjdsp2ThzOxV95h+AUilg/em7OHs7VfX8MR1Bz46LhmfXBoCLykrIFdwRo6yBhFQJam+kpOvXqc5frBZ+0bt7Ak1e0cxn7DnNNCOPAuNvGV+GIrC87u0Kd7FJzFa0+wlpN9Hp89IScdFWe/MOAGp11P88ayRMY6vzJOVkkpWL91ctiRjckNMoF+CDkxOfw6XPu+D0lE44O6WTXY+/7OhNjPzrBJ7/YZ/BdOmPjNfcdM2dhoY5vyELPjh07T7GLDupUTskCALe/P0Ypv53zkAuduSnY+oFqb7oFQpxBFQhT1/A3QPw9DE/rx5zxFqglxaLeVh60dCoxVB7naUeLylS1dRlX7RrQxTioq21OltWLpOZ+d50/966w1VoBtTtbl0etqxpA4DW/zN/nwgb1Rarz9sEAB/F605nCQaqABjckJMpXcoLXh7ix9bNTYHDH7fHgtea45sXG6Fp5WCbHvvj1XomxdOigKAx5Pzg1fvF0ijhhhyIHeDfW3kKa2LuoM/8A6rnz9xOxeZziVi4P86qMpul6WDxtt34om29fxbn5TE6TNjEL1T/cvqfe/ojoEEf8355al98mg4SLxRVWpuehznGnBVHnelqohu8rvg27aaewpomTysWRi1kqHOrsbej6atAYIWix+WbwKw+NxWfEJcXKeRVSqzNsqaPUdSLlu9rK4EVtTZIFDh0/FzzMTsqS44Lq5BTCwv0QVig+Au/bzPxi0gQBFQdv8HQbjbn4eaG3AJxDpX+vxzC9WldodDziypfbSbl2dsvo1W1snB3k+HX1/OzxF+36ottNuon/kGcEFHj11CwGZ2Y+y8DjvwMdNWxAnrheXlmfPHnCrl5AKWrAjnpmnMIBYQDo09prnvlZsPfbJ4++muUqj4pTgy4pKfaEhhqAUPnGeKQfECaoc/unkBB8TXWTBIQIdbW/GnhhJKCUHwup4BwcRLHqcGW5RnVD9g2xbJ9TWJBn6IyVaUvhi6sbZEcgxtyOQqFAofGt8f9zBwsO3ITNx5kYc+le3Y7vgCFKrAptO1CEp6rZ3zOl5lbxaHhhYEaIAZr+gIjY3ZcTMSzdUycbdjNTfeIp8c+X38BGlMHqo/QMla+2l0Mr7FljEIBjDoGQBAv6hrliDQtj4/vAsmxwM/tgEb9LS+LIdXaicfx1DF/jvoiqG3+B9y/ApxZYZtyWFLDoKtDcUB5IP1O8e2A7tonay7S+vpPSUVfh+l248U5jb7X0QTV7iPg4BzblguAU3RUdjIMbsglhQf5IDzIB5/1EvtyPMorgI+nOyI/Wi9LeYYtOYY64QFoVa2s7hmEtaw6fkt1v0ApwMPdsi+/1xcdQ9z0bmbtcyUpHVm5BYiqGKyxfcH+65hUWGlh6/4Rurhb8nWldt68/MTml/G3xWYUdUM2ASnxwOrCZS3ULoTmXrDVO0Hru6B6+gLtJ2oGN89OBCo0BX5/PBliSC0xcLusbyJBA+Uqa8qyJlr7K/N1JNF3DAGooT6vkoGy1O4KPP0hsP1T4Or24s/3/slYQaWhbzbqJ8cVD5hrdxWXOPEOEJdN2fu4A7mtaljsXXMTWlf8vOdl2ve4dsQ+N1Qi+HiKI6u+6hMFAKhZzr9Ymk+61ZXkWIkI1rn9YkI6Fh2Iw74ryWbl98y3u7DprLiQbGpWHoTHF8x76TnIzdfzha3m5oMsHI17YPLxOszcgx5z9iM5w0CTR7EvY3O/nGUcduztX7z8VaLF5rfCId7V1FawL5xkMNLUTsTqDLxO7cAneiRQ6fE0GB6+4krtAwzU7Oi6INZ5HnhtvfHaLN/Sxd+ygqI12FSdbzt9oXt/QdBcr8yQ/kuB8o11TwQJaHYmtyV9wY32iQisIJa5+uPPwJPjip4rbaNmKoUC6PatbfLWZfB/QM8fim+v1UWcRsEFMLihEuWlJyrh+rSuGNexlmrbxc86Y/OYpzC0rXVfXENzx+Hvgrb4MV+CdY/U3HyQjRF/HMfxGw/R6NMtGLX0JGIT0vHEF9tQ65ONeOXXwxiz7CS+3CAuWnnommYH5ie/2okXfzxYbK4eQRDwxuKjeH3RUVXApC7+gaHVzp23Gj03X4lNZxOQkpVb/MkR+8SOsR2mFG1r/Y5Ys2Mo0NDH0JIQ2sGJp69Yo/TRTeDD62IzoUIh9uOp87yuDDQfvn8NePlPILJt8aTRI8XbXvPF+Xd0zQqtVAtuOn4GfBhXtKRGMRYEp5FtNB+3fkcMGGs8V7St7bvm56vLiP3AwL81t5nc2VnrvHr5AUO3AnV7AH0tHMb/6r+Ah1ZfLe1A6Yk37DOMu0x1wD8U8NYRVJq67t17l8XPm7rGr+hOKxMGN1TiKBQKdKovrkq+7p228PF0R+3wACgUCvz9VmsE+Xri77fM/5LZrmyGcXlvIwsWDGE2QeFoqvWn76LTrKI5XPZdScaamDv4ec81ZOXmY/6u4gvIqu9fKDU7D9suJGHHxSQkpYu1NOpBTp6RWiGNyQrNjXXKVDNzBwvpqN2Ys+MyRvxxHC//fKh4+uBKQIthmk1Lbu5izY6xOXd06feHOKJL1wR6QXrWwvMJ1OyzU62dGLRU0QpatF9aKT2/uN29izr/Nh4gzrXj6VM8A+2Lv45FQ+PrPF6/TH2klKlajdSc7LDj58Br6zSbG9WDSnVV2ujerq3jF8CwHeIabNrLkehr+tHerquGqVILoN/vhjvRV31KDAb9tUbStRguvodvHdC5mwb1EWymGLpNHIZviYoW7geIox61P28VpF8yxxoMbqhEKlyVvEEFzV8vzaqUxqnJHdGsShnETe+GNSPbYNvYpzT6rYQGeGNC17pY8abjTZZVb9Jmk9OqV9Z897gj86FrRc1XlxL1r61VICjx1h/HzS9gIZ8gcWTNB9ctz8NCa0+JHWQvGlg7TDJh9cTJBHVNoKdQAC8uNj0vP61gw9TKE+3+JIXUR5gBYsfaBn2Bgav0ZnW1yUfAx3fURoOZwcNLrP2q1k5c2V6fQWuAUlpTBph6vNajLL/YFzIUqBvqG1OzI/DiIvFzXejpD4tGCJatLga7rxv4H+08XbMmy5hKTwDPfVZ8u8HV7AXDaaSaqVrmyQTZoZjIgMaVglX3T03qiJyCApQLsE3NjL1cvZeBI9cfaDQ7LTt6E8uO3tRIN/Hfc+gWVR6l/TyLjdaKTcjAztx7OOZVCxGKh6gQJi6TcD8jBwVKAV4eblAKQJlSRYvZpmbl4dStFLSpESIOdbf16Bg93OQYZi8FS1Y9N6Ric3EZjMLhzr7BuptdOkwBtk3BOeXjWgvtztjmcPcUm2gMqf4M8N4lzSHl0aOAxLNA3Z7AP2/o3k9XUFCjA3Dl8erxtboAWycVT6MdzejrGwQA7l5if6ycdODlv8QatumFNXCP81GfikC7hlJjokMdQYR/KPDiQmCa9vw6asIaAolqc25FthGnFrh3sWjbsB3AbLWalLI1gfuX9edZSFfw9tLvwIpBRY91BVMOiMENkYmC/DwBaP4KXj68FUb+dQLVQvxx5HGn3Q8618ZXm2JlKKFxbabvwO2UbJPTN/1sKwDNIAUo+lrumzsZbhBwzcMLSqWAZp9v00h38bPOqs7cveftx7XkTHzWsz4GRUcCAOKSM/HGkmN486lqeLG5eJGYuSUWNx9mY+ZLjSweAl+kaP+ktEcoF+gDD0cKbsx5fe0nA/evAvEHxL4q3gFArBXzOSkUQLdvjKdr+y7arQ/ATaEcTOyRYT2FAqjXCzi/Ruxv5O0v1noA+oObAcuLb3txERC7SRwlZ2CaA62DGy5X4VIb1n42O88AlvYr3s/IQ8c0AoVe+FV8Lf/9D3jqPbV9tH5wqQdVkU8CfRcC3zx+/QZrZhTFX1e9HsDII8C51UCrt8VmU1PIvFYZgxsiK7SsVhZHJ3RAgVLA0iPxaFWtLGqGBaBtjRB8suYsTt8qWoeqbCkvvNi8En7crbtPjD2YE9ioe6C1OOgVobBvgAJKKKBUCsXm9gHEEV2Vyoh9Va4li8NO/zt1FwNaVoG7mwIT/z2LK0kZeH/VaSw6EIdPe9bH7MerrA9uHalRc6ZPXoESHm4Ko4HQy78cwo5x7eBmw2G3BUrBvAkYtftnGEwbCry+EVAqxQtQ1gPga8v7Ls3fdRVKQcDIZ4xf9OOEx/1x9F2vbHFOe80DGrwAVG9vPO2wHbpHb3kHGJ/52JQ+N4bS6+LmKXbQNtQ0U7uzuH6at9aise4ewHtXioIRdYWvZYgZQW3US+JnxxQe3pqzJQ/ZJN6G1hbn/DHEwRZeZXBDZCWFQgEPd4WqNgIAoioGY83bbfAwKxf9fzmES4kZ+H1oS9QrHyhrcGOtHjmf4SX3Xfgm/yWN7d3n7MO5O2nF0qdm56GiIOCTNWdV247EPUD1jzdgUKsqyMot6pR87k4a+sw/qHpsyurq99Jz8PTXO5GVW4DXWkdicvd6eoOca/fE4MpWwc3Pe67iu62XsXJEtKov19/HbyEyxA/NqpTRvVPllmKzT1lTaxVQ1OxRqqy4ltbaUUZ2KP56M3LyMWOT2IwxoEVllNaqmTNZQASQflfPaC4AIbUtyxcQm7/q9TSc5p0TJs7pYwZjwY0pPrgGPEopPouzNu3AppCpwYiKjsDCJwh4lApUfdp42kJu7kBIzaKJG6tY02+GNTdELsnNTYGy/t7Y8q7ml8u5qZ1w9nYqmkeWQfWP5V0mwlynheo4nV/8YqIrsAFgcJHR3w/dMHis68mZaFVNHJExd+cVnLqZgvmvNFPVjJy5lYrFB+NUAdKiA3FYcewm1oxsg1phjy8ar6xC5uKXMCG3aOizvgkRtWeCLlAKOHM7FfXLB8JNoTBaI/PlBjFYmLDmLP4d2QYn4h9i3MpTAKDqkB6bkI7QAG/NZj6tZokb9zNRupQXAn30dARW13gAkJcNVG5lPK2afLVathwT5krSa8Q+4PZxrQn91OhqKpKKbxlpApvC97xyNBB/UFx3y1o+gaY330hBu3M4IHZszk4BgswcgWVp8O9gS0gwuCGys1LeHmj5+KLdt1lFrDp+C4Ojq+Bacib2XjY8wd/zUREoF+CDBfvFUUZeHm4mTeTnjMb/cwb9W1RGVm4+vt4s9mFad/oO6kUEIrdAie5zigdOWbkFeOevk9j87uPRNVWfQk//v3Aluag5TrvmRhAEfLU5Fn8cvIGW1cpg2JPV0KRyaTz33W7cuJ+FFpFlcCEhDUPbVsWYDrWgS4Ha+mCnbqbgp91XUS5QczHE3w/dwMTHNVjas0Zn5xbgx91XUSc8AG/9eQIebgpc+bKr8ZPk5g60HG44TaDmEG/tJkTBml/YpUKAWp20yvS4SabHHNuszfTaemDrZL2T3imVAqZvuogGFYLQo5EZndZf+Ru4E2N2oKhiq4t7eENxFJWhCQR7/ACsfA1oM7pom1cp3Z2/bdF8VEpHTRP73BCVXN+82AjfvKi5ps3+K8moHR6Amw+ysO9yMl5vWxVpj/IQESR2NFx/+q4quNn5XjtM33gR/53Ss/6Pk2v39U7E3S8a1TV6WYzRfWIT0/HqgiMY8VQ1zN11RSOwAaBRA5OU/gjdZu/Dvcfz/Gy7kIRtF5Iwun1N3Hh83MKO4rO2XcaYDrVw6Np9RAT5QH3GE+0auGkbL2JWv8Ya2yaqNc2tOXkbT9cKVTUHzdt1BT887msEaC6mqm3d6TvYci4RM/pEwdfLwCzBr20A9nwFdNFcrPSF+QcQczNF9Vjya9CY02JtTm3zlv0wWWRbYFjxZRzyCpTwdHfDztgk/LxHnGBOb3Dj5S/2RVEfDu1VqvhEg45C18SM6spUBd7cbWJmet7wV9eaVSRNjlVrAzC4IXI4bWqEAABC/L3RpLL45VvKu+hftX3dcqgaUgqNKgahQrAvfujfBG89XR07Y5MwtG1VTP3vHJYeuakzb2ejHtiYY8+lezoXSz1/J01jpuYF++JUgY2677frHjZ7MSFNNflfnJEZAQz9kB+zPAYAUC20FCoE+yInr3jtm1Ip6By2PuqvkwCAWmH+GPVsTQBA+qM8BGg3Y0W2gVBlDbLzCqA+/aB6YGOu+AdZuJ+Rg7L+3sWee5RXgBM3HqJ5ZDi8NIY8297MrZcwZ8dlrB3V1vCyIeqe/07aQhia4M8aJgafj/IK8MbiY3iqVgiGP6Wjua7qU8D1PUDzobozqPa4+bzTl8DPz2guO2EKWy1NYSEGN0ROxsfTHTvGPa3RP6Re+UDUKy+2u097IQrDn6qOD/8+jSPXH2DewKYI9PHEK78dlqvIDqPr7L0ajzNy8vSk1K3zrL3GEz2WX2D8qnTtXqaqo3Ox55IzUePxGmiP8grwIDMX5QKKgoqk9BzsjE3CkIVHVds+7lpH48K2YH8cPlt3Hj8NaoZO9XWPzFIv5eIDcTgZ/xDfvtRYZx+jyWvPYfLaczoXY/34nzP45+RtvNKqMj7v1dDg6zbkcmI6ygX4PJ56wTSzHwejX6y/gF5NDDRFNXxJXKxUxzIPOfkF8PYwcb0sdYP/A26fAOrYqKbKRKuO38K+K8nYdyVZd3AzYAVw9xRQsUXRtmcnAju05q0Jqy9O1GjuQrXVnxVrCYMri2ubeco7HxiDGyInZGzYc9WQUsVmUD7/aSfsu5yMJ2uGYvelewj09cChq/fx15F4AApsffcp+Hq5w8vdDV9tjsW5O6kY2LIKRlgzE7GD++NQvM3yXnwwTnX/vccdi81ROFrs94NxmPjvOQBAy6pFo67ylYJGYAOInZoHt47E2dtpaFwpGJ+tOw8AePP34zg58Tmdo6KUak1gk9eKx+ncIBx1wgOx+uRtnLqVYrSsey/fwz8nbwMQz6mlwc3Z26mqTuivtY7ElB7mrT5vtNtLr/nimlbhmuVbfjQeH/59Bt+/3Bg9G5vZAbfqU5bN2Cyx7FzDowvTCjyQHtgIFdQnGazUQndicwMbQDz5xvp/2RGDG6ISws/LAx0f/3rv3EC8bV09BGM71i42P8tHXeqo7q97p63BUU+km/ocR6uO3zJ7//m7riKqYhCmbSyaefbw9aLlMVafuK1zv/dXnsbaU3fQq7FmDcaW8wno90TxoclKQUCBUtDoFL34wA2cvZ2K9Jx8k8o66LcjJqUrlFegxNV7GagdFqARqKt3qF90IM7s4ObC3TT0VHvdH/19GtP7RBUlcPcAIqKK7ffh3+KMv6OXxaBRxWBcT87EM3XKFUtnqtO3UlDW3xsVgg1MyGdnjadugVIADo5/VtV/D1WfEtc9M3mCQz0cbKQUwOCGiACDw5wbVAjCtS+74sq9DLy7PAYpWXl4q1119G1WEf+duoP3V522Y0lLjvVn7mL9mbt6n8/WMw9Q4dpZa2I0O5l/+PcZnbUSo/46iTO3UzW2HdRaWd4S326JxY6LSVjxZrRGn7F/Y27jq02xuJ2Sjf4tKsPPyx2jnqmBxPRHqrl31GkP0TfkYVYe1PtiLzt6UzO4MUG7b3YBEGcfLxzVaI5r9zLQY85+AMVHxZkqJSsXwapHpnW6MXaKCs/LiRsp6BalFnTpWvfMVP7hQEaC5oKoDoLBDREZ5eamQK2wAKz/35Ma2ws7PAPA2lFtkK8U8NPuq9h8LlFvXvXLB+qdF4dsq87ETcW2aQc2pkh7lIdAH08kZ+Tgbz21UoWjv1Ydv4XBrSMBiMPk1Ue8LT0iNgveS8/BpnMJxfI4fuMBhiw8ik+er4eXHi/PIQgC0rLzVX1ylFojy6SaGuHkzZRiwc2jvAIUKAX4ebkjJ18JH093ZOTkY+G+6+gaFYHqof5mnc/9V5Kx+9I9vNexNrw8xOaiu6nZiJ62o6jDujVreenww47LKB/so/G/a7Fh24Hza4Gmg4yntTMGN0RksRrl/PH3W60R6u+NymXFMTkzX2qMA1fvQykISMnKhVIA7qRk45VWVeDj6Q4/L3cM/OUwLiakYdf7z2D4kmPw9XJHlwYROHz9Pv47dQcGRkI7jDUFrdHL/QAW5ncyntjFRE3ZAk93BfL0dJrOU5tHp3CSxT2X7mH9ad01UWdup+oMSkb+eRJpj/LxwarTeKl5JRyNe4AXfxRnsR7SJhKTu9cv1p8pJ19/35PCWqC8AiXWnLyNhhWDVJMvaivQ+hD+d+oO3lkqjlRrVzsUu2Lv4c83WmLd6btYeiQes3dcxuUvuharZUrNzsPd1GzUCS8+0d7AX8VO/mGBPhjaVhxttO1Ckvjac/+HuZV2QNljLhJSsuGmUCA8yPpOuhcT0tF73gGLa5U0BFUEot+2Ph8bYHBDRFZpVkXzF2Apbw88Vy/M4D4rRhR1dl71VmvV/QEtK+P7l5sgN1+JU7dSkJqVh/Z1y+FSYgaqh5ZCjQkbNfJZ/HoLbDqboKoBsKcP8t7E0vz2OC7UtPuxHYG+wAYAnv5qp+r+jE0XUaBU4pstl/Smv56se8SY+nw/B6/eR/9fDqkeL9wfh8nd66s6MhfSF6wUdhru1bh8sSY7Xb7eLDarLR/eCnN2XsGsbUXTA+yKFacZGPjrYVQLEWtW8goE/HHohsZSIwDQdsYOpD/Kx78j26CRnrXSbj4omvLA/XFwtF7ZCkO6vItXZh/Gozzx3F35ogs83I0vD3HhbhrqhAcYbM5LzcorNiItO7cAey/fQ9uaIfDzcu7wQCEIDrbalY2lpaUhKCgIqampCAy04/TYRGS14zceIuZmCl5vE6nxxR350XrV/bjp3XDzQRZup2Rj/em7GNOhJvZdSTY4AeD1aV1RdbxzLYVBpln/v7a49TAbb/5u2ai/oW2r4rd91/U+XyHYV++CtHHTu6k+m2VKeWHT6CdRLtAH2y8kYujiY6p0LzWviAnd6qGUl3uxAF7d2amdoADw9p8nEF29LK4mZaBPs4poVa0sft17DZ+vv6BKO+qZGmgeWRqtq4fAy8NN438EAEIDvHF0grhsRkZOPvILlJj07zmsPXUHz0dFYM6ApkbPjb2Zc/1mcENETu9iQhre/vMExj5XC89H6Z7nZMnBONx+mI1xHWsjJTsXOy8mYda2y/iwcx30alIBSqWAQQsOY/8V6zvTEgHAmSkd0XDKFtXj6qGlsH1cu2KBhqlOTeqIxQfjMHOrZi3Y6PY19U48OahVFXzWq4HOYxY2TWnPBF74XHJGDgb+chh9m1XEsKeKVqD/ec9VfL/tMqqX80eramXxcde6AMRmv3ErTqFiGT+MfU73UiXWYHBjAIMbIjLkenImPvr7NCoE++KrvlGYufUS3N0UGNKmKh5k5qJqSCkIgoAjcQ8weMERfNevsWrWYCJj9rz/DJ76eqfxhBJa8noLvLqg+HD9FlXL4KmaITqbDGf1a4yLCen4cfdVAMCxTzogxN8bgiAUq+Uc06Embj/MRu8mFTDgcT8iSfr0aGFwYwCDGyKS2r7LyTh+4yG+26Z5kWhUKRinrFjuoGwpL9zPzLWydETWqxsRiI2jn8T32y4X+5zrIndw49w9hoiIHEDbmiFoWzMEzzeKwJwdV/BUrRC0rFoW5YN9MfW/c1i4Pw4jn6mOZ+uEYcAvh/Bdv8bo2jACDSdvVk2Ut/j1Fpj63zk8UaUMapTzx6XEdMzoEwUBwML91/Hn4Xg8EVkaK46JQ6/nD2yKt/48IeOrppLkwt00ZOXmmxTYAOKweR9PC5azkAhrboiIbEipFHApKR21ygXoXAhTEARk5OQjwMfT6IR1giDgnaUnEeDjgc96NsDoZTEIDfDGztgk3LifhcaVgjUWxnyhSQXVaKKGFYIsmtOGqFBogLfOhWZ1WfjaE1bN8qwLm6UMYHBDRK7m5oMsLD0Sj9daR6KUtwf+OHQDnRuEo0pZzQngcvILIAjiApnqyzoU+qpPFF5sXhH/xtxRrVyurX2dcth+UZyL5bXWkfBwU+BXA6OJqGSaN7ApujaMkDRPBjcGMLghopJOEAT8d/ouLt5Nw2ttIpGVU4CYmyno2bi83pqjc3dSER7og7L+3lh+NB4VS/uhTY0QJKQ+Qqtp2wEAgT4eaFQpWLVG1Bttq+oMfP5+qzXO30nFV5tiTV6/Km56N/y856reeWzIsQR4e+DMVGknuGRwYwCDGyIiad1Lz0GAjwc83BTwcHdDTn4BcvKVCPTxRLPPtuJ+Zi52jHsavl7uCAvwKdY8l5KVi6T0HKRk5eGJyNLIyVciIycfz8/eh4S0R6omjn9jbqvmK/pf+5rwcleoRvoseb0FKpT2Rftvd1v9eupGBOLCXS4RYi2pOxUzuDGAwQ0Rkf1k5ebjYVaexStkZ+bkqxbeLFAKmPTvWTSPLI3eTSoiN1+JX/ddQ7ta5VCvvPh9LggCbqdkY/rGi3iyZgg614/Av6duo3ODcISU8sZfR+IRVTEIpf288Nm686gTEYjZj+eImTugKWqU80fNcv7FArDs3AJcTEhD73kHVNsOjn8W284nYuK/5yx6ba6OwY0dMbghIiJ1hRPc7X6/XbF+StpSs/JwISENLauWUTXhjfj9uGrhz6/6RKF1jbJoO6NoLpvZ/Zvgf4/XpSocTt29UXmkZOdiwmpxuQZ9Mx3XKOePK0kZKBfgjboRgdh96Z7Jr6tOeAAuJqSbnF5qcgY3HApOREQl2t4PnsGDzFyjgQ0ABPl5opXWauHP1i2HTecS4KYAXnpCXL18Svd68PZ0x5M1Q1Ah2BebzyYgr0CJOuEB+HFQM9W+dcID8fOeq/ikWz18uu48tp5PxMCWlfHn4XgsG94KraqVxd3UbIT6eyMnX4n6kzcDEJcMUSgUSM3OQ5CvJxJSH8HDXYFt5xPRsX44gn094eamwNgVMfjnxG2EBnjjp0HNUCc8AMuP3sTJ+BTsvJik6vMUN70bsnMLsPVCIj5bdx51IwKhVAr4X/ua+GZLLI5cf6Dxmr/qG4XoamXxpNo6YlXK+qFyGfFvTAfpZyg2B2tuiIiIrKBUCthyPgGNKgUjIsiy5jdAbFLLyi1AKW8PvfPEJGfkwMNNgWA/L7PyBVCss/ipmymYsOYMPu5SF61rhOjd/0pSOt5YfAyjnq2J56Mi4O3hpspLEARcvZeBiqX9bD6vDZulDGBwQ0RE5HzMuX4bXzudiIiIyIkwuCEiIiKXwuCGiIiIXAqDGyIiInIpDG6IiIjIpTC4ISIiIpfC4IaIiIhcCoMbIiIicikMboiIiMilMLghIiIil8LghoiIiFwKgxsiIiJyKQxuiIiIyKUwuCEiIiKX4iF3AexNEAQA4tLpRERE5BwKr9uF13FDSlxwk56eDgCoVKmSzCUhIiIic6WnpyMoKMhgGoVgSgjkQpRKJe7cuYOAgAAoFApJ805LS0OlSpVw8+ZNBAYGSpo3FeF5tg+eZ/vgebYfnmv7sNV5FgQB6enpKF++PNzcDPeqKXE1N25ubqhYsaJNjxEYGMh/HDvgebYPnmf74Hm2H55r+7DFeTZWY1OIHYqJiIjIpTC4ISIiIpfC4EZC3t7emDx5Mry9veUuikvjebYPnmf74Hm2H55r+3CE81ziOhQTERGRa2PNDREREbkUBjdERETkUhjcEBERkUthcENEREQuhcGNRObOnYvIyEj4+PigZcuWOHLkiNxFcmh79uxB9+7dUb58eSgUCqxZs0bjeUEQMGnSJERERMDX1xcdOnTA5cuXNdI8ePAAAwcORGBgIIKDgzF06FBkZGRopDl9+jSefPJJ+Pj4oFKlSvjqq69s/dIcyrRp0/DEE08gICAA5cqVQ69evRAbG6uR5tGjRxg5ciTKli0Lf39/9OnTB4mJiRpp4uPj0a1bN/j5+aFcuXJ4//33kZ+fr5Fm165daNq0Kby9vVGjRg0sWrTI1i/PYcyfPx9RUVGqScuio6OxceNG1fM8x7Yxffp0KBQKjBkzRrWN59p6U6ZMgUKh0PirU6eO6nmnOMcCWW3ZsmWCl5eXsGDBAuHcuXPCsGHDhODgYCExMVHuojmsDRs2CBMmTBD++ecfAYCwevVqjeenT58uBAUFCWvWrBFOnTol9OjRQ6hataqQnZ2tStO5c2ehUaNGwqFDh4S9e/cKNWrUEPr37696PjU1VQgLCxMGDhwonD17Vli6dKng6+sr/PTTT/Z6mbLr1KmTsHDhQuHs2bNCTEyM0LVrV6Fy5cpCRkaGKs2IESOESpUqCdu3bxeOHTsmtGrVSmjdurXq+fz8fKFBgwZChw4dhJMnTwobNmwQQkJChPHjx6vSXLt2TfDz8xPGjh0rnD9/Xvjhhx8Ed3d3YdOmTXZ9vXJZu3atsH79euHSpUtCbGys8PHHHwuenp7C2bNnBUHgObaFI0eOCJGRkUJUVJQwevRo1Xaea+tNnjxZqF+/vnD37l3V371791TPO8M5ZnAjgRYtWggjR45UPS4oKBDKly8vTJs2TcZSOQ/t4EapVArh4eHC119/rdqWkpIieHt7C0uXLhUEQRDOnz8vABCOHj2qSrNx40ZBoVAIt2/fFgRBEObNmyeULl1ayMnJUaX58MMPhdq1a9v4FTmupKQkAYCwe/duQRDE8+rp6SmsXLlSlebChQsCAOHgwYOCIIiBqJubm5CQkKBKM3/+fCEwMFB1bj/44AOhfv36Gsfq16+f0KlTJ1u/JIdVunRp4ddff+U5toH09HShZs2awtatW4Wnn35aFdzwXEtj8uTJQqNGjXQ+5yznmM1SVsrNzcXx48fRoUMH1TY3Nzd06NABBw8elLFkzuv69etISEjQOKdBQUFo2bKl6pwePHgQwcHBaN68uSpNhw4d4ObmhsOHD6vSPPXUU/Dy8lKl6dSpE2JjY/Hw4UM7vRrHkpqaCgAoU6YMAOD48ePIy8vTONd16tRB5cqVNc51w4YNERYWpkrTqVMnpKWl4dy5c6o06nkUpimJ/wMFBQVYtmwZMjMzER0dzXNsAyNHjkS3bt2KnQ+ea+lcvnwZ5cuXR7Vq1TBw4EDEx8cDcJ5zzODGSsnJySgoKNB4EwEgLCwMCQkJMpXKuRWeN0PnNCEhAeXKldN43sPDA2XKlNFIoysP9WOUJEqlEmPGjEGbNm3QoEEDAOJ58PLyQnBwsEZa7XNt7DzqS5OWlobs7GxbvByHc+bMGfj7+8Pb2xsjRozA6tWrUa9ePZ5jiS1btgwnTpzAtGnTij3Hcy2Nli1bYtGiRdi0aRPmz5+P69ev48knn0R6errTnOMStyo4UUk1cuRInD17Fvv27ZO7KC6pdu3aiImJQWpqKlatWoXBgwdj9+7dchfLpdy8eROjR4/G1q1b4ePjI3dxXFaXLl1U96OiotCyZUtUqVIFK1asgK+vr4wlMx1rbqwUEhICd3f3Yj3FExMTER4eLlOpnFvheTN0TsPDw5GUlKTxfH5+Ph48eKCRRlce6scoKUaNGoV169Zh586dqFixomp7eHg4cnNzkZKSopFe+1wbO4/60gQGBjrNl6G1vLy8UKNGDTRr1gzTpk1Do0aN8P333/McS+j48eNISkpC06ZN4eHhAQ8PD+zevRuzZ8+Gh4cHwsLCeK5tIDg4GLVq1cKVK1ec5vPM4MZKXl5eaNasGbZv367aplQqsX37dkRHR8tYMudVtWpVhIeHa5zTtLQ0HD58WHVOo6OjkZKSguPHj6vS7NixA0qlEi1btlSl2bNnD/Ly8lRptm7ditq1a6N06dJ2ejXyEgQBo0aNwurVq7Fjxw5UrVpV4/lmzZrB09NT41zHxsYiPj5e41yfOXNGI5jcunUrAgMDUa9ePVUa9TwK05Tk/wGlUomcnByeYwm1b98eZ86cQUxMjOqvefPmGDhwoOo+z7X0MjIycPXqVURERDjP51mSbskl3LJlywRvb29h0aJFwvnz54Xhw4cLwcHBGj3FSVN6erpw8uRJ4eTJkwIAYebMmcLJkyeFGzduCIIgDgUPDg4W/v33X+H06dNCz549dQ4Fb9KkiXD48GFh3759Qs2aNTWGgqekpAhhYWHCoEGDhLNnzwrLli0T/Pz8StRQ8LfeeksICgoSdu3apTGsMysrS5VmxIgRQuXKlYUdO3YIx44dE6Kjo4Xo6GjV84XDOjt27CjExMQImzZtEkJDQ3UO63z//feFCxcuCHPnzi1RQ2c/+ugjYffu3cL169eF06dPCx999JGgUCiELVu2CILAc2xL6qOlBIHnWgrjxo0Tdu3aJVy/fl3Yv3+/0KFDByEkJERISkoSBME5zjGDG4n88MMPQuXKlQUvLy+hRYsWwqFDh+QukkPbuXOnAKDY3+DBgwVBEIeDT5w4UQgLCxO8vb2F9u3bC7GxsRp53L9/X+jfv7/g7+8vBAYGCkOGDBHS09M10pw6dUpo27at4O3tLVSoUEGYPn26vV6iQ9B1jgEICxcuVKXJzs4W3n77baF06dKCn5+f0Lt3b+Hu3bsa+cTFxQldunQRfH19hZCQEGHcuHFCXl6eRpqdO3cKjRs3Fry8vIRq1appHMPVvf7660KVKlUELy8vITQ0VGjfvr0qsBEEnmNb0g5ueK6t169fPyEiIkLw8vISKlSoIPTr10+4cuWK6nlnOMcKQRAEaeqAiIiIiOTHPjdERETkUhjcEBERkUthcENEREQuhcENERERuRQGN0RERORSGNwQERGRS2FwQ0RERC6FwQ0RlXgKhQJr1qyRuxhEJBEGN0Qkq9deew0KhaLYX+fOneUuGhE5KQ+5C0BE1LlzZyxcuFBjm7e3t0ylISJnx5obIpKdt7c3wsPDNf4KV25XKBSYP38+unTpAl9fX1SrVg2rVq3S2P/MmTN49tln4evri7Jly2L48OHIyMjQSLNgwQLUr18f3t7eiIiIwKhRozSeT05ORu/eveHn54eaNWti7dq1tn3RRGQzDG6IyOFNnDgRffr0walTpzBw4EC8/PLLuHDhAgAgMzMTnTp1QunSpXH06FGsXLkS27Zt0whe5s+fj5EjR2L48OE4c+YM1q5dixo1amgcY+rUqXjppZdw+vRpdO3aFQMHDsSDBw/s+jqJSCKSLcFJRGSBwYMHC+7u7kKpUqU0/r744gtBEMSVzUeMGKGxT8uWLYW33npLEARB+Pnnn4XSpUsLGRkZqufXr18vuLm5CQkJCYIgCEL58uWFCRMm6C0DAOGTTz5RPc7IyBAACBs3bpTsdRKR/bDPDRHJ7plnnsH8+fM1tpUpU0Z1Pzo6WuO56OhoxMTEAAAuXLiARo0aoVSpUqrn27RpA6VSidjYWCgUCty5cwft27c3WIaoqCjV/VKlSiEwMBBJSUmWviQikhGDGyKSXalSpYo1E0nF19fXpHSenp4ajxUKBZRKpS2KREQ2xj43ROTwDh06VOxx3bp1AQB169bFqVOnkJmZqXp+//79cHNzQ+3atREQEIDIyEhs377drmUmIvmw5oaIZJeTk4OEhASNbR4eHggJCQEArFy5Es2bN0fbtm3x559/4siRI/jtt98AAAMHDsTkyZMxePBgTJkyBffu3cM777yDQYMGISwsDAAwZcoUjBgxAuXKlUOXLl2Qnp6O/fv345133rHvCyUiu2BwQ0Sy27RpEyIiIjS21a5dGxcvXgQgjmRatmwZ3n77bURERGDp0qWoV68eAMDPzw+bN2/G6NGj8cQTT8DPzw99+vTBzJkzVXkNHjwYjx49wnfffYf33nsPISEh6Nu3r/1eIBHZlUIQBEHuQhAR6aNQKLB69Wr06tVL7qIQkZNgnxsiIiJyKQxuiIiIyKWwzw0ROTS2nBORuVhzQ0RERC6FwQ0RERG5FAY3RERE5FIY3BAREZFLYXBDRERELoXBDREREbkUBjdERETkUhjcEBERkUthcENEREQu5f9DpfzjQyhCyQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 3.5335 - accuracy: 0.5121\n",
            "[3.5335209369659424, 0.5120958089828491]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.65      0.61      0.63      2848\n",
            "     class 1       0.26      0.30      0.28      1327\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.46      0.46      0.46      4175\n",
            "weighted avg       0.53      0.51      0.52      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "712/712 [==============================] - 2s 2ms/step - loss: 0.3188 - accuracy: 0.8338\n",
            "[0.3188401460647583, 0.8337723016738892]\n",
            "712/712 [==============================] - 2s 3ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.82      0.85      0.84     11391\n",
            "     class 1       0.85      0.81      0.83     11391\n",
            "\n",
            "    accuracy                           0.83     22782\n",
            "   macro avg       0.83      0.83      0.83     22782\n",
            "weighted avg       0.83      0.83      0.83     22782\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.best"
      ],
      "metadata": {
        "id": "PioVDan-fUtJ",
        "outputId": "8854ba08-3ca4-4a1f-a7e7-08f9f388ef60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2927582859992981"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Best Model Test Only***#\n",
        "#--------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_2', 'class 1']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E7RFuvZPfVAO",
        "outputId": "81be41be-8781-4c38-c8af-5ffc683897d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 3.6311 - accuracy: 0.5207\n",
            "[3.63114070892334, 0.5207185745239258]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.64      0.66      0.65      2848\n",
            "     class 1       0.23      0.22      0.22      1327\n",
            "\n",
            "    accuracy                           0.52      4175\n",
            "   macro avg       0.44      0.44      0.44      4175\n",
            "weighted avg       0.51      0.52      0.52      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGxCAYAAABr1xxGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSbUlEQVR4nO3deVhUZfsH8O+wDPsMorJMAuEOZi5oiporiUsuST9fjQoLJRMtNXEpxS3TbDE00zRzKcuy0letXNLUClREMUXEDQWFAQthBIWBmfP7g5eTkzgxzCCc6fu5rnNdzXme58x9aHBu7uc558gEQRBAREREZIVs6joAIiIiotrCRIeIiIisFhMdIiIislpMdIiIiMhqMdEhIiIiq8VEh4iIiKwWEx0iIiKyWkx0iIiIyGrZ1XUAVDW9Xo/s7Gy4ublBJpPVdThERGQCQRBw69YtqFQq2NjUXk2hpKQEWq3W7OPI5XI4OjpaIKL6h4lOPZWdnQ1fX9+6DoOIiMyQlZWFJk2a1MqxS0pKEODvCnWezuxjeXt7IyMjwyqTHSY69ZSbmxsA4OqJh6Fw5QwjWafOn4yt6xCIaoW+tASX4xeI/5bXBq1WC3WeDhnJ/lC41fx7QnNLj4Dgq9BqtUx06MGpnK5SuNqY9QEmqs9sHazvH1Wiuz2IpQcKN35PGMNEh4iISMJ0gh46Mx7PrRP0lgumHmKiQ0REJGF6CNCj5pmOOWOlgLUuIiIislqs6BAREUmYHnqYM/lk3uj6j4kOERGRhOkEATqh5tNP5oyVAk5dERERkdViRYeIiEjCuBjZOCY6REREEqaHAB0Tnfvi1BURERFZLVZ0iIiIJIxTV8Yx0SEiIpIwXnVlHBMdIiIiCdP/bzNnvDXjGh0iIiIyyeHDhzFkyBCoVCrIZDJs377doL2oqAgTJ05EkyZN4OTkhKCgIKxevdqgT0lJCWJiYtCwYUO4uroiPDwcubm5Bn0yMzMxePBgODs7w9PTE7GxsSgvLzcpViY6REREEqb731VX5mymKi4uRrt27bBy5coq26dOnYrdu3fj888/R1paGiZPnoyJEydix44dYp8pU6Zg586d2Lp1Kw4dOoTs7GyMGDHir/PS6TB48GBotVokJCRg48aN2LBhA+Li4kyKlVNXREREEqYTYObTy00fM3DgQAwcOPC+7QkJCYiMjETv3r0BANHR0fj4449x7NgxDB06FIWFhVi3bh2++OIL9O3bFwCwfv16BAYG4siRI+jatSv27t2Ls2fP4qeffoKXlxfat2+PhQsXYsaMGZg3bx7kcnm1YmVFh4iIiCyqW7du2LFjB65fvw5BEPDzzz/j/Pnz6N+/PwAgOTkZZWVlCA0NFce0bt0afn5+SExMBAAkJiaibdu28PLyEvuEhYVBo9EgNTW12rGwokNERCRhllqMrNFoDPY7ODjAwcGhRsdcsWIFoqOj0aRJE9jZ2cHGxgZr165Fz549AQBqtRpyuRzu7u4G47y8vKBWq8U+dyc5le2VbdXFig4REZGE6SGDzoxNDxkAwNfXF0qlUtwWL15c45hWrFiBI0eOYMeOHUhOTsZ7772HmJgY/PTTT5Y67WpjRYeIiIiQlZUFhUIhvq5pNefOnTt4/fXXsW3bNgwePBgA8OijjyIlJQXvvvsuQkND4e3tDa1Wi4KCAoOqTm5uLry9vQEA3t7eOHbsmMGxK6/KquxTHazoEBERSZheMH8DAIVCYbDVNNEpKytDWVkZbGwMUwxbW1vo9RUTZcHBwbC3t8f+/fvF9vT0dGRmZiIkJAQAEBISgtOnTyMvL0/ss2/fPigUCgQFBVU7HlZ0iIiIJKxyCsqc8aYqKirCxYsXxdcZGRlISUmBh4cH/Pz80KtXL8TGxsLJyQn+/v44dOgQNm3ahPfffx8AoFQqERUVhalTp8LDwwMKhQKTJk1CSEgIunbtCgDo378/goKC8Nxzz2Hp0qVQq9WYPXs2YmJiTErCmOgQERGRSY4fP44+ffqIr6dOnQoAiIyMxIYNG7BlyxbMmjULERERyM/Ph7+/PxYtWoTx48eLY5YtWwYbGxuEh4ejtLQUYWFh+Oijj8R2W1tb7Nq1Cy+//DJCQkLg4uKCyMhILFiwwKRYZYJg5Q+5kCiNRgOlUomb55tC4cYZRrJOQR9NqOsQiGqFrrQEF5e+jsLCQoN1L5ZU+T2RkOoDVzO+J4pu6dGtTU6txlqXWNEhIiKSML0gg16o+dSVOWOlgIkOERGRhNXFGh0p4ZwIERERWS1WdIiIiCRMBxvozKhb6CwYS33ERIeIiEjCBDPX6AhWvkaHU1dERERktVjRISIikjAuRjaOiQ4REZGE6QQb6AQz1uhY+d30OHVFREREVosVHSIiIgnTQwa9GXULPay7pMNEh4iISMK4Rsc4Tl0RERGR1WJFh4iISMLMX4zMqSsiIiKqpyrW6JjxUE8rn7piokNERCRhejMfAWHti5G5RoeIiIisFis6REREEsY1OsYx0SEiIpIwPWx4Hx0jOHVFREREVosVHSIiIgnTCTLoBDNuGGjGWClgokNERCRhOjOvutJx6oqIiIhImljRISIikjC9YAO9GVdd6XnVFREREdVXnLoyjlNXREREZLVY0SEiIpIwPcy7ckpvuVDqJSY6REREEmb+DQOte3KHiQ4REZGEmf8ICOtOdKz77IiIiOhfjRUdIiIiCdNDBj3MWaPDOyMTERFRPcWpK+Os++yIiIjoX40VHSIiIgkz/4aB1l3zYKJDREQkYXpBBr0599Gx8qeXW3caR0RERP9qrOgQERFJmN7MqStrv2GgdZ8dERGRlat8erk5m6kOHz6MIUOGQKVSQSaTYfv27ff0SUtLw9ChQ6FUKuHi4oLOnTsjMzNTbC8pKUFMTAwaNmwIV1dXhIeHIzc31+AYmZmZGDx4MJydneHp6YnY2FiUl5ebFCsTHSIiIjJJcXEx2rVrh5UrV1bZfunSJfTo0QOtW7fGwYMH8fvvv2POnDlwdHQU+0yZMgU7d+7E1q1bcejQIWRnZ2PEiBFiu06nw+DBg6HVapGQkICNGzdiw4YNiIuLMylWTl0RERFJmA4y6My46V9Nxg4cOBADBw68b/sbb7yBQYMGYenSpeK+Zs2aif9dWFiIdevW4YsvvkDfvn0BAOvXr0dgYCCOHDmCrl27Yu/evTh79ix++ukneHl5oX379li4cCFmzJiBefPmQS6XVytWVnSIiIgkzFJTVxqNxmArLS2tWTx6Pb7//nu0bNkSYWFh8PT0RJcuXQymt5KTk1FWVobQ0FBxX+vWreHn54fExEQAQGJiItq2bQsvLy+xT1hYGDQaDVJTU6sdDxMdIiIiCdPhr6pOzbYKvr6+UCqV4rZ48eIaxZOXl4eioiIsWbIEAwYMwN69e/HUU09hxIgROHToEABArVZDLpfD3d3dYKyXlxfUarXY5+4kp7K9sq26OHVFREREyMrKgkKhEF87ODjU6Dh6vR4AMGzYMEyZMgUA0L59eyQkJGD16tXo1auX+cGagBUdIiIiCbPU1JVCoTDYaproNGrUCHZ2dggKCjLYHxgYKF515e3tDa1Wi4KCAoM+ubm58Pb2Fvv8/SqsyteVfaqDiQ4REZGEVT7U05zNkuRyOTp37oz09HSD/efPn4e/vz8AIDg4GPb29ti/f7/Ynp6ejszMTISEhAAAQkJCcPr0aeTl5Yl99u3bB4VCcU8SZQynroiIiMgkRUVFuHjxovg6IyMDKSkp8PDwgJ+fH2JjY/Gf//wHPXv2RJ8+fbB7927s3LkTBw8eBAAolUpERUVh6tSp8PDwgEKhwKRJkxASEoKuXbsCAPr374+goCA899xzWLp0KdRqNWbPno2YmBiTqk1MdIiIiCRMgAx6My4vF2ow9vjx4+jTp4/4eurUqQCAyMhIbNiwAU899RRWr16NxYsX45VXXkGrVq3w7bffokePHuKYZcuWwcbGBuHh4SgtLUVYWBg++ugjsd3W1ha7du3Cyy+/jJCQELi4uCAyMhILFiwwKVaZIAiCyWdItU6j0UCpVOLm+aZQuHGGkaxT0EcT6joEolqhKy3BxaWvo7Cw0GCBryVVfk/EJgyGg6t9jY9TWlSGd7p9X6ux1iV+gxIREZHV4tQVERGRhOkFGfRCzaeuzBkrBUx0iIiIJExn5tPLzRkrBdZ9dkRERPSvxooOERGRhHHqyjgmOkRERBKmhw30ZkzQmDNWCpjoEBERSZhOkEFnRlXGnLFSYN1pHBEREf2rsaJDREQkYVyjYxwTHSIiIgkT7noCeU3HWzPrPjsiIiL6V2NFh4iISMJ0kEFnxkM9zRkrBUx0iIiIJEwvmLfORm/lj/ZmokNW4/QRF2z9yBMXTjsjP9cec9dloNvAQrH9TrEN1i3yQeIeJTQ37eDtq8WwqBt48vk/xT75eXb4ZKEKJw674XaRDXyblWLUq7l4fPBfx3n+sSDkXpMbvPeLs7Lxn0l5tX+SRHdxttfilceOIbRpBjyc7iDtj0ZY/GsPnMnzFPs0bXATU7smorMqB7Y2ely62QCTd4chp8gNADCv1yF0bXINni7FuF1mjxS1N95L7IqMggZ1dVpEFlVvE50rV64gICAAJ0+eRPv27es6HJKAkts2aNrmDsJG52NBVMA97R/PUyHlNzdMX5EJL18tThxyw4pZTdDQqwwhYRoAwDuv+KFIY4t5GzKg9CjHz9sa4K2XHsaKH8+jeds74rGej83BwIi/EiRnV33tnyDR3yzscxAtPPIx46d+uFHsgiGtzmPdkJ0YsuU/yCt2ha+iEJ8/tQ3fpgViZVJnFGnlaO6Rj1KdrXiM1BuNsfN8C+QUuULpUIqYzkn4ZMguPPF5hFkLXOnB0Zu5GNna/z9b99mZoaSkBDExMWjYsCFcXV0RHh6O3Nzcao09deoURo8eDV9fXzg5OSEwMBDx8fG1HDF17nsLY2ao0f2uKs7dzh53wRP/l4923Yrg7avFoGf/RNOgO0hPcTboM+zFP9C6w234+GvxzORcuCh1uPC7k8GxnFz18PAsFzdHZyY69GA52JbjiaaX8W5iCJJzVMjUKLEyqTMyCxUY1SYVAPBql2M4fNUf7yWGIO2PxsjSKPHzlQDk3/nrM7/1bBCSc1TIvqVA2h+NsfxYF/i4FeEht1t1dWpkIj1kZm/WjInOfUyZMgU7d+7E1q1bcejQIWRnZ2PEiBHVGpucnAxPT098/vnnSE1NxRtvvIFZs2bhww8/rOWoyZigTsU4sleJP3LsIQhAym+uuH7ZAcG9bhn0ObTDHZqbttDrgYPb3aEtkeHRbkUGx/r6Q0883eYRTHiiJbZ+1Bi68gd9NvRvZ2ujh52NAG25rcH+Ep0dOvqoIYOAXv5XcaVAiTVP7sIvY9ZjS/i36BeQcd9jOtmV4anW55BV6AZ1kWttnwJZSOWdkc3ZrFmdJjp6vR5Lly5F8+bN4eDgAD8/PyxatKjKvjqdDlFRUQgICICTkxNatWp1T5Xk4MGDeOyxx+Di4gJ3d3d0794dV69eBVBRZenTpw/c3NygUCgQHByM48ePV/lehYWFWLduHd5//3307dsXwcHBWL9+PRISEnDkyJF/PK8XX3wR8fHx6NWrF5o2bYpnn30WL7zwAr777jsTf0JkSRPevA6/liWICG6Dwf7tMDuiKWLeuoa2XYvFPm98fBW6Mhn+r01bPPlwO8TP8MXcdVfwUIBW7DMs6gZmrbqKpVsvYtBzf2LLCi988qaqLk6J/sVul8lxUu2F8Z2S0di5GDYyPYa0PI/2Xrlo7FyMhk534CIvw9iOJ/Frpi/G7RyCny4HIH7AbnRSZRsca1SbMzg+bi2Soz/B436ZGLtzCMr0tvd5ZyJpqdM1OrNmzcLatWuxbNky9OjRAzk5OTh37lyVffV6PZo0aYKtW7eiYcOGSEhIQHR0NHx8fDBy5EiUl5dj+PDhGDduHL788ktotVocO3YMMllFphoREYEOHTpg1apVsLW1RUpKCuzt7at8r+TkZJSVlSE0NFTc17p1a/j5+SExMRFdu3Y1+VwLCwvh4eFx3/bS0lKUlpaKrzUajcnvQcb999NGOJfsjPkbLsOziRanj7hi5esVa3Q69qyo2Gxc6o0ijS2WfHURCo9yJO5WYtH4h/HetgsICCwBAIS/dEM8ZtOgEtjbC4if4YsXZuVA7mDlly9QvTLzp354s8/PODRmE8r1Mpy90Rg/XGyOoMY3IJNVfBYPZDyMTb+3AwCc+7MR2nur8Z82qTie/VdyvutCCyRea4JGzrfxQvsUvN9/LyK2PQWtrt4u46S7cI2OcXX2Kb516xbi4+Px4YcfIjIyEgDQrFkz9OjRo8r+9vb2mD9/vvg6ICAAiYmJ+PrrrzFy5EhoNBoUFhbiySefRLNmzQAAgYGBYv/MzEzExsaidevWAIAWLVrcNza1Wg25XA53d3eD/V5eXlCr1Safa0JCAr766it8//339+2zePFig/Mjyyq9I8OGJT6IW3cFXUIrksimQSW4nOqEb1Z7omPPImRfkWPH+sb4+OdzeLhVRVLTrE0JTh91xY4NjfDq29eqPHarjrehK5chN0sO3+alVfYhqg1ZGiUi/zscTnZlcJFr8cdtF7zXfy+uaRQoKHFEmc4Gl24a/oF1+WYDdPQx/HesSOuAIq0Drha64/dcLyRGfYrQgAz8cPH+/05S/aGHmY+A4Bqd2pGWlobS0lL069ev2mNWrlyJ4OBgNG7cGK6urlizZg0yMzMBAB4eHhgzZgzCwsIwZMgQxMfHIycnRxw7depUjB07FqGhoViyZAkuXbpk8XOqypkzZzBs2DDMnTsX/fv3v2+/WbNmobCwUNyysrIeSHz/FuXlMpSX2cDGxrDiYmMrQPjfOuLSOxW/Dn/vY3tXn6pcTnWCjY0A90ZcqEN14065Pf647QKFQym6+2bhQEYAyvS2OHOjMQLcCwz6PuxeiOxbxtffyADIbXW1FzDRA1RniY6Tk9M/d7rLli1bMG3aNERFRWHv3r1ISUnBCy+8AK32r7UT69evR2JiIrp164avvvoKLVu2FNfUzJs3D6mpqRg8eDAOHDiAoKAgbNu2rcr38vb2hlarRUFBgcH+3NxceHt7Vzvms2fPol+/foiOjsbs2bON9nVwcIBCoTDYyDR3im1w6YwTLp2p+Gyps+S4dMYJedfs4eKmx6MhRVi7UIVTCa5QZ8qx9ysP/PSNh3ivHd/mJVAFlCJ+ui/OnXRG9hU5vlndGCcOu6HbgIo+Z48747u1jXEp1RE5V+U48F0DrJ6rQt/wm3Bz5xcDPVjdfTPRwzcTD7lpENIkCxuG/RcZN92x7VwrAMCnJ9tjYPOLeDrwLPwUhXjmkdPo/fAVbDnzCACgiUKDcR1PIKjxDfi43kJ7bzWWhe1Bqc4WhzP96vLUyASCmVdcCVZe0ZEJglAniwpKSkrg4eGB5cuXY+zYsfe0//0+OpMmTcLZs2exf/9+sU9oaCj++OMPpKSkVPkeISEh6Ny5M5YvX35P2+jRo1FcXIwdO3bc01ZYWIjGjRvjyy+/RHh4OAAgPT0drVu3rvYandTUVPTt2xeRkZFYunTpP/b/O41GA6VSiZvnm0LhZt3zp5ZyKsEV059ufs/+J0bmY9oHmcjPs8Onb/ngxGE33Cqwg+dDFZeYj4i+gf8t5cL1y3Kse0uF1GMuuFNsA1WAFk+Pz0Po0zcBABd+d8KHrzdB1kVHlGll8PbVot/T+RgRfYPrc2og6KMJdR2CpA1odhGTux6Ft2sRCkscsfdyU8QffQxFWgexz4jWaRjX8SS8XItwpcAdHx7rjANXKu4z1di5GAv7HERQ4xtQOpTijztOSM5W4aPjwbjCGwaaRVdagotLX0dhYWGt/eFa+T0R/lMk7F3k/zzgPsqKtfg2dGOtxlqX6myNjqOjI2bMmIHp06dDLpeje/fuuHHjBlJTUxEVFXVP/xYtWmDTpk3Ys2cPAgIC8NlnnyEpKQkBARW/sBkZGVizZg2GDh0KlUqF9PR0XLhwAc8//zzu3LmD2NhYPP300wgICMC1a9eQlJQkJjF/p1QqERUVhalTp8LDwwMKhQKTJk1CSEhItZKcM2fOoG/fvggLC8PUqVPFdT22trZo3LixGT81MqZdtyLsyU65b7uHZzmmfWB8SvChplrEfXLlvu0tHr2D+F0XahghkWXtvtQcuy/dm9zf7btzgfjuXGCVbTduu2D894NrIzSieqNOl9TPmTMHdnZ2iIuLQ3Z2Nnx8fDB+/Pgq+7700ks4efIk/vOf/0Amk2H06NGYMGECfvzxRwCAs7Mzzp07h40bN+LPP/+Ej48PYmJi8NJLL6G8vBx//vknnn/+eeTm5qJRo0YYMWKE0cW/y5Ytg42NDcLDw1FaWoqwsDB89NFH1Tqvb775Bjdu3MDnn3+Ozz//XNzv7++PK1euVP8HRERE9A941ZVxdTZ1RcZx6or+DTh1RdbqQU5dDdv7otlTV//t/6nVTl3xG5SIiIisFhOdGhg/fjxcXV2r3O439UZERFQb+Kwr43jbyxpYsGABpk2bVmWbNZb9iIio/tILZt4w0MqfdcVEpwY8PT3h6elZ12EQEREx0fkHnLoiIiIiq8WKDhERkYSxomMcEx0iIiIJY6JjHKeuiIiIyGqxokNERCRhAmDWJeLWftdgVnSIiIgkrHLqypzNVIcPH8aQIUOgUqkgk8mwffv2+/YdP348ZDIZPvjgA4P9+fn5iIiIgEKhgLu7O6KiolBUVGTQ5/fff8fjjz8OR0dH+Pr61ugh2Ux0iIiIyCTFxcVo164dVq5cabTftm3bcOTIEahUqnvaIiIikJqain379mHXrl04fPgwoqOjxXaNRoP+/fvD398fycnJeOeddzBv3jysWbPGpFg5dUVERCRhdbEYeeDAgRg4cKDRPtevX8ekSZOwZ88eDB482KAtLS0Nu3fvRlJSEjp16gQAWLFiBQYNGoR3330XKpUKmzdvhlarxaeffgq5XI42bdogJSUF77//vkFC9E9Y0SEiIpIwS01daTQag620tLTmMen1eO655xAbG4s2bdrc056YmAh3d3cxyQGA0NBQ2NjY4OjRo2Kfnj17Qi7/64GlYWFhSE9Px82bN6sdCxMdIiIigq+vL5RKpbgtXry4xsd6++23YWdnh1deeaXKdrVafc8TBuzs7ODh4QG1Wi328fLyMuhT+bqyT3Vw6oqIiEjCLDV1lZWVZfC8RgcHhxodLzk5GfHx8Thx4gRksrq/Rw8rOkRERBImCDKzN6DiodR3bzVNdH755Rfk5eXBz88PdnZ2sLOzw9WrV/Haa6/h4YcfBgB4e3sjLy/PYFx5eTny8/Ph7e0t9snNzTXoU/m6sk91MNEhIiKSMD1kZm+W9Nxzz+H3339HSkqKuKlUKsTGxmLPnj0AgJCQEBQUFCA5OVkcd+DAAej1enTp0kXsc/jwYZSVlYl99u3bh1atWqFBgwbVjodTV0RERGSSoqIiXLx4UXydkZGBlJQUeHh4wM/PDw0bNjTob29vD29vb7Rq1QoAEBgYiAEDBmDcuHFYvXo1ysrKMHHiRIwaNUq8FP2ZZ57B/PnzERUVhRkzZuDMmTOIj4/HsmXLTIqViQ4REZGE1cXl5cePH0efPn3E11OnTgUAREZGYsOGDdU6xubNmzFx4kT069cPNjY2CA8Px/Lly8V2pVKJvXv3IiYmBsHBwWjUqBHi4uJMurQcYKJDREQkaXevs6npeFP17t0bglD9h0dcuXLlnn0eHh744osvjI579NFH8csvv5gangGu0SEiIiKrxYoOERGRhNXF1JWUMNEhIiKSsLqYupISTl0RERGR1WJFh4iISMIEM6eurL2iw0SHiIhIwgQAJlwAVeV4a8apKyIiIrJarOgQERFJmB4yyMx4jIOlHwFR3zDRISIikjBedWUcEx0iIiIJ0wsyyHgfnfviGh0iIiKyWqzoEBERSZggmHnVlZVfdsVEh4iISMK4Rsc4Tl0RERGR1WJFh4iISMJY0TGOiQ4REZGE8aor4zh1RURERFaLFR0iIiIJ41VXxjHRISIikrCKRMecNToWDKYe4tQVERERWS1WdIiIiCSMV10Zx0SHiIhIwoT/beaMt2ZMdIiIiCSMFR3juEaHiIiIrBYrOkRERFLGuSujmOgQERFJmZlTV+DUFREREZE0saJDREQkYbwzsnFMdIiIiCSMV10Zx6krIiIislqs6BAREUmZIDNvQbGVV3SY6BAREUkY1+gYx6krIiIislqs6BAREUkZbxhoVLUSnR07dlT7gEOHDq1xMERERGQaXnVlXLUSneHDh1frYDKZDDqdzpx4iIiIyFQPuCpz+PBhvPPOO0hOTkZOTg62bdsm5gplZWWYPXs2fvjhB1y+fBlKpRKhoaFYsmQJVCqVeIz8/HxMmjQJO3fuhI2NDcLDwxEfHw9XV1exz++//46YmBgkJSWhcePGmDRpEqZPn25SrNVao6PX66u1MckhIiKyfsXFxWjXrh1Wrlx5T9vt27dx4sQJzJkzBydOnMB3332H9PT0e2Z8IiIikJqain379mHXrl04fPgwoqOjxXaNRoP+/fvD398fycnJeOeddzBv3jysWbPGpFjNWqNTUlICR0dHcw5BREREZqiLqauBAwdi4MCBVbYplUrs27fPYN+HH36Ixx57DJmZmfDz80NaWhp2796NpKQkdOrUCQCwYsUKDBo0CO+++y5UKhU2b94MrVaLTz/9FHK5HG3atEFKSgref/99g4Ton5h81ZVOp8PChQvx0EMPwdXVFZcvXwYAzJkzB+vWrTP1cERERGQOwQJbLSssLIRMJoO7uzsAIDExEe7u7mKSAwChoaGwsbHB0aNHxT49e/aEXC4X+4SFhSE9PR03b96s9nubnOgsWrQIGzZswNKlSw3e/JFHHsEnn3xi6uGIiIioHtBoNAZbaWmpRY5bUlKCGTNmYPTo0VAoFAAAtVoNT09Pg352dnbw8PCAWq0W+3h5eRn0qXxd2ac6TE50Nm3ahDVr1iAiIgK2trbi/nbt2uHcuXOmHo6IiIjMIrPABvj6+kKpVIrb4sWLzY6srKwMI0eOhCAIWLVqldnHqwmT1+hcv34dzZs3v2e/Xq9HWVmZRYIiIiKiarLQfXSysrLEigsAODg4mBVWZZJz9epVHDhwwODY3t7eyMvLM+hfXl6O/Px8eHt7i31yc3MN+lS+ruxTHSZXdIKCgvDLL7/cs/+bb75Bhw4dTD0cERER1QMKhcJgMyfRqUxyLly4gJ9++gkNGzY0aA8JCUFBQQGSk5PFfQcOHIBer0eXLl3EPocPHzYoouzbtw+tWrVCgwYNqh2LyRWduLg4REZG4vr169Dr9eJlY5s2bcKuXbtMPRwRERGZow7ujFxUVISLFy+KrzMyMpCSkgIPDw/4+Pjg6aefxokTJ7Br1y7odDpxTY2HhwfkcjkCAwMxYMAAjBs3DqtXr0ZZWRkmTpyIUaNGiffaeeaZZzB//nxERUVhxowZOHPmDOLj47Fs2TKTYjW5ojNs2DDs3LkTP/30E1xcXBAXF4e0tDTs3LkTTzzxhKmHIyIiInNUPr3cnM1Ex48fR4cOHcSZnKlTp6JDhw6Ii4vD9evXsWPHDly7dg3t27eHj4+PuCUkJIjH2Lx5M1q3bo1+/fph0KBB6NGjh8E9cpRKJfbu3YuMjAwEBwfjtddeQ1xcnEmXlgM1vI/O448/fs818kRERPTv0Lt3bwhGHnturK2Sh4cHvvjiC6N9Hn300SqXy5iixjcMPH78ONLS0gBUrNsJDg42KxAiIiIynSBUbOaMt2YmJzrXrl3D6NGj8dtvv4k3/ikoKEC3bt2wZcsWNGnSxNIxEhER0f3w6eVGmbxGZ+zYsSgrK0NaWhry8/ORn5+PtLQ06PV6jB07tjZiJCIiovupgzU6UmJyRefQoUNISEhAq1atxH2tWrXCihUr8Pjjj1s0OCIiIiJzmJzo+Pr6VnljQJ1OZ/D4dSIiIqp9MqFiM2e8NTN56uqdd97BpEmTcPz4cXHf8ePH8eqrr+Ldd9+1aHBERET0DyTwUM+6VK2KToMGDSCT/TWHV1xcjC5dusDOrmJ4eXk57Ozs8OKLL2L48OG1EigRERGRqaqV6HzwwQe1HAYRERHViLkLirkYGYiMjKztOIiIiKgmeHm5UTW+YSAAlJSUQKvVGuy7++mkRERERHXJ5MXIxcXFmDhxIjw9PeHi4oIGDRoYbERERPQAcTGyUSYnOtOnT8eBAwewatUqODg44JNPPsH8+fOhUqmwadOm2oiRiIiI7oeJjlEmT13t3LkTmzZtQu/evfHCCy/g8ccfR/PmzeHv74/NmzcjIiKiNuIkIiIiMpnJFZ38/Hw0bdoUQMV6nPz8fABAjx49cPjwYctGR0RERMbxERBGmZzoNG3aFBkZGQCA1q1b4+uvvwZQUempfMgnERERPRiVd0Y2Z7NmJic6L7zwAk6dOgUAmDlzJlauXAlHR0dMmTIFsbGxFg+QiIiIjOAaHaNMXqMzZcoU8b9DQ0Nx7tw5JCcno3nz5nj00UctGhwRERGROcy6jw4A+Pv7w9/f3xKxEBEREVlUtRKd5cuXV/uAr7zySo2DISIiItPIYObTyy0WSf1UrURn2bJl1TqYTCZjokNERET1RrUSncqrrOjB6/hlFGwcHes6DKJaEfBmQl2HQFQryoUyXHxQb8aHehpl9hodIiIiqkN8qKdRJl9eTkRERCQVrOgQERFJGSs6RjHRISIikjBz727MOyMTERERSVSNEp1ffvkFzz77LEJCQnD9+nUAwGeffYZff/3VosERERHRP+AjIIwyOdH59ttvERYWBicnJ5w8eRKlpaUAgMLCQrz11lsWD5CIiIiMYKJjlMmJzptvvonVq1dj7dq1sLe3F/d3794dJ06csGhwREREZByfXm6cyYlOeno6evbsec9+pVKJgoICS8REREREZBEmJzre3t64ePHe+z3++uuvaNq0qUWCIiIiomqqvDOyOZsVMznRGTduHF599VUcPXoUMpkM2dnZ2Lx5M6ZNm4aXX365NmIkIiKi++EaHaNMvo/OzJkzodfr0a9fP9y+fRs9e/aEg4MDpk2bhkmTJtVGjEREREQ1YnKiI5PJ8MYbbyA2NhYXL15EUVERgoKC4OrqWhvxERERkRG8YaBxNb4zslwuR1BQkCVjISIiIlPxERBGmbxGp0+fPujbt+99NyIiIrJuhw8fxpAhQ6BSqSCTybB9+3aDdkEQEBcXBx8fHzg5OSE0NBQXLlww6JOfn4+IiAgoFAq4u7sjKioKRUVFBn1+//13PP7443B0dISvry+WLl1qcqwmJzrt27dHu3btxC0oKAharRYnTpxA27ZtTQ6AiIiIzGDuPXRqUNEpLi5Gu3btsHLlyirbly5diuXLl2P16tU4evQoXFxcEBYWhpKSErFPREQEUlNTsW/fPuzatQuHDx9GdHS02K7RaNC/f3/4+/sjOTkZ77zzDubNm4c1a9aYFKvJU1fLli2rcv+8efPuycSIiIioltXB1NXAgQMxcODAqg8nCPjggw8we/ZsDBs2DACwadMmeHl5Yfv27Rg1ahTS0tKwe/duJCUloVOnTgCAFStWYNCgQXj33XehUqmwefNmaLVafPrpp5DL5WjTpg1SUlLw/vvvGyRE/8RiD/V89tln8emnn1rqcERERCRBGRkZUKvVCA0NFfcplUp06dIFiYmJAIDExES4u7uLSQ4AhIaGwsbGBkePHhX79OzZE3K5XOwTFhaG9PR03Lx5s9rx1Hgx8t8lJibC0dHRUocjIiKi6rBQRUej0RjsdnBwgIODg8mHU6vVAAAvLy+D/V5eXmKbWq2Gp6enQbudnR08PDwM+gQEBNxzjMq2Bg0aVCsekxOdESNGGLwWBAE5OTk4fvw45syZY+rhiIiIyAyWurzc19fXYP/cuXMxb968mh+4njA50VEqlQavbWxs0KpVKyxYsAD9+/e3WGBERET04GRlZUGhUIiva1LNASoeFQUAubm58PHxEffn5uaiffv2Yp+8vDyDceXl5cjPzxfHe3t7Izc316BP5evKPtVhUqKj0+nwwgsvoG3bttUuGREREVH9p1AoDBKdmgoICIC3tzf2798vJjYajQZHjx4VHxUVEhKCgoICJCcnIzg4GABw4MAB6PV6dOnSRezzxhtvoKysDPb29gCAffv2oVWrViblICYtRra1tUX//v35lHIiIqL6og6edVVUVISUlBSkpKQAqFiAnJKSgszMTMhkMkyePBlvvvkmduzYgdOnT+P555+HSqXC8OHDAQCBgYEYMGAAxo0bh2PHjuG3337DxIkTMWrUKKhUKgDAM888A7lcjqioKKSmpuKrr75CfHw8pk6dalKsJk9dPfLII7h8+fI9C4SIiIjowauLR0AcP34cffr0EV9XJh+RkZHYsGEDpk+fjuLiYkRHR6OgoAA9evTA7t27DS5a2rx5MyZOnIh+/frBxsYG4eHhWL58udiuVCqxd+9exMTEIDg4GI0aNUJcXJxJl5ZXnJ8gmHSKu3fvxqxZs7Bw4UIEBwfDxcXFoN0SZS+qKPMplUoEzF8EG17NRlYqYGZiXYdAVCvKhTIcxH9RWFhYa9+Lld8TzWe+BVszvid0JSW4uOT1Wo21LlW7orNgwQK89tprGDRoEABg6NChkMlkYrsgCJDJZNDpdJaPkoiIiO7Pyp9XZY5qJzrz58/H+PHj8fPPP9dmPERERGQKPtTTqGonOpUzXL169aq1YIiIiIgsyaTFyHdPVREREVHdq4vFyFJiUqLTsmXLf0x28vPzzQqIiIiITMCpK6NMSnTmz59/z52RiYiIiOorkxKdUaNG3fMQLiIiIqo7nLoyrtqJDtfnEBER1UOcujLK5KuuiIiIqB5homNUtRMdvV5fm3EQERERWZzJz7oiIiKi+oNrdIxjokNERCRlnLoyyqauAyAiIiKqLazoEBERSRkrOkYx0SEiIpIwrtExjlNXREREZLVY0SEiIpIyTl0ZxUSHiIhIwjh1ZRynroiIiMhqsaJDREQkZZy6MoqJDhERkZQx0TGKiQ4REZGEyf63mTPemnGNDhEREVktVnSIiIikjFNXRjHRISIikjBeXm4cp66IiIjIarGiQ0REJGWcujKKiQ4REZHUWXmyYg5OXREREZHVYkWHiIhIwrgY2TgmOkRERFLGNTpGceqKiIiIrBYrOkRERBLGqSvjmOgQERFJGaeujGKiQ0REJGGs6BjHNTpERERktZjoEBERSZlggc0EOp0Oc+bMQUBAAJycnNCsWTMsXLgQgvDXgQRBQFxcHHx8fODk5ITQ0FBcuHDB4Dj5+fmIiIiAQqGAu7s7oqKiUFRUVJOfgFFMdIiIiKTsASc6b7/9NlatWoUPP/wQaWlpePvtt7F06VKsWLFC7LN06VIsX74cq1evxtGjR+Hi4oKwsDCUlJSIfSIiIpCamop9+/Zh165dOHz4MKKjo2v6U7gvrtEhIiKiaktISMCwYcMwePBgAMDDDz+ML7/8EseOHQNQUc354IMPMHv2bAwbNgwAsGnTJnh5eWH79u0YNWoU0tLSsHv3biQlJaFTp04AgBUrVmDQoEF49913oVKpLBYvKzpEREQSVrkY2ZwNADQajcFWWlpa5ft169YN+/fvx/nz5wEAp06dwq+//oqBAwcCADIyMqBWqxEaGiqOUSqV6NKlCxITEwEAiYmJcHd3F5McAAgNDYWNjQ2OHj1q0Z8PKzpERERSZqHLy319fQ12z507F/Pmzbun+8yZM6HRaNC6dWvY2tpCp9Nh0aJFiIiIAACo1WoAgJeXl8E4Ly8vsU2tVsPT09Og3c7ODh4eHmIfS2GiQ0RERMjKyoJCoRBfOzg4VNnv66+/xubNm/HFF1+gTZs2SElJweTJk6FSqRAZGfmgwq02JjpEREQSJhMEyISal3QqxyoUCoNE535iY2Mxc+ZMjBo1CgDQtm1bXL16FYsXL0ZkZCS8vb0BALm5ufDx8RHH5ebmon379gAAb29v5OXlGRy3vLwc+fn54nhL4RodIiIiKXvAV13dvn0bNjaG6YOtrS30ej0AICAgAN7e3ti/f7/YrtFocPToUYSEhAAAQkJCUFBQgOTkZLHPgQMHoNfr0aVLF9MC+ges6BAREVG1DRkyBIsWLYKfnx/atGmDkydP4v3338eLL74IAJDJZJg8eTLefPNNtGjRAgEBAZgzZw5UKhWGDx8OAAgMDMSAAQMwbtw4rF69GmVlZZg4cSJGjRpl0SuuACY6REREkvagHwGxYsUKzJkzBxMmTEBeXh5UKhVeeuklxMXFiX2mT5+O4uJiREdHo6CgAD169MDu3bvh6Ogo9tm8eTMmTpyIfv36wcbGBuHh4Vi+fHnNT+Q+ZIJgxsQe1RqNRgOlUomA+Ytgc9cHg8iaBMxMrOsQiGpFuVCGg/gvCgsLq7XupSYqvyc6PLMItvKaf0/otCU4+cUbtRprXWJFh4iISML4UE/juBiZiIiIrBYrOkRERFJmoRsGWismOkRERBLGqSvjOHVFREREVosVHSIiIinj1JVRTHSIiIgkztqnn8zBqSsiIiKyWqzoEBERSZkgVGzmjLdiTHSIiIgkjFddGcepKyIiIrJarOgQERFJGa+6MoqJDhERkYTJ9BWbOeOtGRMdshqdPLMxts0ptGl4A17OtzHh5zD8lBVwVw8Br7Q7jpEt0qCQl+LEDW/MPfI4rt5yF3us6vMjAj3+REPHOygsdUBizkN450RX5N1xEfv0UGXhlXZJaO5+E1qdLZJyfbDkeAiuF1vfU3+p/vrPxFx0H1QI3+al0JbY4OxxZ6xb5INrl/56inWDxmUYOycHHXvegrOrHlmXHLAl3hO//uAu9mne9jai3shBy3a3odfJ8OsPSnw8T4WS27Z1cFZUI6zoGGWVa3SuXLkCmUyGlJSUug6FHiBnu3Kcu9kQC44+XmX7uDYpeD7wNOYefRz/98MI3C63x6eh30NuUy72OapW4dVDTyBs+yhMOtQfvm4aLO+1V2xv4qrBqj67cUT9EIbtfBov/jQYDRxK8GHvvVW9JVGteTSkGDs3NMLkJ1tg1qimsLUT8NaXl+HgpBP7xC7PhG+zEswbE4CX+rbEbz8o8frHV9HskdsAAA+vMizZchnZGQ549ckWeCOiKfxblWDaB1l1dVpEFmeViU5dW7NmDXr37g2FQgGZTIaCgoK6Dulf4XC2Hz5IeQz7DKo4lQREBp7GR793xP6sAKQXNMT0X/vA0/k2nvC7IvbakNYOp/7wQnaxG07e8MaaMx3QvnEu7GQVXx5tPG7ARiZg2cnHkFWkxNn8xlh3th0CPf4Q+xA9CG9ENMW+rz1w9bwjLp91wnuT/eDVpAwtHr0j9gnqdBv//bQR0lOcoc50wJfxXigutBX7dAnVoLxchg9ffwjXLjni/ClnLJ/RBI8/WQjVw6V1dWpkosqrrszZrBkTnVpw+/ZtDBgwAK+//npdh0L/4+t6C57Ot5GY00TcV1TmgFM3PNG+sbrKMUp5CYY2vYCTN7xRLlSU8VPzG0MQgPDm52Aj08PVvhTDmp5HQk4TsQ9RXXBRVCTatwr++hyePe6MXkML4OZeDplMQK9hNyF3FPB7gisAwN5Bj/IyGQRBJo7RllR8LbR5rPgBRk9mqbyPjjmbFZNsoqPX67F06VI0b94cDg4O8PPzw6JFi6rsq9PpEBUVhYCAADg5OaFVq1aIj4836HPw4EE89thjcHFxgbu7O7p3746rV68CAE6dOoU+ffrAzc0NCoUCwcHBOH78+H1jmzx5MmbOnImuXbta7oTJLI2cKkr1f5Q4Gez/o8QJjZ3uGOyb1vEIUkZ/gqRRG+DjUoSXfx4gtl0rUuCFn57E1A7HcCZiLU6MXg9v52K8euiJ2j8JovuQyQSMn38dZ44542r6X5/xRS89DFt7Ad+cTcWuK7/j1bevYX7Uw8i+4gAAOPWrGxo0LsPTL+fBzl4PV2U5Xnw9BwDg4VlWJ+dCZGmSXYw8a9YsrF27FsuWLUOPHj2Qk5ODc+fOVdlXr9ejSZMm2Lp1Kxo2bIiEhARER0fDx8cHI0eORHl5OYYPH45x48bhyy+/hFarxbFjxyCTVfyVExERgQ4dOmDVqlWwtbVFSkoK7O3tLXo+paWlKC39q1Ss0WgsenyqvnWp7fDNhdZQud7CpHbJWNr9AKIPDAQgQyPH23gz5BC2XWqJXVdawMVOi1fbJ2FF770Ys+9JALJ/OjyRxU186zr8W5fgteHNDfZHTs+Bq0KPGSObQpNvh5ABhXhj9RW89lRzXDnnhKvnHfHuZD9Ez83Gi7NyoNPJ8N9PGyE/z86gykP1G28YaJwkE51bt24hPj4eH374ISIjIwEAzZo1Q48eParsb29vj/nz54uvAwICkJiYiK+//hojR46ERqNBYWEhnnzySTRr1gwAEBgYKPbPzMxEbGwsWrduDQBo0aKFxc9p8eLFBjGSZf1xxxkA0MjxDm7cdQVVI8c7SLvZ0KDvzVIn3Cx1wpVb7rhU2AC/PP052jfKRcof3ohofQZFWjneOREi9p/2az/88vTnaNcoD6f+8HowJ0T0PzGLrqHLExq89lQz/JEjF/f7+Jdi2It/Irp3K1w9X3El1uWzTmjbpRhDx/yJ5TMrpnF/3tYAP29rAPdGZSi5bQNBAEZE30DOVXmV70f1EK+6MkqSU1dpaWkoLS1Fv379qj1m5cqVCA4ORuPGjeHq6oo1a9YgMzMTAODh4YExY8YgLCwMQ4YMQXx8PHJycsSxU6dOxdixYxEaGoolS5bg0qVLFj+nWbNmobCwUNyysnjVgyVlFbkh77YzQnyui/tc7LVo1zgPKTe87zvO5n9/6shtK9Y/ONmWQ/+3v3QrX9tY+59FVM8IiFl0Dd0GFGL6/zVDbpaDQauDU8XNUfR/u0eKTgfIbO79rBb8YY+S27boNawAZaU2OHHYrdYiJ3qQJJnoODk5/XOnu2zZsgXTpk1DVFQU9u7di5SUFLzwwgvQarVin/Xr1yMxMRHdunXDV199hZYtW+LIkSMAgHnz5iE1NRWDBw/GgQMHEBQUhG3btln0nBwcHKBQKAw2Mo2zXRkCG/yBwAZ/AKi4FDywwR/wcbkFQIaNaW3xcttk9G1yBS3d/8Q73Q8g77Yz9mU+DAB4tFEunm11BoEN/oDK5Ra6el/H+4//hKsaBU7+Lxk6eN0fbRvlIebR4/B3K0CQxw0s7nYQ14pccTa/UR2dOf0bTXzrOvqOuIklMf64U2SDBo3L0KBxGeSOFZlN1kVHXL8sx6tLr6FV+9vw8S9F+Et56NizCAm7leJxhr7wB5q3vY2HmpZiyJg/ELPoOj5d7I1iDRfXSwWvujJOklNXLVq0gJOTE/bv34+xY8f+Y//ffvsN3bp1w4QJE8R9VVVlOnTogA4dOmDWrFkICQnBF198IS4obtmyJVq2bIkpU6Zg9OjRWL9+PZ566inLnRSZ7ZGGefg8bKf4+vXOiQCA7y62xMyEvlib2h5OduVYGHIICrkWyXneiPppMLT6il+DknI7POF3GZPaJ8HZrhx5t53xS7YvJv/eEWX6in/0j6gfwmu/hGJsmxSMbZOCknI7nPzDG2N/GoxSnSR/nUiihoz5EwDw7neG/5a9O9kX+772gK5chtnPNUXU6zmYvzEDTi56ZGfI8e6rvkg68NcfUq3a38Zzr6nh6KLHtYsOWD69CfZ/6/FAz4XMxKeXGyXJf5kdHR0xY8YMTJ8+HXK5HN27d8eNGzeQmpqKqKioe/q3aNECmzZtwp49exAQEIDPPvsMSUlJCAiouN9KRkYG1qxZg6FDh0KlUiE9PR0XLlzA888/jzt37iA2NhZPP/00AgICcO3aNSQlJSE8PPy+8anVaqjValy8eBEAcPr0abi5ucHPzw8eHvwHpLYcy30ILTeNN9JDhuWnOmP5qc5Vtp4vaIjIfUP/8X2+v9Ic319p/o/9iGpTmKrdP/bJznDAwnEPG+3zzqt+FoqIqH6SZKIDAHPmzIGdnR3i4uKQnZ0NHx8fjB9f9ZfcSy+9hJMnT+I///kPZDIZRo8ejQkTJuDHH38EADg7O+PcuXPYuHEj/vzzT/j4+CAmJgYvvfQSysvL8eeff+L5559Hbm4uGjVqhBEjRhhdOLx69WqD9p49ewKomB4bM2aM5X4IRET0r8erroyTCYKV16wkSqPRQKlUImD+Itg4Ov7zACIJCpiZWNchENWKcqEMB/FfFBYW1tqay8rviZABC2BnX/PvifKyEiTujqvVWOuSZCs6RERExIrOP5HkVVdERERE1cGKDhERkZTphYrNnPFWjIkOERGRlPHOyEZx6oqIiIisFis6REREEiaDmYuRLRZJ/cREh4iISMp4Z2SjOHVFREREVosVHSIiIgnjfXSMY0WHiIhIygQLbCa6fv06nn32WTRs2BBOTk5o27Ytjh8//ldIgoC4uDj4+PjAyckJoaGhuHDhgsEx8vPzERERAYVCAXd3d0RFRaGoqMj0YP4BEx0iIiKqtps3b6J79+6wt7fHjz/+iLNnz+K9995DgwYNxD5Lly7F8uXLsXr1ahw9ehQuLi4ICwtDSUmJ2CciIgKpqanYt28fdu3ahcOHDyM6Otri8XLqioiISMJkggCZGQuKTR379ttvw9fXF+vXrxf3BQQEiP8tCAI++OADzJ49G8OGDQMAbNq0CV5eXti+fTtGjRqFtLQ07N69G0lJSejUqRMAYMWKFRg0aBDeffddqFSqGp/P37GiQ0REJGV6C2wm2LFjBzp16oT/+7//g6enJzp06IC1a9eK7RkZGVCr1QgNDRX3KZVKdOnSBYmJFQ/yTUxMhLu7u5jkAEBoaChsbGxw9OhR0wL6B0x0iIiIJKyyomPOBlQ8Df3urbS0tMr3u3z5MlatWoUWLVpgz549ePnll/HKK69g48aNAAC1Wg0A8PLyMhjn5eUltqnVanh6ehq029nZwcPDQ+xjKUx0iIiICL6+vlAqleK2ePHiKvvp9Xp07NgRb731Fjp06IDo6GiMGzcOq1evfsARVw/X6BAREUmZhZ51lZWVBYVCIe52cHCosruPjw+CgoIM9gUGBuLbb78FAHh7ewMAcnNz4ePjI/bJzc1F+/btxT55eXkGxygvL0d+fr443lJY0SEiIpKyyjsjm7MBUCgUBtv9Ep3u3bsjPT3dYN/58+fh7+8PoGJhsre3N/bv3y+2azQaHD16FCEhIQCAkJAQFBQUIDk5Wexz4MAB6PV6dOnSxaI/HlZ0iIiIqNqmTJmCbt264a233sLIkSNx7NgxrFmzBmvWrAEAyGQyTJ48GW+++SZatGiBgIAAzJkzByqVCsOHDwdQUQEaMGCAOOVVVlaGiRMnYtSoURa94gpgokNERCRpD/rOyJ07d8a2bdswa9YsLFiwAAEBAfjggw8QEREh9pk+fTqKi4sRHR2NgoIC9OjRA7t374ajo6PYZ/PmzZg4cSL69esHGxsbhIeHY/ny5TU/kfuQCYKVP81LojQaDZRKJQLmL4LNXR8MImsSMDOxrkMgqhXlQhkO4r8oLCw0WPdiSZXfE71CZsPOrubfE+XlJTiU+GatxlqXuEaHiIiIrBanroiIiCRMpq/YzBlvzZjoEBERSdldV07VeLwV49QVERERWS1WdIiIiKTMQjcMtFZMdIiIiCTsQT+9XGqY6BAREUkZ1+gYxTU6REREZLVY0SEiIpIyAYA5l4hbd0GHiQ4REZGUcY2OcZy6IiIiIqvFig4REZGUCTBzMbLFIqmXmOgQERFJGa+6MopTV0RERGS1WNEhIiKSMj0AmZnjrRgTHSIiIgnjVVfGMdEhIiKSMq7RMYprdIiIiMhqsaJDREQkZazoGMVEh4iISMqY6BjFqSsiIiKyWqzoEBERSRkvLzeKiQ4REZGE8fJy4zh1RURERFaLFR0iIiIp42Jko5joEBERSZleAGRmJCt66050OHVFREREVosVHSIiIinj1JVRTHSIiIgkzcxEB0x0iIiIqL5iRccortEhIiIiq8WKDhERkZTpBZg1/WTlV10x0SEiIpIyQV+xmTPeinHqioiIiKwWKzpERERSxsXIRrGiQ0REJGV6wfzNDEuWLIFMJsPkyZPFfSUlJYiJiUHDhg3h6uqK8PBw5ObmGozLzMzE4MGD4ezsDE9PT8TGxqK8vNysWKrCRIeIiIhqJCkpCR9//DEeffRRg/1TpkzBzp07sXXrVhw6dAjZ2dkYMWKE2K7T6TB48GBotVokJCRg48aN2LBhA+Li4iweIxMdIiIiKaucujJnq4GioiJERERg7dq1aNCggbi/sLAQ69atw/vvv4++ffsiODgY69evR0JCAo4cOQIA2Lt3L86ePYvPP/8c7du3x8CBA7Fw4UKsXLkSWq3WIj+WSkx0iIiIpEyAmYlOzd42JiYGgwcPRmhoqMH+5ORklJWVGexv3bo1/Pz8kJiYCABITExE27Zt4eXlJfYJCwuDRqNBampqzQK6Dy5GJiIiImg0GoPXDg4OcHBwqLLvli1bcOLECSQlJd3TplarIZfL4e7ubrDfy8sLarVa7HN3klPZXtlmSazoEBERSZmFpq58fX2hVCrFbfHixVW+XVZWFl599VVs3rwZjo6OD/JMa4QVHSIiIinT6wGYcdM/fcXYrKwsKBQKcff9qjnJycnIy8tDx44dxX06nQ6HDx/Ghx9+iD179kCr1aKgoMCgqpObmwtvb28AgLe3N44dO2Zw3Mqrsir7WAorOkRERFJmoYqOQqEw2O6X6PTr1w+nT59GSkqKuHXq1AkRERHif9vb22P//v3imPT0dGRmZiIkJAQAEBISgtOnTyMvL0/ss2/fPigUCgQFBVn0x8OKDhEREVWbm5sbHnnkEYN9Li4uaNiwobg/KioKU6dOhYeHBxQKBSZNmoSQkBB07doVANC/f38EBQXhueeew9KlS6FWqzF79mzExMTcN8GqKSY6REREUlYP74y8bNky2NjYIDw8HKWlpQgLC8NHH30kttva2mLXrl14+eWXERISAhcXF0RGRmLBggUWj4WJDhERkZTVg6eXHzx40OC1o6MjVq5ciZUrV953jL+/P3744Qez3/ufcI0OERERWS1WdIiIiCRMEPQQhJpfdWXOWClgokNERCRlgpkP5uTTy4mIiIikiRUdIiIiKRPMXIxs5RUdJjpERERSptcDMjPW2Vj5Gh1OXREREZHVYkWHiIhIyjh1ZRQTHSIiIgkT9HoIZkxd8fJyIiIiqr9Y0TGKa3SIiIjIarGiQ0REJGV6AZCxonM/THSIiIikTBAAmHN5uXUnOpy6IiIiIqvFig4REZGECXoBghlTV4KVV3SY6BAREUmZoId5U1fWfXk5p66IiIjIarGiQ0REJGGcujKOiQ4REZGUcerKKCY69VRlhq0vKanjSIhqT7lQVtchENWKclR8th9EtaQcZWbdGLkyVmslE6y9ZiVR165dg6+vb12HQUREZsjKykKTJk1q5dglJSUICAiAWq02+1je3t7IyMiAo6OjBSKrX5jo1FN6vR7Z2dlwc3ODTCar63Csnkajga+vL7KysqBQKOo6HCKL42f8wRIEAbdu3YJKpYKNTe1d91NSUgKtVmv2ceRyuVUmOQCnruotGxubWvsrgO5PoVDwS4CsGj/jD45Sqaz193B0dLTaBMVSeHk5ERERWS0mOkRERGS1mOgQAXBwcMDcuXPh4OBQ16EQ1Qp+xunfiouRiYiIyGqxokNERERWi4kOERERWS0mOiQZV65cgUwmQ0pKSl2HQlQn+DtAZDomOkTVVFJSgpiYGDRs2BCurq4IDw9Hbm5utcaeOnUKo0ePhq+vL5ycnBAYGIj4+PhajpjIstasWYPevXtDoVBAJpOhoKCgrkMi+kdMdIiqacqUKdi5cye2bt2KQ4cOITs7GyNGjKjW2OTkZHh6euLzzz9Hamoq3njjDcyaNQsffvhhLUdNZDm3b9/GgAED8Prrr9d1KETVJxDVIzqdTnj77beFZs2aCXK5XPD19RXefPNNQRAEISMjQwAgnDx5UhAEQSgvLxdefPFF4eGHHxYcHR2Fli1bCh988IHB8X7++Wehc+fOgrOzs6BUKoVu3boJV65cEQRBEFJSUoTevXsLrq6ugpubm9CxY0chKSmpyrgKCgoEe3t7YevWreK+tLQ0AYCQmJhYo3OdMGGC0KdPnxqNJetVX38H/n5MAMLNmzcteu5EtYGPgKB6ZdasWVi7di2WLVuGHj16ICcnB+fOnauyr16vR5MmTbB161Y0bNgQCQkJiI6Oho+PD0aOHIny8nIMHz4c48aNw5dffgmtVotjx46Jzw6LiIhAhw4dsGrVKtja2iIlJQX29vZVvldycjLKysoQGhoq7mvdujX8/PyQmJiIrl27mnyuhYWF8PDwMHkcWbf6+jtAJFl1nWkRVdJoNIKDg4Owdu3aKtv//tdsVWJiYoTw8HBBEAThzz//FAAIBw8erLKvm5ubsGHDhmrFtnnzZkEul9+zv3PnzsL06dOrdYy7/fbbb4KdnZ2wZ88ek8eS9arPvwN3Y0WHpIRrdKjeSEtLQ2lpKfr161ftMStXrkRwcDAaN24MV1dXrFmzBpmZmQAADw8PjBkzBmFhYRgyZAji4+ORk5Mjjp06dSrGjh2L0NBQLFmyBJcuXbL4OVXlzJkzGDZsGObOnYv+/fs/kPckafi3/A4QPUhMdKjecHJyMqn/li1bMG3aNERFRWHv3r1ISUnBCy+8AK1WK/ZZv349EhMT0a1bN3z11Vdo2bIljhw5AgCYN28eUlNTMXjwYBw4cABBQUHYtm1ble/l7e0NrVZ7z1Umubm58Pb2rnbMZ8+eRb9+/RAdHY3Zs2ebdL5k/erz7wCRZNV1SYmo0p07dwQnJ6dql+0nTpwo9O3b16BPv379hHbt2t33Pbp27SpMmjSpyrZRo0YJQ4YMqbKtcjHyN998I+47d+6cSYuRz5w5I3h6egqxsbHV6k//PvX5d+BunLoiKeFiZKo3HB0dMWPGDEyfPh1yuRzdu3fHjRs3kJqaiqioqHv6t2jRAps2bcKePXsQEBCAzz77DElJSQgICAAAZGRkYM2aNRg6dChUKhXS09Nx4cIFPP/887hz5w5iY2Px9NNPIyAgANeuXUNSUhLCw8OrjE2pVCIqKgpTp06Fh4cHFAoFJk2ahJCQkGotRD5z5gz69u2LsLAwTJ06FWq1GgBga2uLxo0bm/FTI2tSn38HAECtVkOtVuPixYsAgNOnT8PNzQ1+fn5cWE/1V11nWkR30+l0wptvvin4+/sL9vb2gp+fn/DWW28JgnDvX7MlJSXCmDFjBKVSKbi7uwsvv/yyMHPmTPGvWbVaLQwfPlzw8fER5HK54O/vL8TFxQk6nU4oLS0VRo0aJfj6+gpyuVxQqVTCxIkThTt37tw3tjt37ggTJkwQGjRoIDg7OwtPPfWUkJOTU63zmjt3rgDgns3f39+cHxdZofr8O3C/z/H69etr+adCVHN8ejkRERFZLS5GJiIiIqvFRIfIAsaPHw9XV9cqt/Hjx9d1eERE/1qcuiKygLy8PGg0mirbFAoFPD09H3BEREQEMNEhIiIiK8apKyIiIrJaTHSIiIjIajHRISIiIqvFRIeIiIisFhMdIrqvMWPGYPjw4eLr3r17Y/LkyQ88joMHD0Imk93zUNW7yWQybN++vdrHnDdvHtq3b29WXFeuXIFMJkNKSopZxyGi2sNEh0hixowZA5lMBplMBrlcjubNm2PBggUoLy+v9ff+7rvvsHDhwmr1rU5yQkRU2/hQTyIJGjBgANavX4/S0lL88MMPiImJgb29PWbNmnVPX61WC7lcbpH35YMbiUhqWNEhkiAHBwd4e3vD398fL7/8MkJDQ7Fjxw4Af003LVq0CCqVCq1atQIAZGVlYeTIkXB3d4eHhweGDRuGK1euiMfU6XSYOnUq3N3d0bBhQ0yfPh1/v83W36euSktLMWPGDPj6+sLBwQHNmzfHunXrcOXKFfTp0wcA0KBBA8hkMowZMwYAoNfrsXjxYgQEBMDJyQnt2rXDN998Y/A+P/zwA1q2bAknJyf06dPHIM7qmjFjBlq2bAlnZ2c0bdoUc+bMQVlZ2T39Pv74Y/j6+sLZ2RkjR45EYWGhQfsnn3yCwMBAODo6onXr1vjoo49MjoWI6g4THSIr4OTkBK1WK77ev38/0tPTsW/fPuzatQtlZWUICwuDm5sbfvnlF/z2229wdXXFgAEDxHHvvfceNmzYgE8//RS//vor8vPzsW3bNqPv+/zzz+PLL7/E8uXLkZaWho8//hiurq7w9fXFt99+CwBIT09HTk4O4uPjAQCLFy/Gpk2bsHr1aqSmpmLKlCl49tlncejQIQAVCdmIESMwZMgQpKSkYOzYsZg5c6bJPxM3Nzds2LABZ8+eRXx8PNauXYtly5YZ9Ll48SK+/vpr7Ny5E7t378bJkycxYcIEsX3z5s2Ii4vDokWLkJaWhrfeegtz5szBxo0bTY6HiOpIHT45nYhqIDIyUhg2bJggCIKg1+uFffv2CQ4ODsK0adPEdi8vL6G0tFQc89lnnwmtWrUS9Hq9uK+0tFRwcnIS9uzZIwiCIPj4+AhLly4V28vKyoQmTZqI7yUIgtCrVy/h1VdfFQRBENLT0wUAwr59+6qM8+effxYACDdv3hT3lZSUCM7OzkJCQoJB36ioKGH06NGCIAjCrFmzhKCgIIP2GTNm3HOsvwMgbNu27b7t77zzjhAcHCy+njt3rmBraytcu3ZN3Pfjjz8KNjY2Qk5OjiAIgtCsWTPhiy++MDjOwoULhZCQEEEQBCEjI0MAIJw8efK+70tEdYtrdIgkaNeuXXB1dUVZWRn0ej2eeeYZzJs3T2xv27atwbqcU6dO4eLFi3BzczM4TklJCS5duoTCwkLk5OSgS5cuYpudnR06dep0z/RVpZSUFNja2qJXr17VjvvixYu4ffs2nnjiCYP9Wq0WHTp0AACkpaUZxAEAISEh1X6PSl999RWWL1+OS5cuoaioCOXl5VAoFAZ9/Pz88NBDDxm8j16vR3p6Otzc3HDp0iVERUVh3LhxYp/y8nIolUqT4yGiusFEh0iC+vTpg1WrVkEul0OlUsHOzvBX2cXFxeB1UVERgoODsXnz5nuO1bhx4xrF4OTkZPKYoqIiAMD3339vkGAAFeuOLCUxMRERERGYP38+wsLCoFQqsWXLFrz33nsmx7p27dp7Ei9bW1uLxUpEtYuJDpEEubi4oHnz5tXu37FjR3z11Vfw9PS8p6pRycfHB0ePHkXPnj0BVFQukpOT0bFjxyr7t23bFnq9HocOHUJoaOg97ZUVJZ1OJ+4LCgqCg4MDMjMz71sJCgwMFBdWVzpy5Mg/n+RdEhIS4O/vjzfeeEPcd/Xq1Xv6ZWZmIjs7GyqVSnwfGxsbtGrVCl5eXlCpVLh8+TIiIiJMen8iqj+4GJnoXyAiIgKNGjXCsGHD8MsvvyAjIwMHDx7EK6+8gmvXrgEAXn31VSxZsgTbt2/HuXPnMGHCBKP3wHn44YcRGRmJF198Edu3bxeP+fXXXwMA/P39IZPJsGvXLty4cQNFRUVwc3PDtGnTMGXKFGzcuBGXLl3CiRMnsGLFCnGB7/jx43HhwgXExsYiPT0dX3zxBTZs2GDS+bZo0QKZmZnYsmULLl26hOXLl1e5sNrR0RGRkZE4deoUfvnlF7zyyisYOXIkvL29AQDz58/H4sWLsXz5cpw/fx6nT5/G+vXr8f7775sUDxHVHSY6RP8Czs7OOHz4MPz8/DBixAgEBgYiKioKJSUlYoXntddew3PPPYfIyEiEhITAzc0NTz31lNHjrlq1Ck8//TQmTJiA1q1bY9y4cSguLgYAPPTQQ5g/fz5mzpwJLy8vTJw4EQCwcOFCzJkzB4sXL0ZgYCAGDBiA77//HgEBAQAq1s18++232L59O9q1a4fVq1fjrbfeMul8hw4diilTpmDixIlo3749EhISMGfOnHv6NW/eHCNGjMCgQYPQv39/PProowaXj48dOxaffPIJ1q9fj7Zt26JXr17YsGGDGCsR1X8y4X4rDYmIiIgkjhUdIiIislpMdIiIiMhqMdEhIiIiq8VEh4iIiKwWEx0iIiKyWkx0iIiIyGox0SEiIiKrxUSHiIiIrBYTHSIiIrJaTHSIiIjIajHRISIiIqvFRIeIiIis1v8DyOkzBdImv4IAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "712/712 [==============================] - 1s 2ms/step - loss: 0.3174 - accuracy: 0.8317\n",
            "[0.3173559904098511, 0.8316653370857239]\n",
            "712/712 [==============================] - 1s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_2       0.78      0.93      0.85     11391\n",
            "     class 1       0.91      0.74      0.81     11391\n",
            "\n",
            "    accuracy                           0.83     22782\n",
            "   macro avg       0.84      0.83      0.83     22782\n",
            "weighted avg       0.84      0.83      0.83     22782\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/new_df/best_model_by_class1\")"
      ],
      "metadata": {
        "id": "VWrKfxRiRWiB",
        "outputId": "a54373dc-2cef-4297-efe7-36c1c21ebecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCVA7yBE81jHtjamOuU9Hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vajihe-ameri/predict-software-bugs-in-java-classes/blob/main/MLPByClass0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-ChqU41KHj5",
        "outputId": "c96a3fe3-da49-42cd-8b71-dfc38e60f751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2952 sha256=b14d43373af277e09be71202f8304f73051b62b3cccd49088fde9b48f3fbfd09\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn pandas\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons\n",
        "f_measure = tensorflow_addons.metrics.F1Score(num_classes=2, average='macro', threshold=0.5)"
      ],
      "metadata": {
        "id": "ZBLAeiESKP15",
        "outputId": "ae09b0ef-8589-4c8a-bfbe-408fa5990b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_0.csv\")\n",
        "train_features = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_0.csv\")\n",
        "test_target = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_NB_0.csv\")\n",
        "train_target = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_NB_0.csv\")\n"
      ],
      "metadata": {
        "id": "5xz-tCLmKP_v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "file_path = \"/content/drive/MyDrive/new_df/best_model_by_class0.hdf5\""
      ],
      "metadata": {
        "id": "xyLscWXQKw5_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#***Build Model***#\n",
        "#-----------------#\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu', input_dim = train_features.shape[1]))\n",
        "model.add(Dense(80, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(60, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(40, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(20, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OH_PcZrxKctT",
        "outputId": "5e8a3afc-18a6-4c17-f833-4f0922e31b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               8500      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 80)                8080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 60)                4860      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 40)                2440      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                820       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,721\n",
            "Trainable params: 24,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 600, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 1_2', 'class 0']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BytZba-Ld4Z3",
        "outputId": "2980974c-ebff-44d1-8c00-d41f7b1fa27e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1974 - accuracy: 0.8967\n",
            "Epoch 3752: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1994 - accuracy: 0.8947 - val_loss: 4.1873 - val_accuracy: 0.5354\n",
            "Epoch 3753/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2003 - accuracy: 0.8953\n",
            "Epoch 3753: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2029 - accuracy: 0.8937 - val_loss: 4.1330 - val_accuracy: 0.5366\n",
            "Epoch 3754/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2069 - accuracy: 0.8913\n",
            "Epoch 3754: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2092 - accuracy: 0.8898 - val_loss: 4.0116 - val_accuracy: 0.5363\n",
            "Epoch 3755/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2124 - accuracy: 0.8906\n",
            "Epoch 3755: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2146 - accuracy: 0.8896 - val_loss: 4.0870 - val_accuracy: 0.5387\n",
            "Epoch 3756/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2172 - accuracy: 0.8876\n",
            "Epoch 3756: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2206 - accuracy: 0.8850 - val_loss: 4.1287 - val_accuracy: 0.5340\n",
            "Epoch 3757/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2306 - accuracy: 0.8832\n",
            "Epoch 3757: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2295 - accuracy: 0.8837 - val_loss: 4.1662 - val_accuracy: 0.5313\n",
            "Epoch 3758/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2065 - accuracy: 0.8950\n",
            "Epoch 3758: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2086 - accuracy: 0.8933 - val_loss: 4.1559 - val_accuracy: 0.5349\n",
            "Epoch 3759/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2103 - accuracy: 0.8891\n",
            "Epoch 3759: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2116 - accuracy: 0.8878 - val_loss: 4.1204 - val_accuracy: 0.5310\n",
            "Epoch 3760/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2132 - accuracy: 0.8901\n",
            "Epoch 3760: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2155 - accuracy: 0.8873 - val_loss: 4.1510 - val_accuracy: 0.5331\n",
            "Epoch 3761/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2156 - accuracy: 0.8907\n",
            "Epoch 3761: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2168 - accuracy: 0.8892 - val_loss: 4.1194 - val_accuracy: 0.5343\n",
            "Epoch 3762/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2014 - accuracy: 0.8966\n",
            "Epoch 3762: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2032 - accuracy: 0.8947 - val_loss: 4.0182 - val_accuracy: 0.5413\n",
            "Epoch 3763/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2010 - accuracy: 0.8923\n",
            "Epoch 3763: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2011 - accuracy: 0.8917 - val_loss: 4.1602 - val_accuracy: 0.5369\n",
            "Epoch 3764/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.8934\n",
            "Epoch 3764: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2042 - accuracy: 0.8924 - val_loss: 4.1607 - val_accuracy: 0.5284\n",
            "Epoch 3765/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2033 - accuracy: 0.8938\n",
            "Epoch 3765: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2035 - accuracy: 0.8928 - val_loss: 4.3517 - val_accuracy: 0.5369\n",
            "Epoch 3766/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1996 - accuracy: 0.8933\n",
            "Epoch 3766: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2014 - accuracy: 0.8925 - val_loss: 4.1382 - val_accuracy: 0.5284\n",
            "Epoch 3767/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.8957\n",
            "Epoch 3767: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1978 - accuracy: 0.8957 - val_loss: 4.0421 - val_accuracy: 0.5425\n",
            "Epoch 3768/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.8964\n",
            "Epoch 3768: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1961 - accuracy: 0.8964 - val_loss: 4.3210 - val_accuracy: 0.5331\n",
            "Epoch 3769/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1995 - accuracy: 0.8959\n",
            "Epoch 3769: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1993 - accuracy: 0.8961 - val_loss: 4.1440 - val_accuracy: 0.5413\n",
            "Epoch 3770/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2006 - accuracy: 0.8921\n",
            "Epoch 3770: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2017 - accuracy: 0.8923 - val_loss: 4.2925 - val_accuracy: 0.5340\n",
            "Epoch 3771/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2074 - accuracy: 0.8908\n",
            "Epoch 3771: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2076 - accuracy: 0.8906 - val_loss: 4.0636 - val_accuracy: 0.5422\n",
            "Epoch 3772/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2075 - accuracy: 0.8896\n",
            "Epoch 3772: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2082 - accuracy: 0.8896 - val_loss: 4.1734 - val_accuracy: 0.5340\n",
            "Epoch 3773/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2017 - accuracy: 0.8944\n",
            "Epoch 3773: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2034 - accuracy: 0.8926 - val_loss: 4.1070 - val_accuracy: 0.5293\n",
            "Epoch 3774/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.8942\n",
            "Epoch 3774: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2010 - accuracy: 0.8942 - val_loss: 4.1824 - val_accuracy: 0.5378\n",
            "Epoch 3775/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2018 - accuracy: 0.8914\n",
            "Epoch 3775: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2034 - accuracy: 0.8890 - val_loss: 4.1419 - val_accuracy: 0.5302\n",
            "Epoch 3776/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1993 - accuracy: 0.8966\n",
            "Epoch 3776: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2011 - accuracy: 0.8954 - val_loss: 4.1359 - val_accuracy: 0.5340\n",
            "Epoch 3777/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2063 - accuracy: 0.8953\n",
            "Epoch 3777: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2107 - accuracy: 0.8922 - val_loss: 4.0383 - val_accuracy: 0.5369\n",
            "Epoch 3778/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2137 - accuracy: 0.8875\n",
            "Epoch 3778: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2131 - accuracy: 0.8878 - val_loss: 4.1102 - val_accuracy: 0.5231\n",
            "Epoch 3779/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2157 - accuracy: 0.8873\n",
            "Epoch 3779: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2155 - accuracy: 0.8879 - val_loss: 4.2013 - val_accuracy: 0.5393\n",
            "Epoch 3780/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2046 - accuracy: 0.8944\n",
            "Epoch 3780: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2077 - accuracy: 0.8920 - val_loss: 4.0525 - val_accuracy: 0.5390\n",
            "Epoch 3781/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.8859\n",
            "Epoch 3781: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2186 - accuracy: 0.8859 - val_loss: 4.0313 - val_accuracy: 0.5258\n",
            "Epoch 3782/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2087 - accuracy: 0.8920\n",
            "Epoch 3782: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2100 - accuracy: 0.8906 - val_loss: 4.1609 - val_accuracy: 0.5305\n",
            "Epoch 3783/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2014 - accuracy: 0.8937\n",
            "Epoch 3783: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2021 - accuracy: 0.8938 - val_loss: 4.0782 - val_accuracy: 0.5249\n",
            "Epoch 3784/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2075 - accuracy: 0.8877\n",
            "Epoch 3784: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2089 - accuracy: 0.8870 - val_loss: 4.1011 - val_accuracy: 0.5316\n",
            "Epoch 3785/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1984 - accuracy: 0.8963\n",
            "Epoch 3785: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1997 - accuracy: 0.8957 - val_loss: 3.9906 - val_accuracy: 0.5354\n",
            "Epoch 3786/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2084 - accuracy: 0.8892\n",
            "Epoch 3786: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2091 - accuracy: 0.8886 - val_loss: 4.0151 - val_accuracy: 0.5331\n",
            "Epoch 3787/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2020 - accuracy: 0.8968\n",
            "Epoch 3787: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2084 - accuracy: 0.8914 - val_loss: 4.0733 - val_accuracy: 0.5343\n",
            "Epoch 3788/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2061 - accuracy: 0.8923\n",
            "Epoch 3788: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2070 - accuracy: 0.8920 - val_loss: 4.1113 - val_accuracy: 0.5343\n",
            "Epoch 3789/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2002 - accuracy: 0.8958\n",
            "Epoch 3789: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2008 - accuracy: 0.8951 - val_loss: 4.0325 - val_accuracy: 0.5337\n",
            "Epoch 3790/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1994 - accuracy: 0.8944\n",
            "Epoch 3790: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2015 - accuracy: 0.8931 - val_loss: 4.2068 - val_accuracy: 0.5316\n",
            "Epoch 3791/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1923 - accuracy: 0.8977\n",
            "Epoch 3791: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1937 - accuracy: 0.8958 - val_loss: 4.1735 - val_accuracy: 0.5378\n",
            "Epoch 3792/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1924 - accuracy: 0.9003\n",
            "Epoch 3792: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1948 - accuracy: 0.8985 - val_loss: 4.2739 - val_accuracy: 0.5369\n",
            "Epoch 3793/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1910 - accuracy: 0.9002\n",
            "Epoch 3793: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1927 - accuracy: 0.8980 - val_loss: 4.1301 - val_accuracy: 0.5308\n",
            "Epoch 3794/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1987 - accuracy: 0.8962\n",
            "Epoch 3794: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1998 - accuracy: 0.8953 - val_loss: 4.2021 - val_accuracy: 0.5343\n",
            "Epoch 3795/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.8954\n",
            "Epoch 3795: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2039 - accuracy: 0.8954 - val_loss: 4.1887 - val_accuracy: 0.5328\n",
            "Epoch 3796/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2160 - accuracy: 0.8886\n",
            "Epoch 3796: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2161 - accuracy: 0.8881 - val_loss: 4.1543 - val_accuracy: 0.5363\n",
            "Epoch 3797/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2146 - accuracy: 0.8897\n",
            "Epoch 3797: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2161 - accuracy: 0.8889 - val_loss: 4.1091 - val_accuracy: 0.5325\n",
            "Epoch 3798/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.8859\n",
            "Epoch 3798: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2161 - accuracy: 0.8859 - val_loss: 4.1206 - val_accuracy: 0.5410\n",
            "Epoch 3799/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2148 - accuracy: 0.8889\n",
            "Epoch 3799: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2134 - accuracy: 0.8890 - val_loss: 4.1167 - val_accuracy: 0.5328\n",
            "Epoch 3800/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1990 - accuracy: 0.8948\n",
            "Epoch 3800: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1995 - accuracy: 0.8950 - val_loss: 4.2866 - val_accuracy: 0.5387\n",
            "Epoch 3801/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.8928\n",
            "Epoch 3801: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2019 - accuracy: 0.8926 - val_loss: 4.1794 - val_accuracy: 0.5308\n",
            "Epoch 3802/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2044 - accuracy: 0.8902\n",
            "Epoch 3802: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2073 - accuracy: 0.8881 - val_loss: 4.2520 - val_accuracy: 0.5264\n",
            "Epoch 3803/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2058 - accuracy: 0.8935\n",
            "Epoch 3803: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2080 - accuracy: 0.8926 - val_loss: 4.1793 - val_accuracy: 0.5351\n",
            "Epoch 3804/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.8944\n",
            "Epoch 3804: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2015 - accuracy: 0.8944 - val_loss: 4.2050 - val_accuracy: 0.5302\n",
            "Epoch 3805/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2148 - accuracy: 0.8878\n",
            "Epoch 3805: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2159 - accuracy: 0.8868 - val_loss: 4.1460 - val_accuracy: 0.5331\n",
            "Epoch 3806/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2046 - accuracy: 0.8964\n",
            "Epoch 3806: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2075 - accuracy: 0.8942 - val_loss: 4.0473 - val_accuracy: 0.5296\n",
            "Epoch 3807/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2121 - accuracy: 0.8897\n",
            "Epoch 3807: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2131 - accuracy: 0.8881 - val_loss: 4.0500 - val_accuracy: 0.5369\n",
            "Epoch 3808/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2055 - accuracy: 0.8931\n",
            "Epoch 3808: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2052 - accuracy: 0.8929 - val_loss: 4.0069 - val_accuracy: 0.5217\n",
            "Epoch 3809/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1991 - accuracy: 0.8967\n",
            "Epoch 3809: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2029 - accuracy: 0.8928 - val_loss: 4.1302 - val_accuracy: 0.5372\n",
            "Epoch 3810/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1978 - accuracy: 0.8972\n",
            "Epoch 3810: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2004 - accuracy: 0.8964 - val_loss: 4.1064 - val_accuracy: 0.5281\n",
            "Epoch 3811/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2029 - accuracy: 0.8943\n",
            "Epoch 3811: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2040 - accuracy: 0.8930 - val_loss: 4.1956 - val_accuracy: 0.5378\n",
            "Epoch 3812/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2104 - accuracy: 0.8925\n",
            "Epoch 3812: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2170 - accuracy: 0.8899 - val_loss: 4.0917 - val_accuracy: 0.5296\n",
            "Epoch 3813/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2313 - accuracy: 0.8806\n",
            "Epoch 3813: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2313 - accuracy: 0.8808 - val_loss: 4.1082 - val_accuracy: 0.5299\n",
            "Epoch 3814/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2194 - accuracy: 0.8861\n",
            "Epoch 3814: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2197 - accuracy: 0.8844 - val_loss: 4.2593 - val_accuracy: 0.5310\n",
            "Epoch 3815/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.8914\n",
            "Epoch 3815: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2082 - accuracy: 0.8914 - val_loss: 4.2440 - val_accuracy: 0.5357\n",
            "Epoch 3816/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.8913\n",
            "Epoch 3816: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2070 - accuracy: 0.8913 - val_loss: 4.3042 - val_accuracy: 0.5360\n",
            "Epoch 3817/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1965 - accuracy: 0.8946\n",
            "Epoch 3817: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1965 - accuracy: 0.8944 - val_loss: 4.0561 - val_accuracy: 0.5310\n",
            "Epoch 3818/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2014 - accuracy: 0.8933\n",
            "Epoch 3818: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2030 - accuracy: 0.8920 - val_loss: 4.1491 - val_accuracy: 0.5308\n",
            "Epoch 3819/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1999 - accuracy: 0.8925\n",
            "Epoch 3819: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2011 - accuracy: 0.8917 - val_loss: 4.1640 - val_accuracy: 0.5328\n",
            "Epoch 3820/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.8929\n",
            "Epoch 3820: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2031 - accuracy: 0.8929 - val_loss: 4.2222 - val_accuracy: 0.5381\n",
            "Epoch 3821/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1940 - accuracy: 0.8991\n",
            "Epoch 3821: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1984 - accuracy: 0.8956 - val_loss: 4.2707 - val_accuracy: 0.5390\n",
            "Epoch 3822/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2062 - accuracy: 0.8938\n",
            "Epoch 3822: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2070 - accuracy: 0.8929 - val_loss: 4.2893 - val_accuracy: 0.5337\n",
            "Epoch 3823/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2053 - accuracy: 0.8913\n",
            "Epoch 3823: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2062 - accuracy: 0.8904 - val_loss: 4.0837 - val_accuracy: 0.5272\n",
            "Epoch 3824/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1996 - accuracy: 0.8963\n",
            "Epoch 3824: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2005 - accuracy: 0.8955 - val_loss: 4.1476 - val_accuracy: 0.5328\n",
            "Epoch 3825/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2089 - accuracy: 0.8909\n",
            "Epoch 3825: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2127 - accuracy: 0.8888 - val_loss: 4.0884 - val_accuracy: 0.5261\n",
            "Epoch 3826/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2188 - accuracy: 0.8880\n",
            "Epoch 3826: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2222 - accuracy: 0.8860 - val_loss: 3.9751 - val_accuracy: 0.5384\n",
            "Epoch 3827/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.8870\n",
            "Epoch 3827: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2225 - accuracy: 0.8870 - val_loss: 3.9967 - val_accuracy: 0.5422\n",
            "Epoch 3828/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2115 - accuracy: 0.8914\n",
            "Epoch 3828: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2126 - accuracy: 0.8902 - val_loss: 4.2241 - val_accuracy: 0.5343\n",
            "Epoch 3829/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2131 - accuracy: 0.8889\n",
            "Epoch 3829: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2118 - accuracy: 0.8890 - val_loss: 4.1731 - val_accuracy: 0.5357\n",
            "Epoch 3830/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2144 - accuracy: 0.8903\n",
            "Epoch 3830: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2176 - accuracy: 0.8880 - val_loss: 4.1764 - val_accuracy: 0.5293\n",
            "Epoch 3831/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.8938\n",
            "Epoch 3831: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2053 - accuracy: 0.8938 - val_loss: 4.0370 - val_accuracy: 0.5264\n",
            "Epoch 3832/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.8927\n",
            "Epoch 3832: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2033 - accuracy: 0.8927 - val_loss: 4.0653 - val_accuracy: 0.5290\n",
            "Epoch 3833/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.8929\n",
            "Epoch 3833: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2044 - accuracy: 0.8929 - val_loss: 4.0239 - val_accuracy: 0.5346\n",
            "Epoch 3834/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2062 - accuracy: 0.8915\n",
            "Epoch 3834: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2066 - accuracy: 0.8906 - val_loss: 4.0876 - val_accuracy: 0.5281\n",
            "Epoch 3835/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.8904\n",
            "Epoch 3835: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2087 - accuracy: 0.8904 - val_loss: 4.1135 - val_accuracy: 0.5431\n",
            "Epoch 3836/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2166 - accuracy: 0.8881\n",
            "Epoch 3836: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2181 - accuracy: 0.8874 - val_loss: 4.1104 - val_accuracy: 0.5357\n",
            "Epoch 3837/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.8841\n",
            "Epoch 3837: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2204 - accuracy: 0.8841 - val_loss: 3.9863 - val_accuracy: 0.5354\n",
            "Epoch 3838/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1985 - accuracy: 0.8962\n",
            "Epoch 3838: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2010 - accuracy: 0.8947 - val_loss: 4.1812 - val_accuracy: 0.5337\n",
            "Epoch 3839/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2073 - accuracy: 0.8951\n",
            "Epoch 3839: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2099 - accuracy: 0.8939 - val_loss: 4.1467 - val_accuracy: 0.5366\n",
            "Epoch 3840/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.8928\n",
            "Epoch 3840: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2067 - accuracy: 0.8928 - val_loss: 3.9822 - val_accuracy: 0.5360\n",
            "Epoch 3841/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.8926\n",
            "Epoch 3841: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2035 - accuracy: 0.8926 - val_loss: 4.1553 - val_accuracy: 0.5375\n",
            "Epoch 3842/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1940 - accuracy: 0.8998\n",
            "Epoch 3842: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1966 - accuracy: 0.8969 - val_loss: 4.1749 - val_accuracy: 0.5343\n",
            "Epoch 3843/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.8967\n",
            "Epoch 3843: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1978 - accuracy: 0.8967 - val_loss: 4.0878 - val_accuracy: 0.5328\n",
            "Epoch 3844/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1997 - accuracy: 0.8949\n",
            "Epoch 3844: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2005 - accuracy: 0.8947 - val_loss: 4.0450 - val_accuracy: 0.5302\n",
            "Epoch 3845/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2009 - accuracy: 0.8960\n",
            "Epoch 3845: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2024 - accuracy: 0.8946 - val_loss: 4.3409 - val_accuracy: 0.5363\n",
            "Epoch 3846/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2022 - accuracy: 0.8936\n",
            "Epoch 3846: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2057 - accuracy: 0.8901 - val_loss: 4.1117 - val_accuracy: 0.5372\n",
            "Epoch 3847/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1965 - accuracy: 0.8968\n",
            "Epoch 3847: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1990 - accuracy: 0.8951 - val_loss: 4.2452 - val_accuracy: 0.5372\n",
            "Epoch 3848/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.8955\n",
            "Epoch 3848: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2014 - accuracy: 0.8949 - val_loss: 4.1244 - val_accuracy: 0.5340\n",
            "Epoch 3849/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1994 - accuracy: 0.8947\n",
            "Epoch 3849: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1994 - accuracy: 0.8947 - val_loss: 4.2910 - val_accuracy: 0.5372\n",
            "Epoch 3850/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1967 - accuracy: 0.8971\n",
            "Epoch 3850: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1975 - accuracy: 0.8966 - val_loss: 4.1401 - val_accuracy: 0.5387\n",
            "Epoch 3851/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.8969\n",
            "Epoch 3851: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1968 - accuracy: 0.8969 - val_loss: 4.1892 - val_accuracy: 0.5346\n",
            "Epoch 3852/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2075 - accuracy: 0.8898\n",
            "Epoch 3852: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2074 - accuracy: 0.8895 - val_loss: 4.0841 - val_accuracy: 0.5366\n",
            "Epoch 3853/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2171 - accuracy: 0.8898\n",
            "Epoch 3853: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2209 - accuracy: 0.8876 - val_loss: 3.9064 - val_accuracy: 0.5293\n",
            "Epoch 3854/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.8777\n",
            "Epoch 3854: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2446 - accuracy: 0.8777 - val_loss: 3.9796 - val_accuracy: 0.5296\n",
            "Epoch 3855/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2192 - accuracy: 0.8857\n",
            "Epoch 3855: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2204 - accuracy: 0.8850 - val_loss: 4.0663 - val_accuracy: 0.5357\n",
            "Epoch 3856/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2162 - accuracy: 0.8875\n",
            "Epoch 3856: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2190 - accuracy: 0.8849 - val_loss: 4.1519 - val_accuracy: 0.5425\n",
            "Epoch 3857/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2107 - accuracy: 0.8911\n",
            "Epoch 3857: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2130 - accuracy: 0.8900 - val_loss: 3.9813 - val_accuracy: 0.5340\n",
            "Epoch 3858/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2007 - accuracy: 0.8943\n",
            "Epoch 3858: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2003 - accuracy: 0.8946 - val_loss: 4.2421 - val_accuracy: 0.5404\n",
            "Epoch 3859/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1962 - accuracy: 0.8993\n",
            "Epoch 3859: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2009 - accuracy: 0.8964 - val_loss: 3.9037 - val_accuracy: 0.5299\n",
            "Epoch 3860/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2032 - accuracy: 0.8925\n",
            "Epoch 3860: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2033 - accuracy: 0.8925 - val_loss: 4.1401 - val_accuracy: 0.5316\n",
            "Epoch 3861/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1941 - accuracy: 0.8980\n",
            "Epoch 3861: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1963 - accuracy: 0.8969 - val_loss: 4.2001 - val_accuracy: 0.5343\n",
            "Epoch 3862/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1954 - accuracy: 0.8969\n",
            "Epoch 3862: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1966 - accuracy: 0.8958 - val_loss: 4.0600 - val_accuracy: 0.5349\n",
            "Epoch 3863/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2025 - accuracy: 0.8957\n",
            "Epoch 3863: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2041 - accuracy: 0.8944 - val_loss: 3.9052 - val_accuracy: 0.5351\n",
            "Epoch 3864/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2045 - accuracy: 0.8932\n",
            "Epoch 3864: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2081 - accuracy: 0.8906 - val_loss: 4.2130 - val_accuracy: 0.5410\n",
            "Epoch 3865/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2090 - accuracy: 0.8931\n",
            "Epoch 3865: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2091 - accuracy: 0.8923 - val_loss: 4.0979 - val_accuracy: 0.5375\n",
            "Epoch 3866/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2091 - accuracy: 0.8906\n",
            "Epoch 3866: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2138 - accuracy: 0.8874 - val_loss: 4.1711 - val_accuracy: 0.5398\n",
            "Epoch 3867/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2022 - accuracy: 0.8953\n",
            "Epoch 3867: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2028 - accuracy: 0.8934 - val_loss: 4.2250 - val_accuracy: 0.5363\n",
            "Epoch 3868/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1971 - accuracy: 0.8976\n",
            "Epoch 3868: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2000 - accuracy: 0.8951 - val_loss: 4.3054 - val_accuracy: 0.5334\n",
            "Epoch 3869/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1970 - accuracy: 0.8945\n",
            "Epoch 3869: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1969 - accuracy: 0.8945 - val_loss: 4.1287 - val_accuracy: 0.5357\n",
            "Epoch 3870/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2008 - accuracy: 0.8939\n",
            "Epoch 3870: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2001 - accuracy: 0.8940 - val_loss: 4.1803 - val_accuracy: 0.5413\n",
            "Epoch 3871/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1919 - accuracy: 0.8978\n",
            "Epoch 3871: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1944 - accuracy: 0.8960 - val_loss: 4.1920 - val_accuracy: 0.5401\n",
            "Epoch 3872/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1998 - accuracy: 0.8975\n",
            "Epoch 3872: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2046 - accuracy: 0.8943 - val_loss: 4.3600 - val_accuracy: 0.5281\n",
            "Epoch 3873/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2148 - accuracy: 0.8884\n",
            "Epoch 3873: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2183 - accuracy: 0.8868 - val_loss: 4.0418 - val_accuracy: 0.5220\n",
            "Epoch 3874/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2075 - accuracy: 0.8904\n",
            "Epoch 3874: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2102 - accuracy: 0.8877 - val_loss: 4.0321 - val_accuracy: 0.5366\n",
            "Epoch 3875/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2055 - accuracy: 0.8928\n",
            "Epoch 3875: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2073 - accuracy: 0.8914 - val_loss: 3.9027 - val_accuracy: 0.5360\n",
            "Epoch 3876/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2095 - accuracy: 0.8903\n",
            "Epoch 3876: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2121 - accuracy: 0.8873 - val_loss: 3.9506 - val_accuracy: 0.5308\n",
            "Epoch 3877/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2020 - accuracy: 0.8952\n",
            "Epoch 3877: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2016 - accuracy: 0.8955 - val_loss: 4.0305 - val_accuracy: 0.5334\n",
            "Epoch 3878/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2090 - accuracy: 0.8943\n",
            "Epoch 3878: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2147 - accuracy: 0.8887 - val_loss: 4.3358 - val_accuracy: 0.5434\n",
            "Epoch 3879/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2339 - accuracy: 0.8837\n",
            "Epoch 3879: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2351 - accuracy: 0.8824 - val_loss: 4.0041 - val_accuracy: 0.5357\n",
            "Epoch 3880/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2108 - accuracy: 0.8908\n",
            "Epoch 3880: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2113 - accuracy: 0.8903 - val_loss: 4.1410 - val_accuracy: 0.5351\n",
            "Epoch 3881/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2081 - accuracy: 0.8936\n",
            "Epoch 3881: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2133 - accuracy: 0.8910 - val_loss: 3.9446 - val_accuracy: 0.5343\n",
            "Epoch 3882/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.8928\n",
            "Epoch 3882: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2099 - accuracy: 0.8928 - val_loss: 4.1375 - val_accuracy: 0.5395\n",
            "Epoch 3883/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2239 - accuracy: 0.8845\n",
            "Epoch 3883: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2271 - accuracy: 0.8824 - val_loss: 4.1081 - val_accuracy: 0.5278\n",
            "Epoch 3884/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2284 - accuracy: 0.8822\n",
            "Epoch 3884: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2317 - accuracy: 0.8795 - val_loss: 4.2494 - val_accuracy: 0.5328\n",
            "Epoch 3885/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2209 - accuracy: 0.8876\n",
            "Epoch 3885: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2214 - accuracy: 0.8876 - val_loss: 3.9810 - val_accuracy: 0.5351\n",
            "Epoch 3886/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2006 - accuracy: 0.8960\n",
            "Epoch 3886: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2013 - accuracy: 0.8957 - val_loss: 4.0304 - val_accuracy: 0.5354\n",
            "Epoch 3887/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2148 - accuracy: 0.8902\n",
            "Epoch 3887: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2157 - accuracy: 0.8896 - val_loss: 4.1325 - val_accuracy: 0.5349\n",
            "Epoch 3888/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2001 - accuracy: 0.8922\n",
            "Epoch 3888: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2001 - accuracy: 0.8922 - val_loss: 4.0583 - val_accuracy: 0.5375\n",
            "Epoch 3889/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1920 - accuracy: 0.8993\n",
            "Epoch 3889: loss did not improve from 0.19233\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1950 - accuracy: 0.8970 - val_loss: 4.0284 - val_accuracy: 0.5272\n",
            "Epoch 3890/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1849 - accuracy: 0.9046\n",
            "Epoch 3890: loss improved from 0.19233 to 0.19023, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.1902 - accuracy: 0.9005 - val_loss: 4.1307 - val_accuracy: 0.5393\n",
            "Epoch 3891/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1969 - accuracy: 0.8977\n",
            "Epoch 3891: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1998 - accuracy: 0.8955 - val_loss: 4.2181 - val_accuracy: 0.5322\n",
            "Epoch 3892/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2011 - accuracy: 0.8925\n",
            "Epoch 3892: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2018 - accuracy: 0.8919 - val_loss: 4.0917 - val_accuracy: 0.5310\n",
            "Epoch 3893/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1880 - accuracy: 0.9012\n",
            "Epoch 3893: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1923 - accuracy: 0.8980 - val_loss: 4.1033 - val_accuracy: 0.5393\n",
            "Epoch 3894/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2033 - accuracy: 0.8913\n",
            "Epoch 3894: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2035 - accuracy: 0.8910 - val_loss: 4.2097 - val_accuracy: 0.5360\n",
            "Epoch 3895/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1949 - accuracy: 0.8973\n",
            "Epoch 3895: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1977 - accuracy: 0.8956 - val_loss: 4.0675 - val_accuracy: 0.5328\n",
            "Epoch 3896/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1950 - accuracy: 0.8988\n",
            "Epoch 3896: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1957 - accuracy: 0.8982 - val_loss: 4.0420 - val_accuracy: 0.5319\n",
            "Epoch 3897/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1892 - accuracy: 0.8995\n",
            "Epoch 3897: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1938 - accuracy: 0.8966 - val_loss: 4.1368 - val_accuracy: 0.5439\n",
            "Epoch 3898/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1932 - accuracy: 0.8989\n",
            "Epoch 3898: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1940 - accuracy: 0.8984 - val_loss: 4.2074 - val_accuracy: 0.5334\n",
            "Epoch 3899/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1946 - accuracy: 0.8973\n",
            "Epoch 3899: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1953 - accuracy: 0.8970 - val_loss: 4.0273 - val_accuracy: 0.5378\n",
            "Epoch 3900/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1946 - accuracy: 0.8986\n",
            "Epoch 3900: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1966 - accuracy: 0.8964 - val_loss: 4.2485 - val_accuracy: 0.5299\n",
            "Epoch 3901/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2052 - accuracy: 0.8927\n",
            "Epoch 3901: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2059 - accuracy: 0.8918 - val_loss: 3.9888 - val_accuracy: 0.5281\n",
            "Epoch 3902/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2105 - accuracy: 0.8905\n",
            "Epoch 3902: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2117 - accuracy: 0.8881 - val_loss: 4.0345 - val_accuracy: 0.5313\n",
            "Epoch 3903/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1957 - accuracy: 0.8995\n",
            "Epoch 3903: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1974 - accuracy: 0.8976 - val_loss: 4.1651 - val_accuracy: 0.5349\n",
            "Epoch 3904/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2042 - accuracy: 0.8907\n",
            "Epoch 3904: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2042 - accuracy: 0.8907 - val_loss: 4.1377 - val_accuracy: 0.5316\n",
            "Epoch 3905/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1979 - accuracy: 0.8949\n",
            "Epoch 3905: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1997 - accuracy: 0.8933 - val_loss: 4.2342 - val_accuracy: 0.5284\n",
            "Epoch 3906/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1981 - accuracy: 0.8973\n",
            "Epoch 3906: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1986 - accuracy: 0.8965 - val_loss: 4.2153 - val_accuracy: 0.5316\n",
            "Epoch 3907/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1997 - accuracy: 0.8950\n",
            "Epoch 3907: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2009 - accuracy: 0.8941 - val_loss: 4.1659 - val_accuracy: 0.5351\n",
            "Epoch 3908/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.8926\n",
            "Epoch 3908: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2047 - accuracy: 0.8926 - val_loss: 4.0488 - val_accuracy: 0.5319\n",
            "Epoch 3909/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.8953\n",
            "Epoch 3909: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1995 - accuracy: 0.8944 - val_loss: 4.0961 - val_accuracy: 0.5334\n",
            "Epoch 3910/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1941 - accuracy: 0.8942\n",
            "Epoch 3910: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1947 - accuracy: 0.8948 - val_loss: 4.0426 - val_accuracy: 0.5375\n",
            "Epoch 3911/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1969 - accuracy: 0.8992\n",
            "Epoch 3911: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1963 - accuracy: 0.8991 - val_loss: 4.0898 - val_accuracy: 0.5313\n",
            "Epoch 3912/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1961 - accuracy: 0.8982\n",
            "Epoch 3912: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1985 - accuracy: 0.8950 - val_loss: 4.2325 - val_accuracy: 0.5331\n",
            "Epoch 3913/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1979 - accuracy: 0.8964\n",
            "Epoch 3913: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1982 - accuracy: 0.8958 - val_loss: 4.3126 - val_accuracy: 0.5328\n",
            "Epoch 3914/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1987 - accuracy: 0.8937\n",
            "Epoch 3914: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1985 - accuracy: 0.8937 - val_loss: 4.0760 - val_accuracy: 0.5302\n",
            "Epoch 3915/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1992 - accuracy: 0.8947\n",
            "Epoch 3915: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2020 - accuracy: 0.8939 - val_loss: 4.0764 - val_accuracy: 0.5293\n",
            "Epoch 3916/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1955 - accuracy: 0.8991\n",
            "Epoch 3916: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1961 - accuracy: 0.8975 - val_loss: 3.9991 - val_accuracy: 0.5234\n",
            "Epoch 3917/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2083 - accuracy: 0.8911\n",
            "Epoch 3917: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2114 - accuracy: 0.8904 - val_loss: 3.9374 - val_accuracy: 0.5343\n",
            "Epoch 3918/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2150 - accuracy: 0.8897\n",
            "Epoch 3918: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2155 - accuracy: 0.8883 - val_loss: 4.1041 - val_accuracy: 0.5281\n",
            "Epoch 3919/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.8909\n",
            "Epoch 3919: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2071 - accuracy: 0.8909 - val_loss: 4.0426 - val_accuracy: 0.5316\n",
            "Epoch 3920/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2041 - accuracy: 0.8944\n",
            "Epoch 3920: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2055 - accuracy: 0.8934 - val_loss: 4.0725 - val_accuracy: 0.5343\n",
            "Epoch 3921/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2012 - accuracy: 0.8973\n",
            "Epoch 3921: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2065 - accuracy: 0.8931 - val_loss: 4.0948 - val_accuracy: 0.5372\n",
            "Epoch 3922/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.8923\n",
            "Epoch 3922: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2009 - accuracy: 0.8923 - val_loss: 4.2529 - val_accuracy: 0.5340\n",
            "Epoch 3923/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1963 - accuracy: 0.8992\n",
            "Epoch 3923: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2014 - accuracy: 0.8949 - val_loss: 4.2466 - val_accuracy: 0.5375\n",
            "Epoch 3924/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1993 - accuracy: 0.8960\n",
            "Epoch 3924: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2032 - accuracy: 0.8933 - val_loss: 4.0971 - val_accuracy: 0.5422\n",
            "Epoch 3925/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2048 - accuracy: 0.8935\n",
            "Epoch 3925: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2074 - accuracy: 0.8925 - val_loss: 4.0994 - val_accuracy: 0.5334\n",
            "Epoch 3926/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2176 - accuracy: 0.8862\n",
            "Epoch 3926: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2190 - accuracy: 0.8849 - val_loss: 4.2077 - val_accuracy: 0.5328\n",
            "Epoch 3927/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2219 - accuracy: 0.8848\n",
            "Epoch 3927: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2220 - accuracy: 0.8835 - val_loss: 4.0570 - val_accuracy: 0.5378\n",
            "Epoch 3928/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - accuracy: 0.8928\n",
            "Epoch 3928: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2061 - accuracy: 0.8924 - val_loss: 4.0282 - val_accuracy: 0.5299\n",
            "Epoch 3929/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2039 - accuracy: 0.8928\n",
            "Epoch 3929: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2060 - accuracy: 0.8920 - val_loss: 4.1769 - val_accuracy: 0.5296\n",
            "Epoch 3930/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1916 - accuracy: 0.9003\n",
            "Epoch 3930: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1949 - accuracy: 0.8975 - val_loss: 4.1778 - val_accuracy: 0.5331\n",
            "Epoch 3931/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1992 - accuracy: 0.8985\n",
            "Epoch 3931: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1991 - accuracy: 0.8985 - val_loss: 4.0189 - val_accuracy: 0.5325\n",
            "Epoch 3932/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.9005\n",
            "Epoch 3932: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1911 - accuracy: 0.8994 - val_loss: 4.1132 - val_accuracy: 0.5363\n",
            "Epoch 3933/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.8938\n",
            "Epoch 3933: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1996 - accuracy: 0.8938 - val_loss: 4.1980 - val_accuracy: 0.5328\n",
            "Epoch 3934/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1918 - accuracy: 0.8975\n",
            "Epoch 3934: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1942 - accuracy: 0.8950 - val_loss: 4.1515 - val_accuracy: 0.5337\n",
            "Epoch 3935/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1959 - accuracy: 0.8963\n",
            "Epoch 3935: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1979 - accuracy: 0.8945 - val_loss: 4.2550 - val_accuracy: 0.5340\n",
            "Epoch 3936/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2127 - accuracy: 0.8902\n",
            "Epoch 3936: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2118 - accuracy: 0.8904 - val_loss: 4.1878 - val_accuracy: 0.5346\n",
            "Epoch 3937/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2104 - accuracy: 0.8882\n",
            "Epoch 3937: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2117 - accuracy: 0.8875 - val_loss: 4.2117 - val_accuracy: 0.5387\n",
            "Epoch 3938/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2139 - accuracy: 0.8897\n",
            "Epoch 3938: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2143 - accuracy: 0.8888 - val_loss: 4.1797 - val_accuracy: 0.5369\n",
            "Epoch 3939/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2044 - accuracy: 0.8932\n",
            "Epoch 3939: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2060 - accuracy: 0.8921 - val_loss: 4.2391 - val_accuracy: 0.5275\n",
            "Epoch 3940/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2038 - accuracy: 0.8957\n",
            "Epoch 3940: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2065 - accuracy: 0.8932 - val_loss: 4.1524 - val_accuracy: 0.5299\n",
            "Epoch 3941/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2078 - accuracy: 0.8917\n",
            "Epoch 3941: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2078 - accuracy: 0.8918 - val_loss: 4.1363 - val_accuracy: 0.5316\n",
            "Epoch 3942/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1990 - accuracy: 0.8991\n",
            "Epoch 3942: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2028 - accuracy: 0.8961 - val_loss: 4.0866 - val_accuracy: 0.5360\n",
            "Epoch 3943/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1962 - accuracy: 0.8944\n",
            "Epoch 3943: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1973 - accuracy: 0.8933 - val_loss: 4.0625 - val_accuracy: 0.5325\n",
            "Epoch 3944/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.8979\n",
            "Epoch 3944: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1938 - accuracy: 0.8979 - val_loss: 4.0703 - val_accuracy: 0.5322\n",
            "Epoch 3945/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1965 - accuracy: 0.8962\n",
            "Epoch 3945: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1960 - accuracy: 0.8965 - val_loss: 4.1337 - val_accuracy: 0.5322\n",
            "Epoch 3946/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2019 - accuracy: 0.8937\n",
            "Epoch 3946: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2040 - accuracy: 0.8928 - val_loss: 4.2463 - val_accuracy: 0.5340\n",
            "Epoch 3947/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2168 - accuracy: 0.8877\n",
            "Epoch 3947: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2172 - accuracy: 0.8873 - val_loss: 4.1484 - val_accuracy: 0.5360\n",
            "Epoch 3948/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.8895\n",
            "Epoch 3948: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2105 - accuracy: 0.8895 - val_loss: 4.1675 - val_accuracy: 0.5357\n",
            "Epoch 3949/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.8915\n",
            "Epoch 3949: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2067 - accuracy: 0.8915 - val_loss: 4.0472 - val_accuracy: 0.5387\n",
            "Epoch 3950/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2132 - accuracy: 0.8902\n",
            "Epoch 3950: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2133 - accuracy: 0.8890 - val_loss: 3.9633 - val_accuracy: 0.5310\n",
            "Epoch 3951/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2024 - accuracy: 0.8966\n",
            "Epoch 3951: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2059 - accuracy: 0.8931 - val_loss: 4.1656 - val_accuracy: 0.5319\n",
            "Epoch 3952/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2026 - accuracy: 0.8948\n",
            "Epoch 3952: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2058 - accuracy: 0.8933 - val_loss: 4.0210 - val_accuracy: 0.5249\n",
            "Epoch 3953/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2173 - accuracy: 0.8873\n",
            "Epoch 3953: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2196 - accuracy: 0.8855 - val_loss: 4.0406 - val_accuracy: 0.5264\n",
            "Epoch 3954/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2232 - accuracy: 0.8848\n",
            "Epoch 3954: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2266 - accuracy: 0.8826 - val_loss: 4.0752 - val_accuracy: 0.5369\n",
            "Epoch 3955/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2209 - accuracy: 0.8864\n",
            "Epoch 3955: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2228 - accuracy: 0.8854 - val_loss: 4.0630 - val_accuracy: 0.5349\n",
            "Epoch 3956/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2065 - accuracy: 0.8916\n",
            "Epoch 3956: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2071 - accuracy: 0.8925 - val_loss: 3.9334 - val_accuracy: 0.5284\n",
            "Epoch 3957/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1985 - accuracy: 0.8963\n",
            "Epoch 3957: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1999 - accuracy: 0.8952 - val_loss: 4.2301 - val_accuracy: 0.5346\n",
            "Epoch 3958/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2056 - accuracy: 0.8922\n",
            "Epoch 3958: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2057 - accuracy: 0.8923 - val_loss: 4.1739 - val_accuracy: 0.5363\n",
            "Epoch 3959/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1974 - accuracy: 0.8951\n",
            "Epoch 3959: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1993 - accuracy: 0.8933 - val_loss: 4.0858 - val_accuracy: 0.5313\n",
            "Epoch 3960/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1985 - accuracy: 0.8945\n",
            "Epoch 3960: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1983 - accuracy: 0.8943 - val_loss: 4.2354 - val_accuracy: 0.5387\n",
            "Epoch 3961/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2124 - accuracy: 0.8900\n",
            "Epoch 3961: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2134 - accuracy: 0.8898 - val_loss: 4.1170 - val_accuracy: 0.5249\n",
            "Epoch 3962/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2179 - accuracy: 0.8906\n",
            "Epoch 3962: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2190 - accuracy: 0.8895 - val_loss: 4.0497 - val_accuracy: 0.5425\n",
            "Epoch 3963/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2108 - accuracy: 0.8889\n",
            "Epoch 3963: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2120 - accuracy: 0.8878 - val_loss: 4.1442 - val_accuracy: 0.5351\n",
            "Epoch 3964/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2049 - accuracy: 0.8933\n",
            "Epoch 3964: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2066 - accuracy: 0.8917 - val_loss: 3.9370 - val_accuracy: 0.5255\n",
            "Epoch 3965/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1981 - accuracy: 0.8968\n",
            "Epoch 3965: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2014 - accuracy: 0.8944 - val_loss: 4.0945 - val_accuracy: 0.5357\n",
            "Epoch 3966/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1948 - accuracy: 0.8970\n",
            "Epoch 3966: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1969 - accuracy: 0.8962 - val_loss: 4.1793 - val_accuracy: 0.5360\n",
            "Epoch 3967/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1929 - accuracy: 0.8987\n",
            "Epoch 3967: loss did not improve from 0.19023\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1944 - accuracy: 0.8976 - val_loss: 4.0948 - val_accuracy: 0.5363\n",
            "Epoch 3968/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1897 - accuracy: 0.9006\n",
            "Epoch 3968: loss improved from 0.19023 to 0.18935, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 38ms/step - loss: 0.1894 - accuracy: 0.9002 - val_loss: 4.1803 - val_accuracy: 0.5284\n",
            "Epoch 3969/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1910 - accuracy: 0.8997\n",
            "Epoch 3969: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1925 - accuracy: 0.8985 - val_loss: 4.0290 - val_accuracy: 0.5278\n",
            "Epoch 3970/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1993 - accuracy: 0.8944\n",
            "Epoch 3970: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2002 - accuracy: 0.8936 - val_loss: 4.1058 - val_accuracy: 0.5351\n",
            "Epoch 3971/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.8944\n",
            "Epoch 3971: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2009 - accuracy: 0.8944 - val_loss: 4.2540 - val_accuracy: 0.5346\n",
            "Epoch 3972/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2016 - accuracy: 0.8956\n",
            "Epoch 3972: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2039 - accuracy: 0.8940 - val_loss: 3.8625 - val_accuracy: 0.5349\n",
            "Epoch 3973/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2051 - accuracy: 0.8924\n",
            "Epoch 3973: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2057 - accuracy: 0.8919 - val_loss: 4.2424 - val_accuracy: 0.5337\n",
            "Epoch 3974/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2005 - accuracy: 0.8967\n",
            "Epoch 3974: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2004 - accuracy: 0.8957 - val_loss: 4.1419 - val_accuracy: 0.5313\n",
            "Epoch 3975/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1959 - accuracy: 0.8973\n",
            "Epoch 3975: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2018 - accuracy: 0.8936 - val_loss: 4.0369 - val_accuracy: 0.5407\n",
            "Epoch 3976/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1974 - accuracy: 0.8954\n",
            "Epoch 3976: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1971 - accuracy: 0.8955 - val_loss: 4.0295 - val_accuracy: 0.5319\n",
            "Epoch 3977/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1927 - accuracy: 0.9003\n",
            "Epoch 3977: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1946 - accuracy: 0.8989 - val_loss: 4.1634 - val_accuracy: 0.5351\n",
            "Epoch 3978/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2121 - accuracy: 0.8905\n",
            "Epoch 3978: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2175 - accuracy: 0.8868 - val_loss: 4.3727 - val_accuracy: 0.5296\n",
            "Epoch 3979/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2181 - accuracy: 0.8909\n",
            "Epoch 3979: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2200 - accuracy: 0.8897 - val_loss: 4.1837 - val_accuracy: 0.5328\n",
            "Epoch 3980/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2087 - accuracy: 0.8906\n",
            "Epoch 3980: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2088 - accuracy: 0.8905 - val_loss: 4.0464 - val_accuracy: 0.5384\n",
            "Epoch 3981/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2019 - accuracy: 0.8931\n",
            "Epoch 3981: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2061 - accuracy: 0.8906 - val_loss: 4.1961 - val_accuracy: 0.5354\n",
            "Epoch 3982/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1998 - accuracy: 0.8934\n",
            "Epoch 3982: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2001 - accuracy: 0.8926 - val_loss: 4.1787 - val_accuracy: 0.5340\n",
            "Epoch 3983/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2012 - accuracy: 0.8943\n",
            "Epoch 3983: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2039 - accuracy: 0.8931 - val_loss: 3.9394 - val_accuracy: 0.5360\n",
            "Epoch 3984/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2036 - accuracy: 0.8950\n",
            "Epoch 3984: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2065 - accuracy: 0.8931 - val_loss: 4.0519 - val_accuracy: 0.5384\n",
            "Epoch 3985/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.8938\n",
            "Epoch 3985: loss did not improve from 0.18935\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2028 - accuracy: 0.8938 - val_loss: 4.0755 - val_accuracy: 0.5337\n",
            "Epoch 3986/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1880 - accuracy: 0.9023\n",
            "Epoch 3986: loss improved from 0.18935 to 0.18904, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 2s 76ms/step - loss: 0.1890 - accuracy: 0.9009 - val_loss: 4.1650 - val_accuracy: 0.5322\n",
            "Epoch 3987/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1888 - accuracy: 0.9002\n",
            "Epoch 3987: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1892 - accuracy: 0.9003 - val_loss: 4.1182 - val_accuracy: 0.5354\n",
            "Epoch 3988/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1942 - accuracy: 0.8955\n",
            "Epoch 3988: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1950 - accuracy: 0.8947 - val_loss: 4.1863 - val_accuracy: 0.5354\n",
            "Epoch 3989/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.8973\n",
            "Epoch 3989: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1953 - accuracy: 0.8973 - val_loss: 4.1975 - val_accuracy: 0.5354\n",
            "Epoch 3990/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2047 - accuracy: 0.8922\n",
            "Epoch 3990: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2047 - accuracy: 0.8931 - val_loss: 4.3389 - val_accuracy: 0.5390\n",
            "Epoch 3991/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2113 - accuracy: 0.8882\n",
            "Epoch 3991: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2125 - accuracy: 0.8887 - val_loss: 4.0697 - val_accuracy: 0.5287\n",
            "Epoch 3992/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2099 - accuracy: 0.8898\n",
            "Epoch 3992: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2102 - accuracy: 0.8895 - val_loss: 4.2293 - val_accuracy: 0.5316\n",
            "Epoch 3993/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2028 - accuracy: 0.8948\n",
            "Epoch 3993: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2022 - accuracy: 0.8950 - val_loss: 4.1628 - val_accuracy: 0.5351\n",
            "Epoch 3994/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2037 - accuracy: 0.8959\n",
            "Epoch 3994: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2059 - accuracy: 0.8936 - val_loss: 4.1682 - val_accuracy: 0.5343\n",
            "Epoch 3995/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1982 - accuracy: 0.8956\n",
            "Epoch 3995: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1985 - accuracy: 0.8954 - val_loss: 4.2582 - val_accuracy: 0.5284\n",
            "Epoch 3996/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.8946\n",
            "Epoch 3996: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2031 - accuracy: 0.8946 - val_loss: 4.1801 - val_accuracy: 0.5384\n",
            "Epoch 3997/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.8938\n",
            "Epoch 3997: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2021 - accuracy: 0.8938 - val_loss: 4.2227 - val_accuracy: 0.5360\n",
            "Epoch 3998/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1945 - accuracy: 0.8983\n",
            "Epoch 3998: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1955 - accuracy: 0.8972 - val_loss: 4.2943 - val_accuracy: 0.5267\n",
            "Epoch 3999/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1927 - accuracy: 0.8978\n",
            "Epoch 3999: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1937 - accuracy: 0.8971 - val_loss: 4.1575 - val_accuracy: 0.5346\n",
            "Epoch 4000/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1958 - accuracy: 0.8994\n",
            "Epoch 4000: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2015 - accuracy: 0.8965 - val_loss: 4.2589 - val_accuracy: 0.5334\n",
            "Epoch 4001/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2054 - accuracy: 0.8917\n",
            "Epoch 4001: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2069 - accuracy: 0.8905 - val_loss: 4.1565 - val_accuracy: 0.5316\n",
            "Epoch 4002/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2029 - accuracy: 0.8930\n",
            "Epoch 4002: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2054 - accuracy: 0.8914 - val_loss: 4.2902 - val_accuracy: 0.5393\n",
            "Epoch 4003/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2043 - accuracy: 0.8915\n",
            "Epoch 4003: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2057 - accuracy: 0.8905 - val_loss: 4.0708 - val_accuracy: 0.5337\n",
            "Epoch 4004/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2050 - accuracy: 0.8922\n",
            "Epoch 4004: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2057 - accuracy: 0.8916 - val_loss: 4.0999 - val_accuracy: 0.5334\n",
            "Epoch 4005/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1983 - accuracy: 0.8933\n",
            "Epoch 4005: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2000 - accuracy: 0.8925 - val_loss: 4.3658 - val_accuracy: 0.5349\n",
            "Epoch 4006/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.8909\n",
            "Epoch 4006: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2047 - accuracy: 0.8893 - val_loss: 4.1445 - val_accuracy: 0.5369\n",
            "Epoch 4007/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2022 - accuracy: 0.8931\n",
            "Epoch 4007: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2040 - accuracy: 0.8913 - val_loss: 4.0432 - val_accuracy: 0.5278\n",
            "Epoch 4008/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2030 - accuracy: 0.8927\n",
            "Epoch 4008: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2043 - accuracy: 0.8919 - val_loss: 4.0043 - val_accuracy: 0.5264\n",
            "Epoch 4009/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2049 - accuracy: 0.8940\n",
            "Epoch 4009: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2081 - accuracy: 0.8923 - val_loss: 4.1829 - val_accuracy: 0.5360\n",
            "Epoch 4010/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2054 - accuracy: 0.8929\n",
            "Epoch 4010: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2089 - accuracy: 0.8906 - val_loss: 4.2015 - val_accuracy: 0.5275\n",
            "Epoch 4011/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2025 - accuracy: 0.8955\n",
            "Epoch 4011: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2029 - accuracy: 0.8950 - val_loss: 4.1609 - val_accuracy: 0.5360\n",
            "Epoch 4012/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2103 - accuracy: 0.8917\n",
            "Epoch 4012: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2140 - accuracy: 0.8887 - val_loss: 3.9780 - val_accuracy: 0.5237\n",
            "Epoch 4013/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2027 - accuracy: 0.8956\n",
            "Epoch 4013: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2032 - accuracy: 0.8953 - val_loss: 4.1258 - val_accuracy: 0.5334\n",
            "Epoch 4014/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2043 - accuracy: 0.8934\n",
            "Epoch 4014: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2040 - accuracy: 0.8934 - val_loss: 4.2075 - val_accuracy: 0.5322\n",
            "Epoch 4015/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2045 - accuracy: 0.8931\n",
            "Epoch 4015: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2061 - accuracy: 0.8916 - val_loss: 4.1678 - val_accuracy: 0.5278\n",
            "Epoch 4016/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1995 - accuracy: 0.8975\n",
            "Epoch 4016: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2008 - accuracy: 0.8964 - val_loss: 4.0930 - val_accuracy: 0.5378\n",
            "Epoch 4017/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2026 - accuracy: 0.8959\n",
            "Epoch 4017: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2092 - accuracy: 0.8917 - val_loss: 4.1044 - val_accuracy: 0.5351\n",
            "Epoch 4018/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1976 - accuracy: 0.8976\n",
            "Epoch 4018: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1994 - accuracy: 0.8960 - val_loss: 4.1798 - val_accuracy: 0.5378\n",
            "Epoch 4019/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2012 - accuracy: 0.8948\n",
            "Epoch 4019: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2012 - accuracy: 0.8938 - val_loss: 4.1345 - val_accuracy: 0.5349\n",
            "Epoch 4020/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.8979\n",
            "Epoch 4020: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1977 - accuracy: 0.8955 - val_loss: 4.1966 - val_accuracy: 0.5308\n",
            "Epoch 4021/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1952 - accuracy: 0.8974\n",
            "Epoch 4021: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1970 - accuracy: 0.8957 - val_loss: 4.2571 - val_accuracy: 0.5366\n",
            "Epoch 4022/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.2019 - accuracy: 0.8958\n",
            "Epoch 4022: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2072 - accuracy: 0.8928 - val_loss: 4.0371 - val_accuracy: 0.5322\n",
            "Epoch 4023/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2040 - accuracy: 0.8912\n",
            "Epoch 4023: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2047 - accuracy: 0.8912 - val_loss: 4.1399 - val_accuracy: 0.5290\n",
            "Epoch 4024/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1955 - accuracy: 0.8998\n",
            "Epoch 4024: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1976 - accuracy: 0.8985 - val_loss: 4.3164 - val_accuracy: 0.5296\n",
            "Epoch 4025/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1960 - accuracy: 0.8987\n",
            "Epoch 4025: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1992 - accuracy: 0.8962 - val_loss: 4.3090 - val_accuracy: 0.5375\n",
            "Epoch 4026/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2017 - accuracy: 0.8942\n",
            "Epoch 4026: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2045 - accuracy: 0.8922 - val_loss: 3.8837 - val_accuracy: 0.5293\n",
            "Epoch 4027/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.8935\n",
            "Epoch 4027: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2041 - accuracy: 0.8935 - val_loss: 4.1215 - val_accuracy: 0.5422\n",
            "Epoch 4028/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.8961\n",
            "Epoch 4028: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1978 - accuracy: 0.8961 - val_loss: 4.2241 - val_accuracy: 0.5334\n",
            "Epoch 4029/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1965 - accuracy: 0.8953\n",
            "Epoch 4029: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1994 - accuracy: 0.8941 - val_loss: 3.9520 - val_accuracy: 0.5305\n",
            "Epoch 4030/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2037 - accuracy: 0.8942\n",
            "Epoch 4030: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2055 - accuracy: 0.8933 - val_loss: 4.1220 - val_accuracy: 0.5325\n",
            "Epoch 4031/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2018 - accuracy: 0.8943\n",
            "Epoch 4031: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2028 - accuracy: 0.8930 - val_loss: 4.3448 - val_accuracy: 0.5302\n",
            "Epoch 4032/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2125 - accuracy: 0.8918\n",
            "Epoch 4032: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2141 - accuracy: 0.8910 - val_loss: 4.0292 - val_accuracy: 0.5319\n",
            "Epoch 4033/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2078 - accuracy: 0.8938\n",
            "Epoch 4033: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2094 - accuracy: 0.8933 - val_loss: 4.1507 - val_accuracy: 0.5351\n",
            "Epoch 4034/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2115 - accuracy: 0.8900\n",
            "Epoch 4034: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2122 - accuracy: 0.8898 - val_loss: 4.2449 - val_accuracy: 0.5287\n",
            "Epoch 4035/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2188 - accuracy: 0.8901\n",
            "Epoch 4035: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2188 - accuracy: 0.8898 - val_loss: 3.9613 - val_accuracy: 0.5325\n",
            "Epoch 4036/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2081 - accuracy: 0.8932\n",
            "Epoch 4036: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2100 - accuracy: 0.8919 - val_loss: 4.1816 - val_accuracy: 0.5375\n",
            "Epoch 4037/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2058 - accuracy: 0.8956\n",
            "Epoch 4037: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2072 - accuracy: 0.8941 - val_loss: 4.1687 - val_accuracy: 0.5340\n",
            "Epoch 4038/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.8868\n",
            "Epoch 4038: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2138 - accuracy: 0.8868 - val_loss: 4.0395 - val_accuracy: 0.5275\n",
            "Epoch 4039/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2068 - accuracy: 0.8939\n",
            "Epoch 4039: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2069 - accuracy: 0.8922 - val_loss: 4.1024 - val_accuracy: 0.5313\n",
            "Epoch 4040/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1928 - accuracy: 0.8999\n",
            "Epoch 4040: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1949 - accuracy: 0.8977 - val_loss: 4.1876 - val_accuracy: 0.5387\n",
            "Epoch 4041/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1995 - accuracy: 0.8998\n",
            "Epoch 4041: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2036 - accuracy: 0.8955 - val_loss: 4.2342 - val_accuracy: 0.5363\n",
            "Epoch 4042/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2148 - accuracy: 0.8890\n",
            "Epoch 4042: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2143 - accuracy: 0.8896 - val_loss: 3.9683 - val_accuracy: 0.5272\n",
            "Epoch 4043/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2134 - accuracy: 0.8855\n",
            "Epoch 4043: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2131 - accuracy: 0.8857 - val_loss: 4.1032 - val_accuracy: 0.5369\n",
            "Epoch 4044/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2090 - accuracy: 0.8905\n",
            "Epoch 4044: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2085 - accuracy: 0.8909 - val_loss: 4.1256 - val_accuracy: 0.5354\n",
            "Epoch 4045/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2020 - accuracy: 0.8939\n",
            "Epoch 4045: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2021 - accuracy: 0.8939 - val_loss: 4.1851 - val_accuracy: 0.5334\n",
            "Epoch 4046/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2024 - accuracy: 0.8937\n",
            "Epoch 4046: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2025 - accuracy: 0.8936 - val_loss: 4.1328 - val_accuracy: 0.5381\n",
            "Epoch 4047/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.8950\n",
            "Epoch 4047: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2080 - accuracy: 0.8950 - val_loss: 4.0405 - val_accuracy: 0.5278\n",
            "Epoch 4048/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2109 - accuracy: 0.8893\n",
            "Epoch 4048: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2107 - accuracy: 0.8891 - val_loss: 4.0975 - val_accuracy: 0.5319\n",
            "Epoch 4049/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1975 - accuracy: 0.8966\n",
            "Epoch 4049: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1986 - accuracy: 0.8955 - val_loss: 4.2766 - val_accuracy: 0.5442\n",
            "Epoch 4050/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.8978\n",
            "Epoch 4050: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1996 - accuracy: 0.8978 - val_loss: 3.9231 - val_accuracy: 0.5287\n",
            "Epoch 4051/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.8910\n",
            "Epoch 4051: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2104 - accuracy: 0.8910 - val_loss: 4.0227 - val_accuracy: 0.5325\n",
            "Epoch 4052/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2006 - accuracy: 0.8954\n",
            "Epoch 4052: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2044 - accuracy: 0.8933 - val_loss: 3.9845 - val_accuracy: 0.5308\n",
            "Epoch 4053/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2021 - accuracy: 0.8936\n",
            "Epoch 4053: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2026 - accuracy: 0.8931 - val_loss: 4.2777 - val_accuracy: 0.5393\n",
            "Epoch 4054/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.8944\n",
            "Epoch 4054: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2011 - accuracy: 0.8944 - val_loss: 4.2231 - val_accuracy: 0.5419\n",
            "Epoch 4055/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1944 - accuracy: 0.9002\n",
            "Epoch 4055: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.1996 - accuracy: 0.8962 - val_loss: 4.0487 - val_accuracy: 0.5322\n",
            "Epoch 4056/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1956 - accuracy: 0.8956\n",
            "Epoch 4056: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1975 - accuracy: 0.8942 - val_loss: 4.0963 - val_accuracy: 0.5281\n",
            "Epoch 4057/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1929 - accuracy: 0.8987\n",
            "Epoch 4057: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1936 - accuracy: 0.8980 - val_loss: 4.1445 - val_accuracy: 0.5334\n",
            "Epoch 4058/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.8963\n",
            "Epoch 4058: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1949 - accuracy: 0.8963 - val_loss: 4.3347 - val_accuracy: 0.5325\n",
            "Epoch 4059/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1939 - accuracy: 0.8982\n",
            "Epoch 4059: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1947 - accuracy: 0.8975 - val_loss: 4.1479 - val_accuracy: 0.5249\n",
            "Epoch 4060/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1942 - accuracy: 0.8967\n",
            "Epoch 4060: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1959 - accuracy: 0.8958 - val_loss: 4.1423 - val_accuracy: 0.5387\n",
            "Epoch 4061/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2005 - accuracy: 0.8947\n",
            "Epoch 4061: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2017 - accuracy: 0.8949 - val_loss: 4.2340 - val_accuracy: 0.5340\n",
            "Epoch 4062/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1996 - accuracy: 0.8952\n",
            "Epoch 4062: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2011 - accuracy: 0.8945 - val_loss: 4.1114 - val_accuracy: 0.5372\n",
            "Epoch 4063/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2020 - accuracy: 0.8929\n",
            "Epoch 4063: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2020 - accuracy: 0.8931 - val_loss: 4.1973 - val_accuracy: 0.5366\n",
            "Epoch 4064/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1946 - accuracy: 0.8957\n",
            "Epoch 4064: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1964 - accuracy: 0.8945 - val_loss: 4.2106 - val_accuracy: 0.5407\n",
            "Epoch 4065/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2016 - accuracy: 0.8930\n",
            "Epoch 4065: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2021 - accuracy: 0.8922 - val_loss: 4.3232 - val_accuracy: 0.5331\n",
            "Epoch 4066/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1951 - accuracy: 0.8967\n",
            "Epoch 4066: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1966 - accuracy: 0.8958 - val_loss: 4.2333 - val_accuracy: 0.5334\n",
            "Epoch 4067/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1966 - accuracy: 0.8963\n",
            "Epoch 4067: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1985 - accuracy: 0.8939 - val_loss: 4.0448 - val_accuracy: 0.5319\n",
            "Epoch 4068/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1978 - accuracy: 0.8983\n",
            "Epoch 4068: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2001 - accuracy: 0.8953 - val_loss: 4.1897 - val_accuracy: 0.5313\n",
            "Epoch 4069/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1977 - accuracy: 0.8979\n",
            "Epoch 4069: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1986 - accuracy: 0.8968 - val_loss: 4.2725 - val_accuracy: 0.5258\n",
            "Epoch 4070/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.8937\n",
            "Epoch 4070: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2027 - accuracy: 0.8931 - val_loss: 4.3667 - val_accuracy: 0.5299\n",
            "Epoch 4071/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1927 - accuracy: 0.9004\n",
            "Epoch 4071: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1967 - accuracy: 0.8978 - val_loss: 4.1877 - val_accuracy: 0.5369\n",
            "Epoch 4072/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2029 - accuracy: 0.8953\n",
            "Epoch 4072: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2026 - accuracy: 0.8955 - val_loss: 4.2219 - val_accuracy: 0.5363\n",
            "Epoch 4073/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1879 - accuracy: 0.9017\n",
            "Epoch 4073: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1894 - accuracy: 0.9002 - val_loss: 4.2507 - val_accuracy: 0.5328\n",
            "Epoch 4074/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1943 - accuracy: 0.8995\n",
            "Epoch 4074: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1970 - accuracy: 0.8985 - val_loss: 4.0460 - val_accuracy: 0.5269\n",
            "Epoch 4075/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2068 - accuracy: 0.8912\n",
            "Epoch 4075: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2088 - accuracy: 0.8901 - val_loss: 4.2469 - val_accuracy: 0.5395\n",
            "Epoch 4076/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2029 - accuracy: 0.8929\n",
            "Epoch 4076: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2017 - accuracy: 0.8942 - val_loss: 4.1416 - val_accuracy: 0.5357\n",
            "Epoch 4077/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8979\n",
            "Epoch 4077: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1951 - accuracy: 0.8973 - val_loss: 3.9314 - val_accuracy: 0.5269\n",
            "Epoch 4078/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2013 - accuracy: 0.8952\n",
            "Epoch 4078: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2029 - accuracy: 0.8939 - val_loss: 4.0672 - val_accuracy: 0.5313\n",
            "Epoch 4079/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2006 - accuracy: 0.8932\n",
            "Epoch 4079: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2007 - accuracy: 0.8927 - val_loss: 4.1612 - val_accuracy: 0.5331\n",
            "Epoch 4080/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2023 - accuracy: 0.8934\n",
            "Epoch 4080: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2038 - accuracy: 0.8924 - val_loss: 4.1998 - val_accuracy: 0.5343\n",
            "Epoch 4081/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1946 - accuracy: 0.8980\n",
            "Epoch 4081: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1946 - accuracy: 0.8976 - val_loss: 4.0922 - val_accuracy: 0.5281\n",
            "Epoch 4082/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1968 - accuracy: 0.8958\n",
            "Epoch 4082: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1960 - accuracy: 0.8962 - val_loss: 4.1783 - val_accuracy: 0.5354\n",
            "Epoch 4083/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1957 - accuracy: 0.8961\n",
            "Epoch 4083: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1959 - accuracy: 0.8955 - val_loss: 4.3111 - val_accuracy: 0.5346\n",
            "Epoch 4084/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2000 - accuracy: 0.8950\n",
            "Epoch 4084: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2023 - accuracy: 0.8934 - val_loss: 4.3003 - val_accuracy: 0.5287\n",
            "Epoch 4085/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2062 - accuracy: 0.8915\n",
            "Epoch 4085: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2093 - accuracy: 0.8897 - val_loss: 4.2993 - val_accuracy: 0.5419\n",
            "Epoch 4086/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2077 - accuracy: 0.8907\n",
            "Epoch 4086: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2098 - accuracy: 0.8892 - val_loss: 4.3195 - val_accuracy: 0.5372\n",
            "Epoch 4087/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2070 - accuracy: 0.8942\n",
            "Epoch 4087: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2088 - accuracy: 0.8929 - val_loss: 4.2239 - val_accuracy: 0.5384\n",
            "Epoch 4088/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2100 - accuracy: 0.8899\n",
            "Epoch 4088: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2107 - accuracy: 0.8895 - val_loss: 4.1563 - val_accuracy: 0.5278\n",
            "Epoch 4089/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2011 - accuracy: 0.8927\n",
            "Epoch 4089: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2026 - accuracy: 0.8918 - val_loss: 4.2203 - val_accuracy: 0.5293\n",
            "Epoch 4090/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.9023\n",
            "Epoch 4090: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1931 - accuracy: 0.8991 - val_loss: 4.1410 - val_accuracy: 0.5264\n",
            "Epoch 4091/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1937 - accuracy: 0.8957\n",
            "Epoch 4091: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1964 - accuracy: 0.8932 - val_loss: 4.1978 - val_accuracy: 0.5308\n",
            "Epoch 4092/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.8951\n",
            "Epoch 4092: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2013 - accuracy: 0.8947 - val_loss: 4.3240 - val_accuracy: 0.5325\n",
            "Epoch 4093/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1945 - accuracy: 0.8989\n",
            "Epoch 4093: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1946 - accuracy: 0.8989 - val_loss: 4.3209 - val_accuracy: 0.5293\n",
            "Epoch 4094/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.8936\n",
            "Epoch 4094: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2025 - accuracy: 0.8936 - val_loss: 4.1724 - val_accuracy: 0.5316\n",
            "Epoch 4095/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1945 - accuracy: 0.8968\n",
            "Epoch 4095: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1949 - accuracy: 0.8970 - val_loss: 4.0985 - val_accuracy: 0.5354\n",
            "Epoch 4096/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2031 - accuracy: 0.8936\n",
            "Epoch 4096: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2042 - accuracy: 0.8931 - val_loss: 4.3892 - val_accuracy: 0.5255\n",
            "Epoch 4097/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2070 - accuracy: 0.8949\n",
            "Epoch 4097: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2064 - accuracy: 0.8950 - val_loss: 4.2282 - val_accuracy: 0.5372\n",
            "Epoch 4098/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1967 - accuracy: 0.8970\n",
            "Epoch 4098: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1999 - accuracy: 0.8947 - val_loss: 4.2808 - val_accuracy: 0.5293\n",
            "Epoch 4099/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.8923\n",
            "Epoch 4099: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2043 - accuracy: 0.8923 - val_loss: 4.1293 - val_accuracy: 0.5255\n",
            "Epoch 4100/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1946 - accuracy: 0.8991\n",
            "Epoch 4100: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1979 - accuracy: 0.8967 - val_loss: 4.3789 - val_accuracy: 0.5387\n",
            "Epoch 4101/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1934 - accuracy: 0.9000\n",
            "Epoch 4101: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1950 - accuracy: 0.8985 - val_loss: 4.1002 - val_accuracy: 0.5369\n",
            "Epoch 4102/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1911 - accuracy: 0.8975\n",
            "Epoch 4102: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1915 - accuracy: 0.8967 - val_loss: 4.2301 - val_accuracy: 0.5261\n",
            "Epoch 4103/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2067 - accuracy: 0.8936\n",
            "Epoch 4103: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2065 - accuracy: 0.8933 - val_loss: 4.1224 - val_accuracy: 0.5299\n",
            "Epoch 4104/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.9002\n",
            "Epoch 4104: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1954 - accuracy: 0.8981 - val_loss: 4.1589 - val_accuracy: 0.5372\n",
            "Epoch 4105/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1923 - accuracy: 0.8992\n",
            "Epoch 4105: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1938 - accuracy: 0.8983 - val_loss: 4.1924 - val_accuracy: 0.5425\n",
            "Epoch 4106/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1972 - accuracy: 0.8941\n",
            "Epoch 4106: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1994 - accuracy: 0.8923 - val_loss: 4.2361 - val_accuracy: 0.5278\n",
            "Epoch 4107/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.8909\n",
            "Epoch 4107: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2120 - accuracy: 0.8909 - val_loss: 4.1269 - val_accuracy: 0.5293\n",
            "Epoch 4108/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2232 - accuracy: 0.8841\n",
            "Epoch 4108: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2231 - accuracy: 0.8841 - val_loss: 4.0493 - val_accuracy: 0.5302\n",
            "Epoch 4109/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2095 - accuracy: 0.8922\n",
            "Epoch 4109: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2099 - accuracy: 0.8913 - val_loss: 4.2455 - val_accuracy: 0.5281\n",
            "Epoch 4110/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2067 - accuracy: 0.8909\n",
            "Epoch 4110: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2069 - accuracy: 0.8903 - val_loss: 4.1004 - val_accuracy: 0.5313\n",
            "Epoch 4111/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1987 - accuracy: 0.8979\n",
            "Epoch 4111: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1989 - accuracy: 0.8976 - val_loss: 4.0236 - val_accuracy: 0.5398\n",
            "Epoch 4112/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2056 - accuracy: 0.8941\n",
            "Epoch 4112: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2068 - accuracy: 0.8934 - val_loss: 4.2121 - val_accuracy: 0.5258\n",
            "Epoch 4113/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - accuracy: 0.8886\n",
            "Epoch 4113: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2059 - accuracy: 0.8886 - val_loss: 4.1285 - val_accuracy: 0.5334\n",
            "Epoch 4114/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2067 - accuracy: 0.8911\n",
            "Epoch 4114: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2052 - accuracy: 0.8920 - val_loss: 4.1558 - val_accuracy: 0.5369\n",
            "Epoch 4115/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2002 - accuracy: 0.8949\n",
            "Epoch 4115: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2009 - accuracy: 0.8944 - val_loss: 4.1411 - val_accuracy: 0.5275\n",
            "Epoch 4116/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.8959\n",
            "Epoch 4116: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1940 - accuracy: 0.8959 - val_loss: 4.1530 - val_accuracy: 0.5372\n",
            "Epoch 4117/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8991\n",
            "Epoch 4117: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1958 - accuracy: 0.8974 - val_loss: 4.1848 - val_accuracy: 0.5351\n",
            "Epoch 4118/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1999 - accuracy: 0.8953\n",
            "Epoch 4118: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2001 - accuracy: 0.8948 - val_loss: 4.2943 - val_accuracy: 0.5378\n",
            "Epoch 4119/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2077 - accuracy: 0.8968\n",
            "Epoch 4119: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2092 - accuracy: 0.8947 - val_loss: 4.1261 - val_accuracy: 0.5346\n",
            "Epoch 4120/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2025 - accuracy: 0.8931\n",
            "Epoch 4120: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2035 - accuracy: 0.8924 - val_loss: 4.1008 - val_accuracy: 0.5363\n",
            "Epoch 4121/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2049 - accuracy: 0.8968\n",
            "Epoch 4121: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2055 - accuracy: 0.8960 - val_loss: 4.0901 - val_accuracy: 0.5351\n",
            "Epoch 4122/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1990 - accuracy: 0.8984\n",
            "Epoch 4122: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2028 - accuracy: 0.8960 - val_loss: 4.0310 - val_accuracy: 0.5346\n",
            "Epoch 4123/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2012 - accuracy: 0.8940\n",
            "Epoch 4123: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2020 - accuracy: 0.8942 - val_loss: 4.1714 - val_accuracy: 0.5366\n",
            "Epoch 4124/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1951 - accuracy: 0.8998\n",
            "Epoch 4124: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1952 - accuracy: 0.8994 - val_loss: 4.1756 - val_accuracy: 0.5425\n",
            "Epoch 4125/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2010 - accuracy: 0.8936\n",
            "Epoch 4125: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1996 - accuracy: 0.8939 - val_loss: 4.2061 - val_accuracy: 0.5328\n",
            "Epoch 4126/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1928 - accuracy: 0.8974\n",
            "Epoch 4126: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1936 - accuracy: 0.8965 - val_loss: 4.0821 - val_accuracy: 0.5410\n",
            "Epoch 4127/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1958 - accuracy: 0.8967\n",
            "Epoch 4127: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1957 - accuracy: 0.8965 - val_loss: 4.0357 - val_accuracy: 0.5419\n",
            "Epoch 4128/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1911 - accuracy: 0.8990\n",
            "Epoch 4128: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1923 - accuracy: 0.8983 - val_loss: 4.1869 - val_accuracy: 0.5328\n",
            "Epoch 4129/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1894 - accuracy: 0.9026\n",
            "Epoch 4129: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1931 - accuracy: 0.8999 - val_loss: 4.0461 - val_accuracy: 0.5351\n",
            "Epoch 4130/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1993 - accuracy: 0.8967\n",
            "Epoch 4130: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2019 - accuracy: 0.8946 - val_loss: 4.0850 - val_accuracy: 0.5393\n",
            "Epoch 4131/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.8919\n",
            "Epoch 4131: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2065 - accuracy: 0.8919 - val_loss: 4.0126 - val_accuracy: 0.5293\n",
            "Epoch 4132/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2017 - accuracy: 0.8930\n",
            "Epoch 4132: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2043 - accuracy: 0.8909 - val_loss: 4.1958 - val_accuracy: 0.5334\n",
            "Epoch 4133/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.8925\n",
            "Epoch 4133: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2036 - accuracy: 0.8925 - val_loss: 4.1402 - val_accuracy: 0.5349\n",
            "Epoch 4134/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.8920\n",
            "Epoch 4134: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.2058 - accuracy: 0.8920 - val_loss: 4.0679 - val_accuracy: 0.5363\n",
            "Epoch 4135/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1933 - accuracy: 0.9019\n",
            "Epoch 4135: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1966 - accuracy: 0.8993 - val_loss: 4.1469 - val_accuracy: 0.5360\n",
            "Epoch 4136/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.8962\n",
            "Epoch 4136: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1942 - accuracy: 0.8955 - val_loss: 3.8945 - val_accuracy: 0.5319\n",
            "Epoch 4137/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1972 - accuracy: 0.8966\n",
            "Epoch 4137: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1995 - accuracy: 0.8946 - val_loss: 4.1414 - val_accuracy: 0.5305\n",
            "Epoch 4138/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1966 - accuracy: 0.8958\n",
            "Epoch 4138: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1989 - accuracy: 0.8948 - val_loss: 4.1477 - val_accuracy: 0.5337\n",
            "Epoch 4139/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.8948\n",
            "Epoch 4139: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2024 - accuracy: 0.8948 - val_loss: 4.0243 - val_accuracy: 0.5319\n",
            "Epoch 4140/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1960 - accuracy: 0.8963\n",
            "Epoch 4140: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1988 - accuracy: 0.8944 - val_loss: 4.0907 - val_accuracy: 0.5349\n",
            "Epoch 4141/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1949 - accuracy: 0.8965\n",
            "Epoch 4141: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1957 - accuracy: 0.8957 - val_loss: 4.0378 - val_accuracy: 0.5334\n",
            "Epoch 4142/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1917 - accuracy: 0.8980\n",
            "Epoch 4142: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1915 - accuracy: 0.8985 - val_loss: 4.0210 - val_accuracy: 0.5278\n",
            "Epoch 4143/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2042 - accuracy: 0.8938\n",
            "Epoch 4143: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2080 - accuracy: 0.8914 - val_loss: 4.1764 - val_accuracy: 0.5313\n",
            "Epoch 4144/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.8981\n",
            "Epoch 4144: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1981 - accuracy: 0.8982 - val_loss: 4.2199 - val_accuracy: 0.5319\n",
            "Epoch 4145/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1954 - accuracy: 0.8986\n",
            "Epoch 4145: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1959 - accuracy: 0.8977 - val_loss: 4.1011 - val_accuracy: 0.5299\n",
            "Epoch 4146/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1914 - accuracy: 0.8993\n",
            "Epoch 4146: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1930 - accuracy: 0.8973 - val_loss: 4.1694 - val_accuracy: 0.5357\n",
            "Epoch 4147/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1937 - accuracy: 0.8966\n",
            "Epoch 4147: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1979 - accuracy: 0.8947 - val_loss: 4.2081 - val_accuracy: 0.5325\n",
            "Epoch 4148/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2031 - accuracy: 0.8945\n",
            "Epoch 4148: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2016 - accuracy: 0.8942 - val_loss: 3.9516 - val_accuracy: 0.5360\n",
            "Epoch 4149/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.8953\n",
            "Epoch 4149: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2003 - accuracy: 0.8953 - val_loss: 4.1474 - val_accuracy: 0.5372\n",
            "Epoch 4150/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2037 - accuracy: 0.8936\n",
            "Epoch 4150: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2040 - accuracy: 0.8937 - val_loss: 4.1849 - val_accuracy: 0.5302\n",
            "Epoch 4151/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.8923\n",
            "Epoch 4151: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2059 - accuracy: 0.8923 - val_loss: 3.9320 - val_accuracy: 0.5337\n",
            "Epoch 4152/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2062 - accuracy: 0.8907\n",
            "Epoch 4152: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2064 - accuracy: 0.8903 - val_loss: 4.0720 - val_accuracy: 0.5290\n",
            "Epoch 4153/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2104 - accuracy: 0.8912\n",
            "Epoch 4153: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2111 - accuracy: 0.8906 - val_loss: 3.9068 - val_accuracy: 0.5351\n",
            "Epoch 4154/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2009 - accuracy: 0.8940\n",
            "Epoch 4154: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2005 - accuracy: 0.8939 - val_loss: 4.1180 - val_accuracy: 0.5375\n",
            "Epoch 4155/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2017 - accuracy: 0.8942\n",
            "Epoch 4155: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2019 - accuracy: 0.8939 - val_loss: 4.2550 - val_accuracy: 0.5302\n",
            "Epoch 4156/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1993 - accuracy: 0.8927\n",
            "Epoch 4156: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1985 - accuracy: 0.8932 - val_loss: 4.0326 - val_accuracy: 0.5331\n",
            "Epoch 4157/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1969 - accuracy: 0.8978\n",
            "Epoch 4157: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2002 - accuracy: 0.8952 - val_loss: 4.2184 - val_accuracy: 0.5366\n",
            "Epoch 4158/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2079 - accuracy: 0.8910\n",
            "Epoch 4158: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2073 - accuracy: 0.8912 - val_loss: 4.0256 - val_accuracy: 0.5346\n",
            "Epoch 4159/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1984 - accuracy: 0.8964\n",
            "Epoch 4159: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1998 - accuracy: 0.8952 - val_loss: 4.0802 - val_accuracy: 0.5375\n",
            "Epoch 4160/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2067 - accuracy: 0.8907\n",
            "Epoch 4160: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2069 - accuracy: 0.8909 - val_loss: 4.1803 - val_accuracy: 0.5431\n",
            "Epoch 4161/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2029 - accuracy: 0.8919\n",
            "Epoch 4161: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2033 - accuracy: 0.8917 - val_loss: 4.2410 - val_accuracy: 0.5337\n",
            "Epoch 4162/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2060 - accuracy: 0.8903\n",
            "Epoch 4162: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2073 - accuracy: 0.8904 - val_loss: 3.8934 - val_accuracy: 0.5255\n",
            "Epoch 4163/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2070 - accuracy: 0.8917\n",
            "Epoch 4163: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2076 - accuracy: 0.8910 - val_loss: 3.9904 - val_accuracy: 0.5319\n",
            "Epoch 4164/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2069 - accuracy: 0.8925\n",
            "Epoch 4164: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2096 - accuracy: 0.8905 - val_loss: 3.8893 - val_accuracy: 0.5384\n",
            "Epoch 4165/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2030 - accuracy: 0.8939\n",
            "Epoch 4165: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2048 - accuracy: 0.8929 - val_loss: 4.2206 - val_accuracy: 0.5375\n",
            "Epoch 4166/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1972 - accuracy: 0.8975\n",
            "Epoch 4166: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1995 - accuracy: 0.8962 - val_loss: 4.0313 - val_accuracy: 0.5366\n",
            "Epoch 4167/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1930 - accuracy: 0.8979\n",
            "Epoch 4167: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1945 - accuracy: 0.8969 - val_loss: 4.0681 - val_accuracy: 0.5375\n",
            "Epoch 4168/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1993 - accuracy: 0.8974\n",
            "Epoch 4168: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2018 - accuracy: 0.8945 - val_loss: 4.1872 - val_accuracy: 0.5390\n",
            "Epoch 4169/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.8986\n",
            "Epoch 4169: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1968 - accuracy: 0.8986 - val_loss: 4.1113 - val_accuracy: 0.5354\n",
            "Epoch 4170/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1913 - accuracy: 0.8997\n",
            "Epoch 4170: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1944 - accuracy: 0.8983 - val_loss: 4.0912 - val_accuracy: 0.5275\n",
            "Epoch 4171/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1981 - accuracy: 0.8959\n",
            "Epoch 4171: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1977 - accuracy: 0.8962 - val_loss: 4.1478 - val_accuracy: 0.5319\n",
            "Epoch 4172/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2018 - accuracy: 0.8961\n",
            "Epoch 4172: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2029 - accuracy: 0.8953 - val_loss: 4.1220 - val_accuracy: 0.5407\n",
            "Epoch 4173/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2131 - accuracy: 0.8898\n",
            "Epoch 4173: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2137 - accuracy: 0.8891 - val_loss: 4.2116 - val_accuracy: 0.5316\n",
            "Epoch 4174/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2104 - accuracy: 0.8886\n",
            "Epoch 4174: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2100 - accuracy: 0.8891 - val_loss: 4.1471 - val_accuracy: 0.5290\n",
            "Epoch 4175/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2020 - accuracy: 0.8943\n",
            "Epoch 4175: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2025 - accuracy: 0.8928 - val_loss: 4.0785 - val_accuracy: 0.5349\n",
            "Epoch 4176/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1966 - accuracy: 0.8970\n",
            "Epoch 4176: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1973 - accuracy: 0.8968 - val_loss: 4.0698 - val_accuracy: 0.5363\n",
            "Epoch 4177/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1962 - accuracy: 0.8960\n",
            "Epoch 4177: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1977 - accuracy: 0.8944 - val_loss: 4.0694 - val_accuracy: 0.5354\n",
            "Epoch 4178/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2018 - accuracy: 0.8933\n",
            "Epoch 4178: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2025 - accuracy: 0.8928 - val_loss: 4.1470 - val_accuracy: 0.5296\n",
            "Epoch 4179/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2002 - accuracy: 0.8952\n",
            "Epoch 4179: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2002 - accuracy: 0.8951 - val_loss: 4.2414 - val_accuracy: 0.5290\n",
            "Epoch 4180/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2006 - accuracy: 0.8940\n",
            "Epoch 4180: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2005 - accuracy: 0.8937 - val_loss: 4.0118 - val_accuracy: 0.5416\n",
            "Epoch 4181/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1948 - accuracy: 0.8977\n",
            "Epoch 4181: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1957 - accuracy: 0.8974 - val_loss: 4.0898 - val_accuracy: 0.5325\n",
            "Epoch 4182/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2053 - accuracy: 0.8927\n",
            "Epoch 4182: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2064 - accuracy: 0.8923 - val_loss: 4.2615 - val_accuracy: 0.5378\n",
            "Epoch 4183/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2026 - accuracy: 0.8956\n",
            "Epoch 4183: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2035 - accuracy: 0.8949 - val_loss: 4.1895 - val_accuracy: 0.5366\n",
            "Epoch 4184/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.8957\n",
            "Epoch 4184: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1969 - accuracy: 0.8957 - val_loss: 4.2964 - val_accuracy: 0.5390\n",
            "Epoch 4185/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1895 - accuracy: 0.8998\n",
            "Epoch 4185: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1926 - accuracy: 0.8980 - val_loss: 4.1177 - val_accuracy: 0.5346\n",
            "Epoch 4186/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1963 - accuracy: 0.8951\n",
            "Epoch 4186: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1996 - accuracy: 0.8922 - val_loss: 4.2380 - val_accuracy: 0.5340\n",
            "Epoch 4187/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2018 - accuracy: 0.8964\n",
            "Epoch 4187: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2052 - accuracy: 0.8926 - val_loss: 4.1008 - val_accuracy: 0.5401\n",
            "Epoch 4188/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2008 - accuracy: 0.8932\n",
            "Epoch 4188: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2046 - accuracy: 0.8915 - val_loss: 4.0786 - val_accuracy: 0.5322\n",
            "Epoch 4189/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2236 - accuracy: 0.8849\n",
            "Epoch 4189: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2256 - accuracy: 0.8851 - val_loss: 3.9458 - val_accuracy: 0.5398\n",
            "Epoch 4190/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2110 - accuracy: 0.8912\n",
            "Epoch 4190: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2113 - accuracy: 0.8904 - val_loss: 4.1116 - val_accuracy: 0.5240\n",
            "Epoch 4191/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1931 - accuracy: 0.9019\n",
            "Epoch 4191: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1961 - accuracy: 0.9000 - val_loss: 4.0142 - val_accuracy: 0.5290\n",
            "Epoch 4192/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1957 - accuracy: 0.8976\n",
            "Epoch 4192: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1969 - accuracy: 0.8963 - val_loss: 4.2128 - val_accuracy: 0.5319\n",
            "Epoch 4193/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1977 - accuracy: 0.8966\n",
            "Epoch 4193: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1988 - accuracy: 0.8962 - val_loss: 4.1747 - val_accuracy: 0.5375\n",
            "Epoch 4194/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1987 - accuracy: 0.8948\n",
            "Epoch 4194: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2014 - accuracy: 0.8931 - val_loss: 4.0911 - val_accuracy: 0.5340\n",
            "Epoch 4195/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1951 - accuracy: 0.8970\n",
            "Epoch 4195: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1968 - accuracy: 0.8957 - val_loss: 4.1504 - val_accuracy: 0.5313\n",
            "Epoch 4196/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1947 - accuracy: 0.8992\n",
            "Epoch 4196: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1989 - accuracy: 0.8964 - val_loss: 4.1737 - val_accuracy: 0.5357\n",
            "Epoch 4197/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.8970\n",
            "Epoch 4197: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1917 - accuracy: 0.8970 - val_loss: 4.1461 - val_accuracy: 0.5313\n",
            "Epoch 4198/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1900 - accuracy: 0.8999\n",
            "Epoch 4198: loss did not improve from 0.18904\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1922 - accuracy: 0.8993 - val_loss: 4.0107 - val_accuracy: 0.5313\n",
            "Epoch 4199/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9012\n",
            "Epoch 4199: loss improved from 0.18904 to 0.18749, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 0.1875 - accuracy: 0.9012 - val_loss: 4.1672 - val_accuracy: 0.5325\n",
            "Epoch 4200/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1851 - accuracy: 0.9014\n",
            "Epoch 4200: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1879 - accuracy: 0.8998 - val_loss: 4.1505 - val_accuracy: 0.5351\n",
            "Epoch 4201/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1902 - accuracy: 0.8998\n",
            "Epoch 4201: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1908 - accuracy: 0.8996 - val_loss: 4.1433 - val_accuracy: 0.5334\n",
            "Epoch 4202/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1848 - accuracy: 0.9031\n",
            "Epoch 4202: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1881 - accuracy: 0.9011 - val_loss: 4.2137 - val_accuracy: 0.5398\n",
            "Epoch 4203/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1897 - accuracy: 0.8998\n",
            "Epoch 4203: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1953 - accuracy: 0.8967 - val_loss: 4.2441 - val_accuracy: 0.5381\n",
            "Epoch 4204/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1962 - accuracy: 0.8954\n",
            "Epoch 4204: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1968 - accuracy: 0.8950 - val_loss: 4.1393 - val_accuracy: 0.5354\n",
            "Epoch 4205/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1952 - accuracy: 0.8988\n",
            "Epoch 4205: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1971 - accuracy: 0.8964 - val_loss: 4.1338 - val_accuracy: 0.5334\n",
            "Epoch 4206/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2025 - accuracy: 0.8922\n",
            "Epoch 4206: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2038 - accuracy: 0.8914 - val_loss: 3.9846 - val_accuracy: 0.5290\n",
            "Epoch 4207/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2151 - accuracy: 0.8893\n",
            "Epoch 4207: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2164 - accuracy: 0.8878 - val_loss: 4.0522 - val_accuracy: 0.5290\n",
            "Epoch 4208/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2065 - accuracy: 0.8947\n",
            "Epoch 4208: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2101 - accuracy: 0.8923 - val_loss: 3.9999 - val_accuracy: 0.5340\n",
            "Epoch 4209/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.8896\n",
            "Epoch 4209: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2137 - accuracy: 0.8896 - val_loss: 4.2060 - val_accuracy: 0.5366\n",
            "Epoch 4210/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2069 - accuracy: 0.8913\n",
            "Epoch 4210: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2071 - accuracy: 0.8912 - val_loss: 4.2176 - val_accuracy: 0.5337\n",
            "Epoch 4211/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2112 - accuracy: 0.8918\n",
            "Epoch 4211: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2124 - accuracy: 0.8901 - val_loss: 4.0535 - val_accuracy: 0.5343\n",
            "Epoch 4212/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.8917\n",
            "Epoch 4212: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2011 - accuracy: 0.8917 - val_loss: 4.0504 - val_accuracy: 0.5337\n",
            "Epoch 4213/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2005 - accuracy: 0.8937\n",
            "Epoch 4213: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2014 - accuracy: 0.8941 - val_loss: 4.2276 - val_accuracy: 0.5343\n",
            "Epoch 4214/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1947 - accuracy: 0.8965\n",
            "Epoch 4214: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1970 - accuracy: 0.8955 - val_loss: 4.1595 - val_accuracy: 0.5404\n",
            "Epoch 4215/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1976 - accuracy: 0.8950\n",
            "Epoch 4215: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1985 - accuracy: 0.8928 - val_loss: 4.0657 - val_accuracy: 0.5275\n",
            "Epoch 4216/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1912 - accuracy: 0.8999\n",
            "Epoch 4216: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1916 - accuracy: 0.8993 - val_loss: 4.1122 - val_accuracy: 0.5375\n",
            "Epoch 4217/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1943 - accuracy: 0.8982\n",
            "Epoch 4217: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1947 - accuracy: 0.8969 - val_loss: 4.3308 - val_accuracy: 0.5407\n",
            "Epoch 4218/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.8949\n",
            "Epoch 4218: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1954 - accuracy: 0.8949 - val_loss: 4.0386 - val_accuracy: 0.5296\n",
            "Epoch 4219/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.8972\n",
            "Epoch 4219: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1959 - accuracy: 0.8972 - val_loss: 4.1311 - val_accuracy: 0.5428\n",
            "Epoch 4220/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.8939\n",
            "Epoch 4220: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2015 - accuracy: 0.8936 - val_loss: 4.1955 - val_accuracy: 0.5410\n",
            "Epoch 4221/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2014 - accuracy: 0.8928\n",
            "Epoch 4221: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2012 - accuracy: 0.8930 - val_loss: 4.1515 - val_accuracy: 0.5325\n",
            "Epoch 4222/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1930 - accuracy: 0.8962\n",
            "Epoch 4222: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1953 - accuracy: 0.8950 - val_loss: 4.0822 - val_accuracy: 0.5360\n",
            "Epoch 4223/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1963 - accuracy: 0.8983\n",
            "Epoch 4223: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1966 - accuracy: 0.8970 - val_loss: 4.2682 - val_accuracy: 0.5310\n",
            "Epoch 4224/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.8997\n",
            "Epoch 4224: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1920 - accuracy: 0.8997 - val_loss: 4.1436 - val_accuracy: 0.5328\n",
            "Epoch 4225/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1953 - accuracy: 0.8952\n",
            "Epoch 4225: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1953 - accuracy: 0.8950 - val_loss: 4.2923 - val_accuracy: 0.5305\n",
            "Epoch 4226/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1908 - accuracy: 0.9018\n",
            "Epoch 4226: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1943 - accuracy: 0.8992 - val_loss: 4.2491 - val_accuracy: 0.5310\n",
            "Epoch 4227/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1973 - accuracy: 0.8963\n",
            "Epoch 4227: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1982 - accuracy: 0.8948 - val_loss: 4.1818 - val_accuracy: 0.5281\n",
            "Epoch 4228/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2016 - accuracy: 0.8934\n",
            "Epoch 4228: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2025 - accuracy: 0.8928 - val_loss: 4.1546 - val_accuracy: 0.5366\n",
            "Epoch 4229/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1988 - accuracy: 0.8931\n",
            "Epoch 4229: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1980 - accuracy: 0.8939 - val_loss: 4.1875 - val_accuracy: 0.5316\n",
            "Epoch 4230/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1890 - accuracy: 0.9016\n",
            "Epoch 4230: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1908 - accuracy: 0.9005 - val_loss: 4.0863 - val_accuracy: 0.5275\n",
            "Epoch 4231/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1943 - accuracy: 0.9011\n",
            "Epoch 4231: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1967 - accuracy: 0.8996 - val_loss: 4.2347 - val_accuracy: 0.5395\n",
            "Epoch 4232/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1967 - accuracy: 0.8979\n",
            "Epoch 4232: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1999 - accuracy: 0.8964 - val_loss: 4.4476 - val_accuracy: 0.5375\n",
            "Epoch 4233/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2117 - accuracy: 0.8923\n",
            "Epoch 4233: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2110 - accuracy: 0.8927 - val_loss: 4.3898 - val_accuracy: 0.5357\n",
            "Epoch 4234/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2092 - accuracy: 0.8901\n",
            "Epoch 4234: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2074 - accuracy: 0.8913 - val_loss: 3.9892 - val_accuracy: 0.5343\n",
            "Epoch 4235/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1977 - accuracy: 0.8962\n",
            "Epoch 4235: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2010 - accuracy: 0.8933 - val_loss: 4.2539 - val_accuracy: 0.5349\n",
            "Epoch 4236/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.8951\n",
            "Epoch 4236: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1959 - accuracy: 0.8951 - val_loss: 4.1462 - val_accuracy: 0.5310\n",
            "Epoch 4237/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.8971\n",
            "Epoch 4237: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1920 - accuracy: 0.8950 - val_loss: 4.3443 - val_accuracy: 0.5369\n",
            "Epoch 4238/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1888 - accuracy: 0.9028\n",
            "Epoch 4238: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1897 - accuracy: 0.9016 - val_loss: 4.1010 - val_accuracy: 0.5366\n",
            "Epoch 4239/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.8939\n",
            "Epoch 4239: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2022 - accuracy: 0.8939 - val_loss: 4.2922 - val_accuracy: 0.5366\n",
            "Epoch 4240/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2104 - accuracy: 0.8900\n",
            "Epoch 4240: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2119 - accuracy: 0.8889 - val_loss: 4.3355 - val_accuracy: 0.5384\n",
            "Epoch 4241/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2070 - accuracy: 0.8955\n",
            "Epoch 4241: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2062 - accuracy: 0.8958 - val_loss: 4.2308 - val_accuracy: 0.5331\n",
            "Epoch 4242/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.8936\n",
            "Epoch 4242: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2017 - accuracy: 0.8936 - val_loss: 4.2163 - val_accuracy: 0.5351\n",
            "Epoch 4243/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2043 - accuracy: 0.8940\n",
            "Epoch 4243: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2058 - accuracy: 0.8929 - val_loss: 4.1761 - val_accuracy: 0.5363\n",
            "Epoch 4244/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2073 - accuracy: 0.8928\n",
            "Epoch 4244: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2090 - accuracy: 0.8911 - val_loss: 4.1756 - val_accuracy: 0.5334\n",
            "Epoch 4245/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2031 - accuracy: 0.8942\n",
            "Epoch 4245: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2057 - accuracy: 0.8923 - val_loss: 4.3141 - val_accuracy: 0.5390\n",
            "Epoch 4246/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1977 - accuracy: 0.8961\n",
            "Epoch 4246: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2025 - accuracy: 0.8928 - val_loss: 4.0187 - val_accuracy: 0.5308\n",
            "Epoch 4247/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.8909\n",
            "Epoch 4247: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2071 - accuracy: 0.8909 - val_loss: 4.1461 - val_accuracy: 0.5390\n",
            "Epoch 4248/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1992 - accuracy: 0.8960\n",
            "Epoch 4248: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2012 - accuracy: 0.8947 - val_loss: 3.9706 - val_accuracy: 0.5393\n",
            "Epoch 4249/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2084 - accuracy: 0.8920\n",
            "Epoch 4249: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2130 - accuracy: 0.8895 - val_loss: 3.9140 - val_accuracy: 0.5281\n",
            "Epoch 4250/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.8909\n",
            "Epoch 4250: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2127 - accuracy: 0.8909 - val_loss: 4.0474 - val_accuracy: 0.5401\n",
            "Epoch 4251/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2039 - accuracy: 0.8938\n",
            "Epoch 4251: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2067 - accuracy: 0.8916 - val_loss: 4.1018 - val_accuracy: 0.5322\n",
            "Epoch 4252/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2042 - accuracy: 0.8946\n",
            "Epoch 4252: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2059 - accuracy: 0.8936 - val_loss: 4.1107 - val_accuracy: 0.5310\n",
            "Epoch 4253/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2137 - accuracy: 0.8902\n",
            "Epoch 4253: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2156 - accuracy: 0.8889 - val_loss: 3.9911 - val_accuracy: 0.5340\n",
            "Epoch 4254/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2057 - accuracy: 0.8938\n",
            "Epoch 4254: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2053 - accuracy: 0.8941 - val_loss: 4.1748 - val_accuracy: 0.5351\n",
            "Epoch 4255/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2019 - accuracy: 0.8917\n",
            "Epoch 4255: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2031 - accuracy: 0.8906 - val_loss: 4.1050 - val_accuracy: 0.5328\n",
            "Epoch 4256/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1968 - accuracy: 0.8970\n",
            "Epoch 4256: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1982 - accuracy: 0.8962 - val_loss: 4.1216 - val_accuracy: 0.5319\n",
            "Epoch 4257/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1937 - accuracy: 0.8988\n",
            "Epoch 4257: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1952 - accuracy: 0.8976 - val_loss: 4.1161 - val_accuracy: 0.5340\n",
            "Epoch 4258/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1915 - accuracy: 0.8997\n",
            "Epoch 4258: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1948 - accuracy: 0.8973 - val_loss: 4.0532 - val_accuracy: 0.5381\n",
            "Epoch 4259/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1942 - accuracy: 0.8980\n",
            "Epoch 4259: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1946 - accuracy: 0.8970 - val_loss: 4.0453 - val_accuracy: 0.5381\n",
            "Epoch 4260/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1877 - accuracy: 0.9013\n",
            "Epoch 4260: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1898 - accuracy: 0.9001 - val_loss: 4.3357 - val_accuracy: 0.5272\n",
            "Epoch 4261/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1896 - accuracy: 0.8997\n",
            "Epoch 4261: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1925 - accuracy: 0.8985 - val_loss: 4.1934 - val_accuracy: 0.5387\n",
            "Epoch 4262/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1962 - accuracy: 0.8975\n",
            "Epoch 4262: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1986 - accuracy: 0.8959 - val_loss: 4.2194 - val_accuracy: 0.5302\n",
            "Epoch 4263/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1880 - accuracy: 0.9005\n",
            "Epoch 4263: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1894 - accuracy: 0.8992 - val_loss: 4.2007 - val_accuracy: 0.5369\n",
            "Epoch 4264/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.8992\n",
            "Epoch 4264: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1889 - accuracy: 0.8992 - val_loss: 4.2134 - val_accuracy: 0.5269\n",
            "Epoch 4265/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1949 - accuracy: 0.8972\n",
            "Epoch 4265: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1957 - accuracy: 0.8969 - val_loss: 4.2522 - val_accuracy: 0.5340\n",
            "Epoch 4266/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.8974\n",
            "Epoch 4266: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1955 - accuracy: 0.8974 - val_loss: 4.4593 - val_accuracy: 0.5357\n",
            "Epoch 4267/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.8924\n",
            "Epoch 4267: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2086 - accuracy: 0.8924 - val_loss: 4.1245 - val_accuracy: 0.5278\n",
            "Epoch 4268/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2090 - accuracy: 0.8922\n",
            "Epoch 4268: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2091 - accuracy: 0.8912 - val_loss: 4.1646 - val_accuracy: 0.5363\n",
            "Epoch 4269/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1965 - accuracy: 0.8969\n",
            "Epoch 4269: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1993 - accuracy: 0.8941 - val_loss: 4.1282 - val_accuracy: 0.5337\n",
            "Epoch 4270/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2036 - accuracy: 0.8920\n",
            "Epoch 4270: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2043 - accuracy: 0.8911 - val_loss: 4.2570 - val_accuracy: 0.5407\n",
            "Epoch 4271/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1989 - accuracy: 0.8974\n",
            "Epoch 4271: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2017 - accuracy: 0.8939 - val_loss: 4.0991 - val_accuracy: 0.5337\n",
            "Epoch 4272/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2049 - accuracy: 0.8929\n",
            "Epoch 4272: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2063 - accuracy: 0.8919 - val_loss: 4.0442 - val_accuracy: 0.5331\n",
            "Epoch 4273/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2079 - accuracy: 0.8906\n",
            "Epoch 4273: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2092 - accuracy: 0.8910 - val_loss: 4.2358 - val_accuracy: 0.5375\n",
            "Epoch 4274/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2082 - accuracy: 0.8952\n",
            "Epoch 4274: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.2113 - accuracy: 0.8923 - val_loss: 3.9953 - val_accuracy: 0.5401\n",
            "Epoch 4275/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2081 - accuracy: 0.8917\n",
            "Epoch 4275: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2071 - accuracy: 0.8919 - val_loss: 4.0057 - val_accuracy: 0.5290\n",
            "Epoch 4276/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2017 - accuracy: 0.8965\n",
            "Epoch 4276: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2017 - accuracy: 0.8960 - val_loss: 4.0904 - val_accuracy: 0.5351\n",
            "Epoch 4277/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1865 - accuracy: 0.9043\n",
            "Epoch 4277: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1878 - accuracy: 0.9034 - val_loss: 4.1049 - val_accuracy: 0.5337\n",
            "Epoch 4278/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1945 - accuracy: 0.8942\n",
            "Epoch 4278: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1958 - accuracy: 0.8937 - val_loss: 4.1058 - val_accuracy: 0.5393\n",
            "Epoch 4279/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1902 - accuracy: 0.9001\n",
            "Epoch 4279: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1928 - accuracy: 0.8977 - val_loss: 4.1170 - val_accuracy: 0.5308\n",
            "Epoch 4280/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1921 - accuracy: 0.9008\n",
            "Epoch 4280: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1929 - accuracy: 0.9010 - val_loss: 4.0777 - val_accuracy: 0.5360\n",
            "Epoch 4281/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1990 - accuracy: 0.8957\n",
            "Epoch 4281: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1997 - accuracy: 0.8948 - val_loss: 4.2755 - val_accuracy: 0.5334\n",
            "Epoch 4282/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2008 - accuracy: 0.8964\n",
            "Epoch 4282: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2019 - accuracy: 0.8958 - val_loss: 4.2532 - val_accuracy: 0.5340\n",
            "Epoch 4283/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2055 - accuracy: 0.8933\n",
            "Epoch 4283: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2068 - accuracy: 0.8923 - val_loss: 4.1182 - val_accuracy: 0.5290\n",
            "Epoch 4284/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2013 - accuracy: 0.8941\n",
            "Epoch 4284: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2067 - accuracy: 0.8905 - val_loss: 4.1951 - val_accuracy: 0.5363\n",
            "Epoch 4285/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2042 - accuracy: 0.8915\n",
            "Epoch 4285: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2046 - accuracy: 0.8917 - val_loss: 4.1371 - val_accuracy: 0.5284\n",
            "Epoch 4286/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1988 - accuracy: 0.8967\n",
            "Epoch 4286: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2013 - accuracy: 0.8946 - val_loss: 4.2530 - val_accuracy: 0.5272\n",
            "Epoch 4287/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1964 - accuracy: 0.8985\n",
            "Epoch 4287: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1994 - accuracy: 0.8959 - val_loss: 4.1996 - val_accuracy: 0.5234\n",
            "Epoch 4288/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2014 - accuracy: 0.8935\n",
            "Epoch 4288: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2021 - accuracy: 0.8926 - val_loss: 4.2307 - val_accuracy: 0.5351\n",
            "Epoch 4289/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2001 - accuracy: 0.8941\n",
            "Epoch 4289: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2005 - accuracy: 0.8941 - val_loss: 4.2124 - val_accuracy: 0.5299\n",
            "Epoch 4290/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2055 - accuracy: 0.8927\n",
            "Epoch 4290: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2065 - accuracy: 0.8913 - val_loss: 4.3294 - val_accuracy: 0.5354\n",
            "Epoch 4291/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.8909\n",
            "Epoch 4291: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2135 - accuracy: 0.8909 - val_loss: 4.2378 - val_accuracy: 0.5340\n",
            "Epoch 4292/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2151 - accuracy: 0.8898\n",
            "Epoch 4292: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2205 - accuracy: 0.8870 - val_loss: 3.8312 - val_accuracy: 0.5255\n",
            "Epoch 4293/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2138 - accuracy: 0.8874\n",
            "Epoch 4293: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2143 - accuracy: 0.8868 - val_loss: 4.0784 - val_accuracy: 0.5395\n",
            "Epoch 4294/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1934 - accuracy: 0.8973\n",
            "Epoch 4294: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1972 - accuracy: 0.8946 - val_loss: 4.1775 - val_accuracy: 0.5354\n",
            "Epoch 4295/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1927 - accuracy: 0.8988\n",
            "Epoch 4295: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1936 - accuracy: 0.8966 - val_loss: 4.0959 - val_accuracy: 0.5331\n",
            "Epoch 4296/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1957 - accuracy: 0.8963\n",
            "Epoch 4296: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1960 - accuracy: 0.8968 - val_loss: 4.1830 - val_accuracy: 0.5387\n",
            "Epoch 4297/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1954 - accuracy: 0.8974\n",
            "Epoch 4297: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1979 - accuracy: 0.8945 - val_loss: 4.0720 - val_accuracy: 0.5313\n",
            "Epoch 4298/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1942 - accuracy: 0.8977\n",
            "Epoch 4298: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1961 - accuracy: 0.8955 - val_loss: 4.0739 - val_accuracy: 0.5366\n",
            "Epoch 4299/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1970 - accuracy: 0.8954\n",
            "Epoch 4299: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1975 - accuracy: 0.8947 - val_loss: 4.2255 - val_accuracy: 0.5255\n",
            "Epoch 4300/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.8962\n",
            "Epoch 4300: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2004 - accuracy: 0.8962 - val_loss: 4.1761 - val_accuracy: 0.5316\n",
            "Epoch 4301/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.8970\n",
            "Epoch 4301: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1950 - accuracy: 0.8965 - val_loss: 4.2339 - val_accuracy: 0.5331\n",
            "Epoch 4302/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.8980\n",
            "Epoch 4302: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1935 - accuracy: 0.8980 - val_loss: 4.1609 - val_accuracy: 0.5351\n",
            "Epoch 4303/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1949 - accuracy: 0.8979\n",
            "Epoch 4303: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1948 - accuracy: 0.8977 - val_loss: 4.1616 - val_accuracy: 0.5340\n",
            "Epoch 4304/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.8948\n",
            "Epoch 4304: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2029 - accuracy: 0.8948 - val_loss: 4.0602 - val_accuracy: 0.5372\n",
            "Epoch 4305/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2136 - accuracy: 0.8873\n",
            "Epoch 4305: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2144 - accuracy: 0.8871 - val_loss: 4.2364 - val_accuracy: 0.5346\n",
            "Epoch 4306/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2025 - accuracy: 0.8939\n",
            "Epoch 4306: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2033 - accuracy: 0.8931 - val_loss: 4.0821 - val_accuracy: 0.5296\n",
            "Epoch 4307/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1935 - accuracy: 0.8983\n",
            "Epoch 4307: loss did not improve from 0.18749\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1948 - accuracy: 0.8971 - val_loss: 4.1411 - val_accuracy: 0.5310\n",
            "Epoch 4308/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9016\n",
            "Epoch 4308: loss improved from 0.18749 to 0.18639, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 43ms/step - loss: 0.1864 - accuracy: 0.9016 - val_loss: 4.1096 - val_accuracy: 0.5360\n",
            "Epoch 4309/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1890 - accuracy: 0.9014\n",
            "Epoch 4309: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1930 - accuracy: 0.8991 - val_loss: 4.2723 - val_accuracy: 0.5272\n",
            "Epoch 4310/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2022 - accuracy: 0.8930\n",
            "Epoch 4310: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2031 - accuracy: 0.8928 - val_loss: 4.2956 - val_accuracy: 0.5299\n",
            "Epoch 4311/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2019 - accuracy: 0.8953\n",
            "Epoch 4311: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2024 - accuracy: 0.8949 - val_loss: 4.2218 - val_accuracy: 0.5372\n",
            "Epoch 4312/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1875 - accuracy: 0.8987\n",
            "Epoch 4312: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1895 - accuracy: 0.8973 - val_loss: 4.2173 - val_accuracy: 0.5357\n",
            "Epoch 4313/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1985 - accuracy: 0.8955\n",
            "Epoch 4313: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1979 - accuracy: 0.8955 - val_loss: 3.9438 - val_accuracy: 0.5319\n",
            "Epoch 4314/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2079 - accuracy: 0.8934\n",
            "Epoch 4314: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2087 - accuracy: 0.8929 - val_loss: 4.1394 - val_accuracy: 0.5281\n",
            "Epoch 4315/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.8925\n",
            "Epoch 4315: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2113 - accuracy: 0.8925 - val_loss: 4.2009 - val_accuracy: 0.5337\n",
            "Epoch 4316/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2025 - accuracy: 0.8933\n",
            "Epoch 4316: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2031 - accuracy: 0.8933 - val_loss: 4.2901 - val_accuracy: 0.5369\n",
            "Epoch 4317/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2078 - accuracy: 0.8931\n",
            "Epoch 4317: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2078 - accuracy: 0.8931 - val_loss: 4.1750 - val_accuracy: 0.5287\n",
            "Epoch 4318/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2063 - accuracy: 0.8937\n",
            "Epoch 4318: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2088 - accuracy: 0.8925 - val_loss: 4.0821 - val_accuracy: 0.5407\n",
            "Epoch 4319/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2101 - accuracy: 0.8898\n",
            "Epoch 4319: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2102 - accuracy: 0.8901 - val_loss: 4.0435 - val_accuracy: 0.5337\n",
            "Epoch 4320/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1933 - accuracy: 0.8990\n",
            "Epoch 4320: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1947 - accuracy: 0.8976 - val_loss: 4.1497 - val_accuracy: 0.5308\n",
            "Epoch 4321/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1965 - accuracy: 0.8958\n",
            "Epoch 4321: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1987 - accuracy: 0.8945 - val_loss: 4.0631 - val_accuracy: 0.5325\n",
            "Epoch 4322/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1966 - accuracy: 0.8990\n",
            "Epoch 4322: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1992 - accuracy: 0.8968 - val_loss: 4.3013 - val_accuracy: 0.5296\n",
            "Epoch 4323/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1992 - accuracy: 0.8949\n",
            "Epoch 4323: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1989 - accuracy: 0.8948 - val_loss: 4.2245 - val_accuracy: 0.5369\n",
            "Epoch 4324/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1941 - accuracy: 0.8976\n",
            "Epoch 4324: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1963 - accuracy: 0.8961 - val_loss: 4.3541 - val_accuracy: 0.5375\n",
            "Epoch 4325/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1904 - accuracy: 0.8996\n",
            "Epoch 4325: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1908 - accuracy: 0.8995 - val_loss: 4.1960 - val_accuracy: 0.5284\n",
            "Epoch 4326/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2048 - accuracy: 0.8931\n",
            "Epoch 4326: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2064 - accuracy: 0.8915 - val_loss: 4.0637 - val_accuracy: 0.5226\n",
            "Epoch 4327/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2071 - accuracy: 0.8912\n",
            "Epoch 4327: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2084 - accuracy: 0.8902 - val_loss: 4.2800 - val_accuracy: 0.5381\n",
            "Epoch 4328/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1951 - accuracy: 0.8961\n",
            "Epoch 4328: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1952 - accuracy: 0.8960 - val_loss: 4.3093 - val_accuracy: 0.5375\n",
            "Epoch 4329/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2108 - accuracy: 0.8933\n",
            "Epoch 4329: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2123 - accuracy: 0.8930 - val_loss: 4.4492 - val_accuracy: 0.5387\n",
            "Epoch 4330/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2155 - accuracy: 0.8911\n",
            "Epoch 4330: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2172 - accuracy: 0.8893 - val_loss: 4.0562 - val_accuracy: 0.5378\n",
            "Epoch 4331/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2051 - accuracy: 0.8955\n",
            "Epoch 4331: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2089 - accuracy: 0.8923 - val_loss: 4.0478 - val_accuracy: 0.5387\n",
            "Epoch 4332/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2019 - accuracy: 0.8965\n",
            "Epoch 4332: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2068 - accuracy: 0.8928 - val_loss: 4.1808 - val_accuracy: 0.5381\n",
            "Epoch 4333/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2020 - accuracy: 0.8906\n",
            "Epoch 4333: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2032 - accuracy: 0.8901 - val_loss: 4.3492 - val_accuracy: 0.5331\n",
            "Epoch 4334/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1867 - accuracy: 0.9009\n",
            "Epoch 4334: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1896 - accuracy: 0.8992 - val_loss: 4.2601 - val_accuracy: 0.5346\n",
            "Epoch 4335/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1952 - accuracy: 0.8970\n",
            "Epoch 4335: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1954 - accuracy: 0.8966 - val_loss: 4.2865 - val_accuracy: 0.5372\n",
            "Epoch 4336/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1880 - accuracy: 0.9007\n",
            "Epoch 4336: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1920 - accuracy: 0.8977 - val_loss: 4.1831 - val_accuracy: 0.5354\n",
            "Epoch 4337/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.9032\n",
            "Epoch 4337: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1870 - accuracy: 0.9015 - val_loss: 4.2424 - val_accuracy: 0.5296\n",
            "Epoch 4338/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1863 - accuracy: 0.9028\n",
            "Epoch 4338: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1885 - accuracy: 0.9006 - val_loss: 4.4044 - val_accuracy: 0.5378\n",
            "Epoch 4339/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1934 - accuracy: 0.8987\n",
            "Epoch 4339: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1956 - accuracy: 0.8968 - val_loss: 4.3730 - val_accuracy: 0.5340\n",
            "Epoch 4340/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1885 - accuracy: 0.8973\n",
            "Epoch 4340: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1900 - accuracy: 0.8957 - val_loss: 4.4149 - val_accuracy: 0.5299\n",
            "Epoch 4341/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1917 - accuracy: 0.9000\n",
            "Epoch 4341: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1936 - accuracy: 0.8991 - val_loss: 4.3540 - val_accuracy: 0.5334\n",
            "Epoch 4342/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2022 - accuracy: 0.8942\n",
            "Epoch 4342: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2017 - accuracy: 0.8944 - val_loss: 4.1337 - val_accuracy: 0.5296\n",
            "Epoch 4343/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2094 - accuracy: 0.8916\n",
            "Epoch 4343: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2116 - accuracy: 0.8907 - val_loss: 4.1081 - val_accuracy: 0.5366\n",
            "Epoch 4344/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2104 - accuracy: 0.8915\n",
            "Epoch 4344: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2126 - accuracy: 0.8908 - val_loss: 4.2315 - val_accuracy: 0.5310\n",
            "Epoch 4345/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2100 - accuracy: 0.8894\n",
            "Epoch 4345: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2106 - accuracy: 0.8881 - val_loss: 4.0415 - val_accuracy: 0.5378\n",
            "Epoch 4346/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1984 - accuracy: 0.8947\n",
            "Epoch 4346: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1998 - accuracy: 0.8949 - val_loss: 4.2147 - val_accuracy: 0.5346\n",
            "Epoch 4347/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1934 - accuracy: 0.8984\n",
            "Epoch 4347: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1965 - accuracy: 0.8963 - val_loss: 4.1076 - val_accuracy: 0.5284\n",
            "Epoch 4348/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1888 - accuracy: 0.9027\n",
            "Epoch 4348: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1910 - accuracy: 0.9011 - val_loss: 4.1749 - val_accuracy: 0.5431\n",
            "Epoch 4349/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1921 - accuracy: 0.8990\n",
            "Epoch 4349: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1965 - accuracy: 0.8960 - val_loss: 4.2192 - val_accuracy: 0.5360\n",
            "Epoch 4350/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1902 - accuracy: 0.8990\n",
            "Epoch 4350: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1930 - accuracy: 0.8975 - val_loss: 4.3484 - val_accuracy: 0.5293\n",
            "Epoch 4351/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1919 - accuracy: 0.8982\n",
            "Epoch 4351: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1936 - accuracy: 0.8973 - val_loss: 4.3039 - val_accuracy: 0.5325\n",
            "Epoch 4352/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.8983\n",
            "Epoch 4352: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1933 - accuracy: 0.8983 - val_loss: 4.2987 - val_accuracy: 0.5325\n",
            "Epoch 4353/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1980 - accuracy: 0.8947\n",
            "Epoch 4353: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2022 - accuracy: 0.8919 - val_loss: 4.2180 - val_accuracy: 0.5322\n",
            "Epoch 4354/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2033 - accuracy: 0.8975\n",
            "Epoch 4354: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2071 - accuracy: 0.8950 - val_loss: 4.3385 - val_accuracy: 0.5351\n",
            "Epoch 4355/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2009 - accuracy: 0.8954\n",
            "Epoch 4355: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2048 - accuracy: 0.8924 - val_loss: 4.3505 - val_accuracy: 0.5331\n",
            "Epoch 4356/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1998 - accuracy: 0.8946\n",
            "Epoch 4356: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2034 - accuracy: 0.8923 - val_loss: 4.3677 - val_accuracy: 0.5284\n",
            "Epoch 4357/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2066 - accuracy: 0.8903\n",
            "Epoch 4357: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2067 - accuracy: 0.8895 - val_loss: 4.0992 - val_accuracy: 0.5255\n",
            "Epoch 4358/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1970 - accuracy: 0.8978\n",
            "Epoch 4358: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1976 - accuracy: 0.8966 - val_loss: 4.2164 - val_accuracy: 0.5313\n",
            "Epoch 4359/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1984 - accuracy: 0.8969\n",
            "Epoch 4359: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1993 - accuracy: 0.8966 - val_loss: 4.2835 - val_accuracy: 0.5340\n",
            "Epoch 4360/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1970 - accuracy: 0.9015\n",
            "Epoch 4360: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1992 - accuracy: 0.8992 - val_loss: 4.3750 - val_accuracy: 0.5387\n",
            "Epoch 4361/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1956 - accuracy: 0.8961\n",
            "Epoch 4361: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1989 - accuracy: 0.8944 - val_loss: 4.1786 - val_accuracy: 0.5220\n",
            "Epoch 4362/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2021 - accuracy: 0.8966\n",
            "Epoch 4362: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2023 - accuracy: 0.8961 - val_loss: 4.1532 - val_accuracy: 0.5246\n",
            "Epoch 4363/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.8983\n",
            "Epoch 4363: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1980 - accuracy: 0.8983 - val_loss: 4.2556 - val_accuracy: 0.5369\n",
            "Epoch 4364/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.8969\n",
            "Epoch 4364: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1966 - accuracy: 0.8969 - val_loss: 4.1586 - val_accuracy: 0.5337\n",
            "Epoch 4365/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2026 - accuracy: 0.8951\n",
            "Epoch 4365: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2043 - accuracy: 0.8943 - val_loss: 4.3481 - val_accuracy: 0.5305\n",
            "Epoch 4366/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2025 - accuracy: 0.8947\n",
            "Epoch 4366: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2062 - accuracy: 0.8913 - val_loss: 4.2125 - val_accuracy: 0.5343\n",
            "Epoch 4367/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2113 - accuracy: 0.8916\n",
            "Epoch 4367: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2114 - accuracy: 0.8914 - val_loss: 4.0761 - val_accuracy: 0.5308\n",
            "Epoch 4368/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2154 - accuracy: 0.8856\n",
            "Epoch 4368: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2142 - accuracy: 0.8867 - val_loss: 4.2057 - val_accuracy: 0.5337\n",
            "Epoch 4369/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2015 - accuracy: 0.8937\n",
            "Epoch 4369: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2021 - accuracy: 0.8932 - val_loss: 4.2934 - val_accuracy: 0.5305\n",
            "Epoch 4370/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1954 - accuracy: 0.8979\n",
            "Epoch 4370: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1953 - accuracy: 0.8973 - val_loss: 4.2092 - val_accuracy: 0.5299\n",
            "Epoch 4371/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1896 - accuracy: 0.8968\n",
            "Epoch 4371: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1907 - accuracy: 0.8960 - val_loss: 4.0653 - val_accuracy: 0.5351\n",
            "Epoch 4372/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1923 - accuracy: 0.9006\n",
            "Epoch 4372: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1924 - accuracy: 0.9005 - val_loss: 4.4083 - val_accuracy: 0.5366\n",
            "Epoch 4373/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9007\n",
            "Epoch 4373: loss did not improve from 0.18639\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1865 - accuracy: 0.9007 - val_loss: 4.1262 - val_accuracy: 0.5316\n",
            "Epoch 4374/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1838 - accuracy: 0.9050\n",
            "Epoch 4374: loss improved from 0.18639 to 0.18529, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 39ms/step - loss: 0.1853 - accuracy: 0.9031 - val_loss: 4.2911 - val_accuracy: 0.5278\n",
            "Epoch 4375/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1910 - accuracy: 0.9002\n",
            "Epoch 4375: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1916 - accuracy: 0.8991 - val_loss: 4.4139 - val_accuracy: 0.5354\n",
            "Epoch 4376/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1929 - accuracy: 0.8993\n",
            "Epoch 4376: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1927 - accuracy: 0.8988 - val_loss: 4.2071 - val_accuracy: 0.5340\n",
            "Epoch 4377/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1935 - accuracy: 0.8988\n",
            "Epoch 4377: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1957 - accuracy: 0.8965 - val_loss: 4.2000 - val_accuracy: 0.5296\n",
            "Epoch 4378/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1915 - accuracy: 0.8962\n",
            "Epoch 4378: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1926 - accuracy: 0.8952 - val_loss: 4.1831 - val_accuracy: 0.5264\n",
            "Epoch 4379/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1903 - accuracy: 0.8994\n",
            "Epoch 4379: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1929 - accuracy: 0.8980 - val_loss: 4.2775 - val_accuracy: 0.5334\n",
            "Epoch 4380/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1948 - accuracy: 0.8980\n",
            "Epoch 4380: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1945 - accuracy: 0.8976 - val_loss: 4.2246 - val_accuracy: 0.5419\n",
            "Epoch 4381/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1883 - accuracy: 0.9029\n",
            "Epoch 4381: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1940 - accuracy: 0.8992 - val_loss: 4.2963 - val_accuracy: 0.5319\n",
            "Epoch 4382/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1934 - accuracy: 0.9019\n",
            "Epoch 4382: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1933 - accuracy: 0.9017 - val_loss: 4.2406 - val_accuracy: 0.5354\n",
            "Epoch 4383/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2002 - accuracy: 0.8973\n",
            "Epoch 4383: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2012 - accuracy: 0.8961 - val_loss: 4.1661 - val_accuracy: 0.5272\n",
            "Epoch 4384/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2018 - accuracy: 0.8947\n",
            "Epoch 4384: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2046 - accuracy: 0.8937 - val_loss: 4.2786 - val_accuracy: 0.5328\n",
            "Epoch 4385/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.8912\n",
            "Epoch 4385: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2115 - accuracy: 0.8912 - val_loss: 4.3496 - val_accuracy: 0.5308\n",
            "Epoch 4386/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2114 - accuracy: 0.8890\n",
            "Epoch 4386: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2131 - accuracy: 0.8879 - val_loss: 4.1949 - val_accuracy: 0.5390\n",
            "Epoch 4387/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2013 - accuracy: 0.8941\n",
            "Epoch 4387: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2022 - accuracy: 0.8933 - val_loss: 4.0772 - val_accuracy: 0.5354\n",
            "Epoch 4388/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.8990\n",
            "Epoch 4388: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1903 - accuracy: 0.8990 - val_loss: 4.2941 - val_accuracy: 0.5357\n",
            "Epoch 4389/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1894 - accuracy: 0.9011\n",
            "Epoch 4389: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1934 - accuracy: 0.8969 - val_loss: 4.2949 - val_accuracy: 0.5299\n",
            "Epoch 4390/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1866 - accuracy: 0.9019\n",
            "Epoch 4390: loss did not improve from 0.18529\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1893 - accuracy: 0.9000 - val_loss: 4.2726 - val_accuracy: 0.5363\n",
            "Epoch 4391/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1843 - accuracy: 0.9021\n",
            "Epoch 4391: loss improved from 0.18529 to 0.18446, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1845 - accuracy: 0.9018 - val_loss: 4.2385 - val_accuracy: 0.5322\n",
            "Epoch 4392/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1803 - accuracy: 0.9065\n",
            "Epoch 4392: loss improved from 0.18446 to 0.18326, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.1833 - accuracy: 0.9042 - val_loss: 4.3156 - val_accuracy: 0.5354\n",
            "Epoch 4393/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1830 - accuracy: 0.9027\n",
            "Epoch 4393: loss improved from 0.18326 to 0.18308, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 28ms/step - loss: 0.1831 - accuracy: 0.9029 - val_loss: 4.3519 - val_accuracy: 0.5349\n",
            "Epoch 4394/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1851 - accuracy: 0.9008\n",
            "Epoch 4394: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1847 - accuracy: 0.9008 - val_loss: 4.3139 - val_accuracy: 0.5366\n",
            "Epoch 4395/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1884 - accuracy: 0.9017\n",
            "Epoch 4395: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1902 - accuracy: 0.9005 - val_loss: 4.2027 - val_accuracy: 0.5360\n",
            "Epoch 4396/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1900 - accuracy: 0.8991\n",
            "Epoch 4396: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1888 - accuracy: 0.9002 - val_loss: 4.4159 - val_accuracy: 0.5401\n",
            "Epoch 4397/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1908 - accuracy: 0.8993\n",
            "Epoch 4397: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1939 - accuracy: 0.8977 - val_loss: 4.4665 - val_accuracy: 0.5340\n",
            "Epoch 4398/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.8925\n",
            "Epoch 4398: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2039 - accuracy: 0.8925 - val_loss: 4.1350 - val_accuracy: 0.5334\n",
            "Epoch 4399/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1988 - accuracy: 0.8953\n",
            "Epoch 4399: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2019 - accuracy: 0.8936 - val_loss: 4.3007 - val_accuracy: 0.5313\n",
            "Epoch 4400/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2003 - accuracy: 0.8960\n",
            "Epoch 4400: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2025 - accuracy: 0.8947 - val_loss: 4.1884 - val_accuracy: 0.5375\n",
            "Epoch 4401/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2005 - accuracy: 0.8955\n",
            "Epoch 4401: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2024 - accuracy: 0.8953 - val_loss: 4.2055 - val_accuracy: 0.5378\n",
            "Epoch 4402/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1943 - accuracy: 0.8998\n",
            "Epoch 4402: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1947 - accuracy: 0.8985 - val_loss: 4.2673 - val_accuracy: 0.5302\n",
            "Epoch 4403/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1902 - accuracy: 0.8983\n",
            "Epoch 4403: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1911 - accuracy: 0.8977 - val_loss: 4.3121 - val_accuracy: 0.5363\n",
            "Epoch 4404/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1989 - accuracy: 0.8987\n",
            "Epoch 4404: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1999 - accuracy: 0.8973 - val_loss: 4.3185 - val_accuracy: 0.5387\n",
            "Epoch 4405/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.8984\n",
            "Epoch 4405: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1930 - accuracy: 0.8976 - val_loss: 4.2652 - val_accuracy: 0.5384\n",
            "Epoch 4406/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1893 - accuracy: 0.9003\n",
            "Epoch 4406: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1912 - accuracy: 0.8983 - val_loss: 4.2305 - val_accuracy: 0.5328\n",
            "Epoch 4407/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9002\n",
            "Epoch 4407: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1926 - accuracy: 0.9002 - val_loss: 4.3026 - val_accuracy: 0.5393\n",
            "Epoch 4408/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2036 - accuracy: 0.8956\n",
            "Epoch 4408: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2058 - accuracy: 0.8945 - val_loss: 4.4195 - val_accuracy: 0.5378\n",
            "Epoch 4409/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2030 - accuracy: 0.8956\n",
            "Epoch 4409: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2064 - accuracy: 0.8931 - val_loss: 4.0963 - val_accuracy: 0.5293\n",
            "Epoch 4410/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2071 - accuracy: 0.8913\n",
            "Epoch 4410: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2087 - accuracy: 0.8901 - val_loss: 4.3106 - val_accuracy: 0.5325\n",
            "Epoch 4411/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2047 - accuracy: 0.8925\n",
            "Epoch 4411: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2043 - accuracy: 0.8925 - val_loss: 4.4001 - val_accuracy: 0.5340\n",
            "Epoch 4412/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2066 - accuracy: 0.8923\n",
            "Epoch 4412: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2099 - accuracy: 0.8904 - val_loss: 4.4554 - val_accuracy: 0.5331\n",
            "Epoch 4413/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2024 - accuracy: 0.8964\n",
            "Epoch 4413: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2035 - accuracy: 0.8954 - val_loss: 4.0521 - val_accuracy: 0.5305\n",
            "Epoch 4414/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.8940\n",
            "Epoch 4414: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2012 - accuracy: 0.8933 - val_loss: 4.2588 - val_accuracy: 0.5343\n",
            "Epoch 4415/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.8963\n",
            "Epoch 4415: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1963 - accuracy: 0.8963 - val_loss: 4.3488 - val_accuracy: 0.5422\n",
            "Epoch 4416/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2018 - accuracy: 0.8925\n",
            "Epoch 4416: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2081 - accuracy: 0.8891 - val_loss: 4.4311 - val_accuracy: 0.5308\n",
            "Epoch 4417/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2177 - accuracy: 0.8863\n",
            "Epoch 4417: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2196 - accuracy: 0.8849 - val_loss: 4.0644 - val_accuracy: 0.5261\n",
            "Epoch 4418/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2196 - accuracy: 0.8854\n",
            "Epoch 4418: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2213 - accuracy: 0.8839 - val_loss: 3.9928 - val_accuracy: 0.5398\n",
            "Epoch 4419/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2045 - accuracy: 0.8950\n",
            "Epoch 4419: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2056 - accuracy: 0.8942 - val_loss: 4.2552 - val_accuracy: 0.5351\n",
            "Epoch 4420/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2012 - accuracy: 0.8937\n",
            "Epoch 4420: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2017 - accuracy: 0.8934 - val_loss: 4.0834 - val_accuracy: 0.5343\n",
            "Epoch 4421/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1979 - accuracy: 0.8968\n",
            "Epoch 4421: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1991 - accuracy: 0.8953 - val_loss: 4.1588 - val_accuracy: 0.5302\n",
            "Epoch 4422/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.8979\n",
            "Epoch 4422: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1955 - accuracy: 0.8979 - val_loss: 4.3157 - val_accuracy: 0.5308\n",
            "Epoch 4423/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1845 - accuracy: 0.9039\n",
            "Epoch 4423: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1891 - accuracy: 0.9006 - val_loss: 4.1033 - val_accuracy: 0.5305\n",
            "Epoch 4424/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1985 - accuracy: 0.8940\n",
            "Epoch 4424: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2028 - accuracy: 0.8922 - val_loss: 4.2617 - val_accuracy: 0.5305\n",
            "Epoch 4425/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1964 - accuracy: 0.8972\n",
            "Epoch 4425: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1988 - accuracy: 0.8953 - val_loss: 4.3334 - val_accuracy: 0.5322\n",
            "Epoch 4426/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8998\n",
            "Epoch 4426: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1944 - accuracy: 0.8988 - val_loss: 4.3436 - val_accuracy: 0.5381\n",
            "Epoch 4427/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1933 - accuracy: 0.8993\n",
            "Epoch 4427: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1936 - accuracy: 0.8994 - val_loss: 4.1548 - val_accuracy: 0.5401\n",
            "Epoch 4428/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1962 - accuracy: 0.8965\n",
            "Epoch 4428: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1989 - accuracy: 0.8950 - val_loss: 4.1733 - val_accuracy: 0.5316\n",
            "Epoch 4429/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1999 - accuracy: 0.8976\n",
            "Epoch 4429: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2006 - accuracy: 0.8961 - val_loss: 4.3239 - val_accuracy: 0.5293\n",
            "Epoch 4430/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2002 - accuracy: 0.8922\n",
            "Epoch 4430: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2009 - accuracy: 0.8914 - val_loss: 4.3970 - val_accuracy: 0.5331\n",
            "Epoch 4431/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1951 - accuracy: 0.8976\n",
            "Epoch 4431: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1949 - accuracy: 0.8980 - val_loss: 4.2362 - val_accuracy: 0.5340\n",
            "Epoch 4432/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1899 - accuracy: 0.9015\n",
            "Epoch 4432: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1915 - accuracy: 0.8997 - val_loss: 4.3170 - val_accuracy: 0.5284\n",
            "Epoch 4433/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1876 - accuracy: 0.9007\n",
            "Epoch 4433: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1882 - accuracy: 0.8999 - val_loss: 4.1543 - val_accuracy: 0.5325\n",
            "Epoch 4434/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.8969\n",
            "Epoch 4434: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1939 - accuracy: 0.8969 - val_loss: 4.4101 - val_accuracy: 0.5369\n",
            "Epoch 4435/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1935 - accuracy: 0.8975\n",
            "Epoch 4435: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1957 - accuracy: 0.8969 - val_loss: 4.3578 - val_accuracy: 0.5381\n",
            "Epoch 4436/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1970 - accuracy: 0.8998\n",
            "Epoch 4436: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1972 - accuracy: 0.8991 - val_loss: 4.1811 - val_accuracy: 0.5346\n",
            "Epoch 4437/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1937 - accuracy: 0.8978\n",
            "Epoch 4437: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1983 - accuracy: 0.8939 - val_loss: 4.4880 - val_accuracy: 0.5393\n",
            "Epoch 4438/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1968 - accuracy: 0.8984\n",
            "Epoch 4438: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1979 - accuracy: 0.8981 - val_loss: 4.2685 - val_accuracy: 0.5246\n",
            "Epoch 4439/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1867 - accuracy: 0.9001\n",
            "Epoch 4439: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1885 - accuracy: 0.8993 - val_loss: 4.2675 - val_accuracy: 0.5337\n",
            "Epoch 4440/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1873 - accuracy: 0.9030\n",
            "Epoch 4440: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1899 - accuracy: 0.9004 - val_loss: 4.2651 - val_accuracy: 0.5349\n",
            "Epoch 4441/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1903 - accuracy: 0.9020\n",
            "Epoch 4441: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1902 - accuracy: 0.9017 - val_loss: 4.1980 - val_accuracy: 0.5351\n",
            "Epoch 4442/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1954 - accuracy: 0.8975\n",
            "Epoch 4442: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1976 - accuracy: 0.8961 - val_loss: 4.4978 - val_accuracy: 0.5407\n",
            "Epoch 4443/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1983 - accuracy: 0.8968\n",
            "Epoch 4443: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2000 - accuracy: 0.8950 - val_loss: 4.3734 - val_accuracy: 0.5360\n",
            "Epoch 4444/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1924 - accuracy: 0.8991\n",
            "Epoch 4444: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1937 - accuracy: 0.8983 - val_loss: 4.3661 - val_accuracy: 0.5393\n",
            "Epoch 4445/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1983 - accuracy: 0.8944\n",
            "Epoch 4445: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1990 - accuracy: 0.8939 - val_loss: 4.3570 - val_accuracy: 0.5381\n",
            "Epoch 4446/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1908 - accuracy: 0.9010\n",
            "Epoch 4446: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1938 - accuracy: 0.8988 - val_loss: 4.2324 - val_accuracy: 0.5393\n",
            "Epoch 4447/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1933 - accuracy: 0.8964\n",
            "Epoch 4447: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1954 - accuracy: 0.8950 - val_loss: 4.3715 - val_accuracy: 0.5413\n",
            "Epoch 4448/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1897 - accuracy: 0.9018\n",
            "Epoch 4448: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1946 - accuracy: 0.8983 - val_loss: 4.2710 - val_accuracy: 0.5322\n",
            "Epoch 4449/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1978 - accuracy: 0.8977\n",
            "Epoch 4449: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1990 - accuracy: 0.8973 - val_loss: 4.2397 - val_accuracy: 0.5360\n",
            "Epoch 4450/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1987 - accuracy: 0.8956\n",
            "Epoch 4450: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2007 - accuracy: 0.8923 - val_loss: 4.2788 - val_accuracy: 0.5313\n",
            "Epoch 4451/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1967 - accuracy: 0.8987\n",
            "Epoch 4451: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1981 - accuracy: 0.8975 - val_loss: 4.3387 - val_accuracy: 0.5322\n",
            "Epoch 4452/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2002 - accuracy: 0.8952\n",
            "Epoch 4452: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2008 - accuracy: 0.8949 - val_loss: 4.3384 - val_accuracy: 0.5369\n",
            "Epoch 4453/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2041 - accuracy: 0.8941\n",
            "Epoch 4453: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2047 - accuracy: 0.8931 - val_loss: 3.8961 - val_accuracy: 0.5310\n",
            "Epoch 4454/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2101 - accuracy: 0.8898\n",
            "Epoch 4454: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2119 - accuracy: 0.8884 - val_loss: 4.1680 - val_accuracy: 0.5351\n",
            "Epoch 4455/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2006 - accuracy: 0.8962\n",
            "Epoch 4455: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2044 - accuracy: 0.8944 - val_loss: 4.2339 - val_accuracy: 0.5322\n",
            "Epoch 4456/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.8945\n",
            "Epoch 4456: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2031 - accuracy: 0.8945 - val_loss: 4.2080 - val_accuracy: 0.5331\n",
            "Epoch 4457/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2084 - accuracy: 0.8908\n",
            "Epoch 4457: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2090 - accuracy: 0.8902 - val_loss: 4.0898 - val_accuracy: 0.5360\n",
            "Epoch 4458/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2010 - accuracy: 0.8956\n",
            "Epoch 4458: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2016 - accuracy: 0.8947 - val_loss: 4.3369 - val_accuracy: 0.5410\n",
            "Epoch 4459/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1851 - accuracy: 0.9037\n",
            "Epoch 4459: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1892 - accuracy: 0.9004 - val_loss: 4.1482 - val_accuracy: 0.5287\n",
            "Epoch 4460/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1926 - accuracy: 0.8976\n",
            "Epoch 4460: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1947 - accuracy: 0.8964 - val_loss: 4.2725 - val_accuracy: 0.5319\n",
            "Epoch 4461/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1954 - accuracy: 0.8966\n",
            "Epoch 4461: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1967 - accuracy: 0.8962 - val_loss: 4.1704 - val_accuracy: 0.5381\n",
            "Epoch 4462/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.8972\n",
            "Epoch 4462: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1953 - accuracy: 0.8972 - val_loss: 4.3850 - val_accuracy: 0.5287\n",
            "Epoch 4463/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1881 - accuracy: 0.9007\n",
            "Epoch 4463: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1909 - accuracy: 0.8982 - val_loss: 4.2836 - val_accuracy: 0.5322\n",
            "Epoch 4464/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1864 - accuracy: 0.9009\n",
            "Epoch 4464: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1889 - accuracy: 0.8994 - val_loss: 4.2130 - val_accuracy: 0.5354\n",
            "Epoch 4465/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1926 - accuracy: 0.8986\n",
            "Epoch 4465: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1926 - accuracy: 0.8984 - val_loss: 4.4017 - val_accuracy: 0.5346\n",
            "Epoch 4466/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.8979\n",
            "Epoch 4466: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1920 - accuracy: 0.8979 - val_loss: 4.2520 - val_accuracy: 0.5360\n",
            "Epoch 4467/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.9000\n",
            "Epoch 4467: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1888 - accuracy: 0.9000 - val_loss: 4.2397 - val_accuracy: 0.5284\n",
            "Epoch 4468/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1959 - accuracy: 0.8971\n",
            "Epoch 4468: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1991 - accuracy: 0.8958 - val_loss: 4.4432 - val_accuracy: 0.5346\n",
            "Epoch 4469/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2013 - accuracy: 0.8978\n",
            "Epoch 4469: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2037 - accuracy: 0.8955 - val_loss: 4.4104 - val_accuracy: 0.5349\n",
            "Epoch 4470/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1960 - accuracy: 0.8955\n",
            "Epoch 4470: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1974 - accuracy: 0.8944 - val_loss: 4.3677 - val_accuracy: 0.5325\n",
            "Epoch 4471/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1929 - accuracy: 0.8953\n",
            "Epoch 4471: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1940 - accuracy: 0.8952 - val_loss: 4.3984 - val_accuracy: 0.5381\n",
            "Epoch 4472/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2075 - accuracy: 0.8933\n",
            "Epoch 4472: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2082 - accuracy: 0.8929 - val_loss: 4.5672 - val_accuracy: 0.5351\n",
            "Epoch 4473/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2303 - accuracy: 0.8838\n",
            "Epoch 4473: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2311 - accuracy: 0.8830 - val_loss: 4.3497 - val_accuracy: 0.5354\n",
            "Epoch 4474/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2206 - accuracy: 0.8863\n",
            "Epoch 4474: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2204 - accuracy: 0.8865 - val_loss: 4.2513 - val_accuracy: 0.5337\n",
            "Epoch 4475/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1956 - accuracy: 0.8979\n",
            "Epoch 4475: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1981 - accuracy: 0.8955 - val_loss: 4.1939 - val_accuracy: 0.5343\n",
            "Epoch 4476/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1940 - accuracy: 0.8999\n",
            "Epoch 4476: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1940 - accuracy: 0.8990 - val_loss: 4.1449 - val_accuracy: 0.5328\n",
            "Epoch 4477/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2005 - accuracy: 0.8952\n",
            "Epoch 4477: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2013 - accuracy: 0.8949 - val_loss: 4.1112 - val_accuracy: 0.5316\n",
            "Epoch 4478/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1960 - accuracy: 0.8986\n",
            "Epoch 4478: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1969 - accuracy: 0.8976 - val_loss: 4.0123 - val_accuracy: 0.5308\n",
            "Epoch 4479/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.8996\n",
            "Epoch 4479: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1963 - accuracy: 0.8996 - val_loss: 4.3708 - val_accuracy: 0.5319\n",
            "Epoch 4480/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1910 - accuracy: 0.8975\n",
            "Epoch 4480: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1921 - accuracy: 0.8975 - val_loss: 4.2957 - val_accuracy: 0.5325\n",
            "Epoch 4481/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1970 - accuracy: 0.8995\n",
            "Epoch 4481: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1997 - accuracy: 0.8974 - val_loss: 4.1584 - val_accuracy: 0.5278\n",
            "Epoch 4482/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1962 - accuracy: 0.8967\n",
            "Epoch 4482: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1963 - accuracy: 0.8961 - val_loss: 4.0509 - val_accuracy: 0.5366\n",
            "Epoch 4483/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.8972\n",
            "Epoch 4483: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1970 - accuracy: 0.8972 - val_loss: 4.1529 - val_accuracy: 0.5246\n",
            "Epoch 4484/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2025 - accuracy: 0.8948\n",
            "Epoch 4484: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2014 - accuracy: 0.8946 - val_loss: 4.1250 - val_accuracy: 0.5340\n",
            "Epoch 4485/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1983 - accuracy: 0.8958\n",
            "Epoch 4485: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1991 - accuracy: 0.8953 - val_loss: 4.3862 - val_accuracy: 0.5305\n",
            "Epoch 4486/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2038 - accuracy: 0.8924\n",
            "Epoch 4486: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2027 - accuracy: 0.8931 - val_loss: 4.2700 - val_accuracy: 0.5308\n",
            "Epoch 4487/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1972 - accuracy: 0.8996\n",
            "Epoch 4487: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2012 - accuracy: 0.8958 - val_loss: 4.2905 - val_accuracy: 0.5360\n",
            "Epoch 4488/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1885 - accuracy: 0.9009\n",
            "Epoch 4488: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1898 - accuracy: 0.9000 - val_loss: 4.2329 - val_accuracy: 0.5337\n",
            "Epoch 4489/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.9005\n",
            "Epoch 4489: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1910 - accuracy: 0.9005 - val_loss: 4.2591 - val_accuracy: 0.5381\n",
            "Epoch 4490/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.8953\n",
            "Epoch 4490: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 1s 22ms/step - loss: 0.1937 - accuracy: 0.8953 - val_loss: 4.3249 - val_accuracy: 0.5354\n",
            "Epoch 4491/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1931 - accuracy: 0.8987\n",
            "Epoch 4491: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1934 - accuracy: 0.8985 - val_loss: 4.1223 - val_accuracy: 0.5284\n",
            "Epoch 4492/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1921 - accuracy: 0.8975\n",
            "Epoch 4492: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1933 - accuracy: 0.8964 - val_loss: 4.2941 - val_accuracy: 0.5310\n",
            "Epoch 4493/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1937 - accuracy: 0.8992\n",
            "Epoch 4493: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1941 - accuracy: 0.8988 - val_loss: 4.3026 - val_accuracy: 0.5343\n",
            "Epoch 4494/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1845 - accuracy: 0.9022\n",
            "Epoch 4494: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1851 - accuracy: 0.9020 - val_loss: 4.1990 - val_accuracy: 0.5422\n",
            "Epoch 4495/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1859 - accuracy: 0.9010\n",
            "Epoch 4495: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1864 - accuracy: 0.9010 - val_loss: 4.3142 - val_accuracy: 0.5346\n",
            "Epoch 4496/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1857 - accuracy: 0.9015\n",
            "Epoch 4496: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1878 - accuracy: 0.9001 - val_loss: 4.2673 - val_accuracy: 0.5243\n",
            "Epoch 4497/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1941 - accuracy: 0.8998\n",
            "Epoch 4497: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1954 - accuracy: 0.8988 - val_loss: 4.4864 - val_accuracy: 0.5296\n",
            "Epoch 4498/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1951 - accuracy: 0.9004\n",
            "Epoch 4498: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1957 - accuracy: 0.8999 - val_loss: 4.2815 - val_accuracy: 0.5305\n",
            "Epoch 4499/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1978 - accuracy: 0.8960\n",
            "Epoch 4499: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1979 - accuracy: 0.8958 - val_loss: 4.2673 - val_accuracy: 0.5310\n",
            "Epoch 4500/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2023 - accuracy: 0.8939\n",
            "Epoch 4500: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2043 - accuracy: 0.8927 - val_loss: 4.2683 - val_accuracy: 0.5378\n",
            "Epoch 4501/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2060 - accuracy: 0.8917\n",
            "Epoch 4501: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2058 - accuracy: 0.8923 - val_loss: 4.2542 - val_accuracy: 0.5331\n",
            "Epoch 4502/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1985 - accuracy: 0.8949\n",
            "Epoch 4502: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2018 - accuracy: 0.8926 - val_loss: 4.0687 - val_accuracy: 0.5290\n",
            "Epoch 4503/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.8963\n",
            "Epoch 4503: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1990 - accuracy: 0.8963 - val_loss: 4.3537 - val_accuracy: 0.5363\n",
            "Epoch 4504/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1961 - accuracy: 0.8974\n",
            "Epoch 4504: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1973 - accuracy: 0.8967 - val_loss: 4.2397 - val_accuracy: 0.5369\n",
            "Epoch 4505/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1919 - accuracy: 0.8983\n",
            "Epoch 4505: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1912 - accuracy: 0.8988 - val_loss: 4.4418 - val_accuracy: 0.5349\n",
            "Epoch 4506/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.8971\n",
            "Epoch 4506: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1975 - accuracy: 0.8971 - val_loss: 4.3753 - val_accuracy: 0.5299\n",
            "Epoch 4507/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1949 - accuracy: 0.8961\n",
            "Epoch 4507: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1982 - accuracy: 0.8936 - val_loss: 4.3991 - val_accuracy: 0.5395\n",
            "Epoch 4508/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1960 - accuracy: 0.8956\n",
            "Epoch 4508: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1970 - accuracy: 0.8953 - val_loss: 4.1986 - val_accuracy: 0.5310\n",
            "Epoch 4509/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1986 - accuracy: 0.8989\n",
            "Epoch 4509: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1992 - accuracy: 0.8976 - val_loss: 4.2983 - val_accuracy: 0.5316\n",
            "Epoch 4510/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.8969\n",
            "Epoch 4510: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1926 - accuracy: 0.8969 - val_loss: 4.2819 - val_accuracy: 0.5328\n",
            "Epoch 4511/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1988 - accuracy: 0.8962\n",
            "Epoch 4511: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1995 - accuracy: 0.8959 - val_loss: 4.4561 - val_accuracy: 0.5393\n",
            "Epoch 4512/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1904 - accuracy: 0.8996\n",
            "Epoch 4512: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1893 - accuracy: 0.9004 - val_loss: 4.2924 - val_accuracy: 0.5413\n",
            "Epoch 4513/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9024\n",
            "Epoch 4513: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1847 - accuracy: 0.9018 - val_loss: 4.3767 - val_accuracy: 0.5351\n",
            "Epoch 4514/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1884 - accuracy: 0.9004\n",
            "Epoch 4514: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1901 - accuracy: 0.8985 - val_loss: 4.4582 - val_accuracy: 0.5381\n",
            "Epoch 4515/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1888 - accuracy: 0.8994\n",
            "Epoch 4515: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1893 - accuracy: 0.8988 - val_loss: 4.3431 - val_accuracy: 0.5275\n",
            "Epoch 4516/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1914 - accuracy: 0.9010\n",
            "Epoch 4516: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1926 - accuracy: 0.8994 - val_loss: 4.4923 - val_accuracy: 0.5331\n",
            "Epoch 4517/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1937 - accuracy: 0.8980\n",
            "Epoch 4517: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1942 - accuracy: 0.8975 - val_loss: 4.3472 - val_accuracy: 0.5299\n",
            "Epoch 4518/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.8968\n",
            "Epoch 4518: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2015 - accuracy: 0.8968 - val_loss: 4.5448 - val_accuracy: 0.5343\n",
            "Epoch 4519/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2077 - accuracy: 0.8929\n",
            "Epoch 4519: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2092 - accuracy: 0.8910 - val_loss: 4.3784 - val_accuracy: 0.5293\n",
            "Epoch 4520/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2048 - accuracy: 0.8957\n",
            "Epoch 4520: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2043 - accuracy: 0.8953 - val_loss: 4.1005 - val_accuracy: 0.5351\n",
            "Epoch 4521/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1960 - accuracy: 0.8997\n",
            "Epoch 4521: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1951 - accuracy: 0.8994 - val_loss: 4.3376 - val_accuracy: 0.5343\n",
            "Epoch 4522/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.8954\n",
            "Epoch 4522: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1951 - accuracy: 0.8954 - val_loss: 4.2261 - val_accuracy: 0.5252\n",
            "Epoch 4523/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.8972\n",
            "Epoch 4523: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1957 - accuracy: 0.8972 - val_loss: 4.1409 - val_accuracy: 0.5278\n",
            "Epoch 4524/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.8943\n",
            "Epoch 4524: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1980 - accuracy: 0.8943 - val_loss: 4.3865 - val_accuracy: 0.5360\n",
            "Epoch 4525/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2010 - accuracy: 0.8939\n",
            "Epoch 4525: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2004 - accuracy: 0.8942 - val_loss: 4.1408 - val_accuracy: 0.5369\n",
            "Epoch 4526/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2002 - accuracy: 0.8949\n",
            "Epoch 4526: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2019 - accuracy: 0.8938 - val_loss: 4.1077 - val_accuracy: 0.5293\n",
            "Epoch 4527/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1963 - accuracy: 0.8973\n",
            "Epoch 4527: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1966 - accuracy: 0.8975 - val_loss: 4.3392 - val_accuracy: 0.5325\n",
            "Epoch 4528/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1972 - accuracy: 0.8966\n",
            "Epoch 4528: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1980 - accuracy: 0.8957 - val_loss: 4.4127 - val_accuracy: 0.5384\n",
            "Epoch 4529/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1926 - accuracy: 0.8983\n",
            "Epoch 4529: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1931 - accuracy: 0.8979 - val_loss: 4.2010 - val_accuracy: 0.5293\n",
            "Epoch 4530/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1918 - accuracy: 0.8997\n",
            "Epoch 4530: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1929 - accuracy: 0.8982 - val_loss: 4.5317 - val_accuracy: 0.5375\n",
            "Epoch 4531/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1962 - accuracy: 0.8997\n",
            "Epoch 4531: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1981 - accuracy: 0.8984 - val_loss: 4.3680 - val_accuracy: 0.5381\n",
            "Epoch 4532/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1962 - accuracy: 0.8952\n",
            "Epoch 4532: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1962 - accuracy: 0.8947 - val_loss: 4.3459 - val_accuracy: 0.5261\n",
            "Epoch 4533/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1906 - accuracy: 0.9008\n",
            "Epoch 4533: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1937 - accuracy: 0.8983 - val_loss: 4.0358 - val_accuracy: 0.5269\n",
            "Epoch 4534/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1990 - accuracy: 0.8963\n",
            "Epoch 4534: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1997 - accuracy: 0.8957 - val_loss: 4.3971 - val_accuracy: 0.5331\n",
            "Epoch 4535/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1963 - accuracy: 0.8971\n",
            "Epoch 4535: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1955 - accuracy: 0.8978 - val_loss: 4.3426 - val_accuracy: 0.5269\n",
            "Epoch 4536/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1987 - accuracy: 0.8964\n",
            "Epoch 4536: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2020 - accuracy: 0.8944 - val_loss: 4.4339 - val_accuracy: 0.5390\n",
            "Epoch 4537/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2020 - accuracy: 0.8962\n",
            "Epoch 4537: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2007 - accuracy: 0.8962 - val_loss: 4.3484 - val_accuracy: 0.5375\n",
            "Epoch 4538/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1881 - accuracy: 0.9029\n",
            "Epoch 4538: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1912 - accuracy: 0.9002 - val_loss: 4.3417 - val_accuracy: 0.5278\n",
            "Epoch 4539/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1977 - accuracy: 0.8962\n",
            "Epoch 4539: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1989 - accuracy: 0.8952 - val_loss: 4.3834 - val_accuracy: 0.5328\n",
            "Epoch 4540/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2004 - accuracy: 0.8926\n",
            "Epoch 4540: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.2021 - accuracy: 0.8910 - val_loss: 4.1461 - val_accuracy: 0.5325\n",
            "Epoch 4541/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2038 - accuracy: 0.8933\n",
            "Epoch 4541: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2048 - accuracy: 0.8933 - val_loss: 4.3685 - val_accuracy: 0.5302\n",
            "Epoch 4542/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1962 - accuracy: 0.8974\n",
            "Epoch 4542: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2001 - accuracy: 0.8956 - val_loss: 4.1928 - val_accuracy: 0.5281\n",
            "Epoch 4543/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2078 - accuracy: 0.8897\n",
            "Epoch 4543: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2074 - accuracy: 0.8899 - val_loss: 4.2803 - val_accuracy: 0.5354\n",
            "Epoch 4544/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2021 - accuracy: 0.8940\n",
            "Epoch 4544: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2032 - accuracy: 0.8923 - val_loss: 4.0388 - val_accuracy: 0.5255\n",
            "Epoch 4545/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1969 - accuracy: 0.8984\n",
            "Epoch 4545: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1988 - accuracy: 0.8971 - val_loss: 4.1480 - val_accuracy: 0.5331\n",
            "Epoch 4546/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1906 - accuracy: 0.8983\n",
            "Epoch 4546: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1931 - accuracy: 0.8966 - val_loss: 4.4446 - val_accuracy: 0.5387\n",
            "Epoch 4547/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.8966\n",
            "Epoch 4547: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2000 - accuracy: 0.8966 - val_loss: 4.2502 - val_accuracy: 0.5331\n",
            "Epoch 4548/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1968 - accuracy: 0.8942\n",
            "Epoch 4548: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1978 - accuracy: 0.8933 - val_loss: 4.1969 - val_accuracy: 0.5346\n",
            "Epoch 4549/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.8983\n",
            "Epoch 4549: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1943 - accuracy: 0.8983 - val_loss: 4.3924 - val_accuracy: 0.5349\n",
            "Epoch 4550/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.8890\n",
            "Epoch 4550: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2111 - accuracy: 0.8890 - val_loss: 4.2991 - val_accuracy: 0.5281\n",
            "Epoch 4551/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2109 - accuracy: 0.8919\n",
            "Epoch 4551: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2149 - accuracy: 0.8887 - val_loss: 4.1471 - val_accuracy: 0.5226\n",
            "Epoch 4552/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2102 - accuracy: 0.8880\n",
            "Epoch 4552: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2093 - accuracy: 0.8886 - val_loss: 3.9693 - val_accuracy: 0.5310\n",
            "Epoch 4553/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2005 - accuracy: 0.8975\n",
            "Epoch 4553: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2012 - accuracy: 0.8963 - val_loss: 4.1320 - val_accuracy: 0.5346\n",
            "Epoch 4554/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2059 - accuracy: 0.8944\n",
            "Epoch 4554: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2063 - accuracy: 0.8952 - val_loss: 4.3115 - val_accuracy: 0.5375\n",
            "Epoch 4555/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2085 - accuracy: 0.8898\n",
            "Epoch 4555: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2092 - accuracy: 0.8894 - val_loss: 4.3047 - val_accuracy: 0.5448\n",
            "Epoch 4556/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1971 - accuracy: 0.9013\n",
            "Epoch 4556: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2003 - accuracy: 0.8975 - val_loss: 4.2952 - val_accuracy: 0.5407\n",
            "Epoch 4557/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9008\n",
            "Epoch 4557: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1901 - accuracy: 0.9008 - val_loss: 4.1351 - val_accuracy: 0.5234\n",
            "Epoch 4558/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1862 - accuracy: 0.8993\n",
            "Epoch 4558: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1873 - accuracy: 0.8990 - val_loss: 4.2522 - val_accuracy: 0.5381\n",
            "Epoch 4559/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1865 - accuracy: 0.9022\n",
            "Epoch 4559: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1888 - accuracy: 0.9000 - val_loss: 4.4030 - val_accuracy: 0.5334\n",
            "Epoch 4560/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9050\n",
            "Epoch 4560: loss did not improve from 0.18308\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1850 - accuracy: 0.9016 - val_loss: 4.2958 - val_accuracy: 0.5384\n",
            "Epoch 4561/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1811 - accuracy: 0.9040\n",
            "Epoch 4561: loss improved from 0.18308 to 0.18180, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.1818 - accuracy: 0.9046 - val_loss: 4.5323 - val_accuracy: 0.5313\n",
            "Epoch 4562/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1821 - accuracy: 0.9055\n",
            "Epoch 4562: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1848 - accuracy: 0.9034 - val_loss: 4.3728 - val_accuracy: 0.5299\n",
            "Epoch 4563/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9000\n",
            "Epoch 4563: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1901 - accuracy: 0.9000 - val_loss: 4.4131 - val_accuracy: 0.5346\n",
            "Epoch 4564/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1927 - accuracy: 0.9000\n",
            "Epoch 4564: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1932 - accuracy: 0.9000 - val_loss: 4.4324 - val_accuracy: 0.5381\n",
            "Epoch 4565/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1958 - accuracy: 0.8954\n",
            "Epoch 4565: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1977 - accuracy: 0.8939 - val_loss: 4.4335 - val_accuracy: 0.5302\n",
            "Epoch 4566/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1970 - accuracy: 0.8992\n",
            "Epoch 4566: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1981 - accuracy: 0.8985 - val_loss: 4.2951 - val_accuracy: 0.5275\n",
            "Epoch 4567/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1869 - accuracy: 0.9038\n",
            "Epoch 4567: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1907 - accuracy: 0.9005 - val_loss: 4.2553 - val_accuracy: 0.5290\n",
            "Epoch 4568/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2002 - accuracy: 0.8932\n",
            "Epoch 4568: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1999 - accuracy: 0.8939 - val_loss: 4.2052 - val_accuracy: 0.5325\n",
            "Epoch 4569/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1979 - accuracy: 0.8951\n",
            "Epoch 4569: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1986 - accuracy: 0.8950 - val_loss: 4.3826 - val_accuracy: 0.5366\n",
            "Epoch 4570/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1955 - accuracy: 0.8969\n",
            "Epoch 4570: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1965 - accuracy: 0.8959 - val_loss: 4.3247 - val_accuracy: 0.5319\n",
            "Epoch 4571/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1981 - accuracy: 0.8971\n",
            "Epoch 4571: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1989 - accuracy: 0.8960 - val_loss: 4.2959 - val_accuracy: 0.5434\n",
            "Epoch 4572/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1936 - accuracy: 0.8965\n",
            "Epoch 4572: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1956 - accuracy: 0.8966 - val_loss: 4.3727 - val_accuracy: 0.5448\n",
            "Epoch 4573/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1932 - accuracy: 0.8997\n",
            "Epoch 4573: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1967 - accuracy: 0.8961 - val_loss: 4.3431 - val_accuracy: 0.5302\n",
            "Epoch 4574/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.8988\n",
            "Epoch 4574: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1920 - accuracy: 0.8988 - val_loss: 4.3494 - val_accuracy: 0.5331\n",
            "Epoch 4575/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1992 - accuracy: 0.8962\n",
            "Epoch 4575: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2017 - accuracy: 0.8944 - val_loss: 4.2078 - val_accuracy: 0.5360\n",
            "Epoch 4576/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1904 - accuracy: 0.8996\n",
            "Epoch 4576: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1910 - accuracy: 0.8987 - val_loss: 4.4334 - val_accuracy: 0.5381\n",
            "Epoch 4577/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1921 - accuracy: 0.9030\n",
            "Epoch 4577: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1945 - accuracy: 0.9009 - val_loss: 4.4083 - val_accuracy: 0.5395\n",
            "Epoch 4578/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1912 - accuracy: 0.8996\n",
            "Epoch 4578: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1923 - accuracy: 0.8988 - val_loss: 4.3722 - val_accuracy: 0.5322\n",
            "Epoch 4579/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2001 - accuracy: 0.8949\n",
            "Epoch 4579: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2013 - accuracy: 0.8939 - val_loss: 4.3081 - val_accuracy: 0.5372\n",
            "Epoch 4580/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.8951\n",
            "Epoch 4580: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 22ms/step - loss: 0.1966 - accuracy: 0.8951 - val_loss: 4.3073 - val_accuracy: 0.5360\n",
            "Epoch 4581/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.8974\n",
            "Epoch 4581: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1940 - accuracy: 0.8974 - val_loss: 4.1975 - val_accuracy: 0.5290\n",
            "Epoch 4582/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1930 - accuracy: 0.8967\n",
            "Epoch 4582: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1946 - accuracy: 0.8966 - val_loss: 4.2430 - val_accuracy: 0.5346\n",
            "Epoch 4583/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2009 - accuracy: 0.8964\n",
            "Epoch 4583: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2007 - accuracy: 0.8963 - val_loss: 4.4008 - val_accuracy: 0.5296\n",
            "Epoch 4584/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1942 - accuracy: 0.8970\n",
            "Epoch 4584: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1968 - accuracy: 0.8947 - val_loss: 4.2858 - val_accuracy: 0.5375\n",
            "Epoch 4585/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1966 - accuracy: 0.8940\n",
            "Epoch 4585: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1954 - accuracy: 0.8945 - val_loss: 4.3833 - val_accuracy: 0.5328\n",
            "Epoch 4586/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1895 - accuracy: 0.9023\n",
            "Epoch 4586: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1903 - accuracy: 0.9012 - val_loss: 4.2383 - val_accuracy: 0.5351\n",
            "Epoch 4587/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8999\n",
            "Epoch 4587: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1927 - accuracy: 0.9005 - val_loss: 4.3618 - val_accuracy: 0.5390\n",
            "Epoch 4588/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1920 - accuracy: 0.8997\n",
            "Epoch 4588: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1935 - accuracy: 0.8984 - val_loss: 4.4831 - val_accuracy: 0.5296\n",
            "Epoch 4589/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1933 - accuracy: 0.8975\n",
            "Epoch 4589: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1931 - accuracy: 0.8972 - val_loss: 4.1711 - val_accuracy: 0.5351\n",
            "Epoch 4590/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2077 - accuracy: 0.8913\n",
            "Epoch 4590: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2080 - accuracy: 0.8914 - val_loss: 4.2389 - val_accuracy: 0.5302\n",
            "Epoch 4591/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2144 - accuracy: 0.8904\n",
            "Epoch 4591: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2141 - accuracy: 0.8901 - val_loss: 4.0484 - val_accuracy: 0.5267\n",
            "Epoch 4592/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1997 - accuracy: 0.8960\n",
            "Epoch 4592: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2002 - accuracy: 0.8943 - val_loss: 4.2450 - val_accuracy: 0.5349\n",
            "Epoch 4593/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1917 - accuracy: 0.8990\n",
            "Epoch 4593: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1926 - accuracy: 0.8989 - val_loss: 4.2458 - val_accuracy: 0.5322\n",
            "Epoch 4594/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1909 - accuracy: 0.8993\n",
            "Epoch 4594: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1933 - accuracy: 0.8974 - val_loss: 4.2194 - val_accuracy: 0.5278\n",
            "Epoch 4595/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1941 - accuracy: 0.8985\n",
            "Epoch 4595: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1958 - accuracy: 0.8979 - val_loss: 4.1984 - val_accuracy: 0.5357\n",
            "Epoch 4596/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1908 - accuracy: 0.9023\n",
            "Epoch 4596: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1910 - accuracy: 0.9023 - val_loss: 4.3581 - val_accuracy: 0.5316\n",
            "Epoch 4597/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1896 - accuracy: 0.9006\n",
            "Epoch 4597: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1895 - accuracy: 0.9002 - val_loss: 4.3065 - val_accuracy: 0.5296\n",
            "Epoch 4598/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1886 - accuracy: 0.8994\n",
            "Epoch 4598: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1907 - accuracy: 0.8982 - val_loss: 4.2879 - val_accuracy: 0.5308\n",
            "Epoch 4599/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1825 - accuracy: 0.9010\n",
            "Epoch 4599: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1866 - accuracy: 0.8986 - val_loss: 4.2257 - val_accuracy: 0.5302\n",
            "Epoch 4600/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1905 - accuracy: 0.9014\n",
            "Epoch 4600: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1900 - accuracy: 0.9010 - val_loss: 4.4241 - val_accuracy: 0.5369\n",
            "Epoch 4601/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1896 - accuracy: 0.8995\n",
            "Epoch 4601: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1924 - accuracy: 0.8983 - val_loss: 4.4424 - val_accuracy: 0.5325\n",
            "Epoch 4602/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1955 - accuracy: 0.8996\n",
            "Epoch 4602: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1980 - accuracy: 0.8975 - val_loss: 4.3240 - val_accuracy: 0.5331\n",
            "Epoch 4603/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2008 - accuracy: 0.8926\n",
            "Epoch 4603: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2010 - accuracy: 0.8933 - val_loss: 4.2763 - val_accuracy: 0.5305\n",
            "Epoch 4604/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2140 - accuracy: 0.8898\n",
            "Epoch 4604: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2150 - accuracy: 0.8898 - val_loss: 4.1294 - val_accuracy: 0.5331\n",
            "Epoch 4605/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.8865\n",
            "Epoch 4605: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2243 - accuracy: 0.8865 - val_loss: 4.1951 - val_accuracy: 0.5395\n",
            "Epoch 4606/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.8918\n",
            "Epoch 4606: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2048 - accuracy: 0.8918 - val_loss: 4.2267 - val_accuracy: 0.5369\n",
            "Epoch 4607/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1993 - accuracy: 0.8978\n",
            "Epoch 4607: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2000 - accuracy: 0.8962 - val_loss: 4.3119 - val_accuracy: 0.5337\n",
            "Epoch 4608/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.8979\n",
            "Epoch 4608: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1939 - accuracy: 0.8979 - val_loss: 4.2747 - val_accuracy: 0.5290\n",
            "Epoch 4609/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1895 - accuracy: 0.9024\n",
            "Epoch 4609: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1922 - accuracy: 0.9010 - val_loss: 4.3582 - val_accuracy: 0.5390\n",
            "Epoch 4610/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1895 - accuracy: 0.8985\n",
            "Epoch 4610: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1889 - accuracy: 0.8985 - val_loss: 4.3953 - val_accuracy: 0.5360\n",
            "Epoch 4611/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1859 - accuracy: 0.9006\n",
            "Epoch 4611: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1870 - accuracy: 0.9002 - val_loss: 4.2457 - val_accuracy: 0.5375\n",
            "Epoch 4612/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1802 - accuracy: 0.9044\n",
            "Epoch 4612: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1823 - accuracy: 0.9034 - val_loss: 4.2505 - val_accuracy: 0.5375\n",
            "Epoch 4613/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1829 - accuracy: 0.9028\n",
            "Epoch 4613: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1856 - accuracy: 0.9009 - val_loss: 4.4269 - val_accuracy: 0.5319\n",
            "Epoch 4614/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.8984\n",
            "Epoch 4614: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1949 - accuracy: 0.8984 - val_loss: 4.3756 - val_accuracy: 0.5313\n",
            "Epoch 4615/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1842 - accuracy: 0.9043\n",
            "Epoch 4615: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1850 - accuracy: 0.9034 - val_loss: 4.4830 - val_accuracy: 0.5349\n",
            "Epoch 4616/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1888 - accuracy: 0.8989\n",
            "Epoch 4616: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1887 - accuracy: 0.8996 - val_loss: 4.3529 - val_accuracy: 0.5366\n",
            "Epoch 4617/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1858 - accuracy: 0.9048\n",
            "Epoch 4617: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1876 - accuracy: 0.9035 - val_loss: 4.5134 - val_accuracy: 0.5331\n",
            "Epoch 4618/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9036\n",
            "Epoch 4618: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1859 - accuracy: 0.9036 - val_loss: 4.4123 - val_accuracy: 0.5387\n",
            "Epoch 4619/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1917 - accuracy: 0.8977\n",
            "Epoch 4619: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1930 - accuracy: 0.8966 - val_loss: 4.5577 - val_accuracy: 0.5431\n",
            "Epoch 4620/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1922 - accuracy: 0.8974\n",
            "Epoch 4620: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1934 - accuracy: 0.8968 - val_loss: 4.3605 - val_accuracy: 0.5267\n",
            "Epoch 4621/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1891 - accuracy: 0.9024\n",
            "Epoch 4621: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1929 - accuracy: 0.8991 - val_loss: 4.4184 - val_accuracy: 0.5322\n",
            "Epoch 4622/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8971\n",
            "Epoch 4622: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1947 - accuracy: 0.8970 - val_loss: 4.3567 - val_accuracy: 0.5366\n",
            "Epoch 4623/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1969 - accuracy: 0.9016\n",
            "Epoch 4623: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1988 - accuracy: 0.8997 - val_loss: 4.4743 - val_accuracy: 0.5302\n",
            "Epoch 4624/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1976 - accuracy: 0.8954\n",
            "Epoch 4624: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2001 - accuracy: 0.8938 - val_loss: 4.2894 - val_accuracy: 0.5384\n",
            "Epoch 4625/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1999 - accuracy: 0.8949\n",
            "Epoch 4625: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2006 - accuracy: 0.8940 - val_loss: 4.3512 - val_accuracy: 0.5407\n",
            "Epoch 4626/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2050 - accuracy: 0.8926\n",
            "Epoch 4626: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2062 - accuracy: 0.8920 - val_loss: 4.4107 - val_accuracy: 0.5378\n",
            "Epoch 4627/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2022 - accuracy: 0.8974\n",
            "Epoch 4627: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2022 - accuracy: 0.8972 - val_loss: 4.2544 - val_accuracy: 0.5299\n",
            "Epoch 4628/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2003 - accuracy: 0.8940\n",
            "Epoch 4628: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2004 - accuracy: 0.8940 - val_loss: 4.4575 - val_accuracy: 0.5325\n",
            "Epoch 4629/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.8965\n",
            "Epoch 4629: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1928 - accuracy: 0.8965 - val_loss: 4.3785 - val_accuracy: 0.5272\n",
            "Epoch 4630/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.9007\n",
            "Epoch 4630: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1907 - accuracy: 0.9007 - val_loss: 4.4999 - val_accuracy: 0.5360\n",
            "Epoch 4631/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1900 - accuracy: 0.9008\n",
            "Epoch 4631: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1931 - accuracy: 0.8984 - val_loss: 4.5114 - val_accuracy: 0.5334\n",
            "Epoch 4632/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2046 - accuracy: 0.8945\n",
            "Epoch 4632: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2054 - accuracy: 0.8940 - val_loss: 4.2347 - val_accuracy: 0.5305\n",
            "Epoch 4633/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2021 - accuracy: 0.8967\n",
            "Epoch 4633: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2025 - accuracy: 0.8957 - val_loss: 4.2651 - val_accuracy: 0.5308\n",
            "Epoch 4634/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.9030\n",
            "Epoch 4634: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1932 - accuracy: 0.8996 - val_loss: 4.3340 - val_accuracy: 0.5351\n",
            "Epoch 4635/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1950 - accuracy: 0.8975\n",
            "Epoch 4635: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1971 - accuracy: 0.8962 - val_loss: 4.4197 - val_accuracy: 0.5322\n",
            "Epoch 4636/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1956 - accuracy: 0.8974\n",
            "Epoch 4636: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1970 - accuracy: 0.8959 - val_loss: 4.3458 - val_accuracy: 0.5305\n",
            "Epoch 4637/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2097 - accuracy: 0.8923\n",
            "Epoch 4637: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2089 - accuracy: 0.8925 - val_loss: 4.1131 - val_accuracy: 0.5308\n",
            "Epoch 4638/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2073 - accuracy: 0.8906\n",
            "Epoch 4638: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2092 - accuracy: 0.8898 - val_loss: 4.1694 - val_accuracy: 0.5393\n",
            "Epoch 4639/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2072 - accuracy: 0.8924\n",
            "Epoch 4639: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2091 - accuracy: 0.8915 - val_loss: 4.2617 - val_accuracy: 0.5290\n",
            "Epoch 4640/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2179 - accuracy: 0.8870\n",
            "Epoch 4640: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2188 - accuracy: 0.8871 - val_loss: 4.3990 - val_accuracy: 0.5360\n",
            "Epoch 4641/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.8892\n",
            "Epoch 4641: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2068 - accuracy: 0.8892 - val_loss: 4.0900 - val_accuracy: 0.5366\n",
            "Epoch 4642/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2029 - accuracy: 0.8954\n",
            "Epoch 4642: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2042 - accuracy: 0.8946 - val_loss: 4.3650 - val_accuracy: 0.5372\n",
            "Epoch 4643/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1966 - accuracy: 0.8982\n",
            "Epoch 4643: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1995 - accuracy: 0.8962 - val_loss: 4.3560 - val_accuracy: 0.5281\n",
            "Epoch 4644/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1926 - accuracy: 0.8990\n",
            "Epoch 4644: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1934 - accuracy: 0.8980 - val_loss: 4.3887 - val_accuracy: 0.5387\n",
            "Epoch 4645/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1930 - accuracy: 0.8990\n",
            "Epoch 4645: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1953 - accuracy: 0.8973 - val_loss: 4.2463 - val_accuracy: 0.5267\n",
            "Epoch 4646/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1850 - accuracy: 0.9006\n",
            "Epoch 4646: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1868 - accuracy: 0.8986 - val_loss: 4.3967 - val_accuracy: 0.5372\n",
            "Epoch 4647/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1845 - accuracy: 0.9032\n",
            "Epoch 4647: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1882 - accuracy: 0.9010 - val_loss: 4.2202 - val_accuracy: 0.5328\n",
            "Epoch 4648/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1903 - accuracy: 0.9004\n",
            "Epoch 4648: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1912 - accuracy: 0.8977 - val_loss: 4.2973 - val_accuracy: 0.5310\n",
            "Epoch 4649/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1903 - accuracy: 0.9014\n",
            "Epoch 4649: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1899 - accuracy: 0.9010 - val_loss: 4.5062 - val_accuracy: 0.5384\n",
            "Epoch 4650/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1915 - accuracy: 0.9018\n",
            "Epoch 4650: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1937 - accuracy: 0.9009 - val_loss: 4.4047 - val_accuracy: 0.5340\n",
            "Epoch 4651/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9010\n",
            "Epoch 4651: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1885 - accuracy: 0.9010 - val_loss: 4.1624 - val_accuracy: 0.5360\n",
            "Epoch 4652/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1917 - accuracy: 0.9006\n",
            "Epoch 4652: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1937 - accuracy: 0.8996 - val_loss: 4.4224 - val_accuracy: 0.5372\n",
            "Epoch 4653/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1904 - accuracy: 0.8985\n",
            "Epoch 4653: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1919 - accuracy: 0.8985 - val_loss: 4.3116 - val_accuracy: 0.5278\n",
            "Epoch 4654/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.8983\n",
            "Epoch 4654: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1953 - accuracy: 0.8983 - val_loss: 4.5159 - val_accuracy: 0.5325\n",
            "Epoch 4655/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1917 - accuracy: 0.9022\n",
            "Epoch 4655: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1907 - accuracy: 0.9023 - val_loss: 4.4764 - val_accuracy: 0.5390\n",
            "Epoch 4656/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1979 - accuracy: 0.8945\n",
            "Epoch 4656: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1995 - accuracy: 0.8942 - val_loss: 4.2531 - val_accuracy: 0.5346\n",
            "Epoch 4657/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1973 - accuracy: 0.8972\n",
            "Epoch 4657: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1943 - accuracy: 0.8994 - val_loss: 4.2629 - val_accuracy: 0.5395\n",
            "Epoch 4658/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9012\n",
            "Epoch 4658: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1898 - accuracy: 0.9001 - val_loss: 4.3718 - val_accuracy: 0.5287\n",
            "Epoch 4659/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1883 - accuracy: 0.9001\n",
            "Epoch 4659: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1914 - accuracy: 0.8978 - val_loss: 4.4065 - val_accuracy: 0.5328\n",
            "Epoch 4660/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9054\n",
            "Epoch 4660: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1830 - accuracy: 0.9030 - val_loss: 4.3459 - val_accuracy: 0.5343\n",
            "Epoch 4661/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1803 - accuracy: 0.9036\n",
            "Epoch 4661: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1831 - accuracy: 0.9027 - val_loss: 4.2651 - val_accuracy: 0.5293\n",
            "Epoch 4662/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1948 - accuracy: 0.8957\n",
            "Epoch 4662: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1978 - accuracy: 0.8941 - val_loss: 4.4608 - val_accuracy: 0.5269\n",
            "Epoch 4663/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1889 - accuracy: 0.8991\n",
            "Epoch 4663: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1921 - accuracy: 0.8969 - val_loss: 4.5317 - val_accuracy: 0.5343\n",
            "Epoch 4664/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1894 - accuracy: 0.8992\n",
            "Epoch 4664: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1918 - accuracy: 0.8980 - val_loss: 4.3696 - val_accuracy: 0.5296\n",
            "Epoch 4665/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1936 - accuracy: 0.8959\n",
            "Epoch 4665: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1937 - accuracy: 0.8967 - val_loss: 4.4856 - val_accuracy: 0.5305\n",
            "Epoch 4666/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1913 - accuracy: 0.8975\n",
            "Epoch 4666: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1918 - accuracy: 0.8966 - val_loss: 4.3409 - val_accuracy: 0.5322\n",
            "Epoch 4667/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1941 - accuracy: 0.8980\n",
            "Epoch 4667: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1939 - accuracy: 0.8980 - val_loss: 4.3157 - val_accuracy: 0.5384\n",
            "Epoch 4668/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1872 - accuracy: 0.9005\n",
            "Epoch 4668: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1874 - accuracy: 0.8999 - val_loss: 4.5561 - val_accuracy: 0.5313\n",
            "Epoch 4669/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1904 - accuracy: 0.9016\n",
            "Epoch 4669: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1934 - accuracy: 0.8985 - val_loss: 4.5925 - val_accuracy: 0.5302\n",
            "Epoch 4670/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1932 - accuracy: 0.9004\n",
            "Epoch 4670: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1944 - accuracy: 0.8980 - val_loss: 4.4938 - val_accuracy: 0.5299\n",
            "Epoch 4671/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1945 - accuracy: 0.8983\n",
            "Epoch 4671: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1945 - accuracy: 0.8983 - val_loss: 4.4899 - val_accuracy: 0.5381\n",
            "Epoch 4672/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2034 - accuracy: 0.8945\n",
            "Epoch 4672: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2044 - accuracy: 0.8942 - val_loss: 4.5341 - val_accuracy: 0.5252\n",
            "Epoch 4673/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.8909\n",
            "Epoch 4673: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2176 - accuracy: 0.8909 - val_loss: 4.4146 - val_accuracy: 0.5384\n",
            "Epoch 4674/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2258 - accuracy: 0.8858\n",
            "Epoch 4674: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2266 - accuracy: 0.8851 - val_loss: 4.0878 - val_accuracy: 0.5349\n",
            "Epoch 4675/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2042 - accuracy: 0.8929\n",
            "Epoch 4675: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2032 - accuracy: 0.8937 - val_loss: 4.1869 - val_accuracy: 0.5258\n",
            "Epoch 4676/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.8968\n",
            "Epoch 4676: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1965 - accuracy: 0.8968 - val_loss: 4.3084 - val_accuracy: 0.5451\n",
            "Epoch 4677/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1904 - accuracy: 0.9010\n",
            "Epoch 4677: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1919 - accuracy: 0.8985 - val_loss: 4.3195 - val_accuracy: 0.5281\n",
            "Epoch 4678/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1904 - accuracy: 0.8992\n",
            "Epoch 4678: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1903 - accuracy: 0.8996 - val_loss: 4.4476 - val_accuracy: 0.5428\n",
            "Epoch 4679/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1887 - accuracy: 0.8972\n",
            "Epoch 4679: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1899 - accuracy: 0.8967 - val_loss: 4.3238 - val_accuracy: 0.5293\n",
            "Epoch 4680/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1896 - accuracy: 0.8986\n",
            "Epoch 4680: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1924 - accuracy: 0.8964 - val_loss: 4.4065 - val_accuracy: 0.5319\n",
            "Epoch 4681/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1847 - accuracy: 0.9043\n",
            "Epoch 4681: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1874 - accuracy: 0.9024 - val_loss: 4.3427 - val_accuracy: 0.5375\n",
            "Epoch 4682/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.8999\n",
            "Epoch 4682: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1894 - accuracy: 0.8999 - val_loss: 4.3414 - val_accuracy: 0.5322\n",
            "Epoch 4683/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1882 - accuracy: 0.9005\n",
            "Epoch 4683: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1910 - accuracy: 0.8991 - val_loss: 4.3014 - val_accuracy: 0.5354\n",
            "Epoch 4684/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1959 - accuracy: 0.8973\n",
            "Epoch 4684: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1968 - accuracy: 0.8968 - val_loss: 4.4629 - val_accuracy: 0.5381\n",
            "Epoch 4685/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1894 - accuracy: 0.8988\n",
            "Epoch 4685: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1918 - accuracy: 0.8974 - val_loss: 4.2387 - val_accuracy: 0.5322\n",
            "Epoch 4686/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1967 - accuracy: 0.8975\n",
            "Epoch 4686: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1974 - accuracy: 0.8961 - val_loss: 4.5135 - val_accuracy: 0.5305\n",
            "Epoch 4687/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.8981\n",
            "Epoch 4687: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1909 - accuracy: 0.8981 - val_loss: 4.4007 - val_accuracy: 0.5363\n",
            "Epoch 4688/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1897 - accuracy: 0.8998\n",
            "Epoch 4688: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1918 - accuracy: 0.8977 - val_loss: 4.3882 - val_accuracy: 0.5360\n",
            "Epoch 4689/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1967 - accuracy: 0.8964\n",
            "Epoch 4689: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1967 - accuracy: 0.8964 - val_loss: 4.2948 - val_accuracy: 0.5369\n",
            "Epoch 4690/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1836 - accuracy: 0.9043\n",
            "Epoch 4690: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1858 - accuracy: 0.9032 - val_loss: 4.5455 - val_accuracy: 0.5428\n",
            "Epoch 4691/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1886 - accuracy: 0.9014\n",
            "Epoch 4691: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1898 - accuracy: 0.8996 - val_loss: 4.2578 - val_accuracy: 0.5296\n",
            "Epoch 4692/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9062\n",
            "Epoch 4692: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1849 - accuracy: 0.9045 - val_loss: 4.2805 - val_accuracy: 0.5343\n",
            "Epoch 4693/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1857 - accuracy: 0.8989\n",
            "Epoch 4693: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1870 - accuracy: 0.8981 - val_loss: 4.5519 - val_accuracy: 0.5351\n",
            "Epoch 4694/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1990 - accuracy: 0.8957\n",
            "Epoch 4694: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2013 - accuracy: 0.8947 - val_loss: 4.3893 - val_accuracy: 0.5340\n",
            "Epoch 4695/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2021 - accuracy: 0.8945\n",
            "Epoch 4695: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2031 - accuracy: 0.8933 - val_loss: 4.3367 - val_accuracy: 0.5366\n",
            "Epoch 4696/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.8944\n",
            "Epoch 4696: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2021 - accuracy: 0.8944 - val_loss: 4.4573 - val_accuracy: 0.5360\n",
            "Epoch 4697/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1946 - accuracy: 0.8978\n",
            "Epoch 4697: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1959 - accuracy: 0.8971 - val_loss: 4.2781 - val_accuracy: 0.5264\n",
            "Epoch 4698/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1904 - accuracy: 0.8980\n",
            "Epoch 4698: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1906 - accuracy: 0.8977 - val_loss: 4.3788 - val_accuracy: 0.5351\n",
            "Epoch 4699/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1880 - accuracy: 0.9027\n",
            "Epoch 4699: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1897 - accuracy: 0.9017 - val_loss: 4.3342 - val_accuracy: 0.5331\n",
            "Epoch 4700/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1850 - accuracy: 0.9047\n",
            "Epoch 4700: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1872 - accuracy: 0.9029 - val_loss: 4.3504 - val_accuracy: 0.5360\n",
            "Epoch 4701/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1840 - accuracy: 0.9036\n",
            "Epoch 4701: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1864 - accuracy: 0.9018 - val_loss: 4.5577 - val_accuracy: 0.5269\n",
            "Epoch 4702/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2122 - accuracy: 0.8928\n",
            "Epoch 4702: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2103 - accuracy: 0.8931 - val_loss: 4.3599 - val_accuracy: 0.5308\n",
            "Epoch 4703/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1869 - accuracy: 0.9027\n",
            "Epoch 4703: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1890 - accuracy: 0.9013 - val_loss: 4.4362 - val_accuracy: 0.5416\n",
            "Epoch 4704/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1996 - accuracy: 0.8985\n",
            "Epoch 4704: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2020 - accuracy: 0.8971 - val_loss: 4.3444 - val_accuracy: 0.5378\n",
            "Epoch 4705/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.8955\n",
            "Epoch 4705: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1956 - accuracy: 0.8955 - val_loss: 4.5727 - val_accuracy: 0.5360\n",
            "Epoch 4706/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1889 - accuracy: 0.9018\n",
            "Epoch 4706: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1904 - accuracy: 0.8998 - val_loss: 4.2460 - val_accuracy: 0.5331\n",
            "Epoch 4707/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9009\n",
            "Epoch 4707: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1938 - accuracy: 0.9009 - val_loss: 4.5600 - val_accuracy: 0.5308\n",
            "Epoch 4708/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.8987\n",
            "Epoch 4708: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1912 - accuracy: 0.8987 - val_loss: 4.2772 - val_accuracy: 0.5351\n",
            "Epoch 4709/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2032 - accuracy: 0.8955\n",
            "Epoch 4709: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2055 - accuracy: 0.8929 - val_loss: 4.3523 - val_accuracy: 0.5395\n",
            "Epoch 4710/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.8917\n",
            "Epoch 4710: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2070 - accuracy: 0.8917 - val_loss: 4.1786 - val_accuracy: 0.5346\n",
            "Epoch 4711/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2035 - accuracy: 0.8953\n",
            "Epoch 4711: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2023 - accuracy: 0.8964 - val_loss: 4.4275 - val_accuracy: 0.5334\n",
            "Epoch 4712/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1996 - accuracy: 0.8998\n",
            "Epoch 4712: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2027 - accuracy: 0.8977 - val_loss: 4.3450 - val_accuracy: 0.5328\n",
            "Epoch 4713/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1991 - accuracy: 0.8960\n",
            "Epoch 4713: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2012 - accuracy: 0.8956 - val_loss: 4.2099 - val_accuracy: 0.5419\n",
            "Epoch 4714/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.8927\n",
            "Epoch 4714: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2019 - accuracy: 0.8928 - val_loss: 4.1819 - val_accuracy: 0.5322\n",
            "Epoch 4715/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2060 - accuracy: 0.8922\n",
            "Epoch 4715: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2060 - accuracy: 0.8914 - val_loss: 4.2162 - val_accuracy: 0.5316\n",
            "Epoch 4716/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.9024\n",
            "Epoch 4716: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1907 - accuracy: 0.9009 - val_loss: 4.4541 - val_accuracy: 0.5308\n",
            "Epoch 4717/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1989 - accuracy: 0.8944\n",
            "Epoch 4717: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1983 - accuracy: 0.8944 - val_loss: 4.2140 - val_accuracy: 0.5281\n",
            "Epoch 4718/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2062 - accuracy: 0.8971\n",
            "Epoch 4718: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2055 - accuracy: 0.8969 - val_loss: 4.3238 - val_accuracy: 0.5354\n",
            "Epoch 4719/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2002 - accuracy: 0.8963\n",
            "Epoch 4719: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2013 - accuracy: 0.8950 - val_loss: 4.3584 - val_accuracy: 0.5363\n",
            "Epoch 4720/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1908 - accuracy: 0.9015\n",
            "Epoch 4720: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1910 - accuracy: 0.9012 - val_loss: 4.3249 - val_accuracy: 0.5340\n",
            "Epoch 4721/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1832 - accuracy: 0.9026\n",
            "Epoch 4721: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1837 - accuracy: 0.9020 - val_loss: 4.3598 - val_accuracy: 0.5340\n",
            "Epoch 4722/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1821 - accuracy: 0.9044\n",
            "Epoch 4722: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1853 - accuracy: 0.9013 - val_loss: 4.4060 - val_accuracy: 0.5351\n",
            "Epoch 4723/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9035\n",
            "Epoch 4723: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1853 - accuracy: 0.9035 - val_loss: 4.3364 - val_accuracy: 0.5384\n",
            "Epoch 4724/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1813 - accuracy: 0.9068\n",
            "Epoch 4724: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1831 - accuracy: 0.9051 - val_loss: 4.2850 - val_accuracy: 0.5407\n",
            "Epoch 4725/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2007 - accuracy: 0.8976\n",
            "Epoch 4725: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2031 - accuracy: 0.8953 - val_loss: 4.2927 - val_accuracy: 0.5360\n",
            "Epoch 4726/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1978 - accuracy: 0.8964\n",
            "Epoch 4726: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1983 - accuracy: 0.8961 - val_loss: 4.4111 - val_accuracy: 0.5425\n",
            "Epoch 4727/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1873 - accuracy: 0.9024\n",
            "Epoch 4727: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1912 - accuracy: 0.8996 - val_loss: 4.4781 - val_accuracy: 0.5381\n",
            "Epoch 4728/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1910 - accuracy: 0.9006\n",
            "Epoch 4728: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1929 - accuracy: 0.8996 - val_loss: 4.2290 - val_accuracy: 0.5366\n",
            "Epoch 4729/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.8967\n",
            "Epoch 4729: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1942 - accuracy: 0.8967 - val_loss: 4.3980 - val_accuracy: 0.5360\n",
            "Epoch 4730/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.8968\n",
            "Epoch 4730: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1906 - accuracy: 0.8968 - val_loss: 4.3698 - val_accuracy: 0.5366\n",
            "Epoch 4731/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1984 - accuracy: 0.8945\n",
            "Epoch 4731: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1987 - accuracy: 0.8942 - val_loss: 4.3132 - val_accuracy: 0.5313\n",
            "Epoch 4732/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1987 - accuracy: 0.8953\n",
            "Epoch 4732: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1992 - accuracy: 0.8955 - val_loss: 4.2157 - val_accuracy: 0.5372\n",
            "Epoch 4733/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2063 - accuracy: 0.8930\n",
            "Epoch 4733: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2060 - accuracy: 0.8933 - val_loss: 4.2354 - val_accuracy: 0.5398\n",
            "Epoch 4734/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1992 - accuracy: 0.8979\n",
            "Epoch 4734: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2005 - accuracy: 0.8961 - val_loss: 4.2724 - val_accuracy: 0.5316\n",
            "Epoch 4735/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9012\n",
            "Epoch 4735: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1920 - accuracy: 0.9012 - val_loss: 4.3493 - val_accuracy: 0.5340\n",
            "Epoch 4736/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.8994\n",
            "Epoch 4736: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1901 - accuracy: 0.8994 - val_loss: 4.3988 - val_accuracy: 0.5384\n",
            "Epoch 4737/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1808 - accuracy: 0.9054\n",
            "Epoch 4737: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1852 - accuracy: 0.9020 - val_loss: 4.5505 - val_accuracy: 0.5296\n",
            "Epoch 4738/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1983 - accuracy: 0.8957\n",
            "Epoch 4738: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1991 - accuracy: 0.8954 - val_loss: 4.5935 - val_accuracy: 0.5357\n",
            "Epoch 4739/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1981 - accuracy: 0.8971\n",
            "Epoch 4739: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2002 - accuracy: 0.8957 - val_loss: 4.2328 - val_accuracy: 0.5375\n",
            "Epoch 4740/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2005 - accuracy: 0.9000\n",
            "Epoch 4740: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2030 - accuracy: 0.8968 - val_loss: 4.3185 - val_accuracy: 0.5346\n",
            "Epoch 4741/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2178 - accuracy: 0.8899\n",
            "Epoch 4741: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2187 - accuracy: 0.8892 - val_loss: 4.2942 - val_accuracy: 0.5302\n",
            "Epoch 4742/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2007 - accuracy: 0.8958\n",
            "Epoch 4742: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2019 - accuracy: 0.8950 - val_loss: 4.3103 - val_accuracy: 0.5363\n",
            "Epoch 4743/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.9009\n",
            "Epoch 4743: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1974 - accuracy: 0.8962 - val_loss: 4.1592 - val_accuracy: 0.5313\n",
            "Epoch 4744/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2046 - accuracy: 0.8913\n",
            "Epoch 4744: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2067 - accuracy: 0.8900 - val_loss: 4.2334 - val_accuracy: 0.5296\n",
            "Epoch 4745/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2046 - accuracy: 0.8911\n",
            "Epoch 4745: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2057 - accuracy: 0.8909 - val_loss: 4.1192 - val_accuracy: 0.5337\n",
            "Epoch 4746/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9014\n",
            "Epoch 4746: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1858 - accuracy: 0.9014 - val_loss: 4.2173 - val_accuracy: 0.5343\n",
            "Epoch 4747/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1856 - accuracy: 0.9036\n",
            "Epoch 4747: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1857 - accuracy: 0.9027 - val_loss: 4.2865 - val_accuracy: 0.5258\n",
            "Epoch 4748/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1849 - accuracy: 0.9009\n",
            "Epoch 4748: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1892 - accuracy: 0.8994 - val_loss: 4.2434 - val_accuracy: 0.5354\n",
            "Epoch 4749/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9008\n",
            "Epoch 4749: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1892 - accuracy: 0.8994 - val_loss: 4.4574 - val_accuracy: 0.5363\n",
            "Epoch 4750/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1815 - accuracy: 0.9052\n",
            "Epoch 4750: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1846 - accuracy: 0.9018 - val_loss: 4.2293 - val_accuracy: 0.5354\n",
            "Epoch 4751/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1981 - accuracy: 0.8968\n",
            "Epoch 4751: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1994 - accuracy: 0.8963 - val_loss: 4.4717 - val_accuracy: 0.5346\n",
            "Epoch 4752/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2037 - accuracy: 0.8950\n",
            "Epoch 4752: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2052 - accuracy: 0.8945 - val_loss: 4.3696 - val_accuracy: 0.5369\n",
            "Epoch 4753/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2016 - accuracy: 0.8982\n",
            "Epoch 4753: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2040 - accuracy: 0.8962 - val_loss: 4.2220 - val_accuracy: 0.5363\n",
            "Epoch 4754/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1890 - accuracy: 0.8977\n",
            "Epoch 4754: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1891 - accuracy: 0.8974 - val_loss: 4.4248 - val_accuracy: 0.5337\n",
            "Epoch 4755/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1849 - accuracy: 0.9030\n",
            "Epoch 4755: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1855 - accuracy: 0.9027 - val_loss: 4.2393 - val_accuracy: 0.5360\n",
            "Epoch 4756/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1958 - accuracy: 0.8968\n",
            "Epoch 4756: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1950 - accuracy: 0.8974 - val_loss: 4.4838 - val_accuracy: 0.5398\n",
            "Epoch 4757/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2023 - accuracy: 0.8949\n",
            "Epoch 4757: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2024 - accuracy: 0.8949 - val_loss: 4.0473 - val_accuracy: 0.5337\n",
            "Epoch 4758/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2003 - accuracy: 0.8947\n",
            "Epoch 4758: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1987 - accuracy: 0.8959 - val_loss: 4.4439 - val_accuracy: 0.5346\n",
            "Epoch 4759/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1978 - accuracy: 0.8959\n",
            "Epoch 4759: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1995 - accuracy: 0.8950 - val_loss: 4.3566 - val_accuracy: 0.5346\n",
            "Epoch 4760/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1905 - accuracy: 0.9021\n",
            "Epoch 4760: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1928 - accuracy: 0.9004 - val_loss: 4.4143 - val_accuracy: 0.5337\n",
            "Epoch 4761/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1875 - accuracy: 0.9014\n",
            "Epoch 4761: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1878 - accuracy: 0.9013 - val_loss: 4.3819 - val_accuracy: 0.5334\n",
            "Epoch 4762/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9017\n",
            "Epoch 4762: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1840 - accuracy: 0.9012 - val_loss: 4.5225 - val_accuracy: 0.5354\n",
            "Epoch 4763/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1879 - accuracy: 0.8994\n",
            "Epoch 4763: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1882 - accuracy: 0.8988 - val_loss: 4.3702 - val_accuracy: 0.5343\n",
            "Epoch 4764/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1891 - accuracy: 0.9004\n",
            "Epoch 4764: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1914 - accuracy: 0.8985 - val_loss: 4.4876 - val_accuracy: 0.5331\n",
            "Epoch 4765/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1992 - accuracy: 0.8957\n",
            "Epoch 4765: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2013 - accuracy: 0.8953 - val_loss: 4.5175 - val_accuracy: 0.5299\n",
            "Epoch 4766/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1972 - accuracy: 0.8958\n",
            "Epoch 4766: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1960 - accuracy: 0.8958 - val_loss: 4.2475 - val_accuracy: 0.5346\n",
            "Epoch 4767/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1921 - accuracy: 0.8970\n",
            "Epoch 4767: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1933 - accuracy: 0.8969 - val_loss: 4.2469 - val_accuracy: 0.5284\n",
            "Epoch 4768/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.9022\n",
            "Epoch 4768: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1947 - accuracy: 0.9007 - val_loss: 4.5947 - val_accuracy: 0.5372\n",
            "Epoch 4769/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.8949\n",
            "Epoch 4769: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2027 - accuracy: 0.8949 - val_loss: 4.3915 - val_accuracy: 0.5372\n",
            "Epoch 4770/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9007\n",
            "Epoch 4770: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1906 - accuracy: 0.9007 - val_loss: 4.4871 - val_accuracy: 0.5422\n",
            "Epoch 4771/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1937 - accuracy: 0.8985\n",
            "Epoch 4771: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1947 - accuracy: 0.8977 - val_loss: 4.3625 - val_accuracy: 0.5325\n",
            "Epoch 4772/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1958 - accuracy: 0.8967\n",
            "Epoch 4772: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1958 - accuracy: 0.8968 - val_loss: 4.4367 - val_accuracy: 0.5346\n",
            "Epoch 4773/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1952 - accuracy: 0.8974\n",
            "Epoch 4773: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1971 - accuracy: 0.8955 - val_loss: 4.4695 - val_accuracy: 0.5249\n",
            "Epoch 4774/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1868 - accuracy: 0.9018\n",
            "Epoch 4774: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1884 - accuracy: 0.9010 - val_loss: 4.3964 - val_accuracy: 0.5252\n",
            "Epoch 4775/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1888 - accuracy: 0.9003\n",
            "Epoch 4775: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1897 - accuracy: 0.8994 - val_loss: 4.3176 - val_accuracy: 0.5334\n",
            "Epoch 4776/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1875 - accuracy: 0.9028\n",
            "Epoch 4776: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1894 - accuracy: 0.9000 - val_loss: 4.5503 - val_accuracy: 0.5349\n",
            "Epoch 4777/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1843 - accuracy: 0.9033\n",
            "Epoch 4777: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1848 - accuracy: 0.9028 - val_loss: 4.2483 - val_accuracy: 0.5299\n",
            "Epoch 4778/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.9018\n",
            "Epoch 4778: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1879 - accuracy: 0.9007 - val_loss: 4.4860 - val_accuracy: 0.5369\n",
            "Epoch 4779/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1846 - accuracy: 0.9034\n",
            "Epoch 4779: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1852 - accuracy: 0.9035 - val_loss: 4.4317 - val_accuracy: 0.5319\n",
            "Epoch 4780/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1885 - accuracy: 0.9003\n",
            "Epoch 4780: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1893 - accuracy: 0.8993 - val_loss: 4.4569 - val_accuracy: 0.5305\n",
            "Epoch 4781/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1830 - accuracy: 0.9045\n",
            "Epoch 4781: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1833 - accuracy: 0.9043 - val_loss: 4.5252 - val_accuracy: 0.5299\n",
            "Epoch 4782/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1850 - accuracy: 0.9041\n",
            "Epoch 4782: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1888 - accuracy: 0.9016 - val_loss: 4.5710 - val_accuracy: 0.5354\n",
            "Epoch 4783/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1975 - accuracy: 0.8969\n",
            "Epoch 4783: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1987 - accuracy: 0.8961 - val_loss: 4.3849 - val_accuracy: 0.5308\n",
            "Epoch 4784/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.8959\n",
            "Epoch 4784: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2046 - accuracy: 0.8959 - val_loss: 4.5110 - val_accuracy: 0.5387\n",
            "Epoch 4785/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1990 - accuracy: 0.9004\n",
            "Epoch 4785: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2044 - accuracy: 0.8967 - val_loss: 4.3818 - val_accuracy: 0.5369\n",
            "Epoch 4786/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2032 - accuracy: 0.8928\n",
            "Epoch 4786: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2045 - accuracy: 0.8912 - val_loss: 4.2738 - val_accuracy: 0.5255\n",
            "Epoch 4787/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1910 - accuracy: 0.8994\n",
            "Epoch 4787: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1911 - accuracy: 0.8992 - val_loss: 4.4516 - val_accuracy: 0.5369\n",
            "Epoch 4788/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1862 - accuracy: 0.9011\n",
            "Epoch 4788: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1868 - accuracy: 0.9005 - val_loss: 4.3810 - val_accuracy: 0.5378\n",
            "Epoch 4789/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1857 - accuracy: 0.9025\n",
            "Epoch 4789: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1875 - accuracy: 0.9015 - val_loss: 4.3613 - val_accuracy: 0.5261\n",
            "Epoch 4790/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1960 - accuracy: 0.8971\n",
            "Epoch 4790: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1968 - accuracy: 0.8964 - val_loss: 4.5066 - val_accuracy: 0.5322\n",
            "Epoch 4791/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1992 - accuracy: 0.8976\n",
            "Epoch 4791: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2008 - accuracy: 0.8953 - val_loss: 4.4398 - val_accuracy: 0.5360\n",
            "Epoch 4792/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.8979\n",
            "Epoch 4792: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1955 - accuracy: 0.8966 - val_loss: 4.2079 - val_accuracy: 0.5308\n",
            "Epoch 4793/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1987 - accuracy: 0.8964\n",
            "Epoch 4793: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1990 - accuracy: 0.8959 - val_loss: 4.2582 - val_accuracy: 0.5296\n",
            "Epoch 4794/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1977 - accuracy: 0.8981\n",
            "Epoch 4794: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1987 - accuracy: 0.8972 - val_loss: 4.5572 - val_accuracy: 0.5381\n",
            "Epoch 4795/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1927 - accuracy: 0.8990\n",
            "Epoch 4795: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1935 - accuracy: 0.8986 - val_loss: 4.3248 - val_accuracy: 0.5296\n",
            "Epoch 4796/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1915 - accuracy: 0.8989\n",
            "Epoch 4796: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1931 - accuracy: 0.8976 - val_loss: 4.5095 - val_accuracy: 0.5343\n",
            "Epoch 4797/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1944 - accuracy: 0.8998\n",
            "Epoch 4797: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1950 - accuracy: 0.8993 - val_loss: 4.4371 - val_accuracy: 0.5363\n",
            "Epoch 4798/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1984 - accuracy: 0.8945\n",
            "Epoch 4798: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1999 - accuracy: 0.8942 - val_loss: 4.2439 - val_accuracy: 0.5313\n",
            "Epoch 4799/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.8997\n",
            "Epoch 4799: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1898 - accuracy: 0.8997 - val_loss: 4.4078 - val_accuracy: 0.5346\n",
            "Epoch 4800/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.8986\n",
            "Epoch 4800: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1904 - accuracy: 0.8975 - val_loss: 4.3303 - val_accuracy: 0.5363\n",
            "Epoch 4801/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1901 - accuracy: 0.8988\n",
            "Epoch 4801: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1912 - accuracy: 0.8972 - val_loss: 4.5392 - val_accuracy: 0.5319\n",
            "Epoch 4802/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1869 - accuracy: 0.9006\n",
            "Epoch 4802: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1871 - accuracy: 0.9007 - val_loss: 4.3528 - val_accuracy: 0.5390\n",
            "Epoch 4803/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9013\n",
            "Epoch 4803: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1846 - accuracy: 0.9013 - val_loss: 4.4403 - val_accuracy: 0.5305\n",
            "Epoch 4804/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1891 - accuracy: 0.9011\n",
            "Epoch 4804: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1909 - accuracy: 0.8999 - val_loss: 4.4610 - val_accuracy: 0.5322\n",
            "Epoch 4805/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1916 - accuracy: 0.8986\n",
            "Epoch 4805: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1926 - accuracy: 0.8976 - val_loss: 4.4228 - val_accuracy: 0.5393\n",
            "Epoch 4806/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1969 - accuracy: 0.8968\n",
            "Epoch 4806: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1964 - accuracy: 0.8970 - val_loss: 4.5957 - val_accuracy: 0.5378\n",
            "Epoch 4807/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.8989\n",
            "Epoch 4807: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1934 - accuracy: 0.8986 - val_loss: 4.5233 - val_accuracy: 0.5390\n",
            "Epoch 4808/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1928 - accuracy: 0.8982\n",
            "Epoch 4808: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1938 - accuracy: 0.8981 - val_loss: 4.3619 - val_accuracy: 0.5334\n",
            "Epoch 4809/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1935 - accuracy: 0.8997\n",
            "Epoch 4809: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1959 - accuracy: 0.8988 - val_loss: 4.2971 - val_accuracy: 0.5340\n",
            "Epoch 4810/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2006 - accuracy: 0.8949\n",
            "Epoch 4810: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2012 - accuracy: 0.8948 - val_loss: 4.4939 - val_accuracy: 0.5325\n",
            "Epoch 4811/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1994 - accuracy: 0.8954\n",
            "Epoch 4811: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.2017 - accuracy: 0.8942 - val_loss: 4.4101 - val_accuracy: 0.5331\n",
            "Epoch 4812/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.8980\n",
            "Epoch 4812: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1948 - accuracy: 0.8980 - val_loss: 4.3618 - val_accuracy: 0.5308\n",
            "Epoch 4813/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1945 - accuracy: 0.8971\n",
            "Epoch 4813: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1948 - accuracy: 0.8967 - val_loss: 4.1687 - val_accuracy: 0.5325\n",
            "Epoch 4814/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1914 - accuracy: 0.8993\n",
            "Epoch 4814: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1970 - accuracy: 0.8961 - val_loss: 4.2937 - val_accuracy: 0.5267\n",
            "Epoch 4815/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1953 - accuracy: 0.8982\n",
            "Epoch 4815: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1965 - accuracy: 0.8972 - val_loss: 4.4478 - val_accuracy: 0.5351\n",
            "Epoch 4816/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1870 - accuracy: 0.8997\n",
            "Epoch 4816: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1873 - accuracy: 0.8993 - val_loss: 4.4764 - val_accuracy: 0.5305\n",
            "Epoch 4817/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1845 - accuracy: 0.9015\n",
            "Epoch 4817: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1861 - accuracy: 0.9010 - val_loss: 4.4480 - val_accuracy: 0.5290\n",
            "Epoch 4818/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.8996\n",
            "Epoch 4818: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1956 - accuracy: 0.8996 - val_loss: 4.4615 - val_accuracy: 0.5381\n",
            "Epoch 4819/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2043 - accuracy: 0.8916\n",
            "Epoch 4819: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2041 - accuracy: 0.8924 - val_loss: 4.4854 - val_accuracy: 0.5381\n",
            "Epoch 4820/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1985 - accuracy: 0.8952\n",
            "Epoch 4820: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1999 - accuracy: 0.8936 - val_loss: 4.3758 - val_accuracy: 0.5346\n",
            "Epoch 4821/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1970 - accuracy: 0.8985\n",
            "Epoch 4821: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1969 - accuracy: 0.8984 - val_loss: 4.3889 - val_accuracy: 0.5331\n",
            "Epoch 4822/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1945 - accuracy: 0.8957\n",
            "Epoch 4822: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1953 - accuracy: 0.8945 - val_loss: 4.4589 - val_accuracy: 0.5346\n",
            "Epoch 4823/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1900 - accuracy: 0.9006\n",
            "Epoch 4823: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1894 - accuracy: 0.9005 - val_loss: 4.4057 - val_accuracy: 0.5366\n",
            "Epoch 4824/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1916 - accuracy: 0.9048\n",
            "Epoch 4824: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1935 - accuracy: 0.9031 - val_loss: 4.3409 - val_accuracy: 0.5281\n",
            "Epoch 4825/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2029 - accuracy: 0.8955\n",
            "Epoch 4825: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2038 - accuracy: 0.8950 - val_loss: 4.5994 - val_accuracy: 0.5325\n",
            "Epoch 4826/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2033 - accuracy: 0.8928\n",
            "Epoch 4826: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2024 - accuracy: 0.8931 - val_loss: 4.3642 - val_accuracy: 0.5354\n",
            "Epoch 4827/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1910 - accuracy: 0.9008\n",
            "Epoch 4827: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1926 - accuracy: 0.8999 - val_loss: 4.4075 - val_accuracy: 0.5325\n",
            "Epoch 4828/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1936 - accuracy: 0.9006\n",
            "Epoch 4828: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1955 - accuracy: 0.8985 - val_loss: 4.2833 - val_accuracy: 0.5281\n",
            "Epoch 4829/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1960 - accuracy: 0.8964\n",
            "Epoch 4829: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2013 - accuracy: 0.8944 - val_loss: 4.5199 - val_accuracy: 0.5390\n",
            "Epoch 4830/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1979 - accuracy: 0.8985\n",
            "Epoch 4830: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1981 - accuracy: 0.8980 - val_loss: 4.6427 - val_accuracy: 0.5310\n",
            "Epoch 4831/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.2045 - accuracy: 0.8948\n",
            "Epoch 4831: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2072 - accuracy: 0.8927 - val_loss: 4.2644 - val_accuracy: 0.5305\n",
            "Epoch 4832/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1971 - accuracy: 0.8963\n",
            "Epoch 4832: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1974 - accuracy: 0.8966 - val_loss: 4.2427 - val_accuracy: 0.5366\n",
            "Epoch 4833/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1953 - accuracy: 0.8974\n",
            "Epoch 4833: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1994 - accuracy: 0.8953 - val_loss: 4.4026 - val_accuracy: 0.5366\n",
            "Epoch 4834/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1962 - accuracy: 0.8988\n",
            "Epoch 4834: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1968 - accuracy: 0.8977 - val_loss: 4.4290 - val_accuracy: 0.5340\n",
            "Epoch 4835/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9003\n",
            "Epoch 4835: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1938 - accuracy: 0.9003 - val_loss: 4.2663 - val_accuracy: 0.5416\n",
            "Epoch 4836/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1955 - accuracy: 0.8956\n",
            "Epoch 4836: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1967 - accuracy: 0.8944 - val_loss: 4.3426 - val_accuracy: 0.5387\n",
            "Epoch 4837/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1899 - accuracy: 0.9027\n",
            "Epoch 4837: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1913 - accuracy: 0.9019 - val_loss: 4.1892 - val_accuracy: 0.5322\n",
            "Epoch 4838/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1937 - accuracy: 0.9006\n",
            "Epoch 4838: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.1983 - accuracy: 0.8974 - val_loss: 4.3071 - val_accuracy: 0.5340\n",
            "Epoch 4839/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1969 - accuracy: 0.8994\n",
            "Epoch 4839: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2000 - accuracy: 0.8974 - val_loss: 4.2596 - val_accuracy: 0.5287\n",
            "Epoch 4840/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1898 - accuracy: 0.8997\n",
            "Epoch 4840: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1932 - accuracy: 0.8978 - val_loss: 4.1294 - val_accuracy: 0.5346\n",
            "Epoch 4841/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1889 - accuracy: 0.9018\n",
            "Epoch 4841: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1915 - accuracy: 0.8996 - val_loss: 4.4405 - val_accuracy: 0.5322\n",
            "Epoch 4842/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1869 - accuracy: 0.9028\n",
            "Epoch 4842: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1870 - accuracy: 0.9018 - val_loss: 4.5396 - val_accuracy: 0.5340\n",
            "Epoch 4843/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1847 - accuracy: 0.9035\n",
            "Epoch 4843: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1863 - accuracy: 0.9018 - val_loss: 4.4680 - val_accuracy: 0.5334\n",
            "Epoch 4844/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1933 - accuracy: 0.8999\n",
            "Epoch 4844: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1951 - accuracy: 0.8994 - val_loss: 4.3236 - val_accuracy: 0.5331\n",
            "Epoch 4845/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1927 - accuracy: 0.8987\n",
            "Epoch 4845: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1931 - accuracy: 0.8983 - val_loss: 4.3629 - val_accuracy: 0.5343\n",
            "Epoch 4846/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1916 - accuracy: 0.8975\n",
            "Epoch 4846: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1937 - accuracy: 0.8950 - val_loss: 4.2388 - val_accuracy: 0.5340\n",
            "Epoch 4847/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1869 - accuracy: 0.9021\n",
            "Epoch 4847: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1885 - accuracy: 0.9007 - val_loss: 4.5098 - val_accuracy: 0.5387\n",
            "Epoch 4848/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.8963\n",
            "Epoch 4848: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1996 - accuracy: 0.8963 - val_loss: 4.2665 - val_accuracy: 0.5351\n",
            "Epoch 4849/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2034 - accuracy: 0.8955\n",
            "Epoch 4849: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2035 - accuracy: 0.8952 - val_loss: 4.2556 - val_accuracy: 0.5325\n",
            "Epoch 4850/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1999 - accuracy: 0.8962\n",
            "Epoch 4850: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2009 - accuracy: 0.8949 - val_loss: 4.4300 - val_accuracy: 0.5331\n",
            "Epoch 4851/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1941 - accuracy: 0.8993\n",
            "Epoch 4851: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1955 - accuracy: 0.8983 - val_loss: 4.3008 - val_accuracy: 0.5351\n",
            "Epoch 4852/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1992 - accuracy: 0.8974\n",
            "Epoch 4852: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2002 - accuracy: 0.8972 - val_loss: 4.4285 - val_accuracy: 0.5340\n",
            "Epoch 4853/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2027 - accuracy: 0.8937\n",
            "Epoch 4853: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2016 - accuracy: 0.8944 - val_loss: 4.3542 - val_accuracy: 0.5325\n",
            "Epoch 4854/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.8982\n",
            "Epoch 4854: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1897 - accuracy: 0.8982 - val_loss: 4.3403 - val_accuracy: 0.5393\n",
            "Epoch 4855/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1959 - accuracy: 0.8967\n",
            "Epoch 4855: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1956 - accuracy: 0.8968 - val_loss: 4.4194 - val_accuracy: 0.5349\n",
            "Epoch 4856/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1901 - accuracy: 0.9014\n",
            "Epoch 4856: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1912 - accuracy: 0.9009 - val_loss: 4.3146 - val_accuracy: 0.5416\n",
            "Epoch 4857/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1815 - accuracy: 0.9052\n",
            "Epoch 4857: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1839 - accuracy: 0.9027 - val_loss: 4.3189 - val_accuracy: 0.5340\n",
            "Epoch 4858/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1821 - accuracy: 0.9058\n",
            "Epoch 4858: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1831 - accuracy: 0.9049 - val_loss: 4.4775 - val_accuracy: 0.5316\n",
            "Epoch 4859/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1881 - accuracy: 0.8990\n",
            "Epoch 4859: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1901 - accuracy: 0.8975 - val_loss: 4.4368 - val_accuracy: 0.5287\n",
            "Epoch 4860/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1868 - accuracy: 0.8985\n",
            "Epoch 4860: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1869 - accuracy: 0.8984 - val_loss: 4.4006 - val_accuracy: 0.5316\n",
            "Epoch 4861/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1794 - accuracy: 0.9056\n",
            "Epoch 4861: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 0.1821 - accuracy: 0.9032 - val_loss: 4.6137 - val_accuracy: 0.5401\n",
            "Epoch 4862/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1906 - accuracy: 0.8991\n",
            "Epoch 4862: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1904 - accuracy: 0.8990 - val_loss: 4.5543 - val_accuracy: 0.5395\n",
            "Epoch 4863/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.8995\n",
            "Epoch 4863: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1912 - accuracy: 0.8995 - val_loss: 4.3050 - val_accuracy: 0.5284\n",
            "Epoch 4864/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1953 - accuracy: 0.9002\n",
            "Epoch 4864: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1956 - accuracy: 0.9001 - val_loss: 4.3381 - val_accuracy: 0.5328\n",
            "Epoch 4865/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9010\n",
            "Epoch 4865: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1887 - accuracy: 0.9010 - val_loss: 4.3358 - val_accuracy: 0.5331\n",
            "Epoch 4866/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1804 - accuracy: 0.9047\n",
            "Epoch 4866: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1834 - accuracy: 0.9021 - val_loss: 4.4230 - val_accuracy: 0.5384\n",
            "Epoch 4867/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9062\n",
            "Epoch 4867: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1845 - accuracy: 0.9039 - val_loss: 4.3689 - val_accuracy: 0.5275\n",
            "Epoch 4868/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9000\n",
            "Epoch 4868: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1898 - accuracy: 0.9000 - val_loss: 4.2682 - val_accuracy: 0.5258\n",
            "Epoch 4869/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1870 - accuracy: 0.9030\n",
            "Epoch 4869: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1869 - accuracy: 0.9029 - val_loss: 4.3161 - val_accuracy: 0.5258\n",
            "Epoch 4870/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1830 - accuracy: 0.9011\n",
            "Epoch 4870: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1837 - accuracy: 0.9005 - val_loss: 4.4032 - val_accuracy: 0.5281\n",
            "Epoch 4871/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1847 - accuracy: 0.9025\n",
            "Epoch 4871: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1865 - accuracy: 0.9006 - val_loss: 4.4220 - val_accuracy: 0.5351\n",
            "Epoch 4872/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2045 - accuracy: 0.8970\n",
            "Epoch 4872: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2070 - accuracy: 0.8953 - val_loss: 4.4949 - val_accuracy: 0.5390\n",
            "Epoch 4873/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.8933\n",
            "Epoch 4873: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2027 - accuracy: 0.8933 - val_loss: 4.3807 - val_accuracy: 0.5346\n",
            "Epoch 4874/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1953 - accuracy: 0.8963\n",
            "Epoch 4874: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1967 - accuracy: 0.8950 - val_loss: 4.3362 - val_accuracy: 0.5346\n",
            "Epoch 4875/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1887 - accuracy: 0.9037\n",
            "Epoch 4875: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1916 - accuracy: 0.9016 - val_loss: 4.3792 - val_accuracy: 0.5366\n",
            "Epoch 4876/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9005\n",
            "Epoch 4876: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1900 - accuracy: 0.9005 - val_loss: 4.4195 - val_accuracy: 0.5328\n",
            "Epoch 4877/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1874 - accuracy: 0.8975\n",
            "Epoch 4877: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1876 - accuracy: 0.8980 - val_loss: 4.4154 - val_accuracy: 0.5363\n",
            "Epoch 4878/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1885 - accuracy: 0.9009\n",
            "Epoch 4878: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1925 - accuracy: 0.8976 - val_loss: 4.4154 - val_accuracy: 0.5346\n",
            "Epoch 4879/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1919 - accuracy: 0.8997\n",
            "Epoch 4879: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1925 - accuracy: 0.8977 - val_loss: 4.2628 - val_accuracy: 0.5246\n",
            "Epoch 4880/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1946 - accuracy: 0.8973\n",
            "Epoch 4880: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1959 - accuracy: 0.8966 - val_loss: 4.5480 - val_accuracy: 0.5325\n",
            "Epoch 4881/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2032 - accuracy: 0.8946\n",
            "Epoch 4881: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2037 - accuracy: 0.8943 - val_loss: 4.3100 - val_accuracy: 0.5357\n",
            "Epoch 4882/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1983 - accuracy: 0.8940\n",
            "Epoch 4882: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1983 - accuracy: 0.8937 - val_loss: 4.6331 - val_accuracy: 0.5284\n",
            "Epoch 4883/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1978 - accuracy: 0.8958\n",
            "Epoch 4883: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1983 - accuracy: 0.8953 - val_loss: 4.4780 - val_accuracy: 0.5328\n",
            "Epoch 4884/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8985\n",
            "Epoch 4884: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1943 - accuracy: 0.8976 - val_loss: 4.2759 - val_accuracy: 0.5287\n",
            "Epoch 4885/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1811 - accuracy: 0.9050\n",
            "Epoch 4885: loss did not improve from 0.18180\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1840 - accuracy: 0.9025 - val_loss: 4.4963 - val_accuracy: 0.5334\n",
            "Epoch 4886/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1791 - accuracy: 0.9030\n",
            "Epoch 4886: loss improved from 0.18180 to 0.18040, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 35ms/step - loss: 0.1804 - accuracy: 0.9017 - val_loss: 4.5274 - val_accuracy: 0.5398\n",
            "Epoch 4887/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1855 - accuracy: 0.9010\n",
            "Epoch 4887: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1865 - accuracy: 0.8997 - val_loss: 4.3322 - val_accuracy: 0.5375\n",
            "Epoch 4888/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1826 - accuracy: 0.9018\n",
            "Epoch 4888: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1831 - accuracy: 0.9017 - val_loss: 4.6386 - val_accuracy: 0.5378\n",
            "Epoch 4889/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.8950\n",
            "Epoch 4889: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2022 - accuracy: 0.8950 - val_loss: 4.3862 - val_accuracy: 0.5310\n",
            "Epoch 4890/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1946 - accuracy: 0.8998\n",
            "Epoch 4890: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1977 - accuracy: 0.8976 - val_loss: 4.3517 - val_accuracy: 0.5351\n",
            "Epoch 4891/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2049 - accuracy: 0.8908\n",
            "Epoch 4891: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2048 - accuracy: 0.8903 - val_loss: 4.4422 - val_accuracy: 0.5387\n",
            "Epoch 4892/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1891 - accuracy: 0.8993\n",
            "Epoch 4892: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1919 - accuracy: 0.8974 - val_loss: 4.4335 - val_accuracy: 0.5278\n",
            "Epoch 4893/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1942 - accuracy: 0.8980\n",
            "Epoch 4893: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1961 - accuracy: 0.8972 - val_loss: 4.2911 - val_accuracy: 0.5375\n",
            "Epoch 4894/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1976 - accuracy: 0.8968\n",
            "Epoch 4894: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1986 - accuracy: 0.8951 - val_loss: 4.3836 - val_accuracy: 0.5302\n",
            "Epoch 4895/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1888 - accuracy: 0.9033\n",
            "Epoch 4895: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1891 - accuracy: 0.9024 - val_loss: 4.4098 - val_accuracy: 0.5310\n",
            "Epoch 4896/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1867 - accuracy: 0.9054\n",
            "Epoch 4896: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1939 - accuracy: 0.9014 - val_loss: 4.2768 - val_accuracy: 0.5343\n",
            "Epoch 4897/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1884 - accuracy: 0.9018\n",
            "Epoch 4897: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1926 - accuracy: 0.8997 - val_loss: 4.2857 - val_accuracy: 0.5349\n",
            "Epoch 4898/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1965 - accuracy: 0.8963\n",
            "Epoch 4898: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1971 - accuracy: 0.8961 - val_loss: 4.3452 - val_accuracy: 0.5308\n",
            "Epoch 4899/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.8943\n",
            "Epoch 4899: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2026 - accuracy: 0.8943 - val_loss: 4.3095 - val_accuracy: 0.5240\n",
            "Epoch 4900/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1967 - accuracy: 0.9011\n",
            "Epoch 4900: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1978 - accuracy: 0.8998 - val_loss: 4.3117 - val_accuracy: 0.5290\n",
            "Epoch 4901/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1943 - accuracy: 0.8985\n",
            "Epoch 4901: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1945 - accuracy: 0.8973 - val_loss: 4.4347 - val_accuracy: 0.5349\n",
            "Epoch 4902/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1826 - accuracy: 0.9052\n",
            "Epoch 4902: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1838 - accuracy: 0.9041 - val_loss: 4.4113 - val_accuracy: 0.5308\n",
            "Epoch 4903/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9014\n",
            "Epoch 4903: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1859 - accuracy: 0.9014 - val_loss: 4.5691 - val_accuracy: 0.5357\n",
            "Epoch 4904/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1888 - accuracy: 0.8977\n",
            "Epoch 4904: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1898 - accuracy: 0.8975 - val_loss: 4.3680 - val_accuracy: 0.5322\n",
            "Epoch 4905/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1940 - accuracy: 0.8961\n",
            "Epoch 4905: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1947 - accuracy: 0.8959 - val_loss: 4.2953 - val_accuracy: 0.5264\n",
            "Epoch 4906/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2032 - accuracy: 0.8931\n",
            "Epoch 4906: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2025 - accuracy: 0.8928 - val_loss: 4.4750 - val_accuracy: 0.5299\n",
            "Epoch 4907/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2023 - accuracy: 0.8977\n",
            "Epoch 4907: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2068 - accuracy: 0.8941 - val_loss: 4.2657 - val_accuracy: 0.5384\n",
            "Epoch 4908/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1993 - accuracy: 0.8957\n",
            "Epoch 4908: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2010 - accuracy: 0.8938 - val_loss: 4.3181 - val_accuracy: 0.5363\n",
            "Epoch 4909/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2034 - accuracy: 0.8964\n",
            "Epoch 4909: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2079 - accuracy: 0.8940 - val_loss: 4.3181 - val_accuracy: 0.5340\n",
            "Epoch 4910/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2021 - accuracy: 0.8943\n",
            "Epoch 4910: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2023 - accuracy: 0.8939 - val_loss: 4.2936 - val_accuracy: 0.5349\n",
            "Epoch 4911/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1856 - accuracy: 0.9006\n",
            "Epoch 4911: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1874 - accuracy: 0.8985 - val_loss: 4.3677 - val_accuracy: 0.5363\n",
            "Epoch 4912/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1868 - accuracy: 0.8995\n",
            "Epoch 4912: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1877 - accuracy: 0.8990 - val_loss: 4.5248 - val_accuracy: 0.5404\n",
            "Epoch 4913/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1843 - accuracy: 0.9040\n",
            "Epoch 4913: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1852 - accuracy: 0.9027 - val_loss: 4.5592 - val_accuracy: 0.5360\n",
            "Epoch 4914/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1835 - accuracy: 0.9060\n",
            "Epoch 4914: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1844 - accuracy: 0.9043 - val_loss: 4.4255 - val_accuracy: 0.5293\n",
            "Epoch 4915/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.8988\n",
            "Epoch 4915: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1902 - accuracy: 0.8988 - val_loss: 4.4069 - val_accuracy: 0.5346\n",
            "Epoch 4916/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1940 - accuracy: 0.9012\n",
            "Epoch 4916: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1974 - accuracy: 0.8983 - val_loss: 4.4740 - val_accuracy: 0.5305\n",
            "Epoch 4917/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1902 - accuracy: 0.8998\n",
            "Epoch 4917: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1977 - accuracy: 0.8949 - val_loss: 4.2911 - val_accuracy: 0.5357\n",
            "Epoch 4918/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.8845\n",
            "Epoch 4918: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.2215 - accuracy: 0.8845 - val_loss: 4.1315 - val_accuracy: 0.5255\n",
            "Epoch 4919/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2155 - accuracy: 0.8893\n",
            "Epoch 4919: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2170 - accuracy: 0.8887 - val_loss: 4.3718 - val_accuracy: 0.5284\n",
            "Epoch 4920/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1998 - accuracy: 0.8970\n",
            "Epoch 4920: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2001 - accuracy: 0.8961 - val_loss: 4.3968 - val_accuracy: 0.5419\n",
            "Epoch 4921/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1953 - accuracy: 0.8958\n",
            "Epoch 4921: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1979 - accuracy: 0.8944 - val_loss: 4.4885 - val_accuracy: 0.5275\n",
            "Epoch 4922/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.8973\n",
            "Epoch 4922: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1950 - accuracy: 0.8967 - val_loss: 4.2665 - val_accuracy: 0.5281\n",
            "Epoch 4923/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1991 - accuracy: 0.8955\n",
            "Epoch 4923: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1993 - accuracy: 0.8954 - val_loss: 4.1741 - val_accuracy: 0.5384\n",
            "Epoch 4924/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1934 - accuracy: 0.8989\n",
            "Epoch 4924: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1939 - accuracy: 0.8984 - val_loss: 4.4217 - val_accuracy: 0.5351\n",
            "Epoch 4925/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1932 - accuracy: 0.8992\n",
            "Epoch 4925: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1941 - accuracy: 0.8984 - val_loss: 4.1836 - val_accuracy: 0.5299\n",
            "Epoch 4926/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2052 - accuracy: 0.8932\n",
            "Epoch 4926: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2036 - accuracy: 0.8942 - val_loss: 4.2923 - val_accuracy: 0.5319\n",
            "Epoch 4927/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.8962\n",
            "Epoch 4927: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1943 - accuracy: 0.8962 - val_loss: 4.3436 - val_accuracy: 0.5316\n",
            "Epoch 4928/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1881 - accuracy: 0.9053\n",
            "Epoch 4928: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1893 - accuracy: 0.9041 - val_loss: 4.4249 - val_accuracy: 0.5328\n",
            "Epoch 4929/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1832 - accuracy: 0.9052\n",
            "Epoch 4929: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1843 - accuracy: 0.9043 - val_loss: 4.4734 - val_accuracy: 0.5384\n",
            "Epoch 4930/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1891 - accuracy: 0.9018\n",
            "Epoch 4930: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1906 - accuracy: 0.9005 - val_loss: 4.3442 - val_accuracy: 0.5381\n",
            "Epoch 4931/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1827 - accuracy: 0.9030\n",
            "Epoch 4931: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1833 - accuracy: 0.9027 - val_loss: 4.2288 - val_accuracy: 0.5351\n",
            "Epoch 4932/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1869 - accuracy: 0.9034\n",
            "Epoch 4932: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1893 - accuracy: 0.9017 - val_loss: 4.4803 - val_accuracy: 0.5395\n",
            "Epoch 4933/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9023\n",
            "Epoch 4933: loss did not improve from 0.18040\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1839 - accuracy: 0.9023 - val_loss: 4.3943 - val_accuracy: 0.5395\n",
            "Epoch 4934/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9047\n",
            "Epoch 4934: loss improved from 0.18040 to 0.17754, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.1775 - accuracy: 0.9047 - val_loss: 4.5018 - val_accuracy: 0.5375\n",
            "Epoch 4935/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1868 - accuracy: 0.8998\n",
            "Epoch 4935: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1883 - accuracy: 0.8996 - val_loss: 4.4258 - val_accuracy: 0.5278\n",
            "Epoch 4936/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1975 - accuracy: 0.8948\n",
            "Epoch 4936: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1988 - accuracy: 0.8941 - val_loss: 4.5626 - val_accuracy: 0.5384\n",
            "Epoch 4937/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1964 - accuracy: 0.8988\n",
            "Epoch 4937: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1964 - accuracy: 0.8988 - val_loss: 4.5724 - val_accuracy: 0.5363\n",
            "Epoch 4938/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.8936\n",
            "Epoch 4938: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.2044 - accuracy: 0.8936 - val_loss: 4.2949 - val_accuracy: 0.5296\n",
            "Epoch 4939/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1986 - accuracy: 0.8952\n",
            "Epoch 4939: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1989 - accuracy: 0.8947 - val_loss: 4.4262 - val_accuracy: 0.5351\n",
            "Epoch 4940/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1941 - accuracy: 0.8978\n",
            "Epoch 4940: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1947 - accuracy: 0.8975 - val_loss: 4.2456 - val_accuracy: 0.5331\n",
            "Epoch 4941/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1906 - accuracy: 0.9005\n",
            "Epoch 4941: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1924 - accuracy: 0.8991 - val_loss: 4.4510 - val_accuracy: 0.5325\n",
            "Epoch 4942/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1930 - accuracy: 0.9005\n",
            "Epoch 4942: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1942 - accuracy: 0.8995 - val_loss: 4.3990 - val_accuracy: 0.5340\n",
            "Epoch 4943/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1985 - accuracy: 0.8956\n",
            "Epoch 4943: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1974 - accuracy: 0.8956 - val_loss: 4.3179 - val_accuracy: 0.5404\n",
            "Epoch 4944/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1899 - accuracy: 0.8986\n",
            "Epoch 4944: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1905 - accuracy: 0.8982 - val_loss: 4.3641 - val_accuracy: 0.5372\n",
            "Epoch 4945/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1914 - accuracy: 0.9012\n",
            "Epoch 4945: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1913 - accuracy: 0.9007 - val_loss: 4.4416 - val_accuracy: 0.5366\n",
            "Epoch 4946/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1908 - accuracy: 0.8967\n",
            "Epoch 4946: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1904 - accuracy: 0.8964 - val_loss: 4.4025 - val_accuracy: 0.5337\n",
            "Epoch 4947/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1902 - accuracy: 0.9000\n",
            "Epoch 4947: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1933 - accuracy: 0.8979 - val_loss: 4.3898 - val_accuracy: 0.5363\n",
            "Epoch 4948/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9007\n",
            "Epoch 4948: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1936 - accuracy: 0.9007 - val_loss: 4.4496 - val_accuracy: 0.5407\n",
            "Epoch 4949/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1921 - accuracy: 0.9008\n",
            "Epoch 4949: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1945 - accuracy: 0.8995 - val_loss: 4.6283 - val_accuracy: 0.5331\n",
            "Epoch 4950/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1912 - accuracy: 0.8996\n",
            "Epoch 4950: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1919 - accuracy: 0.8989 - val_loss: 4.2800 - val_accuracy: 0.5284\n",
            "Epoch 4951/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9006\n",
            "Epoch 4951: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1909 - accuracy: 0.9006 - val_loss: 4.3356 - val_accuracy: 0.5275\n",
            "Epoch 4952/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9010\n",
            "Epoch 4952: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1873 - accuracy: 0.9010 - val_loss: 4.6210 - val_accuracy: 0.5351\n",
            "Epoch 4953/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1893 - accuracy: 0.9022\n",
            "Epoch 4953: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1922 - accuracy: 0.9002 - val_loss: 4.3150 - val_accuracy: 0.5296\n",
            "Epoch 4954/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1872 - accuracy: 0.9032\n",
            "Epoch 4954: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1878 - accuracy: 0.9029 - val_loss: 4.5322 - val_accuracy: 0.5331\n",
            "Epoch 4955/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1844 - accuracy: 0.9053\n",
            "Epoch 4955: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1846 - accuracy: 0.9050 - val_loss: 4.4692 - val_accuracy: 0.5319\n",
            "Epoch 4956/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1878 - accuracy: 0.9020\n",
            "Epoch 4956: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1901 - accuracy: 0.8999 - val_loss: 4.5602 - val_accuracy: 0.5357\n",
            "Epoch 4957/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1799 - accuracy: 0.9032\n",
            "Epoch 4957: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1820 - accuracy: 0.9024 - val_loss: 4.5532 - val_accuracy: 0.5290\n",
            "Epoch 4958/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.8987\n",
            "Epoch 4958: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1947 - accuracy: 0.8972 - val_loss: 4.3464 - val_accuracy: 0.5340\n",
            "Epoch 4959/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1887 - accuracy: 0.9018\n",
            "Epoch 4959: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1897 - accuracy: 0.9016 - val_loss: 4.4160 - val_accuracy: 0.5246\n",
            "Epoch 4960/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1844 - accuracy: 0.9029\n",
            "Epoch 4960: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1879 - accuracy: 0.9007 - val_loss: 4.3607 - val_accuracy: 0.5357\n",
            "Epoch 4961/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1836 - accuracy: 0.9013\n",
            "Epoch 4961: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1854 - accuracy: 0.8995 - val_loss: 4.4651 - val_accuracy: 0.5375\n",
            "Epoch 4962/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1829 - accuracy: 0.9061\n",
            "Epoch 4962: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1837 - accuracy: 0.9059 - val_loss: 4.3608 - val_accuracy: 0.5363\n",
            "Epoch 4963/5000\n",
            "18/23 [======================>.......] - ETA: 0s - loss: 0.1901 - accuracy: 0.9026\n",
            "Epoch 4963: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1980 - accuracy: 0.8978 - val_loss: 4.4659 - val_accuracy: 0.5351\n",
            "Epoch 4964/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1894 - accuracy: 0.8995\n",
            "Epoch 4964: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1914 - accuracy: 0.8982 - val_loss: 4.3774 - val_accuracy: 0.5354\n",
            "Epoch 4965/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1907 - accuracy: 0.9014\n",
            "Epoch 4965: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1923 - accuracy: 0.8994 - val_loss: 4.4953 - val_accuracy: 0.5384\n",
            "Epoch 4966/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1994 - accuracy: 0.8955\n",
            "Epoch 4966: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2016 - accuracy: 0.8945 - val_loss: 4.4805 - val_accuracy: 0.5366\n",
            "Epoch 4967/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1945 - accuracy: 0.8987\n",
            "Epoch 4967: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1955 - accuracy: 0.8972 - val_loss: 4.3270 - val_accuracy: 0.5378\n",
            "Epoch 4968/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1927 - accuracy: 0.8992\n",
            "Epoch 4968: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1983 - accuracy: 0.8965 - val_loss: 4.1806 - val_accuracy: 0.5284\n",
            "Epoch 4969/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.2113 - accuracy: 0.8940\n",
            "Epoch 4969: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2101 - accuracy: 0.8936 - val_loss: 4.4314 - val_accuracy: 0.5357\n",
            "Epoch 4970/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.2012 - accuracy: 0.8977\n",
            "Epoch 4970: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2020 - accuracy: 0.8968 - val_loss: 4.2863 - val_accuracy: 0.5360\n",
            "Epoch 4971/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9029\n",
            "Epoch 4971: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1904 - accuracy: 0.9029 - val_loss: 4.3847 - val_accuracy: 0.5337\n",
            "Epoch 4972/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1821 - accuracy: 0.9052\n",
            "Epoch 4972: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1840 - accuracy: 0.9033 - val_loss: 4.3793 - val_accuracy: 0.5322\n",
            "Epoch 4973/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1832 - accuracy: 0.9017\n",
            "Epoch 4973: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1845 - accuracy: 0.9016 - val_loss: 4.3357 - val_accuracy: 0.5316\n",
            "Epoch 4974/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1897 - accuracy: 0.8981\n",
            "Epoch 4974: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1908 - accuracy: 0.8977 - val_loss: 4.4383 - val_accuracy: 0.5357\n",
            "Epoch 4975/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1922 - accuracy: 0.8997\n",
            "Epoch 4975: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1932 - accuracy: 0.8990 - val_loss: 4.4367 - val_accuracy: 0.5310\n",
            "Epoch 4976/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1941 - accuracy: 0.8983\n",
            "Epoch 4976: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1939 - accuracy: 0.8982 - val_loss: 4.4607 - val_accuracy: 0.5316\n",
            "Epoch 4977/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1867 - accuracy: 0.9054\n",
            "Epoch 4977: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1927 - accuracy: 0.9021 - val_loss: 4.6106 - val_accuracy: 0.5328\n",
            "Epoch 4978/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1907 - accuracy: 0.9030\n",
            "Epoch 4978: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1954 - accuracy: 0.9002 - val_loss: 4.4934 - val_accuracy: 0.5354\n",
            "Epoch 4979/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1897 - accuracy: 0.9006\n",
            "Epoch 4979: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.1893 - accuracy: 0.9010 - val_loss: 4.4761 - val_accuracy: 0.5343\n",
            "Epoch 4980/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1794 - accuracy: 0.9082\n",
            "Epoch 4980: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1845 - accuracy: 0.9040 - val_loss: 4.4742 - val_accuracy: 0.5351\n",
            "Epoch 4981/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1802 - accuracy: 0.9075\n",
            "Epoch 4981: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1830 - accuracy: 0.9044 - val_loss: 4.3924 - val_accuracy: 0.5375\n",
            "Epoch 4982/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1858 - accuracy: 0.9030\n",
            "Epoch 4982: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1851 - accuracy: 0.9034 - val_loss: 4.6004 - val_accuracy: 0.5305\n",
            "Epoch 4983/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.9005\n",
            "Epoch 4983: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1902 - accuracy: 0.9005 - val_loss: 4.4940 - val_accuracy: 0.5404\n",
            "Epoch 4984/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1808 - accuracy: 0.9054\n",
            "Epoch 4984: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 21ms/step - loss: 0.1840 - accuracy: 0.9032 - val_loss: 4.4668 - val_accuracy: 0.5328\n",
            "Epoch 4985/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1837 - accuracy: 0.9025\n",
            "Epoch 4985: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1835 - accuracy: 0.9028 - val_loss: 4.5212 - val_accuracy: 0.5360\n",
            "Epoch 4986/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1881 - accuracy: 0.9014\n",
            "Epoch 4986: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1941 - accuracy: 0.8978 - val_loss: 4.3807 - val_accuracy: 0.5337\n",
            "Epoch 4987/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.2016 - accuracy: 0.8948\n",
            "Epoch 4987: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2028 - accuracy: 0.8939 - val_loss: 4.3763 - val_accuracy: 0.5366\n",
            "Epoch 4988/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1998 - accuracy: 0.8976\n",
            "Epoch 4988: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1993 - accuracy: 0.8972 - val_loss: 4.5613 - val_accuracy: 0.5296\n",
            "Epoch 4989/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1936 - accuracy: 0.8981\n",
            "Epoch 4989: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1957 - accuracy: 0.8966 - val_loss: 4.3289 - val_accuracy: 0.5310\n",
            "Epoch 4990/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.1909 - accuracy: 0.9011\n",
            "Epoch 4990: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1914 - accuracy: 0.9007 - val_loss: 4.2244 - val_accuracy: 0.5261\n",
            "Epoch 4991/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1952 - accuracy: 0.8976\n",
            "Epoch 4991: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1955 - accuracy: 0.8977 - val_loss: 4.5795 - val_accuracy: 0.5416\n",
            "Epoch 4992/5000\n",
            "22/23 [===========================>..] - ETA: 0s - loss: 0.2030 - accuracy: 0.8931\n",
            "Epoch 4992: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.2036 - accuracy: 0.8924 - val_loss: 4.3316 - val_accuracy: 0.5346\n",
            "Epoch 4993/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.8995\n",
            "Epoch 4993: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.1977 - accuracy: 0.8967 - val_loss: 4.3225 - val_accuracy: 0.5223\n",
            "Epoch 4994/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1908 - accuracy: 0.8992\n",
            "Epoch 4994: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 20ms/step - loss: 0.1919 - accuracy: 0.8982 - val_loss: 4.3726 - val_accuracy: 0.5410\n",
            "Epoch 4995/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1850 - accuracy: 0.9021\n",
            "Epoch 4995: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1868 - accuracy: 0.9010 - val_loss: 4.4715 - val_accuracy: 0.5349\n",
            "Epoch 4996/5000\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 0.1944 - accuracy: 0.8990\n",
            "Epoch 4996: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1975 - accuracy: 0.8979 - val_loss: 4.5014 - val_accuracy: 0.5351\n",
            "Epoch 4997/5000\n",
            "21/23 [==========================>...] - ETA: 0s - loss: 0.1858 - accuracy: 0.9017\n",
            "Epoch 4997: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1860 - accuracy: 0.9010 - val_loss: 4.5890 - val_accuracy: 0.5284\n",
            "Epoch 4998/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9033\n",
            "Epoch 4998: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.1830 - accuracy: 0.9013 - val_loss: 4.3521 - val_accuracy: 0.5328\n",
            "Epoch 4999/5000\n",
            "20/23 [=========================>....] - ETA: 0s - loss: 0.1861 - accuracy: 0.9047\n",
            "Epoch 4999: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.1878 - accuracy: 0.9027 - val_loss: 4.4770 - val_accuracy: 0.5381\n",
            "Epoch 5000/5000\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.8991\n",
            "Epoch 5000: loss did not improve from 0.17754\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.1889 - accuracy: 0.8991 - val_loss: 4.6089 - val_accuracy: 0.5354\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 4.4704 - accuracy: 0.5140\n",
            "[4.4704060554504395, 0.5140119791030884]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.52      0.52      0.52      2133\n",
            "     class 0       0.50      0.51      0.51      2042\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.51      0.51      0.51      4175\n",
            "weighted avg       0.51      0.51      0.51      4175\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "534/534 [==============================] - 1s 2ms/step - loss: 1.0566 - accuracy: 0.8409\n",
            "[1.0566432476043701, 0.8408531546592712]\n",
            "534/534 [==============================] - 1s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.85      0.83      0.84      8533\n",
            "     class 0       0.83      0.85      0.84      8533\n",
            "\n",
            "    accuracy                           0.84     17066\n",
            "   macro avg       0.84      0.84      0.84     17066\n",
            "weighted avg       0.84      0.84      0.84     17066\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.best"
      ],
      "metadata": {
        "id": "4kqxPICBfB3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323cbdf4-f15d-4850-99f8-d4421d5014fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1775376945734024"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Best Model Test Only***#\n",
        "#--------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 1_2', 'class 0']\n",
        "\n",
        "print(\"Test-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jjfj3XzjKvQU",
        "outputId": "3141b726-9e0c-4ba8-f959-1be9dca147fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 4.3956 - accuracy: 0.5071\n",
            "[4.395630836486816, 0.5070658922195435]\n",
            "131/131 [==============================] - 0s 1ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.52      0.53      0.52      2133\n",
            "     class 0       0.50      0.48      0.49      2042\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.51      0.51      0.51      4175\n",
            "weighted avg       0.51      0.51      0.51      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "534/534 [==============================] - 1s 2ms/step - loss: 1.0274 - accuracy: 0.8447\n",
            "[1.0273754596710205, 0.8446618914604187]\n",
            "534/534 [==============================] - 1s 1ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.83      0.86      0.85      8533\n",
            "     class 0       0.86      0.83      0.84      8533\n",
            "\n",
            "    accuracy                           0.84     17066\n",
            "   macro avg       0.85      0.84      0.84     17066\n",
            "weighted avg       0.85      0.84      0.84     17066\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/new_df/best_model_by_class0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LthcmTlDK6hc",
        "outputId": "8e1033f2-b02c-42c6-d69c-b206a080c939"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}
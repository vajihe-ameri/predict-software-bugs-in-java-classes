{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOr0+KyC6PMgwYTmyrdGhk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vajihe-ameri/predict-software-bugs-in-java-classes/blob/main/MLPByClass0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-ChqU41KHj5",
        "outputId": "d04a7a0c-0507-4342-996f-bac8a3773440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2952 sha256=63053bd5d5404b8f737ddfe8889da6e21ef0e7564774e054c202f56e691f786a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn pandas\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons\n",
        "f_measure = tensorflow_addons.metrics.F1Score(num_classes=2, average='macro', threshold=0.5)"
      ],
      "metadata": {
        "id": "ZBLAeiESKP15",
        "outputId": "00e13368-ed10-44bc-96cd-bda5e24d4420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/612.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_0.csv\")\n",
        "train_features = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_0.csv\")\n",
        "test_target = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_NB_0.csv\")\n",
        "train_target = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_NB_0.csv\")\n"
      ],
      "metadata": {
        "id": "5xz-tCLmKP_v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "file_path = \"/content/drive/MyDrive/new_df/best_model_by_class0.hdf5\""
      ],
      "metadata": {
        "id": "xyLscWXQKw5_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#***Build Model***#\n",
        "#-----------------#\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu', input_dim = train_features.shape[1]))\n",
        "model.add(Dense(120, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(140, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(160, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OH_PcZrxKctT",
        "outputId": "59b35a6e-620e-461d-a66e-79b070050ae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               8500      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 120)               12120     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 140)               16940     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 160)               22560     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,281\n",
            "Trainable params: 60,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 500, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 1_2', 'class 0']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BytZba-Ld4Z3",
        "outputId": "b8d02215-ccdb-46af-f83a-09d2f3d5ca0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1767 - accuracy: 0.9051\n",
            "Epoch 3752: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1770 - accuracy: 0.9048 - val_loss: 3.7086 - val_accuracy: 0.5252\n",
            "Epoch 3753/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1821 - accuracy: 0.9039\n",
            "Epoch 3753: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1830 - accuracy: 0.9034 - val_loss: 3.7176 - val_accuracy: 0.5299\n",
            "Epoch 3754/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1814 - accuracy: 0.9027\n",
            "Epoch 3754: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1842 - accuracy: 0.9009 - val_loss: 3.7176 - val_accuracy: 0.5360\n",
            "Epoch 3755/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1820 - accuracy: 0.9028\n",
            "Epoch 3755: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1835 - accuracy: 0.9015 - val_loss: 3.7862 - val_accuracy: 0.5234\n",
            "Epoch 3756/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9044\n",
            "Epoch 3756: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1853 - accuracy: 0.9044 - val_loss: 3.6272 - val_accuracy: 0.5284\n",
            "Epoch 3757/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1975 - accuracy: 0.8960\n",
            "Epoch 3757: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1982 - accuracy: 0.8955 - val_loss: 3.5668 - val_accuracy: 0.5390\n",
            "Epoch 3758/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1852 - accuracy: 0.9027\n",
            "Epoch 3758: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1875 - accuracy: 0.9008 - val_loss: 3.6348 - val_accuracy: 0.5349\n",
            "Epoch 3759/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9002\n",
            "Epoch 3759: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1833 - accuracy: 0.9002 - val_loss: 3.5927 - val_accuracy: 0.5264\n",
            "Epoch 3760/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9015\n",
            "Epoch 3760: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1873 - accuracy: 0.9015 - val_loss: 3.6442 - val_accuracy: 0.5261\n",
            "Epoch 3761/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1825 - accuracy: 0.9041\n",
            "Epoch 3761: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1846 - accuracy: 0.9029 - val_loss: 3.6215 - val_accuracy: 0.5267\n",
            "Epoch 3762/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1838 - accuracy: 0.9033\n",
            "Epoch 3762: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1870 - accuracy: 0.9011 - val_loss: 3.5715 - val_accuracy: 0.5398\n",
            "Epoch 3763/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2009 - accuracy: 0.8951\n",
            "Epoch 3763: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.2024 - accuracy: 0.8942 - val_loss: 3.5523 - val_accuracy: 0.5305\n",
            "Epoch 3764/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.8991\n",
            "Epoch 3764: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1916 - accuracy: 0.8991 - val_loss: 3.6587 - val_accuracy: 0.5284\n",
            "Epoch 3765/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1848 - accuracy: 0.9007\n",
            "Epoch 3765: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1851 - accuracy: 0.9006 - val_loss: 3.7683 - val_accuracy: 0.5340\n",
            "Epoch 3766/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1909 - accuracy: 0.8973\n",
            "Epoch 3766: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1909 - accuracy: 0.8972 - val_loss: 3.4976 - val_accuracy: 0.5284\n",
            "Epoch 3767/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1827 - accuracy: 0.9033\n",
            "Epoch 3767: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1858 - accuracy: 0.9016 - val_loss: 3.7772 - val_accuracy: 0.5372\n",
            "Epoch 3768/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1830 - accuracy: 0.9037\n",
            "Epoch 3768: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1828 - accuracy: 0.9038 - val_loss: 3.6895 - val_accuracy: 0.5284\n",
            "Epoch 3769/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9022\n",
            "Epoch 3769: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1891 - accuracy: 0.9022 - val_loss: 3.6675 - val_accuracy: 0.5243\n",
            "Epoch 3770/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1885 - accuracy: 0.9017\n",
            "Epoch 3770: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1905 - accuracy: 0.9003 - val_loss: 3.6953 - val_accuracy: 0.5366\n",
            "Epoch 3771/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1908 - accuracy: 0.8991\n",
            "Epoch 3771: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1914 - accuracy: 0.8986 - val_loss: 3.5086 - val_accuracy: 0.5354\n",
            "Epoch 3772/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9036\n",
            "Epoch 3772: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1833 - accuracy: 0.9032 - val_loss: 3.9307 - val_accuracy: 0.5287\n",
            "Epoch 3773/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1788 - accuracy: 0.9051\n",
            "Epoch 3773: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1802 - accuracy: 0.9037 - val_loss: 3.7268 - val_accuracy: 0.5269\n",
            "Epoch 3774/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9028\n",
            "Epoch 3774: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1846 - accuracy: 0.9028 - val_loss: 3.7204 - val_accuracy: 0.5305\n",
            "Epoch 3775/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9002\n",
            "Epoch 3775: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1901 - accuracy: 0.9002 - val_loss: 3.6705 - val_accuracy: 0.5252\n",
            "Epoch 3776/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1907 - accuracy: 0.9011\n",
            "Epoch 3776: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1906 - accuracy: 0.9010 - val_loss: 3.6456 - val_accuracy: 0.5351\n",
            "Epoch 3777/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1827 - accuracy: 0.9052\n",
            "Epoch 3777: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1828 - accuracy: 0.9053 - val_loss: 3.6847 - val_accuracy: 0.5308\n",
            "Epoch 3778/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9027\n",
            "Epoch 3778: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1862 - accuracy: 0.9027 - val_loss: 3.7685 - val_accuracy: 0.5366\n",
            "Epoch 3779/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1945 - accuracy: 0.8967\n",
            "Epoch 3779: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1965 - accuracy: 0.8948 - val_loss: 3.5494 - val_accuracy: 0.5264\n",
            "Epoch 3780/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1819 - accuracy: 0.9030\n",
            "Epoch 3780: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1820 - accuracy: 0.9028 - val_loss: 3.6487 - val_accuracy: 0.5308\n",
            "Epoch 3781/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1789 - accuracy: 0.9044\n",
            "Epoch 3781: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1799 - accuracy: 0.9039 - val_loss: 3.6869 - val_accuracy: 0.5272\n",
            "Epoch 3782/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1728 - accuracy: 0.9067\n",
            "Epoch 3782: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1746 - accuracy: 0.9054 - val_loss: 3.7968 - val_accuracy: 0.5296\n",
            "Epoch 3783/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1821 - accuracy: 0.9027\n",
            "Epoch 3783: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1822 - accuracy: 0.9027 - val_loss: 3.7552 - val_accuracy: 0.5281\n",
            "Epoch 3784/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1924 - accuracy: 0.9002\n",
            "Epoch 3784: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1930 - accuracy: 0.8995 - val_loss: 3.6743 - val_accuracy: 0.5346\n",
            "Epoch 3785/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.8991\n",
            "Epoch 3785: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1887 - accuracy: 0.8991 - val_loss: 3.6035 - val_accuracy: 0.5328\n",
            "Epoch 3786/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1831 - accuracy: 0.9032\n",
            "Epoch 3786: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1844 - accuracy: 0.9025 - val_loss: 3.8342 - val_accuracy: 0.5281\n",
            "Epoch 3787/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1919 - accuracy: 0.9006\n",
            "Epoch 3787: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1952 - accuracy: 0.8988 - val_loss: 3.7114 - val_accuracy: 0.5223\n",
            "Epoch 3788/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2003 - accuracy: 0.8972\n",
            "Epoch 3788: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.2013 - accuracy: 0.8953 - val_loss: 3.7621 - val_accuracy: 0.5246\n",
            "Epoch 3789/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1939 - accuracy: 0.9003\n",
            "Epoch 3789: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1941 - accuracy: 0.8999 - val_loss: 3.6094 - val_accuracy: 0.5208\n",
            "Epoch 3790/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.8976\n",
            "Epoch 3790: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1916 - accuracy: 0.8976 - val_loss: 3.7674 - val_accuracy: 0.5278\n",
            "Epoch 3791/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1916 - accuracy: 0.9000\n",
            "Epoch 3791: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1917 - accuracy: 0.8998 - val_loss: 3.5664 - val_accuracy: 0.5325\n",
            "Epoch 3792/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1853 - accuracy: 0.9024\n",
            "Epoch 3792: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1850 - accuracy: 0.9026 - val_loss: 3.8098 - val_accuracy: 0.5278\n",
            "Epoch 3793/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.9040\n",
            "Epoch 3793: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1835 - accuracy: 0.9038 - val_loss: 3.6919 - val_accuracy: 0.5267\n",
            "Epoch 3794/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1822 - accuracy: 0.9059\n",
            "Epoch 3794: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1840 - accuracy: 0.9043 - val_loss: 3.6253 - val_accuracy: 0.5331\n",
            "Epoch 3795/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1884 - accuracy: 0.8981\n",
            "Epoch 3795: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1884 - accuracy: 0.8981 - val_loss: 3.5438 - val_accuracy: 0.5448\n",
            "Epoch 3796/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1945 - accuracy: 0.8966\n",
            "Epoch 3796: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1936 - accuracy: 0.8966 - val_loss: 3.5833 - val_accuracy: 0.5287\n",
            "Epoch 3797/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1866 - accuracy: 0.9046\n",
            "Epoch 3797: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1879 - accuracy: 0.9032 - val_loss: 3.7546 - val_accuracy: 0.5316\n",
            "Epoch 3798/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.8991\n",
            "Epoch 3798: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1860 - accuracy: 0.8991 - val_loss: 3.7206 - val_accuracy: 0.5246\n",
            "Epoch 3799/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9007\n",
            "Epoch 3799: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1873 - accuracy: 0.9007 - val_loss: 3.5989 - val_accuracy: 0.5381\n",
            "Epoch 3800/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9022\n",
            "Epoch 3800: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1869 - accuracy: 0.9024 - val_loss: 3.6925 - val_accuracy: 0.5258\n",
            "Epoch 3801/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1820 - accuracy: 0.9025\n",
            "Epoch 3801: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1822 - accuracy: 0.9022 - val_loss: 3.7698 - val_accuracy: 0.5272\n",
            "Epoch 3802/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9025\n",
            "Epoch 3802: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1843 - accuracy: 0.9025 - val_loss: 3.7677 - val_accuracy: 0.5226\n",
            "Epoch 3803/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1865 - accuracy: 0.9006\n",
            "Epoch 3803: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1859 - accuracy: 0.9007 - val_loss: 3.6613 - val_accuracy: 0.5272\n",
            "Epoch 3804/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1794 - accuracy: 0.9050\n",
            "Epoch 3804: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1808 - accuracy: 0.9040 - val_loss: 3.7498 - val_accuracy: 0.5310\n",
            "Epoch 3805/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9026\n",
            "Epoch 3805: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1815 - accuracy: 0.9026 - val_loss: 3.5940 - val_accuracy: 0.5185\n",
            "Epoch 3806/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9057\n",
            "Epoch 3806: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1826 - accuracy: 0.9057 - val_loss: 3.6364 - val_accuracy: 0.5322\n",
            "Epoch 3807/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9086\n",
            "Epoch 3807: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1752 - accuracy: 0.9073 - val_loss: 3.7416 - val_accuracy: 0.5378\n",
            "Epoch 3808/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1752 - accuracy: 0.9073\n",
            "Epoch 3808: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1778 - accuracy: 0.9055 - val_loss: 3.7279 - val_accuracy: 0.5296\n",
            "Epoch 3809/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1858 - accuracy: 0.9011\n",
            "Epoch 3809: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1860 - accuracy: 0.9013 - val_loss: 3.4895 - val_accuracy: 0.5296\n",
            "Epoch 3810/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.8992\n",
            "Epoch 3810: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1897 - accuracy: 0.8992 - val_loss: 3.7319 - val_accuracy: 0.5281\n",
            "Epoch 3811/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1852 - accuracy: 0.9042\n",
            "Epoch 3811: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1856 - accuracy: 0.9037 - val_loss: 3.6610 - val_accuracy: 0.5299\n",
            "Epoch 3812/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1891 - accuracy: 0.9014\n",
            "Epoch 3812: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1892 - accuracy: 0.9013 - val_loss: 3.6192 - val_accuracy: 0.5316\n",
            "Epoch 3813/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1873 - accuracy: 0.9009\n",
            "Epoch 3813: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1878 - accuracy: 0.9007 - val_loss: 3.7905 - val_accuracy: 0.5278\n",
            "Epoch 3814/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1834 - accuracy: 0.9037\n",
            "Epoch 3814: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1857 - accuracy: 0.9018 - val_loss: 3.8149 - val_accuracy: 0.5205\n",
            "Epoch 3815/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1809 - accuracy: 0.9052\n",
            "Epoch 3815: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1823 - accuracy: 0.9045 - val_loss: 3.7213 - val_accuracy: 0.5372\n",
            "Epoch 3816/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9034\n",
            "Epoch 3816: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1821 - accuracy: 0.9018 - val_loss: 3.7532 - val_accuracy: 0.5272\n",
            "Epoch 3817/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1842 - accuracy: 0.9022\n",
            "Epoch 3817: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1853 - accuracy: 0.9008 - val_loss: 3.6486 - val_accuracy: 0.5354\n",
            "Epoch 3818/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9024\n",
            "Epoch 3818: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.1864 - accuracy: 0.9024 - val_loss: 3.6613 - val_accuracy: 0.5340\n",
            "Epoch 3819/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1755 - accuracy: 0.9083\n",
            "Epoch 3819: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1768 - accuracy: 0.9065 - val_loss: 3.8386 - val_accuracy: 0.5152\n",
            "Epoch 3820/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.8995\n",
            "Epoch 3820: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1833 - accuracy: 0.8995 - val_loss: 3.7126 - val_accuracy: 0.5331\n",
            "Epoch 3821/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1869 - accuracy: 0.9030\n",
            "Epoch 3821: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1878 - accuracy: 0.9024 - val_loss: 3.7330 - val_accuracy: 0.5308\n",
            "Epoch 3822/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.2036 - accuracy: 0.8949\n",
            "Epoch 3822: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.2038 - accuracy: 0.8946 - val_loss: 3.6153 - val_accuracy: 0.5308\n",
            "Epoch 3823/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9013\n",
            "Epoch 3823: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1881 - accuracy: 0.9013 - val_loss: 3.6778 - val_accuracy: 0.5281\n",
            "Epoch 3824/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1852 - accuracy: 0.9043\n",
            "Epoch 3824: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1849 - accuracy: 0.9037 - val_loss: 3.6519 - val_accuracy: 0.5310\n",
            "Epoch 3825/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.9022\n",
            "Epoch 3825: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1825 - accuracy: 0.9026 - val_loss: 3.6166 - val_accuracy: 0.5240\n",
            "Epoch 3826/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1847 - accuracy: 0.8991\n",
            "Epoch 3826: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1847 - accuracy: 0.8997 - val_loss: 3.6774 - val_accuracy: 0.5325\n",
            "Epoch 3827/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1840 - accuracy: 0.9035\n",
            "Epoch 3827: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1847 - accuracy: 0.9028 - val_loss: 3.6905 - val_accuracy: 0.5185\n",
            "Epoch 3828/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.8970\n",
            "Epoch 3828: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1894 - accuracy: 0.8970 - val_loss: 3.7465 - val_accuracy: 0.5351\n",
            "Epoch 3829/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1964 - accuracy: 0.8990\n",
            "Epoch 3829: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1974 - accuracy: 0.8973 - val_loss: 3.6861 - val_accuracy: 0.5378\n",
            "Epoch 3830/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.8967\n",
            "Epoch 3830: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1956 - accuracy: 0.8967 - val_loss: 3.5650 - val_accuracy: 0.5331\n",
            "Epoch 3831/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1870 - accuracy: 0.9034\n",
            "Epoch 3831: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1903 - accuracy: 0.9007 - val_loss: 3.5078 - val_accuracy: 0.5284\n",
            "Epoch 3832/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1972 - accuracy: 0.8949\n",
            "Epoch 3832: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1995 - accuracy: 0.8931 - val_loss: 3.6752 - val_accuracy: 0.5381\n",
            "Epoch 3833/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1926 - accuracy: 0.9000\n",
            "Epoch 3833: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1947 - accuracy: 0.8991 - val_loss: 3.7281 - val_accuracy: 0.5451\n",
            "Epoch 3834/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1958 - accuracy: 0.8981\n",
            "Epoch 3834: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1984 - accuracy: 0.8969 - val_loss: 3.6879 - val_accuracy: 0.5182\n",
            "Epoch 3835/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9027\n",
            "Epoch 3835: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1820 - accuracy: 0.9027 - val_loss: 3.6520 - val_accuracy: 0.5313\n",
            "Epoch 3836/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9087\n",
            "Epoch 3836: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1769 - accuracy: 0.9087 - val_loss: 3.6508 - val_accuracy: 0.5264\n",
            "Epoch 3837/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1802 - accuracy: 0.9060\n",
            "Epoch 3837: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1813 - accuracy: 0.9046 - val_loss: 3.7088 - val_accuracy: 0.5337\n",
            "Epoch 3838/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1811 - accuracy: 0.9026\n",
            "Epoch 3838: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1827 - accuracy: 0.9004 - val_loss: 3.8082 - val_accuracy: 0.5325\n",
            "Epoch 3839/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1801 - accuracy: 0.9036\n",
            "Epoch 3839: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1808 - accuracy: 0.9031 - val_loss: 3.6048 - val_accuracy: 0.5360\n",
            "Epoch 3840/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.8937\n",
            "Epoch 3840: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.2006 - accuracy: 0.8937 - val_loss: 3.6965 - val_accuracy: 0.5319\n",
            "Epoch 3841/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1853 - accuracy: 0.9033\n",
            "Epoch 3841: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1858 - accuracy: 0.9024 - val_loss: 3.7946 - val_accuracy: 0.5349\n",
            "Epoch 3842/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1811 - accuracy: 0.9025\n",
            "Epoch 3842: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1825 - accuracy: 0.9014 - val_loss: 3.6033 - val_accuracy: 0.5334\n",
            "Epoch 3843/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1860 - accuracy: 0.9042\n",
            "Epoch 3843: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1872 - accuracy: 0.9029 - val_loss: 3.7599 - val_accuracy: 0.5193\n",
            "Epoch 3844/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1890 - accuracy: 0.9001\n",
            "Epoch 3844: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1876 - accuracy: 0.9005 - val_loss: 3.7586 - val_accuracy: 0.5308\n",
            "Epoch 3845/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1812 - accuracy: 0.9028\n",
            "Epoch 3845: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1832 - accuracy: 0.9021 - val_loss: 3.6654 - val_accuracy: 0.5223\n",
            "Epoch 3846/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.9067\n",
            "Epoch 3846: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1790 - accuracy: 0.9063 - val_loss: 3.8580 - val_accuracy: 0.5284\n",
            "Epoch 3847/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1980 - accuracy: 0.8990\n",
            "Epoch 3847: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.2015 - accuracy: 0.8975 - val_loss: 3.6477 - val_accuracy: 0.5272\n",
            "Epoch 3848/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1934 - accuracy: 0.9002\n",
            "Epoch 3848: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1947 - accuracy: 0.8994 - val_loss: 3.8586 - val_accuracy: 0.5217\n",
            "Epoch 3849/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1883 - accuracy: 0.9022\n",
            "Epoch 3849: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1885 - accuracy: 0.9015 - val_loss: 3.7153 - val_accuracy: 0.5290\n",
            "Epoch 3850/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9029\n",
            "Epoch 3850: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1808 - accuracy: 0.9018 - val_loss: 3.7939 - val_accuracy: 0.5249\n",
            "Epoch 3851/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1831 - accuracy: 0.9067\n",
            "Epoch 3851: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1838 - accuracy: 0.9049 - val_loss: 3.7559 - val_accuracy: 0.5284\n",
            "Epoch 3852/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1738 - accuracy: 0.9074\n",
            "Epoch 3852: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1750 - accuracy: 0.9063 - val_loss: 3.8251 - val_accuracy: 0.5299\n",
            "Epoch 3853/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1767 - accuracy: 0.9070\n",
            "Epoch 3853: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1785 - accuracy: 0.9051 - val_loss: 3.6046 - val_accuracy: 0.5249\n",
            "Epoch 3854/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9020\n",
            "Epoch 3854: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1811 - accuracy: 0.9018 - val_loss: 3.7070 - val_accuracy: 0.5322\n",
            "Epoch 3855/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1829 - accuracy: 0.8997\n",
            "Epoch 3855: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1826 - accuracy: 0.8999 - val_loss: 3.8377 - val_accuracy: 0.5346\n",
            "Epoch 3856/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1861 - accuracy: 0.9023\n",
            "Epoch 3856: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1855 - accuracy: 0.9029 - val_loss: 3.6700 - val_accuracy: 0.5316\n",
            "Epoch 3857/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9007\n",
            "Epoch 3857: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1903 - accuracy: 0.9007 - val_loss: 3.8438 - val_accuracy: 0.5237\n",
            "Epoch 3858/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1783 - accuracy: 0.9070\n",
            "Epoch 3858: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9038 - val_loss: 3.8463 - val_accuracy: 0.5261\n",
            "Epoch 3859/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1871 - accuracy: 0.9036\n",
            "Epoch 3859: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1883 - accuracy: 0.9019 - val_loss: 3.7433 - val_accuracy: 0.5228\n",
            "Epoch 3860/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.8976\n",
            "Epoch 3860: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1955 - accuracy: 0.8976 - val_loss: 3.7925 - val_accuracy: 0.5305\n",
            "Epoch 3861/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.9011\n",
            "Epoch 3861: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1840 - accuracy: 0.9000 - val_loss: 3.6770 - val_accuracy: 0.5296\n",
            "Epoch 3862/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1797 - accuracy: 0.9050\n",
            "Epoch 3862: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1821 - accuracy: 0.9036 - val_loss: 3.6908 - val_accuracy: 0.5302\n",
            "Epoch 3863/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1852 - accuracy: 0.9022\n",
            "Epoch 3863: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1847 - accuracy: 0.9023 - val_loss: 3.7926 - val_accuracy: 0.5284\n",
            "Epoch 3864/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9024\n",
            "Epoch 3864: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1833 - accuracy: 0.9024 - val_loss: 3.8089 - val_accuracy: 0.5220\n",
            "Epoch 3865/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2033 - accuracy: 0.8950\n",
            "Epoch 3865: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.2043 - accuracy: 0.8941 - val_loss: 3.5347 - val_accuracy: 0.5346\n",
            "Epoch 3866/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1899 - accuracy: 0.8974\n",
            "Epoch 3866: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1893 - accuracy: 0.8980 - val_loss: 3.6774 - val_accuracy: 0.5310\n",
            "Epoch 3867/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1857 - accuracy: 0.9024\n",
            "Epoch 3867: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1850 - accuracy: 0.9021 - val_loss: 3.6022 - val_accuracy: 0.5243\n",
            "Epoch 3868/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1885 - accuracy: 0.9023\n",
            "Epoch 3868: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1884 - accuracy: 0.9023 - val_loss: 3.4901 - val_accuracy: 0.5366\n",
            "Epoch 3869/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1898 - accuracy: 0.9009\n",
            "Epoch 3869: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1914 - accuracy: 0.8990 - val_loss: 3.6023 - val_accuracy: 0.5354\n",
            "Epoch 3870/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1859 - accuracy: 0.9004\n",
            "Epoch 3870: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1861 - accuracy: 0.9002 - val_loss: 3.6136 - val_accuracy: 0.5319\n",
            "Epoch 3871/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9001\n",
            "Epoch 3871: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1869 - accuracy: 0.9001 - val_loss: 3.7418 - val_accuracy: 0.5305\n",
            "Epoch 3872/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1890 - accuracy: 0.9022\n",
            "Epoch 3872: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1905 - accuracy: 0.9012 - val_loss: 3.7785 - val_accuracy: 0.5214\n",
            "Epoch 3873/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1991 - accuracy: 0.8961\n",
            "Epoch 3873: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1991 - accuracy: 0.8961 - val_loss: 3.5491 - val_accuracy: 0.5302\n",
            "Epoch 3874/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9016\n",
            "Epoch 3874: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1882 - accuracy: 0.9014 - val_loss: 3.5397 - val_accuracy: 0.5328\n",
            "Epoch 3875/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1771 - accuracy: 0.9071\n",
            "Epoch 3875: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1782 - accuracy: 0.9057 - val_loss: 3.7379 - val_accuracy: 0.5269\n",
            "Epoch 3876/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1820 - accuracy: 0.9032\n",
            "Epoch 3876: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1818 - accuracy: 0.9035 - val_loss: 3.6331 - val_accuracy: 0.5290\n",
            "Epoch 3877/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1891 - accuracy: 0.9022\n",
            "Epoch 3877: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1918 - accuracy: 0.9008 - val_loss: 3.6469 - val_accuracy: 0.5278\n",
            "Epoch 3878/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1835 - accuracy: 0.9055\n",
            "Epoch 3878: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1856 - accuracy: 0.9042 - val_loss: 3.5554 - val_accuracy: 0.5272\n",
            "Epoch 3879/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9031\n",
            "Epoch 3879: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1861 - accuracy: 0.9031 - val_loss: 3.7855 - val_accuracy: 0.5190\n",
            "Epoch 3880/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1822 - accuracy: 0.9045\n",
            "Epoch 3880: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1836 - accuracy: 0.9027 - val_loss: 3.5827 - val_accuracy: 0.5316\n",
            "Epoch 3881/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9016\n",
            "Epoch 3881: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1873 - accuracy: 0.9016 - val_loss: 3.6540 - val_accuracy: 0.5281\n",
            "Epoch 3882/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1840 - accuracy: 0.9030\n",
            "Epoch 3882: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1841 - accuracy: 0.9021 - val_loss: 3.8187 - val_accuracy: 0.5258\n",
            "Epoch 3883/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1805 - accuracy: 0.9047\n",
            "Epoch 3883: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1818 - accuracy: 0.9040 - val_loss: 3.6794 - val_accuracy: 0.5313\n",
            "Epoch 3884/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1846 - accuracy: 0.9035\n",
            "Epoch 3884: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1858 - accuracy: 0.9024 - val_loss: 3.7180 - val_accuracy: 0.5346\n",
            "Epoch 3885/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1883 - accuracy: 0.9022\n",
            "Epoch 3885: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1898 - accuracy: 0.9012 - val_loss: 3.6284 - val_accuracy: 0.5208\n",
            "Epoch 3886/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1940 - accuracy: 0.8982\n",
            "Epoch 3886: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1936 - accuracy: 0.8979 - val_loss: 3.6940 - val_accuracy: 0.5357\n",
            "Epoch 3887/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1915 - accuracy: 0.8979\n",
            "Epoch 3887: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1919 - accuracy: 0.8975 - val_loss: 3.6962 - val_accuracy: 0.5325\n",
            "Epoch 3888/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1960 - accuracy: 0.8976\n",
            "Epoch 3888: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1968 - accuracy: 0.8965 - val_loss: 3.7397 - val_accuracy: 0.5185\n",
            "Epoch 3889/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2099 - accuracy: 0.8922\n",
            "Epoch 3889: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.2130 - accuracy: 0.8907 - val_loss: 3.6385 - val_accuracy: 0.5243\n",
            "Epoch 3890/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1919 - accuracy: 0.9006\n",
            "Epoch 3890: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1924 - accuracy: 0.9000 - val_loss: 3.6248 - val_accuracy: 0.5272\n",
            "Epoch 3891/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1899 - accuracy: 0.8995\n",
            "Epoch 3891: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1898 - accuracy: 0.8995 - val_loss: 3.5964 - val_accuracy: 0.5281\n",
            "Epoch 3892/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1793 - accuracy: 0.9034\n",
            "Epoch 3892: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1804 - accuracy: 0.9029 - val_loss: 3.5211 - val_accuracy: 0.5346\n",
            "Epoch 3893/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1918 - accuracy: 0.9006\n",
            "Epoch 3893: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1917 - accuracy: 0.8999 - val_loss: 3.8639 - val_accuracy: 0.5234\n",
            "Epoch 3894/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9048\n",
            "Epoch 3894: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1821 - accuracy: 0.9048 - val_loss: 3.6194 - val_accuracy: 0.5319\n",
            "Epoch 3895/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1804 - accuracy: 0.9044\n",
            "Epoch 3895: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1810 - accuracy: 0.9042 - val_loss: 3.6410 - val_accuracy: 0.5299\n",
            "Epoch 3896/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1884 - accuracy: 0.9051\n",
            "Epoch 3896: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1900 - accuracy: 0.9043 - val_loss: 3.5944 - val_accuracy: 0.5281\n",
            "Epoch 3897/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9051\n",
            "Epoch 3897: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1814 - accuracy: 0.9039 - val_loss: 3.8689 - val_accuracy: 0.5299\n",
            "Epoch 3898/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1811 - accuracy: 0.9032\n",
            "Epoch 3898: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1813 - accuracy: 0.9020 - val_loss: 3.7111 - val_accuracy: 0.5293\n",
            "Epoch 3899/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1839 - accuracy: 0.9008\n",
            "Epoch 3899: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1845 - accuracy: 0.9005 - val_loss: 3.6430 - val_accuracy: 0.5293\n",
            "Epoch 3900/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1927 - accuracy: 0.8992\n",
            "Epoch 3900: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1950 - accuracy: 0.8974 - val_loss: 3.4731 - val_accuracy: 0.5313\n",
            "Epoch 3901/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.2070 - accuracy: 0.8932\n",
            "Epoch 3901: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.2063 - accuracy: 0.8936 - val_loss: 3.7525 - val_accuracy: 0.5220\n",
            "Epoch 3902/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1851 - accuracy: 0.9015\n",
            "Epoch 3902: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1854 - accuracy: 0.9012 - val_loss: 3.6374 - val_accuracy: 0.5393\n",
            "Epoch 3903/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1771 - accuracy: 0.9068\n",
            "Epoch 3903: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1781 - accuracy: 0.9062 - val_loss: 3.6028 - val_accuracy: 0.5269\n",
            "Epoch 3904/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1824 - accuracy: 0.9031\n",
            "Epoch 3904: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1829 - accuracy: 0.9027 - val_loss: 3.6290 - val_accuracy: 0.5272\n",
            "Epoch 3905/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1839 - accuracy: 0.9045\n",
            "Epoch 3905: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1851 - accuracy: 0.9038 - val_loss: 3.7344 - val_accuracy: 0.5299\n",
            "Epoch 3906/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9035\n",
            "Epoch 3906: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9035 - val_loss: 3.7013 - val_accuracy: 0.5369\n",
            "Epoch 3907/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1870 - accuracy: 0.9001\n",
            "Epoch 3907: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1876 - accuracy: 0.8997 - val_loss: 3.4897 - val_accuracy: 0.5211\n",
            "Epoch 3908/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1900 - accuracy: 0.9001\n",
            "Epoch 3908: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1911 - accuracy: 0.8983 - val_loss: 3.6393 - val_accuracy: 0.5354\n",
            "Epoch 3909/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1930 - accuracy: 0.8994\n",
            "Epoch 3909: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1936 - accuracy: 0.8982 - val_loss: 3.6874 - val_accuracy: 0.5267\n",
            "Epoch 3910/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1889 - accuracy: 0.9010\n",
            "Epoch 3910: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1895 - accuracy: 0.9004 - val_loss: 3.7496 - val_accuracy: 0.5281\n",
            "Epoch 3911/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1754 - accuracy: 0.9066\n",
            "Epoch 3911: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1777 - accuracy: 0.9052 - val_loss: 3.6317 - val_accuracy: 0.5299\n",
            "Epoch 3912/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1810 - accuracy: 0.9047\n",
            "Epoch 3912: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1819 - accuracy: 0.9036 - val_loss: 3.7812 - val_accuracy: 0.5328\n",
            "Epoch 3913/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9019\n",
            "Epoch 3913: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1841 - accuracy: 0.9014 - val_loss: 3.6416 - val_accuracy: 0.5296\n",
            "Epoch 3914/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1839 - accuracy: 0.9027\n",
            "Epoch 3914: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1861 - accuracy: 0.9015 - val_loss: 3.5832 - val_accuracy: 0.5384\n",
            "Epoch 3915/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1973 - accuracy: 0.8989\n",
            "Epoch 3915: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1966 - accuracy: 0.8990 - val_loss: 3.7093 - val_accuracy: 0.5267\n",
            "Epoch 3916/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.9011\n",
            "Epoch 3916: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1907 - accuracy: 0.9011 - val_loss: 3.6972 - val_accuracy: 0.5290\n",
            "Epoch 3917/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9053\n",
            "Epoch 3917: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1804 - accuracy: 0.9045 - val_loss: 3.6854 - val_accuracy: 0.5278\n",
            "Epoch 3918/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1807 - accuracy: 0.9038\n",
            "Epoch 3918: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1810 - accuracy: 0.9037 - val_loss: 3.6533 - val_accuracy: 0.5267\n",
            "Epoch 3919/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9078\n",
            "Epoch 3919: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1770 - accuracy: 0.9073 - val_loss: 3.7632 - val_accuracy: 0.5220\n",
            "Epoch 3920/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9088\n",
            "Epoch 3920: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1746 - accuracy: 0.9076 - val_loss: 3.6446 - val_accuracy: 0.5308\n",
            "Epoch 3921/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1833 - accuracy: 0.9032\n",
            "Epoch 3921: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1836 - accuracy: 0.9030 - val_loss: 3.7582 - val_accuracy: 0.5261\n",
            "Epoch 3922/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.8995\n",
            "Epoch 3922: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1920 - accuracy: 0.8996 - val_loss: 3.6662 - val_accuracy: 0.5296\n",
            "Epoch 3923/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1835 - accuracy: 0.9003\n",
            "Epoch 3923: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1822 - accuracy: 0.9011 - val_loss: 3.6704 - val_accuracy: 0.5252\n",
            "Epoch 3924/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1852 - accuracy: 0.9012\n",
            "Epoch 3924: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1858 - accuracy: 0.9007 - val_loss: 3.8648 - val_accuracy: 0.5187\n",
            "Epoch 3925/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1842 - accuracy: 0.9034\n",
            "Epoch 3925: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1865 - accuracy: 0.9017 - val_loss: 3.6347 - val_accuracy: 0.5269\n",
            "Epoch 3926/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1807 - accuracy: 0.9042\n",
            "Epoch 3926: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1803 - accuracy: 0.9050 - val_loss: 3.6536 - val_accuracy: 0.5313\n",
            "Epoch 3927/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1782 - accuracy: 0.9073\n",
            "Epoch 3927: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1786 - accuracy: 0.9071 - val_loss: 3.7352 - val_accuracy: 0.5173\n",
            "Epoch 3928/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9024\n",
            "Epoch 3928: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1821 - accuracy: 0.9024 - val_loss: 3.8016 - val_accuracy: 0.5302\n",
            "Epoch 3929/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9002\n",
            "Epoch 3929: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1829 - accuracy: 0.8998 - val_loss: 3.7627 - val_accuracy: 0.5278\n",
            "Epoch 3930/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1871 - accuracy: 0.8993\n",
            "Epoch 3930: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1878 - accuracy: 0.8989 - val_loss: 3.8578 - val_accuracy: 0.5269\n",
            "Epoch 3931/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1840 - accuracy: 0.9026\n",
            "Epoch 3931: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1861 - accuracy: 0.9015 - val_loss: 3.7395 - val_accuracy: 0.5351\n",
            "Epoch 3932/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1764 - accuracy: 0.9065\n",
            "Epoch 3932: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1775 - accuracy: 0.9054 - val_loss: 3.7909 - val_accuracy: 0.5296\n",
            "Epoch 3933/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9071\n",
            "Epoch 3933: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1784 - accuracy: 0.9044 - val_loss: 3.6918 - val_accuracy: 0.5231\n",
            "Epoch 3934/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1854 - accuracy: 0.9014\n",
            "Epoch 3934: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1851 - accuracy: 0.9014 - val_loss: 3.8221 - val_accuracy: 0.5249\n",
            "Epoch 3935/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1812 - accuracy: 0.9042\n",
            "Epoch 3935: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1827 - accuracy: 0.9031 - val_loss: 3.7527 - val_accuracy: 0.5381\n",
            "Epoch 3936/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1877 - accuracy: 0.8992\n",
            "Epoch 3936: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1877 - accuracy: 0.8992 - val_loss: 3.7168 - val_accuracy: 0.5231\n",
            "Epoch 3937/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1772 - accuracy: 0.9077\n",
            "Epoch 3937: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1820 - accuracy: 0.9043 - val_loss: 3.6699 - val_accuracy: 0.5343\n",
            "Epoch 3938/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1914 - accuracy: 0.9010\n",
            "Epoch 3938: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1902 - accuracy: 0.9012 - val_loss: 3.8619 - val_accuracy: 0.5357\n",
            "Epoch 3939/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1846 - accuracy: 0.9037\n",
            "Epoch 3939: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1847 - accuracy: 0.9029 - val_loss: 3.7434 - val_accuracy: 0.5243\n",
            "Epoch 3940/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1788 - accuracy: 0.9058\n",
            "Epoch 3940: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1792 - accuracy: 0.9048 - val_loss: 3.6872 - val_accuracy: 0.5261\n",
            "Epoch 3941/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1842 - accuracy: 0.9026\n",
            "Epoch 3941: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1874 - accuracy: 0.9006 - val_loss: 3.6433 - val_accuracy: 0.5269\n",
            "Epoch 3942/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1946 - accuracy: 0.8990\n",
            "Epoch 3942: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1966 - accuracy: 0.8972 - val_loss: 3.7845 - val_accuracy: 0.5351\n",
            "Epoch 3943/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1872 - accuracy: 0.9013\n",
            "Epoch 3943: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1894 - accuracy: 0.8996 - val_loss: 3.6405 - val_accuracy: 0.5346\n",
            "Epoch 3944/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9029\n",
            "Epoch 3944: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1862 - accuracy: 0.9029 - val_loss: 3.6728 - val_accuracy: 0.5223\n",
            "Epoch 3945/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1809 - accuracy: 0.9048\n",
            "Epoch 3945: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1810 - accuracy: 0.9044 - val_loss: 3.6928 - val_accuracy: 0.5267\n",
            "Epoch 3946/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1789 - accuracy: 0.9039\n",
            "Epoch 3946: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1787 - accuracy: 0.9041 - val_loss: 3.7282 - val_accuracy: 0.5293\n",
            "Epoch 3947/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.9047\n",
            "Epoch 3947: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1774 - accuracy: 0.9047 - val_loss: 3.8176 - val_accuracy: 0.5243\n",
            "Epoch 3948/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1799 - accuracy: 0.9045\n",
            "Epoch 3948: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1819 - accuracy: 0.9036 - val_loss: 3.7677 - val_accuracy: 0.5308\n",
            "Epoch 3949/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1914 - accuracy: 0.8983\n",
            "Epoch 3949: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1914 - accuracy: 0.8983 - val_loss: 3.6633 - val_accuracy: 0.5366\n",
            "Epoch 3950/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1884 - accuracy: 0.9018\n",
            "Epoch 3950: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1900 - accuracy: 0.9000 - val_loss: 3.8054 - val_accuracy: 0.5214\n",
            "Epoch 3951/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1762 - accuracy: 0.9054\n",
            "Epoch 3951: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1762 - accuracy: 0.9053 - val_loss: 3.8124 - val_accuracy: 0.5322\n",
            "Epoch 3952/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1747 - accuracy: 0.9077\n",
            "Epoch 3952: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1776 - accuracy: 0.9053 - val_loss: 3.6660 - val_accuracy: 0.5363\n",
            "Epoch 3953/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1907 - accuracy: 0.9006\n",
            "Epoch 3953: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1903 - accuracy: 0.9005 - val_loss: 3.7163 - val_accuracy: 0.5354\n",
            "Epoch 3954/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.8996\n",
            "Epoch 3954: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1936 - accuracy: 0.8996 - val_loss: 3.6102 - val_accuracy: 0.5349\n",
            "Epoch 3955/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1882 - accuracy: 0.9020\n",
            "Epoch 3955: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1879 - accuracy: 0.9025 - val_loss: 3.6941 - val_accuracy: 0.5401\n",
            "Epoch 3956/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1896 - accuracy: 0.9029\n",
            "Epoch 3956: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1919 - accuracy: 0.9016 - val_loss: 3.5755 - val_accuracy: 0.5290\n",
            "Epoch 3957/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9006\n",
            "Epoch 3957: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1899 - accuracy: 0.8986 - val_loss: 3.8562 - val_accuracy: 0.5387\n",
            "Epoch 3958/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1862 - accuracy: 0.9025\n",
            "Epoch 3958: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1861 - accuracy: 0.9020 - val_loss: 3.7246 - val_accuracy: 0.5252\n",
            "Epoch 3959/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1844 - accuracy: 0.9025\n",
            "Epoch 3959: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1856 - accuracy: 0.9021 - val_loss: 3.7243 - val_accuracy: 0.5269\n",
            "Epoch 3960/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9050\n",
            "Epoch 3960: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1798 - accuracy: 0.9050 - val_loss: 3.6036 - val_accuracy: 0.5243\n",
            "Epoch 3961/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1774 - accuracy: 0.9039\n",
            "Epoch 3961: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1777 - accuracy: 0.9038 - val_loss: 3.7971 - val_accuracy: 0.5357\n",
            "Epoch 3962/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9048\n",
            "Epoch 3962: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1798 - accuracy: 0.9048 - val_loss: 3.7171 - val_accuracy: 0.5284\n",
            "Epoch 3963/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1853 - accuracy: 0.9008\n",
            "Epoch 3963: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1863 - accuracy: 0.9001 - val_loss: 3.8143 - val_accuracy: 0.5313\n",
            "Epoch 3964/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1856 - accuracy: 0.9003\n",
            "Epoch 3964: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1862 - accuracy: 0.9005 - val_loss: 3.7553 - val_accuracy: 0.5214\n",
            "Epoch 3965/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9052\n",
            "Epoch 3965: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1785 - accuracy: 0.9045 - val_loss: 3.6436 - val_accuracy: 0.5305\n",
            "Epoch 3966/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1813 - accuracy: 0.9053\n",
            "Epoch 3966: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1828 - accuracy: 0.9040 - val_loss: 3.4758 - val_accuracy: 0.5354\n",
            "Epoch 3967/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9007\n",
            "Epoch 3967: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1846 - accuracy: 0.9007 - val_loss: 3.8396 - val_accuracy: 0.5226\n",
            "Epoch 3968/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1779 - accuracy: 0.9052\n",
            "Epoch 3968: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1774 - accuracy: 0.9056 - val_loss: 3.7106 - val_accuracy: 0.5316\n",
            "Epoch 3969/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9026\n",
            "Epoch 3969: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1785 - accuracy: 0.9025 - val_loss: 3.8365 - val_accuracy: 0.5190\n",
            "Epoch 3970/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9021\n",
            "Epoch 3970: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1804 - accuracy: 0.9021 - val_loss: 3.6915 - val_accuracy: 0.5381\n",
            "Epoch 3971/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1858 - accuracy: 0.9023\n",
            "Epoch 3971: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1857 - accuracy: 0.9021 - val_loss: 3.6908 - val_accuracy: 0.5299\n",
            "Epoch 3972/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.9015\n",
            "Epoch 3972: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1837 - accuracy: 0.9015 - val_loss: 3.7232 - val_accuracy: 0.5214\n",
            "Epoch 3973/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1876 - accuracy: 0.9000\n",
            "Epoch 3973: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1883 - accuracy: 0.8996 - val_loss: 3.7721 - val_accuracy: 0.5290\n",
            "Epoch 3974/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1892 - accuracy: 0.9019\n",
            "Epoch 3974: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1895 - accuracy: 0.9016 - val_loss: 3.9093 - val_accuracy: 0.5226\n",
            "Epoch 3975/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1884 - accuracy: 0.8990\n",
            "Epoch 3975: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1884 - accuracy: 0.8989 - val_loss: 3.6548 - val_accuracy: 0.5269\n",
            "Epoch 3976/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9028\n",
            "Epoch 3976: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1819 - accuracy: 0.9028 - val_loss: 3.8417 - val_accuracy: 0.5246\n",
            "Epoch 3977/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1914 - accuracy: 0.9013\n",
            "Epoch 3977: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1913 - accuracy: 0.9016 - val_loss: 3.6941 - val_accuracy: 0.5228\n",
            "Epoch 3978/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1871 - accuracy: 0.9043\n",
            "Epoch 3978: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1871 - accuracy: 0.9035 - val_loss: 3.7830 - val_accuracy: 0.5264\n",
            "Epoch 3979/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1787 - accuracy: 0.9049\n",
            "Epoch 3979: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1789 - accuracy: 0.9045 - val_loss: 3.7810 - val_accuracy: 0.5261\n",
            "Epoch 3980/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1819 - accuracy: 0.9070\n",
            "Epoch 3980: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1819 - accuracy: 0.9072 - val_loss: 3.7721 - val_accuracy: 0.5422\n",
            "Epoch 3981/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.8961\n",
            "Epoch 3981: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1981 - accuracy: 0.8961 - val_loss: 3.6436 - val_accuracy: 0.5343\n",
            "Epoch 3982/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.8912\n",
            "Epoch 3982: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.2099 - accuracy: 0.8912 - val_loss: 3.6034 - val_accuracy: 0.5398\n",
            "Epoch 3983/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1984 - accuracy: 0.8977\n",
            "Epoch 3983: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1988 - accuracy: 0.8969 - val_loss: 3.7047 - val_accuracy: 0.5220\n",
            "Epoch 3984/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.2006 - accuracy: 0.8949\n",
            "Epoch 3984: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.2008 - accuracy: 0.8944 - val_loss: 3.6279 - val_accuracy: 0.5343\n",
            "Epoch 3985/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1835 - accuracy: 0.9010\n",
            "Epoch 3985: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1842 - accuracy: 0.9005 - val_loss: 3.6437 - val_accuracy: 0.5193\n",
            "Epoch 3986/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1875 - accuracy: 0.9029\n",
            "Epoch 3986: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1873 - accuracy: 0.9030 - val_loss: 3.7043 - val_accuracy: 0.5369\n",
            "Epoch 3987/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1857 - accuracy: 0.9013\n",
            "Epoch 3987: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1871 - accuracy: 0.9003 - val_loss: 3.5091 - val_accuracy: 0.5369\n",
            "Epoch 3988/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9025\n",
            "Epoch 3988: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1832 - accuracy: 0.9025 - val_loss: 3.7465 - val_accuracy: 0.5296\n",
            "Epoch 3989/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.9047\n",
            "Epoch 3989: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1775 - accuracy: 0.9032 - val_loss: 3.7252 - val_accuracy: 0.5349\n",
            "Epoch 3990/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.9040\n",
            "Epoch 3990: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1803 - accuracy: 0.9040 - val_loss: 3.7245 - val_accuracy: 0.5308\n",
            "Epoch 3991/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9053\n",
            "Epoch 3991: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1753 - accuracy: 0.9042 - val_loss: 3.6222 - val_accuracy: 0.5357\n",
            "Epoch 3992/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9007\n",
            "Epoch 3992: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1826 - accuracy: 0.9007 - val_loss: 3.7497 - val_accuracy: 0.5214\n",
            "Epoch 3993/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1762 - accuracy: 0.9043\n",
            "Epoch 3993: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1775 - accuracy: 0.9026 - val_loss: 3.8526 - val_accuracy: 0.5255\n",
            "Epoch 3994/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1816 - accuracy: 0.9057\n",
            "Epoch 3994: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1816 - accuracy: 0.9052 - val_loss: 3.6926 - val_accuracy: 0.5316\n",
            "Epoch 3995/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1765 - accuracy: 0.9050\n",
            "Epoch 3995: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1783 - accuracy: 0.9035 - val_loss: 3.8434 - val_accuracy: 0.5217\n",
            "Epoch 3996/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1769 - accuracy: 0.9044\n",
            "Epoch 3996: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1776 - accuracy: 0.9033 - val_loss: 3.6808 - val_accuracy: 0.5354\n",
            "Epoch 3997/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1789 - accuracy: 0.9033\n",
            "Epoch 3997: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1827 - accuracy: 0.9012 - val_loss: 3.6525 - val_accuracy: 0.5220\n",
            "Epoch 3998/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.9048\n",
            "Epoch 3998: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1806 - accuracy: 0.9034 - val_loss: 3.7957 - val_accuracy: 0.5296\n",
            "Epoch 3999/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1786 - accuracy: 0.9055\n",
            "Epoch 3999: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1810 - accuracy: 0.9035 - val_loss: 3.7713 - val_accuracy: 0.5284\n",
            "Epoch 4000/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1889 - accuracy: 0.8978\n",
            "Epoch 4000: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1891 - accuracy: 0.8981 - val_loss: 3.6978 - val_accuracy: 0.5299\n",
            "Epoch 4001/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1929 - accuracy: 0.8986\n",
            "Epoch 4001: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1932 - accuracy: 0.8981 - val_loss: 3.7718 - val_accuracy: 0.5196\n",
            "Epoch 4002/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1945 - accuracy: 0.8974\n",
            "Epoch 4002: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1950 - accuracy: 0.8972 - val_loss: 3.5480 - val_accuracy: 0.5372\n",
            "Epoch 4003/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1907 - accuracy: 0.9004\n",
            "Epoch 4003: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1916 - accuracy: 0.8996 - val_loss: 3.7348 - val_accuracy: 0.5246\n",
            "Epoch 4004/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2020 - accuracy: 0.8980\n",
            "Epoch 4004: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.2045 - accuracy: 0.8963 - val_loss: 3.4899 - val_accuracy: 0.5313\n",
            "Epoch 4005/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1857 - accuracy: 0.9045\n",
            "Epoch 4005: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1868 - accuracy: 0.9037 - val_loss: 3.7751 - val_accuracy: 0.5284\n",
            "Epoch 4006/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9061\n",
            "Epoch 4006: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1746 - accuracy: 0.9061 - val_loss: 3.7103 - val_accuracy: 0.5264\n",
            "Epoch 4007/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1832 - accuracy: 0.9038\n",
            "Epoch 4007: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1850 - accuracy: 0.9027 - val_loss: 3.6428 - val_accuracy: 0.5369\n",
            "Epoch 4008/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1874 - accuracy: 0.9007\n",
            "Epoch 4008: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1873 - accuracy: 0.9007 - val_loss: 3.7687 - val_accuracy: 0.5278\n",
            "Epoch 4009/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9040\n",
            "Epoch 4009: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1786 - accuracy: 0.9040 - val_loss: 3.6280 - val_accuracy: 0.5322\n",
            "Epoch 4010/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1824 - accuracy: 0.9017\n",
            "Epoch 4010: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1831 - accuracy: 0.9013 - val_loss: 3.8597 - val_accuracy: 0.5240\n",
            "Epoch 4011/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1945 - accuracy: 0.9003\n",
            "Epoch 4011: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1962 - accuracy: 0.8993 - val_loss: 3.7585 - val_accuracy: 0.5202\n",
            "Epoch 4012/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.2238 - accuracy: 0.8893\n",
            "Epoch 4012: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.2233 - accuracy: 0.8897 - val_loss: 3.6466 - val_accuracy: 0.5395\n",
            "Epoch 4013/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9046\n",
            "Epoch 4013: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1838 - accuracy: 0.9046 - val_loss: 3.7177 - val_accuracy: 0.5387\n",
            "Epoch 4014/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1766 - accuracy: 0.9059\n",
            "Epoch 4014: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1762 - accuracy: 0.9062 - val_loss: 3.6900 - val_accuracy: 0.5375\n",
            "Epoch 4015/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9078\n",
            "Epoch 4015: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1744 - accuracy: 0.9078 - val_loss: 3.7676 - val_accuracy: 0.5255\n",
            "Epoch 4016/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1750 - accuracy: 0.9076\n",
            "Epoch 4016: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1758 - accuracy: 0.9068 - val_loss: 3.6745 - val_accuracy: 0.5269\n",
            "Epoch 4017/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1811 - accuracy: 0.9028\n",
            "Epoch 4017: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1819 - accuracy: 0.9019 - val_loss: 3.6528 - val_accuracy: 0.5290\n",
            "Epoch 4018/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9050\n",
            "Epoch 4018: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1826 - accuracy: 0.9035 - val_loss: 3.6959 - val_accuracy: 0.5302\n",
            "Epoch 4019/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1782 - accuracy: 0.9046\n",
            "Epoch 4019: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1788 - accuracy: 0.9043 - val_loss: 3.6550 - val_accuracy: 0.5328\n",
            "Epoch 4020/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9054\n",
            "Epoch 4020: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1796 - accuracy: 0.9054 - val_loss: 3.8578 - val_accuracy: 0.5226\n",
            "Epoch 4021/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9048\n",
            "Epoch 4021: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1783 - accuracy: 0.9048 - val_loss: 3.8558 - val_accuracy: 0.5284\n",
            "Epoch 4022/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.9051\n",
            "Epoch 4022: loss did not improve from 0.17394\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1764 - accuracy: 0.9045 - val_loss: 3.7381 - val_accuracy: 0.5272\n",
            "Epoch 4023/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1686 - accuracy: 0.9094\n",
            "Epoch 4023: loss improved from 0.17394 to 0.17323, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.1732 - accuracy: 0.9072 - val_loss: 3.7201 - val_accuracy: 0.5155\n",
            "Epoch 4024/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1851 - accuracy: 0.9015\n",
            "Epoch 4024: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1841 - accuracy: 0.9018 - val_loss: 3.7323 - val_accuracy: 0.5331\n",
            "Epoch 4025/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9050\n",
            "Epoch 4025: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1805 - accuracy: 0.9050 - val_loss: 3.7125 - val_accuracy: 0.5334\n",
            "Epoch 4026/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9038\n",
            "Epoch 4026: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1816 - accuracy: 0.9022 - val_loss: 3.7093 - val_accuracy: 0.5284\n",
            "Epoch 4027/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1840 - accuracy: 0.9028\n",
            "Epoch 4027: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1858 - accuracy: 0.9007 - val_loss: 3.7770 - val_accuracy: 0.5310\n",
            "Epoch 4028/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1807 - accuracy: 0.9049\n",
            "Epoch 4028: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1849 - accuracy: 0.9028 - val_loss: 3.7273 - val_accuracy: 0.5228\n",
            "Epoch 4029/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1775 - accuracy: 0.9088\n",
            "Epoch 4029: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1807 - accuracy: 0.9065 - val_loss: 3.6405 - val_accuracy: 0.5255\n",
            "Epoch 4030/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1859 - accuracy: 0.9030\n",
            "Epoch 4030: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1865 - accuracy: 0.9017 - val_loss: 3.6869 - val_accuracy: 0.5290\n",
            "Epoch 4031/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1783 - accuracy: 0.9047\n",
            "Epoch 4031: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1796 - accuracy: 0.9037 - val_loss: 3.8195 - val_accuracy: 0.5228\n",
            "Epoch 4032/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1738 - accuracy: 0.9086\n",
            "Epoch 4032: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1746 - accuracy: 0.9080 - val_loss: 3.6860 - val_accuracy: 0.5308\n",
            "Epoch 4033/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1760 - accuracy: 0.9066\n",
            "Epoch 4033: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1798 - accuracy: 0.9037 - val_loss: 3.7836 - val_accuracy: 0.5325\n",
            "Epoch 4034/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1776 - accuracy: 0.9054\n",
            "Epoch 4034: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1829 - accuracy: 0.9027 - val_loss: 3.8607 - val_accuracy: 0.5217\n",
            "Epoch 4035/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1890 - accuracy: 0.9019\n",
            "Epoch 4035: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1884 - accuracy: 0.9023 - val_loss: 3.7210 - val_accuracy: 0.5231\n",
            "Epoch 4036/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1803 - accuracy: 0.9037\n",
            "Epoch 4036: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1824 - accuracy: 0.9019 - val_loss: 3.6849 - val_accuracy: 0.5255\n",
            "Epoch 4037/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1813 - accuracy: 0.9040\n",
            "Epoch 4037: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9027 - val_loss: 3.7396 - val_accuracy: 0.5299\n",
            "Epoch 4038/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9070\n",
            "Epoch 4038: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1793 - accuracy: 0.9070 - val_loss: 3.7966 - val_accuracy: 0.5267\n",
            "Epoch 4039/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1782 - accuracy: 0.9034\n",
            "Epoch 4039: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1785 - accuracy: 0.9035 - val_loss: 3.6471 - val_accuracy: 0.5249\n",
            "Epoch 4040/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1816 - accuracy: 0.9039\n",
            "Epoch 4040: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1818 - accuracy: 0.9035 - val_loss: 3.8851 - val_accuracy: 0.5252\n",
            "Epoch 4041/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.9032\n",
            "Epoch 4041: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1886 - accuracy: 0.9032 - val_loss: 3.6706 - val_accuracy: 0.5287\n",
            "Epoch 4042/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1819 - accuracy: 0.9015\n",
            "Epoch 4042: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1814 - accuracy: 0.9016 - val_loss: 3.6960 - val_accuracy: 0.5237\n",
            "Epoch 4043/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9056\n",
            "Epoch 4043: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1791 - accuracy: 0.9057 - val_loss: 3.7907 - val_accuracy: 0.5228\n",
            "Epoch 4044/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9039\n",
            "Epoch 4044: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1811 - accuracy: 0.9038 - val_loss: 3.7592 - val_accuracy: 0.5310\n",
            "Epoch 4045/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1766 - accuracy: 0.9069\n",
            "Epoch 4045: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1795 - accuracy: 0.9054 - val_loss: 3.7864 - val_accuracy: 0.5190\n",
            "Epoch 4046/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1844 - accuracy: 0.9037\n",
            "Epoch 4046: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1855 - accuracy: 0.9028 - val_loss: 3.7839 - val_accuracy: 0.5220\n",
            "Epoch 4047/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1827 - accuracy: 0.9041\n",
            "Epoch 4047: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1823 - accuracy: 0.9043 - val_loss: 3.7215 - val_accuracy: 0.5278\n",
            "Epoch 4048/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1765 - accuracy: 0.9062\n",
            "Epoch 4048: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1776 - accuracy: 0.9054 - val_loss: 3.7273 - val_accuracy: 0.5337\n",
            "Epoch 4049/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1886 - accuracy: 0.9018\n",
            "Epoch 4049: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1877 - accuracy: 0.9016 - val_loss: 3.7068 - val_accuracy: 0.5228\n",
            "Epoch 4050/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1775 - accuracy: 0.9042\n",
            "Epoch 4050: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1775 - accuracy: 0.9041 - val_loss: 3.7598 - val_accuracy: 0.5299\n",
            "Epoch 4051/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.8998\n",
            "Epoch 4051: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1880 - accuracy: 0.8998 - val_loss: 3.6195 - val_accuracy: 0.5223\n",
            "Epoch 4052/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1896 - accuracy: 0.8987\n",
            "Epoch 4052: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1897 - accuracy: 0.8989 - val_loss: 3.7507 - val_accuracy: 0.5296\n",
            "Epoch 4053/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1887 - accuracy: 0.9003\n",
            "Epoch 4053: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1890 - accuracy: 0.9003 - val_loss: 3.5291 - val_accuracy: 0.5331\n",
            "Epoch 4054/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1903 - accuracy: 0.8993\n",
            "Epoch 4054: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1907 - accuracy: 0.8992 - val_loss: 3.8698 - val_accuracy: 0.5284\n",
            "Epoch 4055/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1857 - accuracy: 0.9010\n",
            "Epoch 4055: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1854 - accuracy: 0.9011 - val_loss: 3.6051 - val_accuracy: 0.5264\n",
            "Epoch 4056/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9042\n",
            "Epoch 4056: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1828 - accuracy: 0.9042 - val_loss: 3.6741 - val_accuracy: 0.5357\n",
            "Epoch 4057/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1798 - accuracy: 0.9047\n",
            "Epoch 4057: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1790 - accuracy: 0.9053 - val_loss: 3.6469 - val_accuracy: 0.5234\n",
            "Epoch 4058/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.9077\n",
            "Epoch 4058: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1760 - accuracy: 0.9065 - val_loss: 3.6619 - val_accuracy: 0.5313\n",
            "Epoch 4059/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1770 - accuracy: 0.9067\n",
            "Epoch 4059: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1788 - accuracy: 0.9051 - val_loss: 3.7398 - val_accuracy: 0.5252\n",
            "Epoch 4060/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1820 - accuracy: 0.9025\n",
            "Epoch 4060: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1838 - accuracy: 0.9012 - val_loss: 3.8388 - val_accuracy: 0.5308\n",
            "Epoch 4061/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1808 - accuracy: 0.9051\n",
            "Epoch 4061: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1825 - accuracy: 0.9037 - val_loss: 3.8250 - val_accuracy: 0.5223\n",
            "Epoch 4062/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1803 - accuracy: 0.9044\n",
            "Epoch 4062: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1829 - accuracy: 0.9021 - val_loss: 3.8722 - val_accuracy: 0.5372\n",
            "Epoch 4063/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1879 - accuracy: 0.8994\n",
            "Epoch 4063: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1913 - accuracy: 0.8974 - val_loss: 3.7635 - val_accuracy: 0.5302\n",
            "Epoch 4064/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9018\n",
            "Epoch 4064: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1782 - accuracy: 0.9018 - val_loss: 3.8792 - val_accuracy: 0.5325\n",
            "Epoch 4065/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1884 - accuracy: 0.9032\n",
            "Epoch 4065: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1897 - accuracy: 0.9020 - val_loss: 3.6675 - val_accuracy: 0.5272\n",
            "Epoch 4066/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1884 - accuracy: 0.8997\n",
            "Epoch 4066: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1884 - accuracy: 0.8997 - val_loss: 3.5693 - val_accuracy: 0.5334\n",
            "Epoch 4067/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.8970\n",
            "Epoch 4067: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.2030 - accuracy: 0.8970 - val_loss: 3.6741 - val_accuracy: 0.5422\n",
            "Epoch 4068/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1982 - accuracy: 0.8966\n",
            "Epoch 4068: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1981 - accuracy: 0.8967 - val_loss: 3.6518 - val_accuracy: 0.5334\n",
            "Epoch 4069/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1866 - accuracy: 0.9016\n",
            "Epoch 4069: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1866 - accuracy: 0.9015 - val_loss: 3.6136 - val_accuracy: 0.5310\n",
            "Epoch 4070/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1824 - accuracy: 0.9036\n",
            "Epoch 4070: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1831 - accuracy: 0.9029 - val_loss: 3.6714 - val_accuracy: 0.5275\n",
            "Epoch 4071/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9052\n",
            "Epoch 4071: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1808 - accuracy: 0.9038 - val_loss: 3.7511 - val_accuracy: 0.5264\n",
            "Epoch 4072/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9063\n",
            "Epoch 4072: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1768 - accuracy: 0.9063 - val_loss: 3.6760 - val_accuracy: 0.5269\n",
            "Epoch 4073/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1871 - accuracy: 0.9042\n",
            "Epoch 4073: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1896 - accuracy: 0.9021 - val_loss: 3.7329 - val_accuracy: 0.5290\n",
            "Epoch 4074/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1827 - accuracy: 0.9040\n",
            "Epoch 4074: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1841 - accuracy: 0.9028 - val_loss: 3.7232 - val_accuracy: 0.5267\n",
            "Epoch 4075/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1791 - accuracy: 0.9049\n",
            "Epoch 4075: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1791 - accuracy: 0.9048 - val_loss: 3.8281 - val_accuracy: 0.5258\n",
            "Epoch 4076/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1827 - accuracy: 0.9014\n",
            "Epoch 4076: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1840 - accuracy: 0.9006 - val_loss: 3.8058 - val_accuracy: 0.5202\n",
            "Epoch 4077/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1878 - accuracy: 0.9031\n",
            "Epoch 4077: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1875 - accuracy: 0.9032 - val_loss: 3.6533 - val_accuracy: 0.5299\n",
            "Epoch 4078/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1786 - accuracy: 0.9054\n",
            "Epoch 4078: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1784 - accuracy: 0.9058 - val_loss: 3.8729 - val_accuracy: 0.5325\n",
            "Epoch 4079/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9057\n",
            "Epoch 4079: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1804 - accuracy: 0.9042 - val_loss: 3.7741 - val_accuracy: 0.5264\n",
            "Epoch 4080/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9063\n",
            "Epoch 4080: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1796 - accuracy: 0.9063 - val_loss: 3.6944 - val_accuracy: 0.5354\n",
            "Epoch 4081/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1815 - accuracy: 0.9020\n",
            "Epoch 4081: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1812 - accuracy: 0.9022 - val_loss: 3.6621 - val_accuracy: 0.5249\n",
            "Epoch 4082/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1817 - accuracy: 0.9021\n",
            "Epoch 4082: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1816 - accuracy: 0.9021 - val_loss: 3.7508 - val_accuracy: 0.5404\n",
            "Epoch 4083/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1889 - accuracy: 0.9004\n",
            "Epoch 4083: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1879 - accuracy: 0.9005 - val_loss: 3.7207 - val_accuracy: 0.5305\n",
            "Epoch 4084/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1829 - accuracy: 0.9045\n",
            "Epoch 4084: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1833 - accuracy: 0.9041 - val_loss: 3.7080 - val_accuracy: 0.5269\n",
            "Epoch 4085/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9009\n",
            "Epoch 4085: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1865 - accuracy: 0.9009 - val_loss: 3.7261 - val_accuracy: 0.5269\n",
            "Epoch 4086/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.9050\n",
            "Epoch 4086: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1746 - accuracy: 0.9046 - val_loss: 3.7610 - val_accuracy: 0.5272\n",
            "Epoch 4087/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9044\n",
            "Epoch 4087: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1800 - accuracy: 0.9040 - val_loss: 3.7652 - val_accuracy: 0.5287\n",
            "Epoch 4088/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9030\n",
            "Epoch 4088: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1849 - accuracy: 0.9030 - val_loss: 3.7247 - val_accuracy: 0.5284\n",
            "Epoch 4089/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1916 - accuracy: 0.8996\n",
            "Epoch 4089: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1926 - accuracy: 0.8991 - val_loss: 3.7046 - val_accuracy: 0.5395\n",
            "Epoch 4090/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1899 - accuracy: 0.9007\n",
            "Epoch 4090: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1898 - accuracy: 0.9007 - val_loss: 3.7024 - val_accuracy: 0.5319\n",
            "Epoch 4091/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1898 - accuracy: 0.8997\n",
            "Epoch 4091: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1908 - accuracy: 0.8991 - val_loss: 3.6040 - val_accuracy: 0.5340\n",
            "Epoch 4092/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1891 - accuracy: 0.9038\n",
            "Epoch 4092: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1908 - accuracy: 0.9033 - val_loss: 3.7856 - val_accuracy: 0.5255\n",
            "Epoch 4093/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1908 - accuracy: 0.9012\n",
            "Epoch 4093: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1934 - accuracy: 0.8994 - val_loss: 3.7695 - val_accuracy: 0.5167\n",
            "Epoch 4094/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1922 - accuracy: 0.8992\n",
            "Epoch 4094: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1951 - accuracy: 0.8972 - val_loss: 3.7739 - val_accuracy: 0.5228\n",
            "Epoch 4095/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9005\n",
            "Epoch 4095: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1894 - accuracy: 0.9005 - val_loss: 3.5892 - val_accuracy: 0.5278\n",
            "Epoch 4096/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.9038\n",
            "Epoch 4096: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1810 - accuracy: 0.9027 - val_loss: 3.7442 - val_accuracy: 0.5272\n",
            "Epoch 4097/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1726 - accuracy: 0.9102\n",
            "Epoch 4097: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1759 - accuracy: 0.9074 - val_loss: 3.7470 - val_accuracy: 0.5290\n",
            "Epoch 4098/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1818 - accuracy: 0.9023\n",
            "Epoch 4098: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1824 - accuracy: 0.9016 - val_loss: 3.7520 - val_accuracy: 0.5302\n",
            "Epoch 4099/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1735 - accuracy: 0.9078\n",
            "Epoch 4099: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1749 - accuracy: 0.9062 - val_loss: 3.7109 - val_accuracy: 0.5223\n",
            "Epoch 4100/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1766 - accuracy: 0.9050\n",
            "Epoch 4100: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1785 - accuracy: 0.9036 - val_loss: 3.7780 - val_accuracy: 0.5299\n",
            "Epoch 4101/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1857 - accuracy: 0.9015\n",
            "Epoch 4101: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1858 - accuracy: 0.9008 - val_loss: 3.7562 - val_accuracy: 0.5234\n",
            "Epoch 4102/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9047\n",
            "Epoch 4102: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1833 - accuracy: 0.9029 - val_loss: 3.8031 - val_accuracy: 0.5293\n",
            "Epoch 4103/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1787 - accuracy: 0.9069\n",
            "Epoch 4103: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1780 - accuracy: 0.9068 - val_loss: 3.8245 - val_accuracy: 0.5334\n",
            "Epoch 4104/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1739 - accuracy: 0.9068\n",
            "Epoch 4104: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1762 - accuracy: 0.9048 - val_loss: 3.7826 - val_accuracy: 0.5428\n",
            "Epoch 4105/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1867 - accuracy: 0.9048\n",
            "Epoch 4105: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1886 - accuracy: 0.9034 - val_loss: 3.7329 - val_accuracy: 0.5228\n",
            "Epoch 4106/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9032\n",
            "Epoch 4106: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1859 - accuracy: 0.9032 - val_loss: 3.7400 - val_accuracy: 0.5284\n",
            "Epoch 4107/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1780 - accuracy: 0.9035\n",
            "Epoch 4107: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1783 - accuracy: 0.9028 - val_loss: 3.8373 - val_accuracy: 0.5334\n",
            "Epoch 4108/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1881 - accuracy: 0.9022\n",
            "Epoch 4108: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1888 - accuracy: 0.9019 - val_loss: 3.6672 - val_accuracy: 0.5267\n",
            "Epoch 4109/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9014\n",
            "Epoch 4109: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1838 - accuracy: 0.9014 - val_loss: 3.5989 - val_accuracy: 0.5428\n",
            "Epoch 4110/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.8969\n",
            "Epoch 4110: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1974 - accuracy: 0.8969 - val_loss: 3.7535 - val_accuracy: 0.5237\n",
            "Epoch 4111/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1919 - accuracy: 0.8994\n",
            "Epoch 4111: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1916 - accuracy: 0.8996 - val_loss: 3.6699 - val_accuracy: 0.5152\n",
            "Epoch 4112/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.8992\n",
            "Epoch 4112: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1917 - accuracy: 0.8992 - val_loss: 3.6587 - val_accuracy: 0.5264\n",
            "Epoch 4113/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1808 - accuracy: 0.9038\n",
            "Epoch 4113: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1800 - accuracy: 0.9040 - val_loss: 3.7966 - val_accuracy: 0.5360\n",
            "Epoch 4114/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1710 - accuracy: 0.9095\n",
            "Epoch 4114: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1736 - accuracy: 0.9078 - val_loss: 3.7517 - val_accuracy: 0.5346\n",
            "Epoch 4115/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1763 - accuracy: 0.9087\n",
            "Epoch 4115: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1776 - accuracy: 0.9071 - val_loss: 3.8690 - val_accuracy: 0.5296\n",
            "Epoch 4116/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1756 - accuracy: 0.9060\n",
            "Epoch 4116: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1767 - accuracy: 0.9050 - val_loss: 3.7519 - val_accuracy: 0.5372\n",
            "Epoch 4117/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1750 - accuracy: 0.9058\n",
            "Epoch 4117: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1783 - accuracy: 0.9041 - val_loss: 3.7667 - val_accuracy: 0.5363\n",
            "Epoch 4118/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1773 - accuracy: 0.9044\n",
            "Epoch 4118: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1775 - accuracy: 0.9040 - val_loss: 3.8061 - val_accuracy: 0.5208\n",
            "Epoch 4119/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1804 - accuracy: 0.9038\n",
            "Epoch 4119: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1799 - accuracy: 0.9038 - val_loss: 3.7282 - val_accuracy: 0.5290\n",
            "Epoch 4120/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1765 - accuracy: 0.9068\n",
            "Epoch 4120: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1777 - accuracy: 0.9061 - val_loss: 3.8896 - val_accuracy: 0.5310\n",
            "Epoch 4121/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9053\n",
            "Epoch 4121: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1806 - accuracy: 0.9023 - val_loss: 3.6652 - val_accuracy: 0.5445\n",
            "Epoch 4122/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1791 - accuracy: 0.9028\n",
            "Epoch 4122: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1784 - accuracy: 0.9027 - val_loss: 3.8708 - val_accuracy: 0.5313\n",
            "Epoch 4123/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1718 - accuracy: 0.9081\n",
            "Epoch 4123: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1745 - accuracy: 0.9058 - val_loss: 3.7248 - val_accuracy: 0.5246\n",
            "Epoch 4124/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1770 - accuracy: 0.9055\n",
            "Epoch 4124: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1776 - accuracy: 0.9051 - val_loss: 3.8211 - val_accuracy: 0.5357\n",
            "Epoch 4125/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1838 - accuracy: 0.9037\n",
            "Epoch 4125: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1850 - accuracy: 0.9029 - val_loss: 3.7077 - val_accuracy: 0.5343\n",
            "Epoch 4126/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9065\n",
            "Epoch 4126: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1735 - accuracy: 0.9065 - val_loss: 3.8053 - val_accuracy: 0.5360\n",
            "Epoch 4127/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1791 - accuracy: 0.9028\n",
            "Epoch 4127: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1789 - accuracy: 0.9029 - val_loss: 3.7143 - val_accuracy: 0.5284\n",
            "Epoch 4128/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9067\n",
            "Epoch 4128: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1755 - accuracy: 0.9049 - val_loss: 3.9025 - val_accuracy: 0.5264\n",
            "Epoch 4129/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9028\n",
            "Epoch 4129: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1796 - accuracy: 0.9028 - val_loss: 3.8163 - val_accuracy: 0.5328\n",
            "Epoch 4130/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1827 - accuracy: 0.9038\n",
            "Epoch 4130: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1850 - accuracy: 0.9027 - val_loss: 3.6526 - val_accuracy: 0.5375\n",
            "Epoch 4131/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1842 - accuracy: 0.9020\n",
            "Epoch 4131: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1839 - accuracy: 0.9021 - val_loss: 3.8421 - val_accuracy: 0.5308\n",
            "Epoch 4132/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9023\n",
            "Epoch 4132: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1844 - accuracy: 0.9023 - val_loss: 3.6509 - val_accuracy: 0.5334\n",
            "Epoch 4133/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1932 - accuracy: 0.8995\n",
            "Epoch 4133: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1931 - accuracy: 0.8996 - val_loss: 3.6890 - val_accuracy: 0.5354\n",
            "Epoch 4134/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1838 - accuracy: 0.9006\n",
            "Epoch 4134: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1858 - accuracy: 0.8991 - val_loss: 3.6695 - val_accuracy: 0.5316\n",
            "Epoch 4135/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1906 - accuracy: 0.8989\n",
            "Epoch 4135: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1903 - accuracy: 0.8990 - val_loss: 3.7885 - val_accuracy: 0.5340\n",
            "Epoch 4136/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9055\n",
            "Epoch 4136: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1833 - accuracy: 0.9032 - val_loss: 3.7559 - val_accuracy: 0.5261\n",
            "Epoch 4137/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9072\n",
            "Epoch 4137: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1742 - accuracy: 0.9072 - val_loss: 3.8908 - val_accuracy: 0.5284\n",
            "Epoch 4138/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1803 - accuracy: 0.9036\n",
            "Epoch 4138: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1826 - accuracy: 0.9018 - val_loss: 3.6750 - val_accuracy: 0.5258\n",
            "Epoch 4139/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1791 - accuracy: 0.9039\n",
            "Epoch 4139: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1808 - accuracy: 0.9030 - val_loss: 3.7382 - val_accuracy: 0.5228\n",
            "Epoch 4140/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9038\n",
            "Epoch 4140: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1837 - accuracy: 0.9025 - val_loss: 3.7921 - val_accuracy: 0.5310\n",
            "Epoch 4141/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1831 - accuracy: 0.9036\n",
            "Epoch 4141: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1834 - accuracy: 0.9034 - val_loss: 3.7543 - val_accuracy: 0.5305\n",
            "Epoch 4142/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1825 - accuracy: 0.9049\n",
            "Epoch 4142: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1844 - accuracy: 0.9033 - val_loss: 3.7689 - val_accuracy: 0.5252\n",
            "Epoch 4143/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1868 - accuracy: 0.9047\n",
            "Epoch 4143: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1875 - accuracy: 0.9043 - val_loss: 3.8804 - val_accuracy: 0.5264\n",
            "Epoch 4144/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9058\n",
            "Epoch 4144: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1832 - accuracy: 0.9047 - val_loss: 3.5673 - val_accuracy: 0.5313\n",
            "Epoch 4145/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9079\n",
            "Epoch 4145: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1760 - accuracy: 0.9079 - val_loss: 3.7377 - val_accuracy: 0.5308\n",
            "Epoch 4146/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9034\n",
            "Epoch 4146: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1784 - accuracy: 0.9027 - val_loss: 3.8482 - val_accuracy: 0.5337\n",
            "Epoch 4147/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9068\n",
            "Epoch 4147: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1739 - accuracy: 0.9068 - val_loss: 3.7086 - val_accuracy: 0.5390\n",
            "Epoch 4148/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1761 - accuracy: 0.9051\n",
            "Epoch 4148: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1775 - accuracy: 0.9042 - val_loss: 3.6766 - val_accuracy: 0.5252\n",
            "Epoch 4149/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1786 - accuracy: 0.9035\n",
            "Epoch 4149: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1792 - accuracy: 0.9030 - val_loss: 3.7718 - val_accuracy: 0.5316\n",
            "Epoch 4150/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1851 - accuracy: 0.9029\n",
            "Epoch 4150: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1873 - accuracy: 0.9017 - val_loss: 3.8203 - val_accuracy: 0.5334\n",
            "Epoch 4151/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1969 - accuracy: 0.8947\n",
            "Epoch 4151: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1975 - accuracy: 0.8933 - val_loss: 3.8279 - val_accuracy: 0.5334\n",
            "Epoch 4152/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1820 - accuracy: 0.9051\n",
            "Epoch 4152: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1849 - accuracy: 0.9029 - val_loss: 3.7114 - val_accuracy: 0.5284\n",
            "Epoch 4153/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1811 - accuracy: 0.9062\n",
            "Epoch 4153: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1825 - accuracy: 0.9053 - val_loss: 3.8867 - val_accuracy: 0.5354\n",
            "Epoch 4154/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1853 - accuracy: 0.9043\n",
            "Epoch 4154: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1868 - accuracy: 0.9031 - val_loss: 3.6167 - val_accuracy: 0.5249\n",
            "Epoch 4155/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1815 - accuracy: 0.9039\n",
            "Epoch 4155: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1864 - accuracy: 0.9012 - val_loss: 3.7528 - val_accuracy: 0.5381\n",
            "Epoch 4156/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1999 - accuracy: 0.8954\n",
            "Epoch 4156: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1995 - accuracy: 0.8952 - val_loss: 3.6463 - val_accuracy: 0.5313\n",
            "Epoch 4157/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1908 - accuracy: 0.8966\n",
            "Epoch 4157: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1914 - accuracy: 0.8966 - val_loss: 3.7430 - val_accuracy: 0.5302\n",
            "Epoch 4158/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1802 - accuracy: 0.9078\n",
            "Epoch 4158: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1821 - accuracy: 0.9058 - val_loss: 3.6387 - val_accuracy: 0.5217\n",
            "Epoch 4159/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.9061\n",
            "Epoch 4159: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1757 - accuracy: 0.9058 - val_loss: 3.7853 - val_accuracy: 0.5308\n",
            "Epoch 4160/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1728 - accuracy: 0.9082\n",
            "Epoch 4160: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1755 - accuracy: 0.9069 - val_loss: 3.9091 - val_accuracy: 0.5258\n",
            "Epoch 4161/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1786 - accuracy: 0.9023\n",
            "Epoch 4161: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1804 - accuracy: 0.9006 - val_loss: 3.7343 - val_accuracy: 0.5234\n",
            "Epoch 4162/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1850 - accuracy: 0.9025\n",
            "Epoch 4162: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1866 - accuracy: 0.9014 - val_loss: 3.6363 - val_accuracy: 0.5290\n",
            "Epoch 4163/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2020 - accuracy: 0.8944\n",
            "Epoch 4163: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.2024 - accuracy: 0.8941 - val_loss: 3.7789 - val_accuracy: 0.5284\n",
            "Epoch 4164/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1965 - accuracy: 0.8982\n",
            "Epoch 4164: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1999 - accuracy: 0.8966 - val_loss: 3.7056 - val_accuracy: 0.5384\n",
            "Epoch 4165/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.2107 - accuracy: 0.8920\n",
            "Epoch 4165: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.2083 - accuracy: 0.8933 - val_loss: 3.7790 - val_accuracy: 0.5381\n",
            "Epoch 4166/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9040\n",
            "Epoch 4166: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1855 - accuracy: 0.9040 - val_loss: 3.7142 - val_accuracy: 0.5267\n",
            "Epoch 4167/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9018\n",
            "Epoch 4167: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1780 - accuracy: 0.9016 - val_loss: 3.8148 - val_accuracy: 0.5255\n",
            "Epoch 4168/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1736 - accuracy: 0.9099\n",
            "Epoch 4168: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1764 - accuracy: 0.9072 - val_loss: 3.8594 - val_accuracy: 0.5223\n",
            "Epoch 4169/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9029\n",
            "Epoch 4169: loss did not improve from 0.17323\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1789 - accuracy: 0.9029 - val_loss: 3.8009 - val_accuracy: 0.5343\n",
            "Epoch 4170/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1726 - accuracy: 0.9066\n",
            "Epoch 4170: loss improved from 0.17323 to 0.17258, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.1726 - accuracy: 0.9065 - val_loss: 3.7334 - val_accuracy: 0.5267\n",
            "Epoch 4171/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1669 - accuracy: 0.9100\n",
            "Epoch 4171: loss improved from 0.17258 to 0.16889, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1689 - accuracy: 0.9073 - val_loss: 3.8375 - val_accuracy: 0.5278\n",
            "Epoch 4172/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1761 - accuracy: 0.9053\n",
            "Epoch 4172: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1759 - accuracy: 0.9055 - val_loss: 3.7526 - val_accuracy: 0.5349\n",
            "Epoch 4173/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1854 - accuracy: 0.9007\n",
            "Epoch 4173: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1847 - accuracy: 0.9011 - val_loss: 3.7302 - val_accuracy: 0.5334\n",
            "Epoch 4174/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1747 - accuracy: 0.9059\n",
            "Epoch 4174: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1759 - accuracy: 0.9046 - val_loss: 3.7525 - val_accuracy: 0.5308\n",
            "Epoch 4175/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1764 - accuracy: 0.9046\n",
            "Epoch 4175: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1779 - accuracy: 0.9035 - val_loss: 3.9202 - val_accuracy: 0.5208\n",
            "Epoch 4176/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1913 - accuracy: 0.9018\n",
            "Epoch 4176: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1899 - accuracy: 0.9021 - val_loss: 3.9158 - val_accuracy: 0.5290\n",
            "Epoch 4177/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1780 - accuracy: 0.9035\n",
            "Epoch 4177: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1777 - accuracy: 0.9041 - val_loss: 3.7596 - val_accuracy: 0.5328\n",
            "Epoch 4178/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1836 - accuracy: 0.9007\n",
            "Epoch 4178: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1840 - accuracy: 0.9005 - val_loss: 3.7685 - val_accuracy: 0.5337\n",
            "Epoch 4179/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1811 - accuracy: 0.9041\n",
            "Epoch 4179: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1825 - accuracy: 0.9035 - val_loss: 3.7487 - val_accuracy: 0.5284\n",
            "Epoch 4180/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1777 - accuracy: 0.9067\n",
            "Epoch 4180: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1772 - accuracy: 0.9067 - val_loss: 3.8040 - val_accuracy: 0.5208\n",
            "Epoch 4181/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1820 - accuracy: 0.9045\n",
            "Epoch 4181: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1841 - accuracy: 0.9029 - val_loss: 3.7315 - val_accuracy: 0.5287\n",
            "Epoch 4182/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1856 - accuracy: 0.9038\n",
            "Epoch 4182: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1857 - accuracy: 0.9032 - val_loss: 3.8051 - val_accuracy: 0.5302\n",
            "Epoch 4183/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1836 - accuracy: 0.9026\n",
            "Epoch 4183: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1856 - accuracy: 0.9010 - val_loss: 3.7624 - val_accuracy: 0.5299\n",
            "Epoch 4184/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9073\n",
            "Epoch 4184: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1754 - accuracy: 0.9068 - val_loss: 3.8839 - val_accuracy: 0.5322\n",
            "Epoch 4185/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1755 - accuracy: 0.9067\n",
            "Epoch 4185: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1769 - accuracy: 0.9059 - val_loss: 3.8861 - val_accuracy: 0.5363\n",
            "Epoch 4186/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1820 - accuracy: 0.9026\n",
            "Epoch 4186: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1822 - accuracy: 0.9023 - val_loss: 3.6897 - val_accuracy: 0.5220\n",
            "Epoch 4187/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1809 - accuracy: 0.9052\n",
            "Epoch 4187: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1812 - accuracy: 0.9048 - val_loss: 3.9060 - val_accuracy: 0.5299\n",
            "Epoch 4188/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9061\n",
            "Epoch 4188: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1794 - accuracy: 0.9061 - val_loss: 3.7362 - val_accuracy: 0.5293\n",
            "Epoch 4189/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9042\n",
            "Epoch 4189: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1798 - accuracy: 0.9039 - val_loss: 3.7690 - val_accuracy: 0.5237\n",
            "Epoch 4190/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9045\n",
            "Epoch 4190: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1797 - accuracy: 0.9045 - val_loss: 3.6324 - val_accuracy: 0.5366\n",
            "Epoch 4191/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1726 - accuracy: 0.9104\n",
            "Epoch 4191: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1728 - accuracy: 0.9098 - val_loss: 3.8539 - val_accuracy: 0.5284\n",
            "Epoch 4192/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1735 - accuracy: 0.9048\n",
            "Epoch 4192: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1740 - accuracy: 0.9043 - val_loss: 3.6933 - val_accuracy: 0.5305\n",
            "Epoch 4193/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9037\n",
            "Epoch 4193: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1801 - accuracy: 0.9031 - val_loss: 3.9332 - val_accuracy: 0.5252\n",
            "Epoch 4194/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.9054\n",
            "Epoch 4194: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1779 - accuracy: 0.9054 - val_loss: 3.8582 - val_accuracy: 0.5173\n",
            "Epoch 4195/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1873 - accuracy: 0.9025\n",
            "Epoch 4195: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1893 - accuracy: 0.9013 - val_loss: 3.6838 - val_accuracy: 0.5328\n",
            "Epoch 4196/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1824 - accuracy: 0.9021\n",
            "Epoch 4196: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1823 - accuracy: 0.9021 - val_loss: 3.7921 - val_accuracy: 0.5369\n",
            "Epoch 4197/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1775 - accuracy: 0.9038\n",
            "Epoch 4197: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1800 - accuracy: 0.9013 - val_loss: 3.8738 - val_accuracy: 0.5228\n",
            "Epoch 4198/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1759 - accuracy: 0.9058\n",
            "Epoch 4198: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1766 - accuracy: 0.9059 - val_loss: 3.8794 - val_accuracy: 0.5205\n",
            "Epoch 4199/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1758 - accuracy: 0.9059\n",
            "Epoch 4199: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1758 - accuracy: 0.9056 - val_loss: 3.8200 - val_accuracy: 0.5269\n",
            "Epoch 4200/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9055\n",
            "Epoch 4200: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1793 - accuracy: 0.9055 - val_loss: 3.7352 - val_accuracy: 0.5281\n",
            "Epoch 4201/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1805 - accuracy: 0.9037\n",
            "Epoch 4201: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1806 - accuracy: 0.9038 - val_loss: 3.6798 - val_accuracy: 0.5308\n",
            "Epoch 4202/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1844 - accuracy: 0.9015\n",
            "Epoch 4202: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1839 - accuracy: 0.9015 - val_loss: 3.7674 - val_accuracy: 0.5349\n",
            "Epoch 4203/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9026\n",
            "Epoch 4203: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1785 - accuracy: 0.9026 - val_loss: 3.6579 - val_accuracy: 0.5308\n",
            "Epoch 4204/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9019\n",
            "Epoch 4204: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1857 - accuracy: 0.9019 - val_loss: 3.7837 - val_accuracy: 0.5281\n",
            "Epoch 4205/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.9055\n",
            "Epoch 4205: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1807 - accuracy: 0.9048 - val_loss: 3.6493 - val_accuracy: 0.5337\n",
            "Epoch 4206/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1825 - accuracy: 0.9036\n",
            "Epoch 4206: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1829 - accuracy: 0.9029 - val_loss: 3.6072 - val_accuracy: 0.5272\n",
            "Epoch 4207/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1890 - accuracy: 0.9028\n",
            "Epoch 4207: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1884 - accuracy: 0.9032 - val_loss: 3.6448 - val_accuracy: 0.5354\n",
            "Epoch 4208/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1784 - accuracy: 0.9035\n",
            "Epoch 4208: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1784 - accuracy: 0.9035 - val_loss: 3.7438 - val_accuracy: 0.5322\n",
            "Epoch 4209/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1733 - accuracy: 0.9097\n",
            "Epoch 4209: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1747 - accuracy: 0.9079 - val_loss: 3.7796 - val_accuracy: 0.5305\n",
            "Epoch 4210/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9063\n",
            "Epoch 4210: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1766 - accuracy: 0.9063 - val_loss: 3.8037 - val_accuracy: 0.5375\n",
            "Epoch 4211/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1862 - accuracy: 0.9014\n",
            "Epoch 4211: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1870 - accuracy: 0.9008 - val_loss: 3.7487 - val_accuracy: 0.5346\n",
            "Epoch 4212/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1785 - accuracy: 0.9047\n",
            "Epoch 4212: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1797 - accuracy: 0.9039 - val_loss: 3.6947 - val_accuracy: 0.5228\n",
            "Epoch 4213/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1857 - accuracy: 0.9026\n",
            "Epoch 4213: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1864 - accuracy: 0.9021 - val_loss: 3.7621 - val_accuracy: 0.5258\n",
            "Epoch 4214/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1817 - accuracy: 0.9042\n",
            "Epoch 4214: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1826 - accuracy: 0.9035 - val_loss: 3.7548 - val_accuracy: 0.5284\n",
            "Epoch 4215/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1809 - accuracy: 0.9040\n",
            "Epoch 4215: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1823 - accuracy: 0.9032 - val_loss: 3.7147 - val_accuracy: 0.5269\n",
            "Epoch 4216/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1847 - accuracy: 0.9052\n",
            "Epoch 4216: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1876 - accuracy: 0.9027 - val_loss: 3.7555 - val_accuracy: 0.5243\n",
            "Epoch 4217/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1926 - accuracy: 0.8992\n",
            "Epoch 4217: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1952 - accuracy: 0.8976 - val_loss: 3.7135 - val_accuracy: 0.5340\n",
            "Epoch 4218/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.8999\n",
            "Epoch 4218: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1909 - accuracy: 0.8999 - val_loss: 3.6131 - val_accuracy: 0.5214\n",
            "Epoch 4219/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9037\n",
            "Epoch 4219: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1857 - accuracy: 0.9022 - val_loss: 3.6972 - val_accuracy: 0.5357\n",
            "Epoch 4220/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1927 - accuracy: 0.8991\n",
            "Epoch 4220: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1940 - accuracy: 0.8974 - val_loss: 3.8384 - val_accuracy: 0.5316\n",
            "Epoch 4221/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9042\n",
            "Epoch 4221: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1817 - accuracy: 0.9037 - val_loss: 3.7123 - val_accuracy: 0.5334\n",
            "Epoch 4222/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1901 - accuracy: 0.8999\n",
            "Epoch 4222: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1902 - accuracy: 0.8996 - val_loss: 3.7098 - val_accuracy: 0.5223\n",
            "Epoch 4223/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1854 - accuracy: 0.9010\n",
            "Epoch 4223: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1845 - accuracy: 0.9008 - val_loss: 3.6237 - val_accuracy: 0.5387\n",
            "Epoch 4224/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9038\n",
            "Epoch 4224: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1783 - accuracy: 0.9038 - val_loss: 3.8473 - val_accuracy: 0.5246\n",
            "Epoch 4225/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9041\n",
            "Epoch 4225: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1796 - accuracy: 0.9041 - val_loss: 3.7765 - val_accuracy: 0.5351\n",
            "Epoch 4226/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1720 - accuracy: 0.9104\n",
            "Epoch 4226: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1733 - accuracy: 0.9093 - val_loss: 3.7350 - val_accuracy: 0.5357\n",
            "Epoch 4227/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.9073\n",
            "Epoch 4227: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1803 - accuracy: 0.9073 - val_loss: 3.6702 - val_accuracy: 0.5375\n",
            "Epoch 4228/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1777 - accuracy: 0.9045\n",
            "Epoch 4228: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1803 - accuracy: 0.9033 - val_loss: 3.7852 - val_accuracy: 0.5337\n",
            "Epoch 4229/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.9021\n",
            "Epoch 4229: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1928 - accuracy: 0.8998 - val_loss: 3.6516 - val_accuracy: 0.5223\n",
            "Epoch 4230/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1861 - accuracy: 0.9025\n",
            "Epoch 4230: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1864 - accuracy: 0.9021 - val_loss: 3.7162 - val_accuracy: 0.5407\n",
            "Epoch 4231/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1850 - accuracy: 0.9017\n",
            "Epoch 4231: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1861 - accuracy: 0.9010 - val_loss: 3.7636 - val_accuracy: 0.5343\n",
            "Epoch 4232/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9052\n",
            "Epoch 4232: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1796 - accuracy: 0.9038 - val_loss: 3.7189 - val_accuracy: 0.5308\n",
            "Epoch 4233/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9077\n",
            "Epoch 4233: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1809 - accuracy: 0.9070 - val_loss: 3.7123 - val_accuracy: 0.5328\n",
            "Epoch 4234/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1838 - accuracy: 0.9026\n",
            "Epoch 4234: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1843 - accuracy: 0.9018 - val_loss: 3.6372 - val_accuracy: 0.5319\n",
            "Epoch 4235/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9035\n",
            "Epoch 4235: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1813 - accuracy: 0.9035 - val_loss: 3.7955 - val_accuracy: 0.5287\n",
            "Epoch 4236/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1844 - accuracy: 0.9013\n",
            "Epoch 4236: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1843 - accuracy: 0.9012 - val_loss: 3.6707 - val_accuracy: 0.5325\n",
            "Epoch 4237/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1729 - accuracy: 0.9079\n",
            "Epoch 4237: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1762 - accuracy: 0.9059 - val_loss: 3.8461 - val_accuracy: 0.5331\n",
            "Epoch 4238/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1779 - accuracy: 0.9055\n",
            "Epoch 4238: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1787 - accuracy: 0.9049 - val_loss: 3.8518 - val_accuracy: 0.5349\n",
            "Epoch 4239/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1806 - accuracy: 0.9027\n",
            "Epoch 4239: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1804 - accuracy: 0.9029 - val_loss: 3.7931 - val_accuracy: 0.5384\n",
            "Epoch 4240/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1752 - accuracy: 0.9070\n",
            "Epoch 4240: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1769 - accuracy: 0.9065 - val_loss: 3.7635 - val_accuracy: 0.5217\n",
            "Epoch 4241/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9029\n",
            "Epoch 4241: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1772 - accuracy: 0.9029 - val_loss: 3.8240 - val_accuracy: 0.5272\n",
            "Epoch 4242/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1757 - accuracy: 0.9053\n",
            "Epoch 4242: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1791 - accuracy: 0.9027 - val_loss: 3.7438 - val_accuracy: 0.5319\n",
            "Epoch 4243/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1841 - accuracy: 0.9030\n",
            "Epoch 4243: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1840 - accuracy: 0.9028 - val_loss: 3.9843 - val_accuracy: 0.5308\n",
            "Epoch 4244/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1881 - accuracy: 0.8995\n",
            "Epoch 4244: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1882 - accuracy: 0.8992 - val_loss: 3.5904 - val_accuracy: 0.5196\n",
            "Epoch 4245/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1863 - accuracy: 0.9032\n",
            "Epoch 4245: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1868 - accuracy: 0.9026 - val_loss: 3.7194 - val_accuracy: 0.5275\n",
            "Epoch 4246/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1846 - accuracy: 0.9033\n",
            "Epoch 4246: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1861 - accuracy: 0.9025 - val_loss: 3.8278 - val_accuracy: 0.5331\n",
            "Epoch 4247/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1762 - accuracy: 0.9047\n",
            "Epoch 4247: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1765 - accuracy: 0.9046 - val_loss: 3.7045 - val_accuracy: 0.5351\n",
            "Epoch 4248/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9062\n",
            "Epoch 4248: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1760 - accuracy: 0.9062 - val_loss: 3.7205 - val_accuracy: 0.5343\n",
            "Epoch 4249/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1805 - accuracy: 0.9038\n",
            "Epoch 4249: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1824 - accuracy: 0.9025 - val_loss: 3.8261 - val_accuracy: 0.5258\n",
            "Epoch 4250/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1903 - accuracy: 0.8972\n",
            "Epoch 4250: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1899 - accuracy: 0.8974 - val_loss: 3.8397 - val_accuracy: 0.5340\n",
            "Epoch 4251/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9038\n",
            "Epoch 4251: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1808 - accuracy: 0.9038 - val_loss: 3.7341 - val_accuracy: 0.5375\n",
            "Epoch 4252/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1798 - accuracy: 0.9060\n",
            "Epoch 4252: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1800 - accuracy: 0.9061 - val_loss: 3.7613 - val_accuracy: 0.5290\n",
            "Epoch 4253/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1853 - accuracy: 0.9046\n",
            "Epoch 4253: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1860 - accuracy: 0.9034 - val_loss: 3.7254 - val_accuracy: 0.5325\n",
            "Epoch 4254/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1777 - accuracy: 0.9049\n",
            "Epoch 4254: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1777 - accuracy: 0.9049 - val_loss: 3.9243 - val_accuracy: 0.5287\n",
            "Epoch 4255/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1772 - accuracy: 0.9062\n",
            "Epoch 4255: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1760 - accuracy: 0.9067 - val_loss: 3.6957 - val_accuracy: 0.5275\n",
            "Epoch 4256/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1728 - accuracy: 0.9086\n",
            "Epoch 4256: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1737 - accuracy: 0.9086 - val_loss: 3.8106 - val_accuracy: 0.5325\n",
            "Epoch 4257/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1884 - accuracy: 0.8997\n",
            "Epoch 4257: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1886 - accuracy: 0.8991 - val_loss: 3.7899 - val_accuracy: 0.5390\n",
            "Epoch 4258/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1807 - accuracy: 0.9020\n",
            "Epoch 4258: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1807 - accuracy: 0.9018 - val_loss: 3.8221 - val_accuracy: 0.5305\n",
            "Epoch 4259/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1858 - accuracy: 0.9032\n",
            "Epoch 4259: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1868 - accuracy: 0.9027 - val_loss: 3.7881 - val_accuracy: 0.5281\n",
            "Epoch 4260/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1766 - accuracy: 0.9047\n",
            "Epoch 4260: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1770 - accuracy: 0.9046 - val_loss: 3.8136 - val_accuracy: 0.5249\n",
            "Epoch 4261/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1866 - accuracy: 0.9008\n",
            "Epoch 4261: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1870 - accuracy: 0.9004 - val_loss: 3.7542 - val_accuracy: 0.5264\n",
            "Epoch 4262/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1739 - accuracy: 0.9061\n",
            "Epoch 4262: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1746 - accuracy: 0.9059 - val_loss: 3.8455 - val_accuracy: 0.5331\n",
            "Epoch 4263/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.9040\n",
            "Epoch 4263: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1778 - accuracy: 0.9042 - val_loss: 3.9206 - val_accuracy: 0.5349\n",
            "Epoch 4264/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1845 - accuracy: 0.9035\n",
            "Epoch 4264: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1845 - accuracy: 0.9035 - val_loss: 3.6706 - val_accuracy: 0.5272\n",
            "Epoch 4265/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1846 - accuracy: 0.9054\n",
            "Epoch 4265: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1856 - accuracy: 0.9040 - val_loss: 3.8490 - val_accuracy: 0.5293\n",
            "Epoch 4266/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1903 - accuracy: 0.9018\n",
            "Epoch 4266: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1907 - accuracy: 0.9015 - val_loss: 3.6405 - val_accuracy: 0.5346\n",
            "Epoch 4267/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1866 - accuracy: 0.9005\n",
            "Epoch 4267: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1868 - accuracy: 0.9006 - val_loss: 3.6780 - val_accuracy: 0.5302\n",
            "Epoch 4268/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9039\n",
            "Epoch 4268: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1810 - accuracy: 0.9039 - val_loss: 3.8545 - val_accuracy: 0.5366\n",
            "Epoch 4269/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9055\n",
            "Epoch 4269: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1778 - accuracy: 0.9055 - val_loss: 3.7902 - val_accuracy: 0.5246\n",
            "Epoch 4270/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9007\n",
            "Epoch 4270: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1836 - accuracy: 0.9007 - val_loss: 3.7157 - val_accuracy: 0.5337\n",
            "Epoch 4271/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9055\n",
            "Epoch 4271: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1769 - accuracy: 0.9055 - val_loss: 3.7549 - val_accuracy: 0.5328\n",
            "Epoch 4272/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9060\n",
            "Epoch 4272: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1754 - accuracy: 0.9060 - val_loss: 3.7596 - val_accuracy: 0.5369\n",
            "Epoch 4273/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1757 - accuracy: 0.9053\n",
            "Epoch 4273: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1755 - accuracy: 0.9054 - val_loss: 3.7472 - val_accuracy: 0.5349\n",
            "Epoch 4274/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.9067\n",
            "Epoch 4274: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1750 - accuracy: 0.9067 - val_loss: 3.6015 - val_accuracy: 0.5214\n",
            "Epoch 4275/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1739 - accuracy: 0.9086\n",
            "Epoch 4275: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1757 - accuracy: 0.9074 - val_loss: 3.8504 - val_accuracy: 0.5240\n",
            "Epoch 4276/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.9070\n",
            "Epoch 4276: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1747 - accuracy: 0.9053 - val_loss: 3.9095 - val_accuracy: 0.5249\n",
            "Epoch 4277/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1797 - accuracy: 0.9050\n",
            "Epoch 4277: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1792 - accuracy: 0.9051 - val_loss: 3.8216 - val_accuracy: 0.5407\n",
            "Epoch 4278/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1803 - accuracy: 0.9057\n",
            "Epoch 4278: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1799 - accuracy: 0.9058 - val_loss: 3.8753 - val_accuracy: 0.5308\n",
            "Epoch 4279/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.9068\n",
            "Epoch 4279: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1717 - accuracy: 0.9059 - val_loss: 3.6861 - val_accuracy: 0.5340\n",
            "Epoch 4280/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1871 - accuracy: 0.9026\n",
            "Epoch 4280: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1878 - accuracy: 0.9023 - val_loss: 3.7661 - val_accuracy: 0.5252\n",
            "Epoch 4281/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9050\n",
            "Epoch 4281: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1802 - accuracy: 0.9048 - val_loss: 3.7760 - val_accuracy: 0.5331\n",
            "Epoch 4282/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1741 - accuracy: 0.9064\n",
            "Epoch 4282: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1772 - accuracy: 0.9053 - val_loss: 3.7368 - val_accuracy: 0.5349\n",
            "Epoch 4283/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1802 - accuracy: 0.9044\n",
            "Epoch 4283: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1802 - accuracy: 0.9036 - val_loss: 3.7683 - val_accuracy: 0.5302\n",
            "Epoch 4284/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1742 - accuracy: 0.9082\n",
            "Epoch 4284: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1744 - accuracy: 0.9074 - val_loss: 3.6358 - val_accuracy: 0.5302\n",
            "Epoch 4285/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.9063\n",
            "Epoch 4285: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1737 - accuracy: 0.9064 - val_loss: 3.8600 - val_accuracy: 0.5269\n",
            "Epoch 4286/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1801 - accuracy: 0.9071\n",
            "Epoch 4286: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1805 - accuracy: 0.9057 - val_loss: 3.7755 - val_accuracy: 0.5290\n",
            "Epoch 4287/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1742 - accuracy: 0.9081\n",
            "Epoch 4287: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1755 - accuracy: 0.9066 - val_loss: 3.8439 - val_accuracy: 0.5322\n",
            "Epoch 4288/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1701 - accuracy: 0.9086\n",
            "Epoch 4288: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1714 - accuracy: 0.9078 - val_loss: 3.7998 - val_accuracy: 0.5331\n",
            "Epoch 4289/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1787 - accuracy: 0.9066\n",
            "Epoch 4289: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1814 - accuracy: 0.9044 - val_loss: 3.7106 - val_accuracy: 0.5401\n",
            "Epoch 4290/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.9075\n",
            "Epoch 4290: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1792 - accuracy: 0.9075 - val_loss: 3.6909 - val_accuracy: 0.5281\n",
            "Epoch 4291/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1837 - accuracy: 0.9017\n",
            "Epoch 4291: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1850 - accuracy: 0.9006 - val_loss: 3.6854 - val_accuracy: 0.5249\n",
            "Epoch 4292/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1752 - accuracy: 0.9053\n",
            "Epoch 4292: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1788 - accuracy: 0.9032 - val_loss: 3.7851 - val_accuracy: 0.5261\n",
            "Epoch 4293/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.9078\n",
            "Epoch 4293: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1765 - accuracy: 0.9078 - val_loss: 3.7834 - val_accuracy: 0.5261\n",
            "Epoch 4294/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1759 - accuracy: 0.9084\n",
            "Epoch 4294: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1799 - accuracy: 0.9052 - val_loss: 3.7683 - val_accuracy: 0.5255\n",
            "Epoch 4295/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9051\n",
            "Epoch 4295: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1828 - accuracy: 0.9051 - val_loss: 3.7395 - val_accuracy: 0.5319\n",
            "Epoch 4296/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9009\n",
            "Epoch 4296: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1885 - accuracy: 0.9009 - val_loss: 3.7482 - val_accuracy: 0.5255\n",
            "Epoch 4297/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.8923\n",
            "Epoch 4297: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.2059 - accuracy: 0.8923 - val_loss: 3.7468 - val_accuracy: 0.5372\n",
            "Epoch 4298/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1849 - accuracy: 0.9016\n",
            "Epoch 4298: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1864 - accuracy: 0.9002 - val_loss: 3.5602 - val_accuracy: 0.5275\n",
            "Epoch 4299/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1809 - accuracy: 0.9057\n",
            "Epoch 4299: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1815 - accuracy: 0.9053 - val_loss: 3.6706 - val_accuracy: 0.5381\n",
            "Epoch 4300/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9056\n",
            "Epoch 4300: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1807 - accuracy: 0.9053 - val_loss: 3.8360 - val_accuracy: 0.5328\n",
            "Epoch 4301/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1732 - accuracy: 0.9083\n",
            "Epoch 4301: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1757 - accuracy: 0.9060 - val_loss: 3.8091 - val_accuracy: 0.5313\n",
            "Epoch 4302/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9079\n",
            "Epoch 4302: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1734 - accuracy: 0.9079 - val_loss: 3.8411 - val_accuracy: 0.5305\n",
            "Epoch 4303/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1763 - accuracy: 0.9061\n",
            "Epoch 4303: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1776 - accuracy: 0.9056 - val_loss: 3.7153 - val_accuracy: 0.5302\n",
            "Epoch 4304/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9070\n",
            "Epoch 4304: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1761 - accuracy: 0.9073 - val_loss: 3.7185 - val_accuracy: 0.5310\n",
            "Epoch 4305/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1717 - accuracy: 0.9095\n",
            "Epoch 4305: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1732 - accuracy: 0.9081 - val_loss: 3.9152 - val_accuracy: 0.5302\n",
            "Epoch 4306/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1758 - accuracy: 0.9070\n",
            "Epoch 4306: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1766 - accuracy: 0.9059 - val_loss: 3.6858 - val_accuracy: 0.5366\n",
            "Epoch 4307/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1848 - accuracy: 0.9006\n",
            "Epoch 4307: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1856 - accuracy: 0.8998 - val_loss: 3.7015 - val_accuracy: 0.5296\n",
            "Epoch 4308/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1847 - accuracy: 0.9019\n",
            "Epoch 4308: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1842 - accuracy: 0.9024 - val_loss: 3.8485 - val_accuracy: 0.5278\n",
            "Epoch 4309/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9050\n",
            "Epoch 4309: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1814 - accuracy: 0.9050 - val_loss: 3.8694 - val_accuracy: 0.5252\n",
            "Epoch 4310/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9066\n",
            "Epoch 4310: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1744 - accuracy: 0.9066 - val_loss: 3.7309 - val_accuracy: 0.5249\n",
            "Epoch 4311/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1844 - accuracy: 0.9046\n",
            "Epoch 4311: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1849 - accuracy: 0.9042 - val_loss: 3.7701 - val_accuracy: 0.5410\n",
            "Epoch 4312/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1878 - accuracy: 0.9003\n",
            "Epoch 4312: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1872 - accuracy: 0.9005 - val_loss: 3.7423 - val_accuracy: 0.5334\n",
            "Epoch 4313/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1949 - accuracy: 0.8983\n",
            "Epoch 4313: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1974 - accuracy: 0.8968 - val_loss: 3.6169 - val_accuracy: 0.5325\n",
            "Epoch 4314/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1901 - accuracy: 0.9008\n",
            "Epoch 4314: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1903 - accuracy: 0.9008 - val_loss: 3.9051 - val_accuracy: 0.5228\n",
            "Epoch 4315/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1882 - accuracy: 0.9019\n",
            "Epoch 4315: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1893 - accuracy: 0.9010 - val_loss: 3.6429 - val_accuracy: 0.5313\n",
            "Epoch 4316/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1859 - accuracy: 0.9013\n",
            "Epoch 4316: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1859 - accuracy: 0.9014 - val_loss: 3.8362 - val_accuracy: 0.5275\n",
            "Epoch 4317/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1744 - accuracy: 0.9069\n",
            "Epoch 4317: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1749 - accuracy: 0.9064 - val_loss: 3.8385 - val_accuracy: 0.5240\n",
            "Epoch 4318/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.9090\n",
            "Epoch 4318: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1743 - accuracy: 0.9073 - val_loss: 3.8773 - val_accuracy: 0.5331\n",
            "Epoch 4319/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1859 - accuracy: 0.9025\n",
            "Epoch 4319: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1858 - accuracy: 0.9023 - val_loss: 3.8320 - val_accuracy: 0.5395\n",
            "Epoch 4320/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9064\n",
            "Epoch 4320: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1826 - accuracy: 0.9064 - val_loss: 3.8710 - val_accuracy: 0.5343\n",
            "Epoch 4321/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1801 - accuracy: 0.9041\n",
            "Epoch 4321: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1803 - accuracy: 0.9039 - val_loss: 3.8211 - val_accuracy: 0.5287\n",
            "Epoch 4322/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1852 - accuracy: 0.9035\n",
            "Epoch 4322: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1863 - accuracy: 0.9023 - val_loss: 3.7757 - val_accuracy: 0.5310\n",
            "Epoch 4323/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1726 - accuracy: 0.9097\n",
            "Epoch 4323: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1739 - accuracy: 0.9083 - val_loss: 3.8144 - val_accuracy: 0.5363\n",
            "Epoch 4324/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9046\n",
            "Epoch 4324: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1787 - accuracy: 0.9046 - val_loss: 3.8032 - val_accuracy: 0.5275\n",
            "Epoch 4325/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1719 - accuracy: 0.9082\n",
            "Epoch 4325: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1742 - accuracy: 0.9064 - val_loss: 3.8861 - val_accuracy: 0.5334\n",
            "Epoch 4326/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1737 - accuracy: 0.9067\n",
            "Epoch 4326: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1742 - accuracy: 0.9064 - val_loss: 3.6331 - val_accuracy: 0.5293\n",
            "Epoch 4327/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9080\n",
            "Epoch 4327: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1722 - accuracy: 0.9080 - val_loss: 3.7694 - val_accuracy: 0.5267\n",
            "Epoch 4328/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1753 - accuracy: 0.9052\n",
            "Epoch 4328: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1766 - accuracy: 0.9048 - val_loss: 3.7468 - val_accuracy: 0.5319\n",
            "Epoch 4329/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1762 - accuracy: 0.9039\n",
            "Epoch 4329: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1762 - accuracy: 0.9039 - val_loss: 3.6849 - val_accuracy: 0.5310\n",
            "Epoch 4330/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1727 - accuracy: 0.9065\n",
            "Epoch 4330: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1741 - accuracy: 0.9054 - val_loss: 3.8404 - val_accuracy: 0.5393\n",
            "Epoch 4331/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1749 - accuracy: 0.9068\n",
            "Epoch 4331: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1753 - accuracy: 0.9069 - val_loss: 3.7144 - val_accuracy: 0.5322\n",
            "Epoch 4332/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1729 - accuracy: 0.9079\n",
            "Epoch 4332: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1733 - accuracy: 0.9075 - val_loss: 3.8543 - val_accuracy: 0.5378\n",
            "Epoch 4333/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9082\n",
            "Epoch 4333: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1775 - accuracy: 0.9078 - val_loss: 3.8304 - val_accuracy: 0.5296\n",
            "Epoch 4334/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1746 - accuracy: 0.9077\n",
            "Epoch 4334: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1780 - accuracy: 0.9054 - val_loss: 3.7134 - val_accuracy: 0.5425\n",
            "Epoch 4335/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1840 - accuracy: 0.9028\n",
            "Epoch 4335: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1845 - accuracy: 0.9024 - val_loss: 3.7698 - val_accuracy: 0.5310\n",
            "Epoch 4336/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1842 - accuracy: 0.9046\n",
            "Epoch 4336: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1867 - accuracy: 0.9024 - val_loss: 3.7764 - val_accuracy: 0.5372\n",
            "Epoch 4337/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1781 - accuracy: 0.9062\n",
            "Epoch 4337: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1790 - accuracy: 0.9056 - val_loss: 3.9301 - val_accuracy: 0.5240\n",
            "Epoch 4338/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1778 - accuracy: 0.9064\n",
            "Epoch 4338: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1806 - accuracy: 0.9035 - val_loss: 3.7997 - val_accuracy: 0.5290\n",
            "Epoch 4339/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9043\n",
            "Epoch 4339: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1801 - accuracy: 0.9043 - val_loss: 3.6845 - val_accuracy: 0.5299\n",
            "Epoch 4340/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1824 - accuracy: 0.8999\n",
            "Epoch 4340: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1848 - accuracy: 0.8988 - val_loss: 3.8022 - val_accuracy: 0.5366\n",
            "Epoch 4341/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1833 - accuracy: 0.9019\n",
            "Epoch 4341: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1833 - accuracy: 0.9018 - val_loss: 3.9504 - val_accuracy: 0.5349\n",
            "Epoch 4342/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1806 - accuracy: 0.9067\n",
            "Epoch 4342: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1813 - accuracy: 0.9059 - val_loss: 3.6402 - val_accuracy: 0.5252\n",
            "Epoch 4343/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1835 - accuracy: 0.9038\n",
            "Epoch 4343: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1840 - accuracy: 0.9031 - val_loss: 3.6312 - val_accuracy: 0.5337\n",
            "Epoch 4344/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1742 - accuracy: 0.9085\n",
            "Epoch 4344: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1752 - accuracy: 0.9075 - val_loss: 3.8326 - val_accuracy: 0.5228\n",
            "Epoch 4345/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1856 - accuracy: 0.9027\n",
            "Epoch 4345: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1854 - accuracy: 0.9024 - val_loss: 3.7310 - val_accuracy: 0.5334\n",
            "Epoch 4346/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1752 - accuracy: 0.9052\n",
            "Epoch 4346: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1768 - accuracy: 0.9040 - val_loss: 3.8270 - val_accuracy: 0.5269\n",
            "Epoch 4347/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1733 - accuracy: 0.9077\n",
            "Epoch 4347: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1757 - accuracy: 0.9058 - val_loss: 3.7663 - val_accuracy: 0.5202\n",
            "Epoch 4348/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1793 - accuracy: 0.9067\n",
            "Epoch 4348: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1797 - accuracy: 0.9060 - val_loss: 3.9633 - val_accuracy: 0.5354\n",
            "Epoch 4349/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1769 - accuracy: 0.9070\n",
            "Epoch 4349: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1787 - accuracy: 0.9051 - val_loss: 3.7632 - val_accuracy: 0.5281\n",
            "Epoch 4350/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9023\n",
            "Epoch 4350: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1906 - accuracy: 0.9023 - val_loss: 3.8497 - val_accuracy: 0.5363\n",
            "Epoch 4351/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1900 - accuracy: 0.9006\n",
            "Epoch 4351: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1882 - accuracy: 0.9016 - val_loss: 3.8328 - val_accuracy: 0.5252\n",
            "Epoch 4352/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1853 - accuracy: 0.9010\n",
            "Epoch 4352: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1881 - accuracy: 0.8993 - val_loss: 3.8377 - val_accuracy: 0.5267\n",
            "Epoch 4353/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1911 - accuracy: 0.9032\n",
            "Epoch 4353: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1939 - accuracy: 0.9010 - val_loss: 3.6847 - val_accuracy: 0.5316\n",
            "Epoch 4354/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1784 - accuracy: 0.9061\n",
            "Epoch 4354: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1794 - accuracy: 0.9046 - val_loss: 3.8627 - val_accuracy: 0.5167\n",
            "Epoch 4355/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1765 - accuracy: 0.9075\n",
            "Epoch 4355: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1780 - accuracy: 0.9062 - val_loss: 3.8318 - val_accuracy: 0.5322\n",
            "Epoch 4356/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9093\n",
            "Epoch 4356: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1720 - accuracy: 0.9093 - val_loss: 3.9055 - val_accuracy: 0.5305\n",
            "Epoch 4357/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.9077\n",
            "Epoch 4357: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1778 - accuracy: 0.9066 - val_loss: 3.6783 - val_accuracy: 0.5228\n",
            "Epoch 4358/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.8994\n",
            "Epoch 4358: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1892 - accuracy: 0.8994 - val_loss: 3.8249 - val_accuracy: 0.5310\n",
            "Epoch 4359/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9036\n",
            "Epoch 4359: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1802 - accuracy: 0.9036 - val_loss: 3.7252 - val_accuracy: 0.5267\n",
            "Epoch 4360/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9090\n",
            "Epoch 4360: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1742 - accuracy: 0.9090 - val_loss: 3.7857 - val_accuracy: 0.5290\n",
            "Epoch 4361/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1783 - accuracy: 0.9017\n",
            "Epoch 4361: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1774 - accuracy: 0.9027 - val_loss: 3.8147 - val_accuracy: 0.5351\n",
            "Epoch 4362/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1736 - accuracy: 0.9078\n",
            "Epoch 4362: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1748 - accuracy: 0.9062 - val_loss: 3.8517 - val_accuracy: 0.5220\n",
            "Epoch 4363/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1733 - accuracy: 0.9054\n",
            "Epoch 4363: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1764 - accuracy: 0.9039 - val_loss: 3.8557 - val_accuracy: 0.5299\n",
            "Epoch 4364/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1832 - accuracy: 0.9026\n",
            "Epoch 4364: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1861 - accuracy: 0.9010 - val_loss: 3.7738 - val_accuracy: 0.5296\n",
            "Epoch 4365/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1841 - accuracy: 0.9059\n",
            "Epoch 4365: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1854 - accuracy: 0.9043 - val_loss: 3.7858 - val_accuracy: 0.5214\n",
            "Epoch 4366/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1851 - accuracy: 0.9030\n",
            "Epoch 4366: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1850 - accuracy: 0.9026 - val_loss: 3.7822 - val_accuracy: 0.5337\n",
            "Epoch 4367/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9068\n",
            "Epoch 4367: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1832 - accuracy: 0.9068 - val_loss: 3.8640 - val_accuracy: 0.5249\n",
            "Epoch 4368/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1727 - accuracy: 0.9084\n",
            "Epoch 4368: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1774 - accuracy: 0.9049 - val_loss: 3.8819 - val_accuracy: 0.5269\n",
            "Epoch 4369/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.8973\n",
            "Epoch 4369: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1965 - accuracy: 0.8973 - val_loss: 3.7930 - val_accuracy: 0.5363\n",
            "Epoch 4370/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1841 - accuracy: 0.9037\n",
            "Epoch 4370: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1850 - accuracy: 0.9023 - val_loss: 3.7216 - val_accuracy: 0.5258\n",
            "Epoch 4371/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1867 - accuracy: 0.9041\n",
            "Epoch 4371: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1876 - accuracy: 0.9034 - val_loss: 3.8033 - val_accuracy: 0.5249\n",
            "Epoch 4372/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9057\n",
            "Epoch 4372: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1820 - accuracy: 0.9039 - val_loss: 3.7419 - val_accuracy: 0.5269\n",
            "Epoch 4373/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1778 - accuracy: 0.9035\n",
            "Epoch 4373: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1798 - accuracy: 0.9020 - val_loss: 3.9970 - val_accuracy: 0.5293\n",
            "Epoch 4374/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9043\n",
            "Epoch 4374: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.1779 - accuracy: 0.9043 - val_loss: 3.8432 - val_accuracy: 0.5205\n",
            "Epoch 4375/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1762 - accuracy: 0.9081\n",
            "Epoch 4375: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1762 - accuracy: 0.9081 - val_loss: 3.8109 - val_accuracy: 0.5308\n",
            "Epoch 4376/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1939 - accuracy: 0.9001\n",
            "Epoch 4376: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1937 - accuracy: 0.9005 - val_loss: 3.6134 - val_accuracy: 0.5267\n",
            "Epoch 4377/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.9078\n",
            "Epoch 4377: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1758 - accuracy: 0.9074 - val_loss: 3.9888 - val_accuracy: 0.5281\n",
            "Epoch 4378/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1800 - accuracy: 0.9028\n",
            "Epoch 4378: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1805 - accuracy: 0.9025 - val_loss: 3.6640 - val_accuracy: 0.5351\n",
            "Epoch 4379/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1774 - accuracy: 0.9073\n",
            "Epoch 4379: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1784 - accuracy: 0.9058 - val_loss: 3.7914 - val_accuracy: 0.5281\n",
            "Epoch 4380/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9044\n",
            "Epoch 4380: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1773 - accuracy: 0.9044 - val_loss: 3.8453 - val_accuracy: 0.5366\n",
            "Epoch 4381/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1823 - accuracy: 0.9054\n",
            "Epoch 4381: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1835 - accuracy: 0.9046 - val_loss: 3.8066 - val_accuracy: 0.5302\n",
            "Epoch 4382/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1745 - accuracy: 0.9057\n",
            "Epoch 4382: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1736 - accuracy: 0.9061 - val_loss: 3.8597 - val_accuracy: 0.5310\n",
            "Epoch 4383/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1696 - accuracy: 0.9082\n",
            "Epoch 4383: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1718 - accuracy: 0.9068 - val_loss: 3.8435 - val_accuracy: 0.5237\n",
            "Epoch 4384/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1768 - accuracy: 0.9071\n",
            "Epoch 4384: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1794 - accuracy: 0.9058 - val_loss: 3.8889 - val_accuracy: 0.5313\n",
            "Epoch 4385/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9065\n",
            "Epoch 4385: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1765 - accuracy: 0.9065 - val_loss: 3.7476 - val_accuracy: 0.5284\n",
            "Epoch 4386/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1797 - accuracy: 0.9060\n",
            "Epoch 4386: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1795 - accuracy: 0.9062 - val_loss: 4.0237 - val_accuracy: 0.5354\n",
            "Epoch 4387/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9070\n",
            "Epoch 4387: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1765 - accuracy: 0.9070 - val_loss: 3.8148 - val_accuracy: 0.5252\n",
            "Epoch 4388/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1763 - accuracy: 0.9055\n",
            "Epoch 4388: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1766 - accuracy: 0.9056 - val_loss: 3.8777 - val_accuracy: 0.5390\n",
            "Epoch 4389/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1774 - accuracy: 0.9061\n",
            "Epoch 4389: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1786 - accuracy: 0.9052 - val_loss: 3.7266 - val_accuracy: 0.5272\n",
            "Epoch 4390/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1792 - accuracy: 0.9058\n",
            "Epoch 4390: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1815 - accuracy: 0.9046 - val_loss: 3.8899 - val_accuracy: 0.5334\n",
            "Epoch 4391/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.8985\n",
            "Epoch 4391: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1937 - accuracy: 0.8985 - val_loss: 3.7703 - val_accuracy: 0.5343\n",
            "Epoch 4392/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1786 - accuracy: 0.9071\n",
            "Epoch 4392: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1812 - accuracy: 0.9046 - val_loss: 3.7039 - val_accuracy: 0.5343\n",
            "Epoch 4393/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1757 - accuracy: 0.9059\n",
            "Epoch 4393: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1763 - accuracy: 0.9054 - val_loss: 3.8267 - val_accuracy: 0.5310\n",
            "Epoch 4394/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1737 - accuracy: 0.9083\n",
            "Epoch 4394: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1763 - accuracy: 0.9068 - val_loss: 3.8369 - val_accuracy: 0.5261\n",
            "Epoch 4395/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9045\n",
            "Epoch 4395: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1798 - accuracy: 0.9045 - val_loss: 3.7712 - val_accuracy: 0.5334\n",
            "Epoch 4396/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9046\n",
            "Epoch 4396: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1797 - accuracy: 0.9046 - val_loss: 3.7883 - val_accuracy: 0.5328\n",
            "Epoch 4397/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1738 - accuracy: 0.9081\n",
            "Epoch 4397: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1755 - accuracy: 0.9064 - val_loss: 3.7504 - val_accuracy: 0.5349\n",
            "Epoch 4398/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1712 - accuracy: 0.9095\n",
            "Epoch 4398: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1742 - accuracy: 0.9073 - val_loss: 3.8404 - val_accuracy: 0.5272\n",
            "Epoch 4399/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1775 - accuracy: 0.9057\n",
            "Epoch 4399: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1773 - accuracy: 0.9057 - val_loss: 3.7512 - val_accuracy: 0.5240\n",
            "Epoch 4400/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.8999\n",
            "Epoch 4400: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1868 - accuracy: 0.8999 - val_loss: 3.8822 - val_accuracy: 0.5310\n",
            "Epoch 4401/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1798 - accuracy: 0.9037\n",
            "Epoch 4401: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1811 - accuracy: 0.9024 - val_loss: 3.8870 - val_accuracy: 0.5258\n",
            "Epoch 4402/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1755 - accuracy: 0.9080\n",
            "Epoch 4402: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1769 - accuracy: 0.9059 - val_loss: 3.7529 - val_accuracy: 0.5310\n",
            "Epoch 4403/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1794 - accuracy: 0.9040\n",
            "Epoch 4403: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1794 - accuracy: 0.9038 - val_loss: 3.9044 - val_accuracy: 0.5316\n",
            "Epoch 4404/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1777 - accuracy: 0.9050\n",
            "Epoch 4404: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1777 - accuracy: 0.9051 - val_loss: 3.8437 - val_accuracy: 0.5284\n",
            "Epoch 4405/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1697 - accuracy: 0.9085\n",
            "Epoch 4405: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1724 - accuracy: 0.9067 - val_loss: 3.8541 - val_accuracy: 0.5246\n",
            "Epoch 4406/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9011\n",
            "Epoch 4406: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1878 - accuracy: 0.9010 - val_loss: 3.6616 - val_accuracy: 0.5281\n",
            "Epoch 4407/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1851 - accuracy: 0.9051\n",
            "Epoch 4407: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1855 - accuracy: 0.9050 - val_loss: 3.8529 - val_accuracy: 0.5372\n",
            "Epoch 4408/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1916 - accuracy: 0.9002\n",
            "Epoch 4408: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1927 - accuracy: 0.8996 - val_loss: 3.8915 - val_accuracy: 0.5185\n",
            "Epoch 4409/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1894 - accuracy: 0.9035\n",
            "Epoch 4409: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1920 - accuracy: 0.9015 - val_loss: 3.7745 - val_accuracy: 0.5249\n",
            "Epoch 4410/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.8962\n",
            "Epoch 4410: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1906 - accuracy: 0.8962 - val_loss: 3.7166 - val_accuracy: 0.5249\n",
            "Epoch 4411/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9046\n",
            "Epoch 4411: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1796 - accuracy: 0.9046 - val_loss: 3.7266 - val_accuracy: 0.5313\n",
            "Epoch 4412/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9048\n",
            "Epoch 4412: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1757 - accuracy: 0.9048 - val_loss: 3.8442 - val_accuracy: 0.5340\n",
            "Epoch 4413/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1762 - accuracy: 0.9058\n",
            "Epoch 4413: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1771 - accuracy: 0.9051 - val_loss: 3.7902 - val_accuracy: 0.5357\n",
            "Epoch 4414/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9037\n",
            "Epoch 4414: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1832 - accuracy: 0.9037 - val_loss: 3.7753 - val_accuracy: 0.5278\n",
            "Epoch 4415/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1822 - accuracy: 0.9036\n",
            "Epoch 4415: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1821 - accuracy: 0.9038 - val_loss: 3.7727 - val_accuracy: 0.5313\n",
            "Epoch 4416/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1811 - accuracy: 0.9036\n",
            "Epoch 4416: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1809 - accuracy: 0.9038 - val_loss: 3.8123 - val_accuracy: 0.5231\n",
            "Epoch 4417/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1737 - accuracy: 0.9076\n",
            "Epoch 4417: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1733 - accuracy: 0.9081 - val_loss: 3.8062 - val_accuracy: 0.5302\n",
            "Epoch 4418/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1759 - accuracy: 0.9061\n",
            "Epoch 4418: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1775 - accuracy: 0.9043 - val_loss: 3.7677 - val_accuracy: 0.5269\n",
            "Epoch 4419/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9098\n",
            "Epoch 4419: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1707 - accuracy: 0.9098 - val_loss: 3.8213 - val_accuracy: 0.5334\n",
            "Epoch 4420/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1790 - accuracy: 0.9038\n",
            "Epoch 4420: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1785 - accuracy: 0.9038 - val_loss: 3.7619 - val_accuracy: 0.5372\n",
            "Epoch 4421/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1832 - accuracy: 0.9020\n",
            "Epoch 4421: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1832 - accuracy: 0.9013 - val_loss: 3.6891 - val_accuracy: 0.5308\n",
            "Epoch 4422/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1804 - accuracy: 0.9060\n",
            "Epoch 4422: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1815 - accuracy: 0.9054 - val_loss: 3.9383 - val_accuracy: 0.5284\n",
            "Epoch 4423/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1915 - accuracy: 0.8998\n",
            "Epoch 4423: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1916 - accuracy: 0.8994 - val_loss: 3.7484 - val_accuracy: 0.5340\n",
            "Epoch 4424/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1904 - accuracy: 0.8982\n",
            "Epoch 4424: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1919 - accuracy: 0.8977 - val_loss: 3.6682 - val_accuracy: 0.5308\n",
            "Epoch 4425/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1885 - accuracy: 0.8999\n",
            "Epoch 4425: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1898 - accuracy: 0.8984 - val_loss: 4.0098 - val_accuracy: 0.5246\n",
            "Epoch 4426/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9045\n",
            "Epoch 4426: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9037 - val_loss: 3.7587 - val_accuracy: 0.5404\n",
            "Epoch 4427/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9039\n",
            "Epoch 4427: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1861 - accuracy: 0.9039 - val_loss: 3.6964 - val_accuracy: 0.5337\n",
            "Epoch 4428/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1864 - accuracy: 0.9038\n",
            "Epoch 4428: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1867 - accuracy: 0.9035 - val_loss: 3.7279 - val_accuracy: 0.5278\n",
            "Epoch 4429/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1771 - accuracy: 0.9044\n",
            "Epoch 4429: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1786 - accuracy: 0.9037 - val_loss: 3.6741 - val_accuracy: 0.5243\n",
            "Epoch 4430/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1748 - accuracy: 0.9060\n",
            "Epoch 4430: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1762 - accuracy: 0.9043 - val_loss: 3.8547 - val_accuracy: 0.5308\n",
            "Epoch 4431/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1768 - accuracy: 0.9062\n",
            "Epoch 4431: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1776 - accuracy: 0.9051 - val_loss: 3.7469 - val_accuracy: 0.5267\n",
            "Epoch 4432/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9016\n",
            "Epoch 4432: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1810 - accuracy: 0.9016 - val_loss: 3.8404 - val_accuracy: 0.5267\n",
            "Epoch 4433/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1833 - accuracy: 0.9050\n",
            "Epoch 4433: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1856 - accuracy: 0.9028 - val_loss: 3.9189 - val_accuracy: 0.5246\n",
            "Epoch 4434/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9070\n",
            "Epoch 4434: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1799 - accuracy: 0.9062 - val_loss: 3.8897 - val_accuracy: 0.5223\n",
            "Epoch 4435/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1838 - accuracy: 0.9038\n",
            "Epoch 4435: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1830 - accuracy: 0.9033 - val_loss: 3.6551 - val_accuracy: 0.5290\n",
            "Epoch 4436/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1707 - accuracy: 0.9075\n",
            "Epoch 4436: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1720 - accuracy: 0.9063 - val_loss: 3.9393 - val_accuracy: 0.5331\n",
            "Epoch 4437/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1726 - accuracy: 0.9073\n",
            "Epoch 4437: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1730 - accuracy: 0.9068 - val_loss: 3.8401 - val_accuracy: 0.5310\n",
            "Epoch 4438/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1781 - accuracy: 0.9065\n",
            "Epoch 4438: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1783 - accuracy: 0.9064 - val_loss: 3.8433 - val_accuracy: 0.5322\n",
            "Epoch 4439/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9016\n",
            "Epoch 4439: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1846 - accuracy: 0.9016 - val_loss: 3.8185 - val_accuracy: 0.5431\n",
            "Epoch 4440/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1820 - accuracy: 0.9050\n",
            "Epoch 4440: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1836 - accuracy: 0.9037 - val_loss: 3.7330 - val_accuracy: 0.5328\n",
            "Epoch 4441/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1762 - accuracy: 0.9082\n",
            "Epoch 4441: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1770 - accuracy: 0.9079 - val_loss: 3.8590 - val_accuracy: 0.5249\n",
            "Epoch 4442/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1749 - accuracy: 0.9059\n",
            "Epoch 4442: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1758 - accuracy: 0.9059 - val_loss: 3.8980 - val_accuracy: 0.5278\n",
            "Epoch 4443/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9051\n",
            "Epoch 4443: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1800 - accuracy: 0.9051 - val_loss: 3.9305 - val_accuracy: 0.5196\n",
            "Epoch 4444/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1835 - accuracy: 0.9046\n",
            "Epoch 4444: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1834 - accuracy: 0.9046 - val_loss: 3.8855 - val_accuracy: 0.5331\n",
            "Epoch 4445/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1785 - accuracy: 0.9042\n",
            "Epoch 4445: loss did not improve from 0.16889\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1785 - accuracy: 0.9042 - val_loss: 3.7779 - val_accuracy: 0.5290\n",
            "Epoch 4446/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1670 - accuracy: 0.9107\n",
            "Epoch 4446: loss improved from 0.16889 to 0.16745, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 45ms/step - loss: 0.1675 - accuracy: 0.9103 - val_loss: 3.8419 - val_accuracy: 0.5278\n",
            "Epoch 4447/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1678 - accuracy: 0.9093\n",
            "Epoch 4447: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1680 - accuracy: 0.9093 - val_loss: 3.7083 - val_accuracy: 0.5351\n",
            "Epoch 4448/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9052\n",
            "Epoch 4448: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1747 - accuracy: 0.9052 - val_loss: 3.8286 - val_accuracy: 0.5313\n",
            "Epoch 4449/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1702 - accuracy: 0.9065\n",
            "Epoch 4449: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1714 - accuracy: 0.9060 - val_loss: 3.8948 - val_accuracy: 0.5284\n",
            "Epoch 4450/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1719 - accuracy: 0.9104\n",
            "Epoch 4450: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1755 - accuracy: 0.9074 - val_loss: 3.9267 - val_accuracy: 0.5202\n",
            "Epoch 4451/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9070\n",
            "Epoch 4451: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1776 - accuracy: 0.9057 - val_loss: 3.6143 - val_accuracy: 0.5275\n",
            "Epoch 4452/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9038\n",
            "Epoch 4452: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1801 - accuracy: 0.9039 - val_loss: 3.7732 - val_accuracy: 0.5366\n",
            "Epoch 4453/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.9076\n",
            "Epoch 4453: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1702 - accuracy: 0.9080 - val_loss: 3.9178 - val_accuracy: 0.5272\n",
            "Epoch 4454/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9073\n",
            "Epoch 4454: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1733 - accuracy: 0.9073 - val_loss: 3.7692 - val_accuracy: 0.5275\n",
            "Epoch 4455/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1762 - accuracy: 0.9057\n",
            "Epoch 4455: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1762 - accuracy: 0.9057 - val_loss: 3.8674 - val_accuracy: 0.5220\n",
            "Epoch 4456/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1831 - accuracy: 0.9045\n",
            "Epoch 4456: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1828 - accuracy: 0.9043 - val_loss: 3.8226 - val_accuracy: 0.5217\n",
            "Epoch 4457/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9073\n",
            "Epoch 4457: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1794 - accuracy: 0.9073 - val_loss: 3.7363 - val_accuracy: 0.5272\n",
            "Epoch 4458/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1815 - accuracy: 0.9063\n",
            "Epoch 4458: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1839 - accuracy: 0.9051 - val_loss: 3.6704 - val_accuracy: 0.5349\n",
            "Epoch 4459/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1836 - accuracy: 0.9032\n",
            "Epoch 4459: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1831 - accuracy: 0.9036 - val_loss: 4.0078 - val_accuracy: 0.5299\n",
            "Epoch 4460/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1752 - accuracy: 0.9078\n",
            "Epoch 4460: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1764 - accuracy: 0.9071 - val_loss: 3.7014 - val_accuracy: 0.5384\n",
            "Epoch 4461/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1998 - accuracy: 0.8983\n",
            "Epoch 4461: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1991 - accuracy: 0.8980 - val_loss: 3.7054 - val_accuracy: 0.5369\n",
            "Epoch 4462/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9048\n",
            "Epoch 4462: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1821 - accuracy: 0.9048 - val_loss: 3.5909 - val_accuracy: 0.5313\n",
            "Epoch 4463/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1794 - accuracy: 0.9060\n",
            "Epoch 4463: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1801 - accuracy: 0.9054 - val_loss: 3.9326 - val_accuracy: 0.5281\n",
            "Epoch 4464/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9068\n",
            "Epoch 4464: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1742 - accuracy: 0.9068 - val_loss: 3.7967 - val_accuracy: 0.5220\n",
            "Epoch 4465/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9051\n",
            "Epoch 4465: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1824 - accuracy: 0.9051 - val_loss: 3.7469 - val_accuracy: 0.5357\n",
            "Epoch 4466/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1824 - accuracy: 0.9033\n",
            "Epoch 4466: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1851 - accuracy: 0.9011 - val_loss: 3.8483 - val_accuracy: 0.5354\n",
            "Epoch 4467/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9043\n",
            "Epoch 4467: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1779 - accuracy: 0.9043 - val_loss: 3.7473 - val_accuracy: 0.5343\n",
            "Epoch 4468/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1884 - accuracy: 0.9030\n",
            "Epoch 4468: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1891 - accuracy: 0.9024 - val_loss: 3.6418 - val_accuracy: 0.5237\n",
            "Epoch 4469/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1857 - accuracy: 0.9018\n",
            "Epoch 4469: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1857 - accuracy: 0.9010 - val_loss: 3.7175 - val_accuracy: 0.5284\n",
            "Epoch 4470/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9073\n",
            "Epoch 4470: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1770 - accuracy: 0.9073 - val_loss: 3.7792 - val_accuracy: 0.5290\n",
            "Epoch 4471/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1727 - accuracy: 0.9071\n",
            "Epoch 4471: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1717 - accuracy: 0.9068 - val_loss: 3.7961 - val_accuracy: 0.5310\n",
            "Epoch 4472/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9097\n",
            "Epoch 4472: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1699 - accuracy: 0.9097 - val_loss: 3.9085 - val_accuracy: 0.5278\n",
            "Epoch 4473/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1776 - accuracy: 0.9059\n",
            "Epoch 4473: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1793 - accuracy: 0.9047 - val_loss: 3.7489 - val_accuracy: 0.5249\n",
            "Epoch 4474/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1765 - accuracy: 0.9063\n",
            "Epoch 4474: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1758 - accuracy: 0.9060 - val_loss: 3.8147 - val_accuracy: 0.5275\n",
            "Epoch 4475/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1761 - accuracy: 0.9067\n",
            "Epoch 4475: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1769 - accuracy: 0.9062 - val_loss: 3.7321 - val_accuracy: 0.5308\n",
            "Epoch 4476/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1749 - accuracy: 0.9068\n",
            "Epoch 4476: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1748 - accuracy: 0.9070 - val_loss: 3.8218 - val_accuracy: 0.5325\n",
            "Epoch 4477/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9094\n",
            "Epoch 4477: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1712 - accuracy: 0.9094 - val_loss: 3.8354 - val_accuracy: 0.5316\n",
            "Epoch 4478/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.9081\n",
            "Epoch 4478: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1743 - accuracy: 0.9062 - val_loss: 3.9809 - val_accuracy: 0.5187\n",
            "Epoch 4479/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1824 - accuracy: 0.9030\n",
            "Epoch 4479: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1820 - accuracy: 0.9032 - val_loss: 3.7799 - val_accuracy: 0.5290\n",
            "Epoch 4480/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1860 - accuracy: 0.9036\n",
            "Epoch 4480: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1851 - accuracy: 0.9039 - val_loss: 3.7203 - val_accuracy: 0.5319\n",
            "Epoch 4481/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9074\n",
            "Epoch 4481: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1772 - accuracy: 0.9074 - val_loss: 3.8298 - val_accuracy: 0.5234\n",
            "Epoch 4482/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9065\n",
            "Epoch 4482: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1755 - accuracy: 0.9065 - val_loss: 3.8415 - val_accuracy: 0.5299\n",
            "Epoch 4483/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9109\n",
            "Epoch 4483: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1726 - accuracy: 0.9096 - val_loss: 3.8067 - val_accuracy: 0.5290\n",
            "Epoch 4484/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1730 - accuracy: 0.9101\n",
            "Epoch 4484: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1733 - accuracy: 0.9095 - val_loss: 3.8552 - val_accuracy: 0.5269\n",
            "Epoch 4485/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1770 - accuracy: 0.9057\n",
            "Epoch 4485: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1791 - accuracy: 0.9044 - val_loss: 3.7778 - val_accuracy: 0.5337\n",
            "Epoch 4486/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9044\n",
            "Epoch 4486: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1757 - accuracy: 0.9044 - val_loss: 3.7311 - val_accuracy: 0.5343\n",
            "Epoch 4487/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1753 - accuracy: 0.9089\n",
            "Epoch 4487: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1758 - accuracy: 0.9081 - val_loss: 3.8327 - val_accuracy: 0.5261\n",
            "Epoch 4488/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9073\n",
            "Epoch 4488: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1742 - accuracy: 0.9073 - val_loss: 3.7399 - val_accuracy: 0.5305\n",
            "Epoch 4489/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.9054\n",
            "Epoch 4489: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1769 - accuracy: 0.9036 - val_loss: 3.7416 - val_accuracy: 0.5381\n",
            "Epoch 4490/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1814 - accuracy: 0.9082\n",
            "Epoch 4490: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1829 - accuracy: 0.9065 - val_loss: 3.7898 - val_accuracy: 0.5220\n",
            "Epoch 4491/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9030\n",
            "Epoch 4491: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1812 - accuracy: 0.9034 - val_loss: 3.7534 - val_accuracy: 0.5346\n",
            "Epoch 4492/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1855 - accuracy: 0.9028\n",
            "Epoch 4492: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1875 - accuracy: 0.9016 - val_loss: 3.6556 - val_accuracy: 0.5445\n",
            "Epoch 4493/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1851 - accuracy: 0.9030\n",
            "Epoch 4493: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1851 - accuracy: 0.9016 - val_loss: 3.7820 - val_accuracy: 0.5351\n",
            "Epoch 4494/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1779 - accuracy: 0.9038\n",
            "Epoch 4494: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1790 - accuracy: 0.9030 - val_loss: 3.7481 - val_accuracy: 0.5401\n",
            "Epoch 4495/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1905 - accuracy: 0.8999\n",
            "Epoch 4495: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1906 - accuracy: 0.8998 - val_loss: 3.6569 - val_accuracy: 0.5281\n",
            "Epoch 4496/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1792 - accuracy: 0.9052\n",
            "Epoch 4496: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1816 - accuracy: 0.9030 - val_loss: 3.8000 - val_accuracy: 0.5328\n",
            "Epoch 4497/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1830 - accuracy: 0.9034\n",
            "Epoch 4497: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9027 - val_loss: 3.8119 - val_accuracy: 0.5281\n",
            "Epoch 4498/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9058\n",
            "Epoch 4498: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1801 - accuracy: 0.9058 - val_loss: 3.7329 - val_accuracy: 0.5308\n",
            "Epoch 4499/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9068\n",
            "Epoch 4499: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1737 - accuracy: 0.9068 - val_loss: 3.9620 - val_accuracy: 0.5246\n",
            "Epoch 4500/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9032\n",
            "Epoch 4500: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1814 - accuracy: 0.9032 - val_loss: 3.7203 - val_accuracy: 0.5296\n",
            "Epoch 4501/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1734 - accuracy: 0.9082\n",
            "Epoch 4501: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1737 - accuracy: 0.9082 - val_loss: 4.0084 - val_accuracy: 0.5275\n",
            "Epoch 4502/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9061\n",
            "Epoch 4502: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1813 - accuracy: 0.9041 - val_loss: 3.8518 - val_accuracy: 0.5269\n",
            "Epoch 4503/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1703 - accuracy: 0.9085\n",
            "Epoch 4503: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1727 - accuracy: 0.9060 - val_loss: 3.8518 - val_accuracy: 0.5313\n",
            "Epoch 4504/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1782 - accuracy: 0.9055\n",
            "Epoch 4504: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1788 - accuracy: 0.9051 - val_loss: 3.7756 - val_accuracy: 0.5261\n",
            "Epoch 4505/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1779 - accuracy: 0.9043\n",
            "Epoch 4505: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1777 - accuracy: 0.9045 - val_loss: 3.7876 - val_accuracy: 0.5226\n",
            "Epoch 4506/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9067\n",
            "Epoch 4506: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1712 - accuracy: 0.9067 - val_loss: 3.8749 - val_accuracy: 0.5281\n",
            "Epoch 4507/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9065\n",
            "Epoch 4507: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1717 - accuracy: 0.9065 - val_loss: 3.8878 - val_accuracy: 0.5287\n",
            "Epoch 4508/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1777 - accuracy: 0.9063\n",
            "Epoch 4508: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1781 - accuracy: 0.9061 - val_loss: 3.8529 - val_accuracy: 0.5305\n",
            "Epoch 4509/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1753 - accuracy: 0.9055\n",
            "Epoch 4509: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1755 - accuracy: 0.9053 - val_loss: 3.9737 - val_accuracy: 0.5357\n",
            "Epoch 4510/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9046\n",
            "Epoch 4510: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1746 - accuracy: 0.9046 - val_loss: 3.9236 - val_accuracy: 0.5205\n",
            "Epoch 4511/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1724 - accuracy: 0.9062\n",
            "Epoch 4511: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1727 - accuracy: 0.9060 - val_loss: 3.7775 - val_accuracy: 0.5346\n",
            "Epoch 4512/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1715 - accuracy: 0.9078\n",
            "Epoch 4512: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1710 - accuracy: 0.9085 - val_loss: 3.8496 - val_accuracy: 0.5369\n",
            "Epoch 4513/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1780 - accuracy: 0.9079\n",
            "Epoch 4513: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1782 - accuracy: 0.9076 - val_loss: 3.8354 - val_accuracy: 0.5319\n",
            "Epoch 4514/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9006\n",
            "Epoch 4514: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1855 - accuracy: 0.9006 - val_loss: 3.7261 - val_accuracy: 0.5349\n",
            "Epoch 4515/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9058\n",
            "Epoch 4515: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1775 - accuracy: 0.9058 - val_loss: 3.8639 - val_accuracy: 0.5211\n",
            "Epoch 4516/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1899 - accuracy: 0.9021\n",
            "Epoch 4516: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1917 - accuracy: 0.9007 - val_loss: 3.8970 - val_accuracy: 0.5272\n",
            "Epoch 4517/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9021\n",
            "Epoch 4517: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1860 - accuracy: 0.9021 - val_loss: 3.7947 - val_accuracy: 0.5246\n",
            "Epoch 4518/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9076\n",
            "Epoch 4518: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1748 - accuracy: 0.9076 - val_loss: 3.8301 - val_accuracy: 0.5302\n",
            "Epoch 4519/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9056\n",
            "Epoch 4519: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1825 - accuracy: 0.9046 - val_loss: 3.8090 - val_accuracy: 0.5217\n",
            "Epoch 4520/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9025\n",
            "Epoch 4520: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1875 - accuracy: 0.9025 - val_loss: 3.8053 - val_accuracy: 0.5346\n",
            "Epoch 4521/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9007\n",
            "Epoch 4521: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1874 - accuracy: 0.9007 - val_loss: 3.7498 - val_accuracy: 0.5287\n",
            "Epoch 4522/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9075\n",
            "Epoch 4522: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1790 - accuracy: 0.9068 - val_loss: 3.7035 - val_accuracy: 0.5264\n",
            "Epoch 4523/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9086\n",
            "Epoch 4523: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1744 - accuracy: 0.9086 - val_loss: 3.7504 - val_accuracy: 0.5302\n",
            "Epoch 4524/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1757 - accuracy: 0.9056\n",
            "Epoch 4524: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1785 - accuracy: 0.9038 - val_loss: 3.7184 - val_accuracy: 0.5208\n",
            "Epoch 4525/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1745 - accuracy: 0.9057\n",
            "Epoch 4525: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1775 - accuracy: 0.9038 - val_loss: 3.7216 - val_accuracy: 0.5267\n",
            "Epoch 4526/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1739 - accuracy: 0.9085\n",
            "Epoch 4526: loss did not improve from 0.16745\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1742 - accuracy: 0.9078 - val_loss: 3.9367 - val_accuracy: 0.5313\n",
            "Epoch 4527/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1655 - accuracy: 0.9134\n",
            "Epoch 4527: loss improved from 0.16745 to 0.16704, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.1670 - accuracy: 0.9122 - val_loss: 3.7847 - val_accuracy: 0.5284\n",
            "Epoch 4528/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9085\n",
            "Epoch 4528: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1718 - accuracy: 0.9085 - val_loss: 3.8607 - val_accuracy: 0.5287\n",
            "Epoch 4529/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1724 - accuracy: 0.9082\n",
            "Epoch 4529: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1762 - accuracy: 0.9061 - val_loss: 3.7920 - val_accuracy: 0.5340\n",
            "Epoch 4530/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1731 - accuracy: 0.9085\n",
            "Epoch 4530: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1735 - accuracy: 0.9083 - val_loss: 3.8710 - val_accuracy: 0.5287\n",
            "Epoch 4531/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1716 - accuracy: 0.9105\n",
            "Epoch 4531: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1751 - accuracy: 0.9073 - val_loss: 3.9377 - val_accuracy: 0.5252\n",
            "Epoch 4532/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1760 - accuracy: 0.9060\n",
            "Epoch 4532: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1794 - accuracy: 0.9038 - val_loss: 3.8291 - val_accuracy: 0.5246\n",
            "Epoch 4533/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1837 - accuracy: 0.9022\n",
            "Epoch 4533: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1847 - accuracy: 0.9012 - val_loss: 3.8011 - val_accuracy: 0.5363\n",
            "Epoch 4534/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1818 - accuracy: 0.9013\n",
            "Epoch 4534: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1818 - accuracy: 0.9013 - val_loss: 3.7513 - val_accuracy: 0.5278\n",
            "Epoch 4535/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.9089\n",
            "Epoch 4535: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1732 - accuracy: 0.9074 - val_loss: 3.9079 - val_accuracy: 0.5325\n",
            "Epoch 4536/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1892 - accuracy: 0.9007\n",
            "Epoch 4536: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1889 - accuracy: 0.9007 - val_loss: 3.8587 - val_accuracy: 0.5343\n",
            "Epoch 4537/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1810 - accuracy: 0.9050\n",
            "Epoch 4537: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1813 - accuracy: 0.9047 - val_loss: 3.7477 - val_accuracy: 0.5354\n",
            "Epoch 4538/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1754 - accuracy: 0.9051\n",
            "Epoch 4538: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1780 - accuracy: 0.9034 - val_loss: 3.8939 - val_accuracy: 0.5255\n",
            "Epoch 4539/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9068\n",
            "Epoch 4539: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1801 - accuracy: 0.9068 - val_loss: 3.8474 - val_accuracy: 0.5384\n",
            "Epoch 4540/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1751 - accuracy: 0.9078\n",
            "Epoch 4540: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1755 - accuracy: 0.9071 - val_loss: 3.8227 - val_accuracy: 0.5328\n",
            "Epoch 4541/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1722 - accuracy: 0.9066\n",
            "Epoch 4541: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1725 - accuracy: 0.9065 - val_loss: 3.9001 - val_accuracy: 0.5322\n",
            "Epoch 4542/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.9075\n",
            "Epoch 4542: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1744 - accuracy: 0.9068 - val_loss: 3.8337 - val_accuracy: 0.5272\n",
            "Epoch 4543/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9099\n",
            "Epoch 4543: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1696 - accuracy: 0.9099 - val_loss: 3.8624 - val_accuracy: 0.5246\n",
            "Epoch 4544/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1761 - accuracy: 0.9067\n",
            "Epoch 4544: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1760 - accuracy: 0.9067 - val_loss: 3.6610 - val_accuracy: 0.5278\n",
            "Epoch 4545/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1837 - accuracy: 0.9033\n",
            "Epoch 4545: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1824 - accuracy: 0.9043 - val_loss: 3.8417 - val_accuracy: 0.5231\n",
            "Epoch 4546/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1822 - accuracy: 0.9035\n",
            "Epoch 4546: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1825 - accuracy: 0.9028 - val_loss: 3.7960 - val_accuracy: 0.5328\n",
            "Epoch 4547/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1824 - accuracy: 0.9030\n",
            "Epoch 4547: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1870 - accuracy: 0.8998 - val_loss: 3.9716 - val_accuracy: 0.5234\n",
            "Epoch 4548/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9002\n",
            "Epoch 4548: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1892 - accuracy: 0.9002 - val_loss: 3.6843 - val_accuracy: 0.5325\n",
            "Epoch 4549/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9062\n",
            "Epoch 4549: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1814 - accuracy: 0.9055 - val_loss: 4.0165 - val_accuracy: 0.5228\n",
            "Epoch 4550/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1757 - accuracy: 0.9070\n",
            "Epoch 4550: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1760 - accuracy: 0.9062 - val_loss: 3.8293 - val_accuracy: 0.5319\n",
            "Epoch 4551/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1762 - accuracy: 0.9066\n",
            "Epoch 4551: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1763 - accuracy: 0.9065 - val_loss: 3.8883 - val_accuracy: 0.5234\n",
            "Epoch 4552/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1830 - accuracy: 0.9049\n",
            "Epoch 4552: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1836 - accuracy: 0.9043 - val_loss: 3.8773 - val_accuracy: 0.5363\n",
            "Epoch 4553/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1786 - accuracy: 0.9059\n",
            "Epoch 4553: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1792 - accuracy: 0.9057 - val_loss: 3.7630 - val_accuracy: 0.5214\n",
            "Epoch 4554/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1798 - accuracy: 0.9048\n",
            "Epoch 4554: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1794 - accuracy: 0.9037 - val_loss: 3.7584 - val_accuracy: 0.5343\n",
            "Epoch 4555/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9087\n",
            "Epoch 4555: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1701 - accuracy: 0.9087 - val_loss: 3.8148 - val_accuracy: 0.5313\n",
            "Epoch 4556/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1728 - accuracy: 0.9092\n",
            "Epoch 4556: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1736 - accuracy: 0.9069 - val_loss: 3.8243 - val_accuracy: 0.5340\n",
            "Epoch 4557/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1763 - accuracy: 0.9050\n",
            "Epoch 4557: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1767 - accuracy: 0.9051 - val_loss: 3.7187 - val_accuracy: 0.5278\n",
            "Epoch 4558/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9065\n",
            "Epoch 4558: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1792 - accuracy: 0.9054 - val_loss: 3.7120 - val_accuracy: 0.5305\n",
            "Epoch 4559/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1700 - accuracy: 0.9111\n",
            "Epoch 4559: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1718 - accuracy: 0.9101 - val_loss: 3.8315 - val_accuracy: 0.5293\n",
            "Epoch 4560/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1683 - accuracy: 0.9100\n",
            "Epoch 4560: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1685 - accuracy: 0.9099 - val_loss: 3.7898 - val_accuracy: 0.5313\n",
            "Epoch 4561/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1788 - accuracy: 0.9034\n",
            "Epoch 4561: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1783 - accuracy: 0.9037 - val_loss: 3.7921 - val_accuracy: 0.5231\n",
            "Epoch 4562/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9051\n",
            "Epoch 4562: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1739 - accuracy: 0.9051 - val_loss: 3.7833 - val_accuracy: 0.5319\n",
            "Epoch 4563/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9052\n",
            "Epoch 4563: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1740 - accuracy: 0.9052 - val_loss: 3.9836 - val_accuracy: 0.5267\n",
            "Epoch 4564/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1751 - accuracy: 0.9055\n",
            "Epoch 4564: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1751 - accuracy: 0.9054 - val_loss: 3.8442 - val_accuracy: 0.5305\n",
            "Epoch 4565/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1794 - accuracy: 0.9036\n",
            "Epoch 4565: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1786 - accuracy: 0.9040 - val_loss: 3.7637 - val_accuracy: 0.5308\n",
            "Epoch 4566/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1703 - accuracy: 0.9090\n",
            "Epoch 4566: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1708 - accuracy: 0.9087 - val_loss: 3.7690 - val_accuracy: 0.5419\n",
            "Epoch 4567/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1772 - accuracy: 0.9068\n",
            "Epoch 4567: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1787 - accuracy: 0.9059 - val_loss: 3.7537 - val_accuracy: 0.5331\n",
            "Epoch 4568/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9076\n",
            "Epoch 4568: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1819 - accuracy: 0.9076 - val_loss: 3.7988 - val_accuracy: 0.5351\n",
            "Epoch 4569/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1789 - accuracy: 0.9060\n",
            "Epoch 4569: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1797 - accuracy: 0.9054 - val_loss: 3.8233 - val_accuracy: 0.5296\n",
            "Epoch 4570/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1940 - accuracy: 0.8988\n",
            "Epoch 4570: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1934 - accuracy: 0.8991 - val_loss: 3.8872 - val_accuracy: 0.5208\n",
            "Epoch 4571/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1751 - accuracy: 0.9061\n",
            "Epoch 4571: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1750 - accuracy: 0.9063 - val_loss: 3.7434 - val_accuracy: 0.5354\n",
            "Epoch 4572/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1750 - accuracy: 0.9075\n",
            "Epoch 4572: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1771 - accuracy: 0.9061 - val_loss: 3.9134 - val_accuracy: 0.5302\n",
            "Epoch 4573/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1772 - accuracy: 0.9093\n",
            "Epoch 4573: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1791 - accuracy: 0.9081 - val_loss: 3.9497 - val_accuracy: 0.5343\n",
            "Epoch 4574/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1867 - accuracy: 0.9043\n",
            "Epoch 4574: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1881 - accuracy: 0.9027 - val_loss: 3.8842 - val_accuracy: 0.5337\n",
            "Epoch 4575/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1827 - accuracy: 0.9030\n",
            "Epoch 4575: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1830 - accuracy: 0.9028 - val_loss: 3.8735 - val_accuracy: 0.5363\n",
            "Epoch 4576/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1957 - accuracy: 0.8992\n",
            "Epoch 4576: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1953 - accuracy: 0.8997 - val_loss: 3.5689 - val_accuracy: 0.5164\n",
            "Epoch 4577/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.8984\n",
            "Epoch 4577: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1932 - accuracy: 0.8984 - val_loss: 4.0270 - val_accuracy: 0.5305\n",
            "Epoch 4578/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1768 - accuracy: 0.9064\n",
            "Epoch 4578: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1787 - accuracy: 0.9051 - val_loss: 3.7887 - val_accuracy: 0.5267\n",
            "Epoch 4579/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1780 - accuracy: 0.9068\n",
            "Epoch 4579: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1801 - accuracy: 0.9048 - val_loss: 3.9766 - val_accuracy: 0.5220\n",
            "Epoch 4580/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9038\n",
            "Epoch 4580: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1812 - accuracy: 0.9038 - val_loss: 3.9119 - val_accuracy: 0.5278\n",
            "Epoch 4581/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9040\n",
            "Epoch 4581: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1761 - accuracy: 0.9045 - val_loss: 3.7307 - val_accuracy: 0.5343\n",
            "Epoch 4582/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1806 - accuracy: 0.9072\n",
            "Epoch 4582: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1816 - accuracy: 0.9051 - val_loss: 3.6503 - val_accuracy: 0.5287\n",
            "Epoch 4583/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1755 - accuracy: 0.9076\n",
            "Epoch 4583: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1774 - accuracy: 0.9062 - val_loss: 3.8766 - val_accuracy: 0.5299\n",
            "Epoch 4584/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1900 - accuracy: 0.9000\n",
            "Epoch 4584: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1899 - accuracy: 0.8996 - val_loss: 3.7498 - val_accuracy: 0.5349\n",
            "Epoch 4585/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9032\n",
            "Epoch 4585: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1798 - accuracy: 0.9032 - val_loss: 3.7007 - val_accuracy: 0.5305\n",
            "Epoch 4586/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1816 - accuracy: 0.9033\n",
            "Epoch 4586: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1817 - accuracy: 0.9031 - val_loss: 3.8149 - val_accuracy: 0.5331\n",
            "Epoch 4587/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9065\n",
            "Epoch 4587: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1730 - accuracy: 0.9065 - val_loss: 3.9098 - val_accuracy: 0.5220\n",
            "Epoch 4588/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1799 - accuracy: 0.9059\n",
            "Epoch 4588: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1797 - accuracy: 0.9062 - val_loss: 3.7348 - val_accuracy: 0.5281\n",
            "Epoch 4589/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1708 - accuracy: 0.9090\n",
            "Epoch 4589: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1722 - accuracy: 0.9076 - val_loss: 3.8086 - val_accuracy: 0.5334\n",
            "Epoch 4590/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1723 - accuracy: 0.9066\n",
            "Epoch 4590: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1721 - accuracy: 0.9065 - val_loss: 3.8918 - val_accuracy: 0.5278\n",
            "Epoch 4591/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1815 - accuracy: 0.9045\n",
            "Epoch 4591: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1822 - accuracy: 0.9048 - val_loss: 3.8239 - val_accuracy: 0.5228\n",
            "Epoch 4592/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1782 - accuracy: 0.9049\n",
            "Epoch 4592: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1783 - accuracy: 0.9045 - val_loss: 3.8137 - val_accuracy: 0.5346\n",
            "Epoch 4593/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9059\n",
            "Epoch 4593: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1785 - accuracy: 0.9059 - val_loss: 3.8332 - val_accuracy: 0.5322\n",
            "Epoch 4594/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1715 - accuracy: 0.9065\n",
            "Epoch 4594: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1719 - accuracy: 0.9062 - val_loss: 3.9287 - val_accuracy: 0.5313\n",
            "Epoch 4595/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1752 - accuracy: 0.9084\n",
            "Epoch 4595: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1761 - accuracy: 0.9079 - val_loss: 3.7854 - val_accuracy: 0.5272\n",
            "Epoch 4596/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9018\n",
            "Epoch 4596: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1852 - accuracy: 0.9018 - val_loss: 3.7082 - val_accuracy: 0.5252\n",
            "Epoch 4597/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9065\n",
            "Epoch 4597: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1809 - accuracy: 0.9051 - val_loss: 3.7932 - val_accuracy: 0.5287\n",
            "Epoch 4598/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1775 - accuracy: 0.9056\n",
            "Epoch 4598: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1778 - accuracy: 0.9052 - val_loss: 3.6747 - val_accuracy: 0.5299\n",
            "Epoch 4599/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9059\n",
            "Epoch 4599: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1756 - accuracy: 0.9059 - val_loss: 3.8221 - val_accuracy: 0.5190\n",
            "Epoch 4600/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1734 - accuracy: 0.9110\n",
            "Epoch 4600: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1751 - accuracy: 0.9096 - val_loss: 3.7548 - val_accuracy: 0.5349\n",
            "Epoch 4601/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9054\n",
            "Epoch 4601: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1787 - accuracy: 0.9054 - val_loss: 3.9673 - val_accuracy: 0.5296\n",
            "Epoch 4602/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.9054\n",
            "Epoch 4602: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1753 - accuracy: 0.9053 - val_loss: 3.7839 - val_accuracy: 0.5264\n",
            "Epoch 4603/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1745 - accuracy: 0.9099\n",
            "Epoch 4603: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1764 - accuracy: 0.9086 - val_loss: 3.7688 - val_accuracy: 0.5319\n",
            "Epoch 4604/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1873 - accuracy: 0.9016\n",
            "Epoch 4604: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1876 - accuracy: 0.9017 - val_loss: 3.8891 - val_accuracy: 0.5275\n",
            "Epoch 4605/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9028\n",
            "Epoch 4605: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1806 - accuracy: 0.9022 - val_loss: 3.7893 - val_accuracy: 0.5284\n",
            "Epoch 4606/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1719 - accuracy: 0.9071\n",
            "Epoch 4606: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1751 - accuracy: 0.9052 - val_loss: 3.6788 - val_accuracy: 0.5366\n",
            "Epoch 4607/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.9065\n",
            "Epoch 4607: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1761 - accuracy: 0.9058 - val_loss: 3.7791 - val_accuracy: 0.5237\n",
            "Epoch 4608/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1850 - accuracy: 0.9036\n",
            "Epoch 4608: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1851 - accuracy: 0.9035 - val_loss: 3.6720 - val_accuracy: 0.5272\n",
            "Epoch 4609/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1838 - accuracy: 0.9030\n",
            "Epoch 4609: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1847 - accuracy: 0.9029 - val_loss: 3.7804 - val_accuracy: 0.5275\n",
            "Epoch 4610/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1853 - accuracy: 0.9044\n",
            "Epoch 4610: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1862 - accuracy: 0.9031 - val_loss: 3.7428 - val_accuracy: 0.5313\n",
            "Epoch 4611/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1817 - accuracy: 0.9036\n",
            "Epoch 4611: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1813 - accuracy: 0.9036 - val_loss: 3.6988 - val_accuracy: 0.5375\n",
            "Epoch 4612/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.9092\n",
            "Epoch 4612: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1755 - accuracy: 0.9087 - val_loss: 4.0605 - val_accuracy: 0.5234\n",
            "Epoch 4613/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9059\n",
            "Epoch 4613: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1793 - accuracy: 0.9059 - val_loss: 3.8477 - val_accuracy: 0.5246\n",
            "Epoch 4614/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1906 - accuracy: 0.8994\n",
            "Epoch 4614: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1910 - accuracy: 0.8983 - val_loss: 3.8349 - val_accuracy: 0.5302\n",
            "Epoch 4615/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1779 - accuracy: 0.9070\n",
            "Epoch 4615: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1788 - accuracy: 0.9061 - val_loss: 4.0118 - val_accuracy: 0.5313\n",
            "Epoch 4616/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1742 - accuracy: 0.9076\n",
            "Epoch 4616: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1738 - accuracy: 0.9076 - val_loss: 4.0177 - val_accuracy: 0.5185\n",
            "Epoch 4617/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1737 - accuracy: 0.9094\n",
            "Epoch 4617: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1753 - accuracy: 0.9081 - val_loss: 3.8177 - val_accuracy: 0.5308\n",
            "Epoch 4618/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1783 - accuracy: 0.9029\n",
            "Epoch 4618: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1773 - accuracy: 0.9035 - val_loss: 3.7517 - val_accuracy: 0.5401\n",
            "Epoch 4619/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1787 - accuracy: 0.9041\n",
            "Epoch 4619: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1810 - accuracy: 0.9027 - val_loss: 3.8888 - val_accuracy: 0.5363\n",
            "Epoch 4620/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1820 - accuracy: 0.9038\n",
            "Epoch 4620: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9029 - val_loss: 3.8304 - val_accuracy: 0.5346\n",
            "Epoch 4621/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1891 - accuracy: 0.9004\n",
            "Epoch 4621: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1883 - accuracy: 0.9010 - val_loss: 3.8929 - val_accuracy: 0.5258\n",
            "Epoch 4622/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9025\n",
            "Epoch 4622: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1824 - accuracy: 0.9025 - val_loss: 3.8676 - val_accuracy: 0.5310\n",
            "Epoch 4623/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1789 - accuracy: 0.9066\n",
            "Epoch 4623: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1800 - accuracy: 0.9057 - val_loss: 3.8065 - val_accuracy: 0.5190\n",
            "Epoch 4624/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1742 - accuracy: 0.9075\n",
            "Epoch 4624: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1760 - accuracy: 0.9062 - val_loss: 3.8750 - val_accuracy: 0.5349\n",
            "Epoch 4625/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9098\n",
            "Epoch 4625: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1699 - accuracy: 0.9098 - val_loss: 3.8206 - val_accuracy: 0.5313\n",
            "Epoch 4626/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1830 - accuracy: 0.9044\n",
            "Epoch 4626: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1826 - accuracy: 0.9046 - val_loss: 4.0236 - val_accuracy: 0.5217\n",
            "Epoch 4627/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9070\n",
            "Epoch 4627: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1790 - accuracy: 0.9070 - val_loss: 3.8481 - val_accuracy: 0.5267\n",
            "Epoch 4628/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.9027\n",
            "Epoch 4628: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1809 - accuracy: 0.9027 - val_loss: 3.7367 - val_accuracy: 0.5287\n",
            "Epoch 4629/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9058\n",
            "Epoch 4629: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1795 - accuracy: 0.9063 - val_loss: 3.8196 - val_accuracy: 0.5296\n",
            "Epoch 4630/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.9068\n",
            "Epoch 4630: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1721 - accuracy: 0.9073 - val_loss: 3.7455 - val_accuracy: 0.5264\n",
            "Epoch 4631/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1732 - accuracy: 0.9080\n",
            "Epoch 4631: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1745 - accuracy: 0.9073 - val_loss: 3.8069 - val_accuracy: 0.5340\n",
            "Epoch 4632/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1804 - accuracy: 0.9033\n",
            "Epoch 4632: loss did not improve from 0.16704\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1812 - accuracy: 0.9027 - val_loss: 3.9225 - val_accuracy: 0.5278\n",
            "Epoch 4633/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.9130\n",
            "Epoch 4633: loss improved from 0.16704 to 0.16604, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 2s 63ms/step - loss: 0.1660 - accuracy: 0.9108 - val_loss: 3.9369 - val_accuracy: 0.5290\n",
            "Epoch 4634/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1706 - accuracy: 0.9100\n",
            "Epoch 4634: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1710 - accuracy: 0.9100 - val_loss: 3.9523 - val_accuracy: 0.5217\n",
            "Epoch 4635/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1686 - accuracy: 0.9088\n",
            "Epoch 4635: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1706 - accuracy: 0.9078 - val_loss: 4.0020 - val_accuracy: 0.5269\n",
            "Epoch 4636/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1739 - accuracy: 0.9071\n",
            "Epoch 4636: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1740 - accuracy: 0.9068 - val_loss: 3.7962 - val_accuracy: 0.5237\n",
            "Epoch 4637/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9078\n",
            "Epoch 4637: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1747 - accuracy: 0.9078 - val_loss: 3.9705 - val_accuracy: 0.5249\n",
            "Epoch 4638/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1728 - accuracy: 0.9102\n",
            "Epoch 4638: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1732 - accuracy: 0.9100 - val_loss: 3.8019 - val_accuracy: 0.5337\n",
            "Epoch 4639/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1756 - accuracy: 0.9067\n",
            "Epoch 4639: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1758 - accuracy: 0.9065 - val_loss: 3.9456 - val_accuracy: 0.5346\n",
            "Epoch 4640/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1753 - accuracy: 0.9054\n",
            "Epoch 4640: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1764 - accuracy: 0.9057 - val_loss: 3.7834 - val_accuracy: 0.5231\n",
            "Epoch 4641/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1674 - accuracy: 0.9104\n",
            "Epoch 4641: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1664 - accuracy: 0.9093 - val_loss: 3.9148 - val_accuracy: 0.5278\n",
            "Epoch 4642/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.9096\n",
            "Epoch 4642: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1753 - accuracy: 0.9084 - val_loss: 3.8948 - val_accuracy: 0.5343\n",
            "Epoch 4643/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1735 - accuracy: 0.9100\n",
            "Epoch 4643: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1743 - accuracy: 0.9096 - val_loss: 3.8420 - val_accuracy: 0.5296\n",
            "Epoch 4644/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1877 - accuracy: 0.9057\n",
            "Epoch 4644: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1890 - accuracy: 0.9045 - val_loss: 3.7276 - val_accuracy: 0.5284\n",
            "Epoch 4645/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1758 - accuracy: 0.9083\n",
            "Epoch 4645: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1767 - accuracy: 0.9073 - val_loss: 3.8933 - val_accuracy: 0.5275\n",
            "Epoch 4646/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1845 - accuracy: 0.9039\n",
            "Epoch 4646: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1846 - accuracy: 0.9040 - val_loss: 3.8509 - val_accuracy: 0.5269\n",
            "Epoch 4647/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9087\n",
            "Epoch 4647: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1730 - accuracy: 0.9087 - val_loss: 3.8374 - val_accuracy: 0.5196\n",
            "Epoch 4648/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1754 - accuracy: 0.9061\n",
            "Epoch 4648: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1782 - accuracy: 0.9037 - val_loss: 3.7641 - val_accuracy: 0.5261\n",
            "Epoch 4649/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1845 - accuracy: 0.9054\n",
            "Epoch 4649: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1862 - accuracy: 0.9041 - val_loss: 3.7509 - val_accuracy: 0.5296\n",
            "Epoch 4650/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9086\n",
            "Epoch 4650: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1804 - accuracy: 0.9060 - val_loss: 3.8457 - val_accuracy: 0.5290\n",
            "Epoch 4651/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1799 - accuracy: 0.9056\n",
            "Epoch 4651: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1791 - accuracy: 0.9056 - val_loss: 3.8245 - val_accuracy: 0.5322\n",
            "Epoch 4652/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9055\n",
            "Epoch 4652: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1764 - accuracy: 0.9055 - val_loss: 3.8302 - val_accuracy: 0.5243\n",
            "Epoch 4653/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1725 - accuracy: 0.9093\n",
            "Epoch 4653: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1749 - accuracy: 0.9070 - val_loss: 3.8454 - val_accuracy: 0.5281\n",
            "Epoch 4654/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1784 - accuracy: 0.9069\n",
            "Epoch 4654: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1804 - accuracy: 0.9051 - val_loss: 3.9679 - val_accuracy: 0.5272\n",
            "Epoch 4655/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9070\n",
            "Epoch 4655: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1715 - accuracy: 0.9070 - val_loss: 3.9105 - val_accuracy: 0.5328\n",
            "Epoch 4656/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9090\n",
            "Epoch 4656: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1677 - accuracy: 0.9090 - val_loss: 3.7996 - val_accuracy: 0.5278\n",
            "Epoch 4657/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.9086\n",
            "Epoch 4657: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1769 - accuracy: 0.9068 - val_loss: 3.7714 - val_accuracy: 0.5308\n",
            "Epoch 4658/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1743 - accuracy: 0.9089\n",
            "Epoch 4658: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 16ms/step - loss: 0.1754 - accuracy: 0.9080 - val_loss: 3.9802 - val_accuracy: 0.5337\n",
            "Epoch 4659/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1754 - accuracy: 0.9074\n",
            "Epoch 4659: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1760 - accuracy: 0.9067 - val_loss: 3.9324 - val_accuracy: 0.5217\n",
            "Epoch 4660/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9088\n",
            "Epoch 4660: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1719 - accuracy: 0.9088 - val_loss: 3.9502 - val_accuracy: 0.5313\n",
            "Epoch 4661/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1797 - accuracy: 0.9040\n",
            "Epoch 4661: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1801 - accuracy: 0.9038 - val_loss: 3.8089 - val_accuracy: 0.5226\n",
            "Epoch 4662/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1746 - accuracy: 0.9061\n",
            "Epoch 4662: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1755 - accuracy: 0.9054 - val_loss: 3.8418 - val_accuracy: 0.5357\n",
            "Epoch 4663/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1802 - accuracy: 0.9058\n",
            "Epoch 4663: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1806 - accuracy: 0.9055 - val_loss: 3.9677 - val_accuracy: 0.5319\n",
            "Epoch 4664/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1832 - accuracy: 0.9045\n",
            "Epoch 4664: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1835 - accuracy: 0.9043 - val_loss: 3.8419 - val_accuracy: 0.5246\n",
            "Epoch 4665/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9074\n",
            "Epoch 4665: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1809 - accuracy: 0.9061 - val_loss: 3.7606 - val_accuracy: 0.5343\n",
            "Epoch 4666/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1762 - accuracy: 0.9051\n",
            "Epoch 4666: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1760 - accuracy: 0.9049 - val_loss: 3.7897 - val_accuracy: 0.5284\n",
            "Epoch 4667/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1764 - accuracy: 0.9074\n",
            "Epoch 4667: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1767 - accuracy: 0.9073 - val_loss: 4.0467 - val_accuracy: 0.5190\n",
            "Epoch 4668/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1841 - accuracy: 0.9045\n",
            "Epoch 4668: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1853 - accuracy: 0.9038 - val_loss: 3.8669 - val_accuracy: 0.5313\n",
            "Epoch 4669/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1827 - accuracy: 0.9022\n",
            "Epoch 4669: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1830 - accuracy: 0.9020 - val_loss: 3.8735 - val_accuracy: 0.5275\n",
            "Epoch 4670/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1748 - accuracy: 0.9060\n",
            "Epoch 4670: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1754 - accuracy: 0.9055 - val_loss: 3.8561 - val_accuracy: 0.5246\n",
            "Epoch 4671/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1751 - accuracy: 0.9063\n",
            "Epoch 4671: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1765 - accuracy: 0.9056 - val_loss: 3.7502 - val_accuracy: 0.5275\n",
            "Epoch 4672/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.9060\n",
            "Epoch 4672: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1817 - accuracy: 0.9032 - val_loss: 3.9654 - val_accuracy: 0.5231\n",
            "Epoch 4673/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1874 - accuracy: 0.8999\n",
            "Epoch 4673: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1868 - accuracy: 0.9005 - val_loss: 3.7745 - val_accuracy: 0.5290\n",
            "Epoch 4674/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9088\n",
            "Epoch 4674: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1722 - accuracy: 0.9088 - val_loss: 3.8759 - val_accuracy: 0.5264\n",
            "Epoch 4675/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1663 - accuracy: 0.9102\n",
            "Epoch 4675: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1668 - accuracy: 0.9101 - val_loss: 3.8033 - val_accuracy: 0.5337\n",
            "Epoch 4676/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1683 - accuracy: 0.9097\n",
            "Epoch 4676: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1704 - accuracy: 0.9083 - val_loss: 3.8895 - val_accuracy: 0.5234\n",
            "Epoch 4677/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1863 - accuracy: 0.9027\n",
            "Epoch 4677: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1877 - accuracy: 0.9020 - val_loss: 3.8083 - val_accuracy: 0.5387\n",
            "Epoch 4678/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1900 - accuracy: 0.9008\n",
            "Epoch 4678: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1902 - accuracy: 0.9007 - val_loss: 3.8436 - val_accuracy: 0.5325\n",
            "Epoch 4679/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1784 - accuracy: 0.9048\n",
            "Epoch 4679: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1799 - accuracy: 0.9035 - val_loss: 3.8719 - val_accuracy: 0.5228\n",
            "Epoch 4680/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1772 - accuracy: 0.9042\n",
            "Epoch 4680: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1773 - accuracy: 0.9040 - val_loss: 3.8441 - val_accuracy: 0.5308\n",
            "Epoch 4681/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.9058\n",
            "Epoch 4681: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1759 - accuracy: 0.9055 - val_loss: 3.6400 - val_accuracy: 0.5313\n",
            "Epoch 4682/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1757 - accuracy: 0.9074\n",
            "Epoch 4682: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1766 - accuracy: 0.9062 - val_loss: 4.0532 - val_accuracy: 0.5240\n",
            "Epoch 4683/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1700 - accuracy: 0.9120\n",
            "Epoch 4683: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1711 - accuracy: 0.9111 - val_loss: 3.8927 - val_accuracy: 0.5310\n",
            "Epoch 4684/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1697 - accuracy: 0.9110\n",
            "Epoch 4684: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1718 - accuracy: 0.9096 - val_loss: 3.7016 - val_accuracy: 0.5313\n",
            "Epoch 4685/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.9046\n",
            "Epoch 4685: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1807 - accuracy: 0.9038 - val_loss: 3.9639 - val_accuracy: 0.5349\n",
            "Epoch 4686/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1714 - accuracy: 0.9113\n",
            "Epoch 4686: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1726 - accuracy: 0.9101 - val_loss: 3.7324 - val_accuracy: 0.5375\n",
            "Epoch 4687/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1834 - accuracy: 0.9033\n",
            "Epoch 4687: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1827 - accuracy: 0.9030 - val_loss: 3.8657 - val_accuracy: 0.5199\n",
            "Epoch 4688/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1790 - accuracy: 0.9076\n",
            "Epoch 4688: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1794 - accuracy: 0.9073 - val_loss: 3.7736 - val_accuracy: 0.5252\n",
            "Epoch 4689/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1903 - accuracy: 0.9031\n",
            "Epoch 4689: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1915 - accuracy: 0.9025 - val_loss: 3.8695 - val_accuracy: 0.5305\n",
            "Epoch 4690/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1808 - accuracy: 0.9054\n",
            "Epoch 4690: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1794 - accuracy: 0.9063 - val_loss: 3.8315 - val_accuracy: 0.5284\n",
            "Epoch 4691/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9055\n",
            "Epoch 4691: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1768 - accuracy: 0.9055 - val_loss: 3.8275 - val_accuracy: 0.5343\n",
            "Epoch 4692/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1725 - accuracy: 0.9101\n",
            "Epoch 4692: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1729 - accuracy: 0.9097 - val_loss: 3.8477 - val_accuracy: 0.5375\n",
            "Epoch 4693/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1810 - accuracy: 0.9058\n",
            "Epoch 4693: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1828 - accuracy: 0.9056 - val_loss: 3.7948 - val_accuracy: 0.5296\n",
            "Epoch 4694/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1725 - accuracy: 0.9082\n",
            "Epoch 4694: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1749 - accuracy: 0.9068 - val_loss: 3.9074 - val_accuracy: 0.5331\n",
            "Epoch 4695/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1766 - accuracy: 0.9064\n",
            "Epoch 4695: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1762 - accuracy: 0.9064 - val_loss: 3.8735 - val_accuracy: 0.5284\n",
            "Epoch 4696/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1703 - accuracy: 0.9098\n",
            "Epoch 4696: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1711 - accuracy: 0.9090 - val_loss: 3.7620 - val_accuracy: 0.5313\n",
            "Epoch 4697/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.9077\n",
            "Epoch 4697: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1761 - accuracy: 0.9068 - val_loss: 3.8106 - val_accuracy: 0.5267\n",
            "Epoch 4698/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.9055\n",
            "Epoch 4698: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1779 - accuracy: 0.9043 - val_loss: 3.8912 - val_accuracy: 0.5299\n",
            "Epoch 4699/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9066\n",
            "Epoch 4699: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1853 - accuracy: 0.9052 - val_loss: 3.7058 - val_accuracy: 0.5281\n",
            "Epoch 4700/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1819 - accuracy: 0.9042\n",
            "Epoch 4700: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1812 - accuracy: 0.9047 - val_loss: 3.8711 - val_accuracy: 0.5384\n",
            "Epoch 4701/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1798 - accuracy: 0.9045\n",
            "Epoch 4701: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1797 - accuracy: 0.9048 - val_loss: 3.7477 - val_accuracy: 0.5381\n",
            "Epoch 4702/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.9070\n",
            "Epoch 4702: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1772 - accuracy: 0.9071 - val_loss: 3.7798 - val_accuracy: 0.5278\n",
            "Epoch 4703/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1757 - accuracy: 0.9068\n",
            "Epoch 4703: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1770 - accuracy: 0.9057 - val_loss: 3.8111 - val_accuracy: 0.5272\n",
            "Epoch 4704/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1823 - accuracy: 0.9049\n",
            "Epoch 4704: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1833 - accuracy: 0.9038 - val_loss: 4.0371 - val_accuracy: 0.5208\n",
            "Epoch 4705/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1862 - accuracy: 0.9039\n",
            "Epoch 4705: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1871 - accuracy: 0.9038 - val_loss: 3.7696 - val_accuracy: 0.5322\n",
            "Epoch 4706/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1749 - accuracy: 0.9073\n",
            "Epoch 4706: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1754 - accuracy: 0.9070 - val_loss: 3.7632 - val_accuracy: 0.5261\n",
            "Epoch 4707/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1727 - accuracy: 0.9090\n",
            "Epoch 4707: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1730 - accuracy: 0.9079 - val_loss: 3.8562 - val_accuracy: 0.5322\n",
            "Epoch 4708/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1685 - accuracy: 0.9119\n",
            "Epoch 4708: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1687 - accuracy: 0.9117 - val_loss: 3.8805 - val_accuracy: 0.5190\n",
            "Epoch 4709/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1748 - accuracy: 0.9074\n",
            "Epoch 4709: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1768 - accuracy: 0.9057 - val_loss: 3.9338 - val_accuracy: 0.5278\n",
            "Epoch 4710/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1762 - accuracy: 0.9058\n",
            "Epoch 4710: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1767 - accuracy: 0.9051 - val_loss: 3.8395 - val_accuracy: 0.5313\n",
            "Epoch 4711/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.9094\n",
            "Epoch 4711: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1703 - accuracy: 0.9081 - val_loss: 3.8041 - val_accuracy: 0.5278\n",
            "Epoch 4712/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9083\n",
            "Epoch 4712: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1729 - accuracy: 0.9083 - val_loss: 4.0016 - val_accuracy: 0.5269\n",
            "Epoch 4713/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1766 - accuracy: 0.9075\n",
            "Epoch 4713: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1772 - accuracy: 0.9070 - val_loss: 3.9435 - val_accuracy: 0.5214\n",
            "Epoch 4714/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1883 - accuracy: 0.9039\n",
            "Epoch 4714: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1888 - accuracy: 0.9028 - val_loss: 3.8613 - val_accuracy: 0.5381\n",
            "Epoch 4715/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1784 - accuracy: 0.9051\n",
            "Epoch 4715: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1803 - accuracy: 0.9039 - val_loss: 3.8518 - val_accuracy: 0.5299\n",
            "Epoch 4716/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1808 - accuracy: 0.9087\n",
            "Epoch 4716: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1804 - accuracy: 0.9088 - val_loss: 3.8098 - val_accuracy: 0.5372\n",
            "Epoch 4717/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1740 - accuracy: 0.9103\n",
            "Epoch 4717: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1737 - accuracy: 0.9103 - val_loss: 3.8836 - val_accuracy: 0.5322\n",
            "Epoch 4718/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1734 - accuracy: 0.9086\n",
            "Epoch 4718: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1733 - accuracy: 0.9081 - val_loss: 3.8852 - val_accuracy: 0.5369\n",
            "Epoch 4719/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1667 - accuracy: 0.9110\n",
            "Epoch 4719: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1689 - accuracy: 0.9098 - val_loss: 3.8739 - val_accuracy: 0.5264\n",
            "Epoch 4720/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1782 - accuracy: 0.9066\n",
            "Epoch 4720: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1791 - accuracy: 0.9061 - val_loss: 3.8836 - val_accuracy: 0.5375\n",
            "Epoch 4721/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1729 - accuracy: 0.9081\n",
            "Epoch 4721: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1734 - accuracy: 0.9079 - val_loss: 3.9269 - val_accuracy: 0.5337\n",
            "Epoch 4722/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1805 - accuracy: 0.9036\n",
            "Epoch 4722: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1810 - accuracy: 0.9038 - val_loss: 3.7625 - val_accuracy: 0.5293\n",
            "Epoch 4723/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1807 - accuracy: 0.9066\n",
            "Epoch 4723: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1805 - accuracy: 0.9068 - val_loss: 3.7363 - val_accuracy: 0.5354\n",
            "Epoch 4724/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9064\n",
            "Epoch 4724: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1754 - accuracy: 0.9064 - val_loss: 3.8360 - val_accuracy: 0.5343\n",
            "Epoch 4725/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1695 - accuracy: 0.9105\n",
            "Epoch 4725: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1712 - accuracy: 0.9086 - val_loss: 3.9447 - val_accuracy: 0.5258\n",
            "Epoch 4726/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1777 - accuracy: 0.9045\n",
            "Epoch 4726: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1797 - accuracy: 0.9030 - val_loss: 3.8762 - val_accuracy: 0.5325\n",
            "Epoch 4727/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1785 - accuracy: 0.9063\n",
            "Epoch 4727: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1793 - accuracy: 0.9056 - val_loss: 3.8479 - val_accuracy: 0.5363\n",
            "Epoch 4728/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1812 - accuracy: 0.9027\n",
            "Epoch 4728: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1824 - accuracy: 0.9022 - val_loss: 4.0192 - val_accuracy: 0.5290\n",
            "Epoch 4729/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1789 - accuracy: 0.9042\n",
            "Epoch 4729: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1795 - accuracy: 0.9035 - val_loss: 3.8407 - val_accuracy: 0.5310\n",
            "Epoch 4730/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.9057\n",
            "Epoch 4730: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1764 - accuracy: 0.9051 - val_loss: 3.8025 - val_accuracy: 0.5223\n",
            "Epoch 4731/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1890 - accuracy: 0.9018\n",
            "Epoch 4731: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1887 - accuracy: 0.9022 - val_loss: 3.8258 - val_accuracy: 0.5246\n",
            "Epoch 4732/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1853 - accuracy: 0.9034\n",
            "Epoch 4732: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1856 - accuracy: 0.9033 - val_loss: 3.8273 - val_accuracy: 0.5267\n",
            "Epoch 4733/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9055\n",
            "Epoch 4733: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1786 - accuracy: 0.9050 - val_loss: 3.7972 - val_accuracy: 0.5226\n",
            "Epoch 4734/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1790 - accuracy: 0.9035\n",
            "Epoch 4734: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1788 - accuracy: 0.9037 - val_loss: 3.7668 - val_accuracy: 0.5310\n",
            "Epoch 4735/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1654 - accuracy: 0.9112\n",
            "Epoch 4735: loss did not improve from 0.16604\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1661 - accuracy: 0.9100 - val_loss: 3.8841 - val_accuracy: 0.5284\n",
            "Epoch 4736/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1624 - accuracy: 0.9117\n",
            "Epoch 4736: loss improved from 0.16604 to 0.16373, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 44ms/step - loss: 0.1637 - accuracy: 0.9112 - val_loss: 3.9157 - val_accuracy: 0.5278\n",
            "Epoch 4737/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1703 - accuracy: 0.9090\n",
            "Epoch 4737: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1713 - accuracy: 0.9081 - val_loss: 3.7789 - val_accuracy: 0.5281\n",
            "Epoch 4738/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1689 - accuracy: 0.9092\n",
            "Epoch 4738: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1692 - accuracy: 0.9090 - val_loss: 3.9605 - val_accuracy: 0.5357\n",
            "Epoch 4739/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.9056\n",
            "Epoch 4739: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1773 - accuracy: 0.9054 - val_loss: 3.9736 - val_accuracy: 0.5252\n",
            "Epoch 4740/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1715 - accuracy: 0.9104\n",
            "Epoch 4740: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1717 - accuracy: 0.9103 - val_loss: 3.8639 - val_accuracy: 0.5267\n",
            "Epoch 4741/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9073\n",
            "Epoch 4741: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1785 - accuracy: 0.9073 - val_loss: 3.9356 - val_accuracy: 0.5269\n",
            "Epoch 4742/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1701 - accuracy: 0.9116\n",
            "Epoch 4742: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1735 - accuracy: 0.9093 - val_loss: 3.8916 - val_accuracy: 0.5316\n",
            "Epoch 4743/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1807 - accuracy: 0.9038\n",
            "Epoch 4743: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1808 - accuracy: 0.9035 - val_loss: 3.7466 - val_accuracy: 0.5237\n",
            "Epoch 4744/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9062\n",
            "Epoch 4744: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1722 - accuracy: 0.9054 - val_loss: 3.9341 - val_accuracy: 0.5240\n",
            "Epoch 4745/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1717 - accuracy: 0.9091\n",
            "Epoch 4745: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1742 - accuracy: 0.9062 - val_loss: 3.8998 - val_accuracy: 0.5305\n",
            "Epoch 4746/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1738 - accuracy: 0.9077\n",
            "Epoch 4746: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1733 - accuracy: 0.9079 - val_loss: 3.8233 - val_accuracy: 0.5269\n",
            "Epoch 4747/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1688 - accuracy: 0.9081\n",
            "Epoch 4747: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1691 - accuracy: 0.9077 - val_loss: 3.8329 - val_accuracy: 0.5299\n",
            "Epoch 4748/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1670 - accuracy: 0.9114\n",
            "Epoch 4748: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1684 - accuracy: 0.9106 - val_loss: 3.8053 - val_accuracy: 0.5360\n",
            "Epoch 4749/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1634 - accuracy: 0.9135\n",
            "Epoch 4749: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1644 - accuracy: 0.9124 - val_loss: 3.9880 - val_accuracy: 0.5337\n",
            "Epoch 4750/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1670 - accuracy: 0.9116\n",
            "Epoch 4750: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1672 - accuracy: 0.9109 - val_loss: 3.9342 - val_accuracy: 0.5375\n",
            "Epoch 4751/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1688 - accuracy: 0.9118\n",
            "Epoch 4751: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1699 - accuracy: 0.9108 - val_loss: 3.7658 - val_accuracy: 0.5269\n",
            "Epoch 4752/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1736 - accuracy: 0.9067\n",
            "Epoch 4752: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1739 - accuracy: 0.9068 - val_loss: 4.0013 - val_accuracy: 0.5375\n",
            "Epoch 4753/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.9071\n",
            "Epoch 4753: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1757 - accuracy: 0.9054 - val_loss: 4.0285 - val_accuracy: 0.5357\n",
            "Epoch 4754/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1710 - accuracy: 0.9105\n",
            "Epoch 4754: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1720 - accuracy: 0.9096 - val_loss: 3.9171 - val_accuracy: 0.5272\n",
            "Epoch 4755/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9030\n",
            "Epoch 4755: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1843 - accuracy: 0.9030 - val_loss: 3.8413 - val_accuracy: 0.5343\n",
            "Epoch 4756/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9021\n",
            "Epoch 4756: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1881 - accuracy: 0.9021 - val_loss: 3.6461 - val_accuracy: 0.5340\n",
            "Epoch 4757/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1849 - accuracy: 0.9024\n",
            "Epoch 4757: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1846 - accuracy: 0.9024 - val_loss: 3.9378 - val_accuracy: 0.5249\n",
            "Epoch 4758/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9039\n",
            "Epoch 4758: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1891 - accuracy: 0.9025 - val_loss: 3.9222 - val_accuracy: 0.5272\n",
            "Epoch 4759/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1821 - accuracy: 0.9058\n",
            "Epoch 4759: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1829 - accuracy: 0.9051 - val_loss: 3.8180 - val_accuracy: 0.5243\n",
            "Epoch 4760/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1735 - accuracy: 0.9059\n",
            "Epoch 4760: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1742 - accuracy: 0.9051 - val_loss: 3.8899 - val_accuracy: 0.5284\n",
            "Epoch 4761/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1765 - accuracy: 0.9084\n",
            "Epoch 4761: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1785 - accuracy: 0.9070 - val_loss: 3.8926 - val_accuracy: 0.5343\n",
            "Epoch 4762/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9016\n",
            "Epoch 4762: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1822 - accuracy: 0.9016 - val_loss: 3.8671 - val_accuracy: 0.5278\n",
            "Epoch 4763/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1711 - accuracy: 0.9110\n",
            "Epoch 4763: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1729 - accuracy: 0.9096 - val_loss: 4.0066 - val_accuracy: 0.5299\n",
            "Epoch 4764/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9073\n",
            "Epoch 4764: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1731 - accuracy: 0.9073 - val_loss: 4.0065 - val_accuracy: 0.5296\n",
            "Epoch 4765/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9071\n",
            "Epoch 4765: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1739 - accuracy: 0.9071 - val_loss: 3.9029 - val_accuracy: 0.5340\n",
            "Epoch 4766/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1751 - accuracy: 0.9068\n",
            "Epoch 4766: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1754 - accuracy: 0.9066 - val_loss: 3.8946 - val_accuracy: 0.5349\n",
            "Epoch 4767/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1735 - accuracy: 0.9058\n",
            "Epoch 4767: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1732 - accuracy: 0.9064 - val_loss: 3.8702 - val_accuracy: 0.5284\n",
            "Epoch 4768/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 0.9073\n",
            "Epoch 4768: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1714 - accuracy: 0.9071 - val_loss: 3.8426 - val_accuracy: 0.5313\n",
            "Epoch 4769/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1782 - accuracy: 0.9074\n",
            "Epoch 4769: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1790 - accuracy: 0.9065 - val_loss: 3.6871 - val_accuracy: 0.5290\n",
            "Epoch 4770/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1846 - accuracy: 0.9028\n",
            "Epoch 4770: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1846 - accuracy: 0.9024 - val_loss: 3.8928 - val_accuracy: 0.5264\n",
            "Epoch 4771/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1758 - accuracy: 0.9075\n",
            "Epoch 4771: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1775 - accuracy: 0.9061 - val_loss: 3.9730 - val_accuracy: 0.5369\n",
            "Epoch 4772/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9081\n",
            "Epoch 4772: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1723 - accuracy: 0.9081 - val_loss: 3.8144 - val_accuracy: 0.5340\n",
            "Epoch 4773/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1757 - accuracy: 0.9083\n",
            "Epoch 4773: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1770 - accuracy: 0.9070 - val_loss: 3.8898 - val_accuracy: 0.5275\n",
            "Epoch 4774/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1680 - accuracy: 0.9097\n",
            "Epoch 4774: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1701 - accuracy: 0.9076 - val_loss: 3.8814 - val_accuracy: 0.5316\n",
            "Epoch 4775/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1727 - accuracy: 0.9066\n",
            "Epoch 4775: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1734 - accuracy: 0.9059 - val_loss: 3.8190 - val_accuracy: 0.5308\n",
            "Epoch 4776/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9063\n",
            "Epoch 4776: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1749 - accuracy: 0.9063 - val_loss: 3.8278 - val_accuracy: 0.5322\n",
            "Epoch 4777/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.9087\n",
            "Epoch 4777: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1712 - accuracy: 0.9084 - val_loss: 3.8772 - val_accuracy: 0.5269\n",
            "Epoch 4778/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9038\n",
            "Epoch 4778: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1820 - accuracy: 0.9038 - val_loss: 3.9929 - val_accuracy: 0.5264\n",
            "Epoch 4779/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9087\n",
            "Epoch 4779: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1697 - accuracy: 0.9087 - val_loss: 3.8175 - val_accuracy: 0.5267\n",
            "Epoch 4780/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1716 - accuracy: 0.9107\n",
            "Epoch 4780: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1718 - accuracy: 0.9105 - val_loss: 3.9204 - val_accuracy: 0.5261\n",
            "Epoch 4781/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1770 - accuracy: 0.9060\n",
            "Epoch 4781: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1784 - accuracy: 0.9045 - val_loss: 3.8920 - val_accuracy: 0.5337\n",
            "Epoch 4782/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1840 - accuracy: 0.9039\n",
            "Epoch 4782: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1860 - accuracy: 0.9029 - val_loss: 3.6788 - val_accuracy: 0.5246\n",
            "Epoch 4783/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1777 - accuracy: 0.9058\n",
            "Epoch 4783: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1774 - accuracy: 0.9058 - val_loss: 3.8977 - val_accuracy: 0.5290\n",
            "Epoch 4784/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1758 - accuracy: 0.9083\n",
            "Epoch 4784: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1759 - accuracy: 0.9081 - val_loss: 3.8432 - val_accuracy: 0.5401\n",
            "Epoch 4785/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1745 - accuracy: 0.9082\n",
            "Epoch 4785: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1746 - accuracy: 0.9084 - val_loss: 3.8190 - val_accuracy: 0.5334\n",
            "Epoch 4786/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1708 - accuracy: 0.9095\n",
            "Epoch 4786: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1728 - accuracy: 0.9081 - val_loss: 3.8752 - val_accuracy: 0.5272\n",
            "Epoch 4787/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1685 - accuracy: 0.9096\n",
            "Epoch 4787: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1716 - accuracy: 0.9074 - val_loss: 3.9775 - val_accuracy: 0.5337\n",
            "Epoch 4788/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1743 - accuracy: 0.9075\n",
            "Epoch 4788: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1764 - accuracy: 0.9059 - val_loss: 3.8294 - val_accuracy: 0.5346\n",
            "Epoch 4789/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9095\n",
            "Epoch 4789: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1742 - accuracy: 0.9073 - val_loss: 3.8518 - val_accuracy: 0.5214\n",
            "Epoch 4790/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9030\n",
            "Epoch 4790: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1855 - accuracy: 0.9030 - val_loss: 3.7879 - val_accuracy: 0.5328\n",
            "Epoch 4791/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1780 - accuracy: 0.9050\n",
            "Epoch 4791: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1785 - accuracy: 0.9043 - val_loss: 3.7395 - val_accuracy: 0.5322\n",
            "Epoch 4792/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1949 - accuracy: 0.8978\n",
            "Epoch 4792: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1964 - accuracy: 0.8972 - val_loss: 4.0885 - val_accuracy: 0.5217\n",
            "Epoch 4793/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.2111 - accuracy: 0.8950\n",
            "Epoch 4793: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.2105 - accuracy: 0.8956 - val_loss: 3.6393 - val_accuracy: 0.5214\n",
            "Epoch 4794/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9046\n",
            "Epoch 4794: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1797 - accuracy: 0.9038 - val_loss: 3.8528 - val_accuracy: 0.5278\n",
            "Epoch 4795/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1699 - accuracy: 0.9099\n",
            "Epoch 4795: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1703 - accuracy: 0.9095 - val_loss: 3.8773 - val_accuracy: 0.5378\n",
            "Epoch 4796/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1694 - accuracy: 0.9074\n",
            "Epoch 4796: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1692 - accuracy: 0.9075 - val_loss: 3.6802 - val_accuracy: 0.5313\n",
            "Epoch 4797/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1941 - accuracy: 0.8994\n",
            "Epoch 4797: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1949 - accuracy: 0.8992 - val_loss: 3.8601 - val_accuracy: 0.5313\n",
            "Epoch 4798/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1741 - accuracy: 0.9087\n",
            "Epoch 4798: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1761 - accuracy: 0.9076 - val_loss: 3.7903 - val_accuracy: 0.5272\n",
            "Epoch 4799/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1720 - accuracy: 0.9110\n",
            "Epoch 4799: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1718 - accuracy: 0.9111 - val_loss: 3.7641 - val_accuracy: 0.5325\n",
            "Epoch 4800/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1715 - accuracy: 0.9091\n",
            "Epoch 4800: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1714 - accuracy: 0.9092 - val_loss: 3.7976 - val_accuracy: 0.5261\n",
            "Epoch 4801/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1771 - accuracy: 0.9082\n",
            "Epoch 4801: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1771 - accuracy: 0.9082 - val_loss: 3.7405 - val_accuracy: 0.5264\n",
            "Epoch 4802/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1693 - accuracy: 0.9126\n",
            "Epoch 4802: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1693 - accuracy: 0.9128 - val_loss: 3.8279 - val_accuracy: 0.5319\n",
            "Epoch 4803/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1786 - accuracy: 0.9066\n",
            "Epoch 4803: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1805 - accuracy: 0.9061 - val_loss: 3.9981 - val_accuracy: 0.5299\n",
            "Epoch 4804/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9041\n",
            "Epoch 4804: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1890 - accuracy: 0.9041 - val_loss: 3.8712 - val_accuracy: 0.5284\n",
            "Epoch 4805/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9048\n",
            "Epoch 4805: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1778 - accuracy: 0.9048 - val_loss: 3.7657 - val_accuracy: 0.5305\n",
            "Epoch 4806/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1722 - accuracy: 0.9087\n",
            "Epoch 4806: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1722 - accuracy: 0.9087 - val_loss: 3.9780 - val_accuracy: 0.5243\n",
            "Epoch 4807/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1746 - accuracy: 0.9073\n",
            "Epoch 4807: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1738 - accuracy: 0.9069 - val_loss: 3.7893 - val_accuracy: 0.5299\n",
            "Epoch 4808/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9088\n",
            "Epoch 4808: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1722 - accuracy: 0.9088 - val_loss: 3.9833 - val_accuracy: 0.5305\n",
            "Epoch 4809/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1719 - accuracy: 0.9118\n",
            "Epoch 4809: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1735 - accuracy: 0.9106 - val_loss: 3.8738 - val_accuracy: 0.5264\n",
            "Epoch 4810/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1707 - accuracy: 0.9108\n",
            "Epoch 4810: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1711 - accuracy: 0.9101 - val_loss: 3.8861 - val_accuracy: 0.5343\n",
            "Epoch 4811/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1704 - accuracy: 0.9084\n",
            "Epoch 4811: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1703 - accuracy: 0.9086 - val_loss: 3.9599 - val_accuracy: 0.5384\n",
            "Epoch 4812/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9112\n",
            "Epoch 4812: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1728 - accuracy: 0.9106 - val_loss: 3.8324 - val_accuracy: 0.5249\n",
            "Epoch 4813/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1738 - accuracy: 0.9082\n",
            "Epoch 4813: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1745 - accuracy: 0.9070 - val_loss: 3.8098 - val_accuracy: 0.5302\n",
            "Epoch 4814/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1790 - accuracy: 0.9059\n",
            "Epoch 4814: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1801 - accuracy: 0.9053 - val_loss: 3.9487 - val_accuracy: 0.5217\n",
            "Epoch 4815/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1667 - accuracy: 0.9106\n",
            "Epoch 4815: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1703 - accuracy: 0.9087 - val_loss: 3.9129 - val_accuracy: 0.5310\n",
            "Epoch 4816/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1831 - accuracy: 0.9023\n",
            "Epoch 4816: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1829 - accuracy: 0.9025 - val_loss: 3.8110 - val_accuracy: 0.5343\n",
            "Epoch 4817/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1861 - accuracy: 0.9047\n",
            "Epoch 4817: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1867 - accuracy: 0.9041 - val_loss: 3.7177 - val_accuracy: 0.5231\n",
            "Epoch 4818/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1696 - accuracy: 0.9084\n",
            "Epoch 4818: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1701 - accuracy: 0.9077 - val_loss: 3.9151 - val_accuracy: 0.5337\n",
            "Epoch 4819/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1650 - accuracy: 0.9098\n",
            "Epoch 4819: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1671 - accuracy: 0.9083 - val_loss: 3.8995 - val_accuracy: 0.5272\n",
            "Epoch 4820/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9087\n",
            "Epoch 4820: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1706 - accuracy: 0.9087 - val_loss: 3.7230 - val_accuracy: 0.5319\n",
            "Epoch 4821/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1735 - accuracy: 0.9087\n",
            "Epoch 4821: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1768 - accuracy: 0.9064 - val_loss: 3.9931 - val_accuracy: 0.5363\n",
            "Epoch 4822/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1820 - accuracy: 0.9054\n",
            "Epoch 4822: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1834 - accuracy: 0.9046 - val_loss: 3.8161 - val_accuracy: 0.5472\n",
            "Epoch 4823/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1830 - accuracy: 0.9028\n",
            "Epoch 4823: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1851 - accuracy: 0.9019 - val_loss: 3.9043 - val_accuracy: 0.5228\n",
            "Epoch 4824/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1975 - accuracy: 0.8991\n",
            "Epoch 4824: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1957 - accuracy: 0.8999 - val_loss: 3.7528 - val_accuracy: 0.5404\n",
            "Epoch 4825/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1913 - accuracy: 0.9000\n",
            "Epoch 4825: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1911 - accuracy: 0.9000 - val_loss: 3.7118 - val_accuracy: 0.5316\n",
            "Epoch 4826/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1825 - accuracy: 0.9063\n",
            "Epoch 4826: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1822 - accuracy: 0.9053 - val_loss: 3.9982 - val_accuracy: 0.5290\n",
            "Epoch 4827/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1714 - accuracy: 0.9083\n",
            "Epoch 4827: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1715 - accuracy: 0.9081 - val_loss: 3.8644 - val_accuracy: 0.5296\n",
            "Epoch 4828/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1675 - accuracy: 0.9139\n",
            "Epoch 4828: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1695 - accuracy: 0.9122 - val_loss: 3.9085 - val_accuracy: 0.5278\n",
            "Epoch 4829/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1711 - accuracy: 0.9073\n",
            "Epoch 4829: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1716 - accuracy: 0.9063 - val_loss: 3.8299 - val_accuracy: 0.5296\n",
            "Epoch 4830/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1701 - accuracy: 0.9091\n",
            "Epoch 4830: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1704 - accuracy: 0.9090 - val_loss: 3.8853 - val_accuracy: 0.5431\n",
            "Epoch 4831/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9018\n",
            "Epoch 4831: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1797 - accuracy: 0.9018 - val_loss: 3.7855 - val_accuracy: 0.5360\n",
            "Epoch 4832/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1729 - accuracy: 0.9089\n",
            "Epoch 4832: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1746 - accuracy: 0.9073 - val_loss: 3.8763 - val_accuracy: 0.5240\n",
            "Epoch 4833/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1693 - accuracy: 0.9102\n",
            "Epoch 4833: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1701 - accuracy: 0.9095 - val_loss: 3.7652 - val_accuracy: 0.5363\n",
            "Epoch 4834/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9049\n",
            "Epoch 4834: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1757 - accuracy: 0.9049 - val_loss: 3.9806 - val_accuracy: 0.5234\n",
            "Epoch 4835/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9045\n",
            "Epoch 4835: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1785 - accuracy: 0.9045 - val_loss: 3.8105 - val_accuracy: 0.5410\n",
            "Epoch 4836/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1788 - accuracy: 0.9055\n",
            "Epoch 4836: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1798 - accuracy: 0.9046 - val_loss: 3.7001 - val_accuracy: 0.5340\n",
            "Epoch 4837/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1816 - accuracy: 0.9033\n",
            "Epoch 4837: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1818 - accuracy: 0.9033 - val_loss: 3.8351 - val_accuracy: 0.5308\n",
            "Epoch 4838/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9039\n",
            "Epoch 4838: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1793 - accuracy: 0.9040 - val_loss: 3.9564 - val_accuracy: 0.5228\n",
            "Epoch 4839/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1843 - accuracy: 0.9041\n",
            "Epoch 4839: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1864 - accuracy: 0.9022 - val_loss: 3.8314 - val_accuracy: 0.5322\n",
            "Epoch 4840/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1880 - accuracy: 0.9003\n",
            "Epoch 4840: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1887 - accuracy: 0.8997 - val_loss: 3.7882 - val_accuracy: 0.5334\n",
            "Epoch 4841/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9056\n",
            "Epoch 4841: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1717 - accuracy: 0.9056 - val_loss: 3.6878 - val_accuracy: 0.5305\n",
            "Epoch 4842/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.9083\n",
            "Epoch 4842: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1727 - accuracy: 0.9069 - val_loss: 3.8706 - val_accuracy: 0.5272\n",
            "Epoch 4843/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1711 - accuracy: 0.9081\n",
            "Epoch 4843: loss did not improve from 0.16373\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1725 - accuracy: 0.9074 - val_loss: 3.7914 - val_accuracy: 0.5246\n",
            "Epoch 4844/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1623 - accuracy: 0.9111\n",
            "Epoch 4844: loss improved from 0.16373 to 0.16317, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "28/28 [==============================] - 1s 42ms/step - loss: 0.1632 - accuracy: 0.9105 - val_loss: 4.0168 - val_accuracy: 0.5237\n",
            "Epoch 4845/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1717 - accuracy: 0.9104\n",
            "Epoch 4845: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1718 - accuracy: 0.9098 - val_loss: 3.8519 - val_accuracy: 0.5316\n",
            "Epoch 4846/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9087\n",
            "Epoch 4846: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1683 - accuracy: 0.9087 - val_loss: 3.8194 - val_accuracy: 0.5272\n",
            "Epoch 4847/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1704 - accuracy: 0.9102\n",
            "Epoch 4847: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1721 - accuracy: 0.9084 - val_loss: 3.7747 - val_accuracy: 0.5360\n",
            "Epoch 4848/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.9054\n",
            "Epoch 4848: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1824 - accuracy: 0.9054 - val_loss: 3.9196 - val_accuracy: 0.5369\n",
            "Epoch 4849/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1663 - accuracy: 0.9098\n",
            "Epoch 4849: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1686 - accuracy: 0.9082 - val_loss: 4.0224 - val_accuracy: 0.5290\n",
            "Epoch 4850/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9086\n",
            "Epoch 4850: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1752 - accuracy: 0.9086 - val_loss: 4.0348 - val_accuracy: 0.5258\n",
            "Epoch 4851/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1814 - accuracy: 0.9053\n",
            "Epoch 4851: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1824 - accuracy: 0.9051 - val_loss: 3.9337 - val_accuracy: 0.5296\n",
            "Epoch 4852/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1743 - accuracy: 0.9064\n",
            "Epoch 4852: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1744 - accuracy: 0.9063 - val_loss: 3.7875 - val_accuracy: 0.5316\n",
            "Epoch 4853/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1737 - accuracy: 0.9076\n",
            "Epoch 4853: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1735 - accuracy: 0.9076 - val_loss: 3.8409 - val_accuracy: 0.5269\n",
            "Epoch 4854/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1787 - accuracy: 0.9050\n",
            "Epoch 4854: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1791 - accuracy: 0.9046 - val_loss: 3.9334 - val_accuracy: 0.5322\n",
            "Epoch 4855/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.9078\n",
            "Epoch 4855: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1733 - accuracy: 0.9068 - val_loss: 4.0625 - val_accuracy: 0.5261\n",
            "Epoch 4856/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9067\n",
            "Epoch 4856: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1724 - accuracy: 0.9067 - val_loss: 3.8601 - val_accuracy: 0.5331\n",
            "Epoch 4857/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1788 - accuracy: 0.9060\n",
            "Epoch 4857: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1782 - accuracy: 0.9065 - val_loss: 3.9594 - val_accuracy: 0.5290\n",
            "Epoch 4858/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1801 - accuracy: 0.9084\n",
            "Epoch 4858: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1802 - accuracy: 0.9082 - val_loss: 3.9382 - val_accuracy: 0.5281\n",
            "Epoch 4859/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1712 - accuracy: 0.9102\n",
            "Epoch 4859: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1722 - accuracy: 0.9092 - val_loss: 3.8444 - val_accuracy: 0.5258\n",
            "Epoch 4860/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1741 - accuracy: 0.9086\n",
            "Epoch 4860: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1745 - accuracy: 0.9083 - val_loss: 3.9274 - val_accuracy: 0.5217\n",
            "Epoch 4861/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9096\n",
            "Epoch 4861: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1706 - accuracy: 0.9096 - val_loss: 4.0185 - val_accuracy: 0.5255\n",
            "Epoch 4862/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1711 - accuracy: 0.9092\n",
            "Epoch 4862: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1734 - accuracy: 0.9078 - val_loss: 3.8652 - val_accuracy: 0.5228\n",
            "Epoch 4863/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1692 - accuracy: 0.9102\n",
            "Epoch 4863: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1716 - accuracy: 0.9081 - val_loss: 3.8446 - val_accuracy: 0.5208\n",
            "Epoch 4864/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1697 - accuracy: 0.9107\n",
            "Epoch 4864: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1721 - accuracy: 0.9093 - val_loss: 4.0617 - val_accuracy: 0.5281\n",
            "Epoch 4865/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9059\n",
            "Epoch 4865: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1741 - accuracy: 0.9043 - val_loss: 3.9819 - val_accuracy: 0.5284\n",
            "Epoch 4866/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1719 - accuracy: 0.9084\n",
            "Epoch 4866: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1737 - accuracy: 0.9070 - val_loss: 3.8085 - val_accuracy: 0.5328\n",
            "Epoch 4867/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.9080\n",
            "Epoch 4867: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1752 - accuracy: 0.9076 - val_loss: 3.8653 - val_accuracy: 0.5425\n",
            "Epoch 4868/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9057\n",
            "Epoch 4868: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1772 - accuracy: 0.9057 - val_loss: 3.8482 - val_accuracy: 0.5334\n",
            "Epoch 4869/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1861 - accuracy: 0.9029\n",
            "Epoch 4869: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1863 - accuracy: 0.9016 - val_loss: 3.7263 - val_accuracy: 0.5334\n",
            "Epoch 4870/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1822 - accuracy: 0.9059\n",
            "Epoch 4870: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1817 - accuracy: 0.9060 - val_loss: 3.8090 - val_accuracy: 0.5199\n",
            "Epoch 4871/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1793 - accuracy: 0.9024\n",
            "Epoch 4871: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1792 - accuracy: 0.9028 - val_loss: 3.8161 - val_accuracy: 0.5369\n",
            "Epoch 4872/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1718 - accuracy: 0.9061\n",
            "Epoch 4872: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1745 - accuracy: 0.9043 - val_loss: 3.7916 - val_accuracy: 0.5319\n",
            "Epoch 4873/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9085\n",
            "Epoch 4873: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1716 - accuracy: 0.9085 - val_loss: 3.7899 - val_accuracy: 0.5290\n",
            "Epoch 4874/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1662 - accuracy: 0.9100\n",
            "Epoch 4874: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1678 - accuracy: 0.9084 - val_loss: 3.9394 - val_accuracy: 0.5375\n",
            "Epoch 4875/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9079\n",
            "Epoch 4875: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1725 - accuracy: 0.9079 - val_loss: 3.8458 - val_accuracy: 0.5281\n",
            "Epoch 4876/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1796 - accuracy: 0.9054\n",
            "Epoch 4876: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1812 - accuracy: 0.9041 - val_loss: 3.9859 - val_accuracy: 0.5387\n",
            "Epoch 4877/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1762 - accuracy: 0.9057\n",
            "Epoch 4877: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1763 - accuracy: 0.9052 - val_loss: 3.8337 - val_accuracy: 0.5264\n",
            "Epoch 4878/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1694 - accuracy: 0.9097\n",
            "Epoch 4878: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1716 - accuracy: 0.9084 - val_loss: 3.8362 - val_accuracy: 0.5366\n",
            "Epoch 4879/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.9049\n",
            "Epoch 4879: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1801 - accuracy: 0.9043 - val_loss: 3.8450 - val_accuracy: 0.5290\n",
            "Epoch 4880/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1749 - accuracy: 0.9065\n",
            "Epoch 4880: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1749 - accuracy: 0.9065 - val_loss: 3.7840 - val_accuracy: 0.5302\n",
            "Epoch 4881/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9040\n",
            "Epoch 4881: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1821 - accuracy: 0.9040 - val_loss: 3.6232 - val_accuracy: 0.5354\n",
            "Epoch 4882/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9037\n",
            "Epoch 4882: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1830 - accuracy: 0.9037 - val_loss: 3.8643 - val_accuracy: 0.5343\n",
            "Epoch 4883/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1688 - accuracy: 0.9117\n",
            "Epoch 4883: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1725 - accuracy: 0.9087 - val_loss: 3.8151 - val_accuracy: 0.5246\n",
            "Epoch 4884/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1696 - accuracy: 0.9098\n",
            "Epoch 4884: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1699 - accuracy: 0.9091 - val_loss: 3.8114 - val_accuracy: 0.5267\n",
            "Epoch 4885/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1729 - accuracy: 0.9089\n",
            "Epoch 4885: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1756 - accuracy: 0.9079 - val_loss: 4.1364 - val_accuracy: 0.5261\n",
            "Epoch 4886/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1805 - accuracy: 0.9050\n",
            "Epoch 4886: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1807 - accuracy: 0.9041 - val_loss: 3.7608 - val_accuracy: 0.5278\n",
            "Epoch 4887/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1675 - accuracy: 0.9116\n",
            "Epoch 4887: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1690 - accuracy: 0.9103 - val_loss: 3.8248 - val_accuracy: 0.5246\n",
            "Epoch 4888/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1712 - accuracy: 0.9095\n",
            "Epoch 4888: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1705 - accuracy: 0.9108 - val_loss: 3.9063 - val_accuracy: 0.5293\n",
            "Epoch 4889/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9106\n",
            "Epoch 4889: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1696 - accuracy: 0.9106 - val_loss: 3.8707 - val_accuracy: 0.5322\n",
            "Epoch 4890/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9074\n",
            "Epoch 4890: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1752 - accuracy: 0.9074 - val_loss: 3.8009 - val_accuracy: 0.5322\n",
            "Epoch 4891/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.9079\n",
            "Epoch 4891: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1739 - accuracy: 0.9079 - val_loss: 3.8938 - val_accuracy: 0.5302\n",
            "Epoch 4892/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1773 - accuracy: 0.9061\n",
            "Epoch 4892: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1771 - accuracy: 0.9063 - val_loss: 3.9805 - val_accuracy: 0.5261\n",
            "Epoch 4893/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1732 - accuracy: 0.9089\n",
            "Epoch 4893: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1747 - accuracy: 0.9073 - val_loss: 3.8821 - val_accuracy: 0.5313\n",
            "Epoch 4894/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1842 - accuracy: 0.9030\n",
            "Epoch 4894: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1847 - accuracy: 0.9024 - val_loss: 3.6697 - val_accuracy: 0.5223\n",
            "Epoch 4895/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.8972\n",
            "Epoch 4895: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.2010 - accuracy: 0.8972 - val_loss: 3.8304 - val_accuracy: 0.5302\n",
            "Epoch 4896/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1841 - accuracy: 0.9025\n",
            "Epoch 4896: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1841 - accuracy: 0.9020 - val_loss: 3.9123 - val_accuracy: 0.5310\n",
            "Epoch 4897/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1863 - accuracy: 0.9031\n",
            "Epoch 4897: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1905 - accuracy: 0.9017 - val_loss: 3.7327 - val_accuracy: 0.5349\n",
            "Epoch 4898/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1906 - accuracy: 0.9014\n",
            "Epoch 4898: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1907 - accuracy: 0.9013 - val_loss: 3.9048 - val_accuracy: 0.5281\n",
            "Epoch 4899/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1809 - accuracy: 0.9069\n",
            "Epoch 4899: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1819 - accuracy: 0.9062 - val_loss: 3.6717 - val_accuracy: 0.5343\n",
            "Epoch 4900/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1802 - accuracy: 0.9050\n",
            "Epoch 4900: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1817 - accuracy: 0.9043 - val_loss: 3.7616 - val_accuracy: 0.5302\n",
            "Epoch 4901/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1760 - accuracy: 0.9077\n",
            "Epoch 4901: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1785 - accuracy: 0.9065 - val_loss: 3.7729 - val_accuracy: 0.5237\n",
            "Epoch 4902/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1793 - accuracy: 0.9058\n",
            "Epoch 4902: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1798 - accuracy: 0.9051 - val_loss: 3.7448 - val_accuracy: 0.5287\n",
            "Epoch 4903/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1691 - accuracy: 0.9102\n",
            "Epoch 4903: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1732 - accuracy: 0.9075 - val_loss: 3.7589 - val_accuracy: 0.5275\n",
            "Epoch 4904/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1724 - accuracy: 0.9095\n",
            "Epoch 4904: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1730 - accuracy: 0.9089 - val_loss: 3.8179 - val_accuracy: 0.5234\n",
            "Epoch 4905/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1686 - accuracy: 0.9083\n",
            "Epoch 4905: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1684 - accuracy: 0.9087 - val_loss: 3.8575 - val_accuracy: 0.5325\n",
            "Epoch 4906/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1701 - accuracy: 0.9108\n",
            "Epoch 4906: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1723 - accuracy: 0.9094 - val_loss: 3.9444 - val_accuracy: 0.5346\n",
            "Epoch 4907/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1723 - accuracy: 0.9083\n",
            "Epoch 4907: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1713 - accuracy: 0.9079 - val_loss: 3.7941 - val_accuracy: 0.5246\n",
            "Epoch 4908/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.9068\n",
            "Epoch 4908: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1752 - accuracy: 0.9070 - val_loss: 3.9189 - val_accuracy: 0.5334\n",
            "Epoch 4909/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1697 - accuracy: 0.9086\n",
            "Epoch 4909: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1735 - accuracy: 0.9062 - val_loss: 3.8049 - val_accuracy: 0.5287\n",
            "Epoch 4910/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1757 - accuracy: 0.9089\n",
            "Epoch 4910: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1761 - accuracy: 0.9084 - val_loss: 3.7169 - val_accuracy: 0.5231\n",
            "Epoch 4911/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9084\n",
            "Epoch 4911: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1718 - accuracy: 0.9084 - val_loss: 3.8049 - val_accuracy: 0.5284\n",
            "Epoch 4912/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1717 - accuracy: 0.9098\n",
            "Epoch 4912: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1726 - accuracy: 0.9095 - val_loss: 3.7772 - val_accuracy: 0.5305\n",
            "Epoch 4913/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1667 - accuracy: 0.9102\n",
            "Epoch 4913: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1673 - accuracy: 0.9100 - val_loss: 3.8877 - val_accuracy: 0.5255\n",
            "Epoch 4914/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1808 - accuracy: 0.9061\n",
            "Epoch 4914: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1827 - accuracy: 0.9042 - val_loss: 3.8940 - val_accuracy: 0.5220\n",
            "Epoch 4915/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9090\n",
            "Epoch 4915: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1720 - accuracy: 0.9090 - val_loss: 3.8504 - val_accuracy: 0.5310\n",
            "Epoch 4916/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9072\n",
            "Epoch 4916: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1706 - accuracy: 0.9072 - val_loss: 3.8496 - val_accuracy: 0.5331\n",
            "Epoch 4917/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9066\n",
            "Epoch 4917: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1799 - accuracy: 0.9059 - val_loss: 3.7915 - val_accuracy: 0.5357\n",
            "Epoch 4918/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.9078\n",
            "Epoch 4918: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1758 - accuracy: 0.9058 - val_loss: 3.9199 - val_accuracy: 0.5328\n",
            "Epoch 4919/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1735 - accuracy: 0.9087\n",
            "Epoch 4919: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1745 - accuracy: 0.9079 - val_loss: 3.7257 - val_accuracy: 0.5322\n",
            "Epoch 4920/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.9106\n",
            "Epoch 4920: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1739 - accuracy: 0.9093 - val_loss: 3.8398 - val_accuracy: 0.5249\n",
            "Epoch 4921/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1846 - accuracy: 0.9038\n",
            "Epoch 4921: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1861 - accuracy: 0.9035 - val_loss: 3.8025 - val_accuracy: 0.5310\n",
            "Epoch 4922/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1748 - accuracy: 0.9074\n",
            "Epoch 4922: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1749 - accuracy: 0.9065 - val_loss: 3.8982 - val_accuracy: 0.5313\n",
            "Epoch 4923/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1705 - accuracy: 0.9110\n",
            "Epoch 4923: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.1723 - accuracy: 0.9098 - val_loss: 3.7963 - val_accuracy: 0.5340\n",
            "Epoch 4924/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1707 - accuracy: 0.9070\n",
            "Epoch 4924: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1722 - accuracy: 0.9062 - val_loss: 3.8664 - val_accuracy: 0.5281\n",
            "Epoch 4925/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.9053\n",
            "Epoch 4925: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1781 - accuracy: 0.9052 - val_loss: 3.8320 - val_accuracy: 0.5284\n",
            "Epoch 4926/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1740 - accuracy: 0.9058\n",
            "Epoch 4926: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1750 - accuracy: 0.9052 - val_loss: 3.7226 - val_accuracy: 0.5316\n",
            "Epoch 4927/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.9074\n",
            "Epoch 4927: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1776 - accuracy: 0.9051 - val_loss: 3.9509 - val_accuracy: 0.5302\n",
            "Epoch 4928/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9078\n",
            "Epoch 4928: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1731 - accuracy: 0.9078 - val_loss: 3.8221 - val_accuracy: 0.5237\n",
            "Epoch 4929/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1890 - accuracy: 0.8992\n",
            "Epoch 4929: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1905 - accuracy: 0.8988 - val_loss: 3.8486 - val_accuracy: 0.5328\n",
            "Epoch 4930/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.2024 - accuracy: 0.8955\n",
            "Epoch 4930: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.2023 - accuracy: 0.8955 - val_loss: 3.7490 - val_accuracy: 0.5290\n",
            "Epoch 4931/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1741 - accuracy: 0.9091\n",
            "Epoch 4931: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1754 - accuracy: 0.9075 - val_loss: 3.7945 - val_accuracy: 0.5375\n",
            "Epoch 4932/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9093\n",
            "Epoch 4932: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1711 - accuracy: 0.9090 - val_loss: 3.8594 - val_accuracy: 0.5340\n",
            "Epoch 4933/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9076\n",
            "Epoch 4933: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1704 - accuracy: 0.9076 - val_loss: 3.7815 - val_accuracy: 0.5287\n",
            "Epoch 4934/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1700 - accuracy: 0.9109\n",
            "Epoch 4934: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1703 - accuracy: 0.9103 - val_loss: 3.8784 - val_accuracy: 0.5340\n",
            "Epoch 4935/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1773 - accuracy: 0.9036\n",
            "Epoch 4935: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1776 - accuracy: 0.9036 - val_loss: 3.9344 - val_accuracy: 0.5234\n",
            "Epoch 4936/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9129\n",
            "Epoch 4936: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1669 - accuracy: 0.9129 - val_loss: 3.8897 - val_accuracy: 0.5187\n",
            "Epoch 4937/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9095\n",
            "Epoch 4937: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1697 - accuracy: 0.9095 - val_loss: 3.8987 - val_accuracy: 0.5351\n",
            "Epoch 4938/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9072\n",
            "Epoch 4938: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1720 - accuracy: 0.9072 - val_loss: 3.8139 - val_accuracy: 0.5308\n",
            "Epoch 4939/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1651 - accuracy: 0.9135\n",
            "Epoch 4939: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1679 - accuracy: 0.9119 - val_loss: 3.7617 - val_accuracy: 0.5343\n",
            "Epoch 4940/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1737 - accuracy: 0.9089\n",
            "Epoch 4940: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1746 - accuracy: 0.9077 - val_loss: 3.8045 - val_accuracy: 0.5296\n",
            "Epoch 4941/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9046\n",
            "Epoch 4941: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1793 - accuracy: 0.9046 - val_loss: 3.8819 - val_accuracy: 0.5185\n",
            "Epoch 4942/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9090\n",
            "Epoch 4942: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1765 - accuracy: 0.9076 - val_loss: 3.8855 - val_accuracy: 0.5349\n",
            "Epoch 4943/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1699 - accuracy: 0.9112\n",
            "Epoch 4943: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1699 - accuracy: 0.9111 - val_loss: 3.8454 - val_accuracy: 0.5334\n",
            "Epoch 4944/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1781 - accuracy: 0.9042\n",
            "Epoch 4944: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1802 - accuracy: 0.9029 - val_loss: 3.8431 - val_accuracy: 0.5331\n",
            "Epoch 4945/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1801 - accuracy: 0.9072\n",
            "Epoch 4945: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1799 - accuracy: 0.9062 - val_loss: 3.8281 - val_accuracy: 0.5275\n",
            "Epoch 4946/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1750 - accuracy: 0.9099\n",
            "Epoch 4946: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1746 - accuracy: 0.9101 - val_loss: 3.8857 - val_accuracy: 0.5316\n",
            "Epoch 4947/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9076\n",
            "Epoch 4947: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1786 - accuracy: 0.9076 - val_loss: 3.7873 - val_accuracy: 0.5287\n",
            "Epoch 4948/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.9068\n",
            "Epoch 4948: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1716 - accuracy: 0.9068 - val_loss: 3.9550 - val_accuracy: 0.5237\n",
            "Epoch 4949/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.9079\n",
            "Epoch 4949: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1715 - accuracy: 0.9077 - val_loss: 3.8373 - val_accuracy: 0.5313\n",
            "Epoch 4950/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1746 - accuracy: 0.9097\n",
            "Epoch 4950: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1752 - accuracy: 0.9090 - val_loss: 3.9485 - val_accuracy: 0.5208\n",
            "Epoch 4951/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.9105\n",
            "Epoch 4951: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1702 - accuracy: 0.9107 - val_loss: 4.0100 - val_accuracy: 0.5258\n",
            "Epoch 4952/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1701 - accuracy: 0.9118\n",
            "Epoch 4952: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1708 - accuracy: 0.9115 - val_loss: 3.6230 - val_accuracy: 0.5278\n",
            "Epoch 4953/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1668 - accuracy: 0.9110\n",
            "Epoch 4953: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1676 - accuracy: 0.9098 - val_loss: 3.8670 - val_accuracy: 0.5234\n",
            "Epoch 4954/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1681 - accuracy: 0.9106\n",
            "Epoch 4954: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1701 - accuracy: 0.9097 - val_loss: 3.8360 - val_accuracy: 0.5390\n",
            "Epoch 4955/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1853 - accuracy: 0.9057\n",
            "Epoch 4955: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1853 - accuracy: 0.9058 - val_loss: 3.8447 - val_accuracy: 0.5366\n",
            "Epoch 4956/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1669 - accuracy: 0.9124\n",
            "Epoch 4956: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1670 - accuracy: 0.9124 - val_loss: 3.8754 - val_accuracy: 0.5310\n",
            "Epoch 4957/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9090\n",
            "Epoch 4957: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.1708 - accuracy: 0.9090 - val_loss: 3.8163 - val_accuracy: 0.5340\n",
            "Epoch 4958/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9056\n",
            "Epoch 4958: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 26ms/step - loss: 0.1799 - accuracy: 0.9056 - val_loss: 3.9605 - val_accuracy: 0.5228\n",
            "Epoch 4959/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1787 - accuracy: 0.9061\n",
            "Epoch 4959: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1804 - accuracy: 0.9047 - val_loss: 3.9429 - val_accuracy: 0.5334\n",
            "Epoch 4960/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1675 - accuracy: 0.9142\n",
            "Epoch 4960: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1681 - accuracy: 0.9138 - val_loss: 3.7603 - val_accuracy: 0.5278\n",
            "Epoch 4961/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1718 - accuracy: 0.9076\n",
            "Epoch 4961: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1728 - accuracy: 0.9065 - val_loss: 3.9109 - val_accuracy: 0.5378\n",
            "Epoch 4962/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9095\n",
            "Epoch 4962: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1705 - accuracy: 0.9095 - val_loss: 3.9051 - val_accuracy: 0.5240\n",
            "Epoch 4963/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9082\n",
            "Epoch 4963: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1697 - accuracy: 0.9082 - val_loss: 3.9124 - val_accuracy: 0.5290\n",
            "Epoch 4964/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.9111\n",
            "Epoch 4964: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1720 - accuracy: 0.9092 - val_loss: 3.9568 - val_accuracy: 0.5313\n",
            "Epoch 4965/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1693 - accuracy: 0.9104\n",
            "Epoch 4965: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1694 - accuracy: 0.9103 - val_loss: 3.9280 - val_accuracy: 0.5325\n",
            "Epoch 4966/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1706 - accuracy: 0.9103\n",
            "Epoch 4966: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1713 - accuracy: 0.9100 - val_loss: 3.7497 - val_accuracy: 0.5246\n",
            "Epoch 4967/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1880 - accuracy: 0.9021\n",
            "Epoch 4967: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1893 - accuracy: 0.9015 - val_loss: 3.8335 - val_accuracy: 0.5381\n",
            "Epoch 4968/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1896 - accuracy: 0.9018\n",
            "Epoch 4968: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1895 - accuracy: 0.9013 - val_loss: 3.7593 - val_accuracy: 0.5375\n",
            "Epoch 4969/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1834 - accuracy: 0.9036\n",
            "Epoch 4969: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1814 - accuracy: 0.9044 - val_loss: 3.8796 - val_accuracy: 0.5278\n",
            "Epoch 4970/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.9094\n",
            "Epoch 4970: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1747 - accuracy: 0.9070 - val_loss: 3.7729 - val_accuracy: 0.5354\n",
            "Epoch 4971/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9080\n",
            "Epoch 4971: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1749 - accuracy: 0.9080 - val_loss: 3.7347 - val_accuracy: 0.5302\n",
            "Epoch 4972/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1757 - accuracy: 0.9066\n",
            "Epoch 4972: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1757 - accuracy: 0.9065 - val_loss: 3.7575 - val_accuracy: 0.5290\n",
            "Epoch 4973/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9059\n",
            "Epoch 4973: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1771 - accuracy: 0.9059 - val_loss: 3.7748 - val_accuracy: 0.5316\n",
            "Epoch 4974/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9062\n",
            "Epoch 4974: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1742 - accuracy: 0.9062 - val_loss: 3.8260 - val_accuracy: 0.5261\n",
            "Epoch 4975/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1765 - accuracy: 0.9072\n",
            "Epoch 4975: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1773 - accuracy: 0.9060 - val_loss: 3.8172 - val_accuracy: 0.5243\n",
            "Epoch 4976/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1768 - accuracy: 0.9046\n",
            "Epoch 4976: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1766 - accuracy: 0.9046 - val_loss: 3.8246 - val_accuracy: 0.5313\n",
            "Epoch 4977/5000\n",
            "24/28 [========================>.....] - ETA: 0s - loss: 0.1681 - accuracy: 0.9111\n",
            "Epoch 4977: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1694 - accuracy: 0.9107 - val_loss: 3.7596 - val_accuracy: 0.5281\n",
            "Epoch 4978/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9083\n",
            "Epoch 4978: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1729 - accuracy: 0.9081 - val_loss: 3.8986 - val_accuracy: 0.5334\n",
            "Epoch 4979/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1786 - accuracy: 0.9063\n",
            "Epoch 4979: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1813 - accuracy: 0.9050 - val_loss: 3.8370 - val_accuracy: 0.5310\n",
            "Epoch 4980/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.9082\n",
            "Epoch 4980: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1784 - accuracy: 0.9060 - val_loss: 3.9251 - val_accuracy: 0.5302\n",
            "Epoch 4981/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1725 - accuracy: 0.9085\n",
            "Epoch 4981: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1732 - accuracy: 0.9070 - val_loss: 3.8133 - val_accuracy: 0.5264\n",
            "Epoch 4982/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9087\n",
            "Epoch 4982: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1689 - accuracy: 0.9087 - val_loss: 3.8416 - val_accuracy: 0.5252\n",
            "Epoch 4983/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1700 - accuracy: 0.9099\n",
            "Epoch 4983: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 17ms/step - loss: 0.1712 - accuracy: 0.9088 - val_loss: 3.8741 - val_accuracy: 0.5436\n",
            "Epoch 4984/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1865 - accuracy: 0.9038\n",
            "Epoch 4984: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 18ms/step - loss: 0.1876 - accuracy: 0.9032 - val_loss: 3.8189 - val_accuracy: 0.5357\n",
            "Epoch 4985/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1696 - accuracy: 0.9095\n",
            "Epoch 4985: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1709 - accuracy: 0.9086 - val_loss: 3.8893 - val_accuracy: 0.5281\n",
            "Epoch 4986/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9081\n",
            "Epoch 4986: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 0s 18ms/step - loss: 0.1727 - accuracy: 0.9071 - val_loss: 3.8298 - val_accuracy: 0.5410\n",
            "Epoch 4987/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1847 - accuracy: 0.9042\n",
            "Epoch 4987: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1844 - accuracy: 0.9043 - val_loss: 3.7596 - val_accuracy: 0.5296\n",
            "Epoch 4988/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1686 - accuracy: 0.9109\n",
            "Epoch 4988: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 19ms/step - loss: 0.1683 - accuracy: 0.9107 - val_loss: 3.8543 - val_accuracy: 0.5363\n",
            "Epoch 4989/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9106\n",
            "Epoch 4989: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 25ms/step - loss: 0.1685 - accuracy: 0.9106 - val_loss: 3.8831 - val_accuracy: 0.5240\n",
            "Epoch 4990/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1720 - accuracy: 0.9085\n",
            "Epoch 4990: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1729 - accuracy: 0.9079 - val_loss: 3.9736 - val_accuracy: 0.5275\n",
            "Epoch 4991/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1675 - accuracy: 0.9100\n",
            "Epoch 4991: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1680 - accuracy: 0.9098 - val_loss: 3.7037 - val_accuracy: 0.5240\n",
            "Epoch 4992/5000\n",
            "27/28 [===========================>..] - ETA: 0s - loss: 0.1692 - accuracy: 0.9100\n",
            "Epoch 4992: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 21ms/step - loss: 0.1692 - accuracy: 0.9098 - val_loss: 3.8584 - val_accuracy: 0.5316\n",
            "Epoch 4993/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1725 - accuracy: 0.9084\n",
            "Epoch 4993: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1748 - accuracy: 0.9070 - val_loss: 3.8401 - val_accuracy: 0.5334\n",
            "Epoch 4994/5000\n",
            "25/28 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9038\n",
            "Epoch 4994: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 22ms/step - loss: 0.1820 - accuracy: 0.9040 - val_loss: 3.6702 - val_accuracy: 0.5293\n",
            "Epoch 4995/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1694 - accuracy: 0.9110\n",
            "Epoch 4995: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1709 - accuracy: 0.9095 - val_loss: 3.8924 - val_accuracy: 0.5261\n",
            "Epoch 4996/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.9053\n",
            "Epoch 4996: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1743 - accuracy: 0.9053 - val_loss: 3.8245 - val_accuracy: 0.5322\n",
            "Epoch 4997/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9092\n",
            "Epoch 4997: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 20ms/step - loss: 0.1673 - accuracy: 0.9092 - val_loss: 3.8372 - val_accuracy: 0.5293\n",
            "Epoch 4998/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1703 - accuracy: 0.9096\n",
            "Epoch 4998: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1712 - accuracy: 0.9087 - val_loss: 3.8149 - val_accuracy: 0.5375\n",
            "Epoch 4999/5000\n",
            "26/28 [==========================>...] - ETA: 0s - loss: 0.1722 - accuracy: 0.9060\n",
            "Epoch 4999: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 23ms/step - loss: 0.1728 - accuracy: 0.9057 - val_loss: 3.9582 - val_accuracy: 0.5308\n",
            "Epoch 5000/5000\n",
            "28/28 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9080\n",
            "Epoch 5000: loss did not improve from 0.16317\n",
            "28/28 [==============================] - 1s 24ms/step - loss: 0.1709 - accuracy: 0.9080 - val_loss: 3.8922 - val_accuracy: 0.5393\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABuqklEQVR4nO3dd3hTZfsH8G+60j2gGwotmzLKhoIKSLUMkSIoIgoowouCP3gBFRyAOIoD1wuCqIADLKIsQcCyZc9C2TJboAs60pm2yfn9cWjaNOlIm+Yk6fdzXbmS85znnHPngObmPEsmCIIAIiIiIithI3UARERERMbE5IaIiIisCpMbIiIisipMboiIiMiqMLkhIiIiq8LkhoiIiKwKkxsiIiKyKkxuiIiIyKowuSEiIiKrwuSGiMyeTCbD/PnzDT7u5s2bkMlkWLVqVaX19u7dC5lMhr1799YoPiIyL0xuiKhaVq1aBZlMBplMhgMHDujsFwQBQUFBkMlkeOKJJySIkIhIxOSGiAzi6OiINWvW6JTv27cPt2/fhlwulyAqIqJSTG6IyCCDBw/GunXrUFxcrFW+Zs0adO3aFf7+/hJFRkQkYnJDRAYZPXo07t+/j9jYWE1ZYWEhfv/9dzz33HN6j8nNzcXMmTMRFBQEuVyO1q1b47PPPoMgCFr1lEol/vvf/8LHxwdubm548skncfv2bb3nvHPnDl566SX4+flBLpejXbt2WLFihfG+KIB169aha9eucHJygre3N55//nncuXNHq05ycjJefPFFNG7cGHK5HAEBARg2bBhu3rypqXPixAlERkbC29sbTk5OCAkJwUsvvWTUWImolJ3UARCRZQkODkZ4eDh+/fVXDBo0CACwbds2ZGVl4dlnn8XXX3+tVV8QBDz55JPYs2cPJkyYgE6dOmHHjh14/fXXcefOHXzxxReaui+//DJ++eUXPPfcc+jduzd2796NIUOG6MSQkpKCXr16QSaTYerUqfDx8cG2bdswYcIEKBQKTJ8+vdbfc9WqVXjxxRfRvXt3REdHIyUlBV999RUOHjyI06dPw9PTEwAwYsQInD9/Hq+99hqCg4ORmpqK2NhYJCQkaLYff/xx+Pj4YPbs2fD09MTNmzexfv36WsdIRBUQiIiqYeXKlQIA4fjx48LixYsFNzc3IS8vTxAEQXj66aeF/v37C4IgCE2bNhWGDBmiOW7jxo0CAOGDDz7QOt/IkSMFmUwmXL16VRAEQYiLixMACK+++qpWveeee04AIMybN09TNmHCBCEgIEC4d++eVt1nn31W8PDw0MR148YNAYCwcuXKSr/bnj17BADCnj17BEEQhMLCQsHX11do3769kJ+fr6m3ZcsWAYAwd+5cQRAEISMjQwAgfPrppxWee8OGDZr7RkSmwWYpIjLYM888g/z8fGzZsgXZ2dnYsmVLhU1Sf/31F2xtbfF///d/WuUzZ86EIAjYtm2bph4AnXrln8IIgoA//vgDQ4cOhSAIuHfvnuYVGRmJrKwsnDp1qlbf78SJE0hNTcWrr74KR0dHTfmQIUPQpk0bbN26FQDg5OQEBwcH7N27FxkZGXrPVfKEZ8uWLSgqKqpVXERUPUxuiMhgPj4+iIiIwJo1a7B+/XqoVCqMHDlSb91bt24hMDAQbm5uWuVt27bV7C95t7GxQfPmzbXqtW7dWms7LS0NmZmZWL58OXx8fLReL774IgAgNTW1Vt+vJKby1waANm3aaPbL5XJ8/PHH2LZtG/z8/PDII4/gk08+QXJysqZ+3759MWLECLz33nvw9vbGsGHDsHLlSiiVylrFSEQVY58bIqqR5557DhMnTkRycjIGDRqkeUJR19RqNQDg+eefx7hx4/TW6dixo0liAcQnS0OHDsXGjRuxY8cOvPvuu4iOjsbu3bvRuXNnyGQy/P777zhy5Aj+/PNP7NixAy+99BIWLVqEI0eOwNXV1WSxEtUXfHJDRDUyfPhw2NjY4MiRIxU2SQFA06ZNcffuXWRnZ2uVX7p0SbO/5F2tVuPatWta9S5fvqy1XTKSSqVSISIiQu/L19e3Vt+tJKby1y4pK9lfonnz5pg5cyb+/vtvnDt3DoWFhVi0aJFWnV69euHDDz/EiRMnsHr1apw/fx4xMTG1ipOI9GNyQ0Q14urqiqVLl2L+/PkYOnRohfUGDx4MlUqFxYsXa5V/8cUXkMlkmhFXJe/lR1t9+eWXWtu2trYYMWIE/vjjD5w7d07nemlpaTX5Olq6desGX19fLFu2TKv5aNu2bbh48aJmBFdeXh4KCgq0jm3evDnc3Nw0x2VkZOgMee/UqRMAsGmKqI6wWYqIaqyiZqGyhg4div79++Ptt9/GzZs3ERYWhr///hubNm3C9OnTNX1sOnXqhNGjR+Obb75BVlYWevfujV27duHq1as651y4cCH27NmDnj17YuLEiQgNDUV6ejpOnTqFnTt3Ij09vVbfy97eHh9//DFefPFF9O3bF6NHj9YMBQ8ODsZ///tfAMCVK1cwYMAAPPPMMwgNDYWdnR02bNiAlJQUPPvsswCAH3/8Ed988w2GDx+O5s2bIzs7G9999x3c3d0xePDgWsVJRPoxuSGiOmVjY4PNmzdj7ty5WLt2LVauXIng4GB8+umnmDlzplbdFStWwMfHB6tXr8bGjRvx6KOPYuvWrQgKCtKq5+fnh2PHjmHBggVYv349vvnmGzRs2BDt2rXDxx9/bJS4x48fD2dnZyxcuBBvvvkmXFxcMHz4cHz88cea/kVBQUEYPXo0du3ahZ9//hl2dnZo06YNfvvtN4wYMQKA2KH42LFjiImJQUpKCjw8PNCjRw+sXr0aISEhRomViLTJhPLPS4mIiIgsGPvcEBERkVVhckNERERWhckNERERWRUmN0RERGRVmNwQERGRVWFyQ0RERFal3s1zo1arcffuXbi5uUEmk0kdDhEREVWDIAjIzs5GYGAgbGwqfzZT75Kbu3fv6kwIRkRERJYhMTERjRs3rrROvUtu3NzcAIg3x93dXeJoiIiIqDoUCgWCgoI0v+OVqXfJTUlTlLu7O5MbIiIiC1OdLiXsUExERERWhckNERERWRUmN0RERGRV6l2fGyIish4qlQpFRUVSh0FG4uDgUOUw7+pgckNERBZHEAQkJycjMzNT6lDIiGxsbBASEgIHB4danYfJDRERWZySxMbX1xfOzs6clNUKlEyym5SUhCZNmtTqz5TJDRERWRSVSqVJbBo2bCh1OGREPj4+uHv3LoqLi2Fvb1/j87BDMRERWZSSPjbOzs4SR0LGVtIcpVKpanUeJjdERGSR2BRlfYz1Z8rkhoiIiKwKkxsiIiILFhwcjC+//FLqMMwKkxsiIiITkMlklb7mz59fo/MeP34ckyZNMm6wFo6jpYiIiEwgKSlJ83nt2rWYO3cuLl++rClzdXXVfBYEASqVCnZ2Vf9M+/j4GDfQiqhVgI2taa5VS3xyQ0REZAL+/v6al4eHB2QymWb70qVLcHNzw7Zt29C1a1fI5XIcOHAA165dw7Bhw+Dn5wdXV1d0794dO3fu1Dpv+WYpmUyG77//HsOHD4ezszNatmyJzZs31y54xV0g+SygzK7deUyEyQ0REVk8QRCQV1gsyUsQBKN9j9mzZ2PhwoW4ePEiOnbsiJycHAwePBi7du3C6dOnMXDgQAwdOhQJCQmVnue9997DM888g7Nnz2Lw4MEYM2YM0tPTax5YTor4rrhT83OYEJuliIjI4uUXqRA6d4ck176wIBLODsb5OV2wYAEee+wxzXaDBg0QFham2X7//fexYcMGbN68GVOnTq3wPOPHj8fo0aMBAB999BG+/vprHDt2DAMHDjRKnOaOT26IiIjMRLdu3bS2c3JyMGvWLLRt2xaenp5wdXXFxYsXq3xy07FjR81nFxcXuLu7IzU1teIDBEF8WQk+uSEiIovnZG+LCwsiJbu2sbi4uGhtz5o1C7Gxsfjss8/QokULODk5YeTIkSgsLKz0POWXLpDJZFCr1forCwKQdgmQ2QDerQArmByRyQ0REVk8mUxmtKYhc3Lw4EGMHz8ew4cPByA+ybl586ZxL6JSAsUFDzYEAExuiIiIqI60bNkS69evx9ChQyGTyfDuu+/qfwKTnwEUKABHd+3y2jQ1FRcCqRcgJjyWhX1uiIiIzNTnn38OLy8v9O7dG0OHDkVkZCS6dOmiXUlQAYW5QPq1MmVqIDdNHL6dcr7yi1SUu2QlVrLTvMkEY45hq4WFCxdizpw5mDZtWqXTSK9btw7vvvsubt68iZYtW+Ljjz/G4MGDq30dhUIBDw8PZGVlwd3dveoDiIjIrBQUFODGjRsICQmBo6Oj1OFIL/MWkPdgmHdgZ/E9OwXIvltax7kBYCsH3Px1jy8qANIuip8DwsS+NwBwNw46yY29E+DTxpjRa6nsz9aQ32+zeHJz/PhxfPvtt1q9u/U5dOgQRo8ejQkTJuD06dOIiopCVFQUzp07Z6JIiYiIaqmoAMhOFmf8rfE58oG0K2JTVNk+MvkZ4nthjnb9vHQgOwkoVgKqIiA/E0g+ByjL1dNiFs8+akTy5CYnJwdjxozBd999By8vr0rrfvXVVxg4cCBef/11tG3bFu+//z66dOmCxYsXmyhaIiKiWkq7KCYa2Q+WY1BmiwkPICYe6uLSukX52klQ+g3g3r/ie1G5pigAyLhZedKUehFIOQdk3ADURcD9f/XXK4mnKiWJknk0AmlIntxMmTIFQ4YMQURERJV1Dx8+rFMvMjIShw8frvAYpVIJhUKh9SIiIgNd3QncPS11FJZPVVT6uTBXTF7uXxUTHnWxmHgkx4v7CxTiEO200vWnUJApPpVRKSu+Rt79SgLQk4QUZJV+LsoHCvPE61ZH6kUxUcq7V736JiLpaKmYmBicOnUKx48fr1b95ORk+Pn5aZX5+fkhOTm5wmOio6Px3nvv1SpOIqJ6LeMm8MsI8fP8rEqr1juCIDb12MmrNz9MSrluFEX5pZ+LyyUs+Q/60VSWyADQGbqtuAPIPaqOpUTZvjn3rlT/OEDszAyIiZiLiRbwrAbJntwkJiZi2rRpWL16dZ12CJszZw6ysrI0r8TExDq7FhGRVcqsfDbcei3rtvjUpWTtJWNJOV/afwYQRz8ZQllHSWjZZEzLgwSrMFf76ZREJHtyc/LkSaSmpmoNaVOpVNi/fz8WL14MpVIJW1vtWR/9/f2RkqL9FyglJQX+/np6fz8gl8shl8uNGzwRERFQ2hyTnax/JFKJgizAxoCfXFW5GYgVSYCrb+UxmIpaBdiUm5VZUInNWSVPfkpGbUlEsic3AwYMQHx8POLi4jSvbt26YcyYMYiLi9NJbAAgPDwcu3bt0iqLjY1FeHi4qcImIqqHLH/GWkkV5gLp1w1v8ikrN03seGwuigoejNR6oDAHyCnTRSSnknWsTECyJzdubm5o3769VpmLiwsaNmyoKR87diwaNWqE6OhoAMC0adPQt29fLFq0CEOGDEFMTAxOnDiB5cuXmzx+IqJ6wwrWGpJUdZOaCpt8AEAwn+Tm3r9AsZ5Yy3ZMVtyp+EmTCUg+WqoyCQkJSEpK0mz37t0ba9aswfLlyxEWFobff/8dGzdu1EmSiIjIwu37FDiyVOootAkCcP9aBX2QajgUumxfnawq+oSWdDCWmr7ExsyYVXKzd+9erdmJ9+7di1WrVmnVefrpp3H58mUolUqcO3fOoNmJiYjIAmTdAfZ8AGyfXbuJ7owpP0Mcoq1UVDzU+u7p0jlqivJKh3pXpria88k80G/kREyf+6lmO7jnEHz53epKj5E16oKN2/cYdJ26PI8pmFVyQ0RE5sjEzVJlf/ANHSVUXnayOJOvoUlScaGYmAiCOGFexs3SYc9AxZPWJcc/mCfmstgclVf6tGXouGkYOGZKad2iPM3Hf46egqxRF5y9YFi/nON//YJJzz9l0DFVmb9oGTo99qxOedLpvzGofx+jXquuMLkhIqLKmbrPTdnr1Tq5SRJn8s0zoElHEIDU8+JEdvkZ4sR5hki/Xvo5t7Rj7YTRUYjdfxS37+oOG1+5djO6hYWiY2grgy7l09ALzk5OhsVXQ/6+3pDLHUxyrdpickNEROZFVuanyWjNUuWSpOxksf+MqkjsqFv2SUzZ5CTzlv7TVZZ0CfpjfiLiYfg09MKq3/7UKs/JzcO6LTsRFdkPo1+dg0ZdI+HcvDc6DHgGv27cXtmX0mmW+vd6Ah55agIcm/VCaL8RiN1/ROeYNz/8Cq0eioJz895oFj4U737yDYqKxLlpVq3djPc+X44zF65A1qgLZI26YNXazQB0m6XiL/6LR5+eBKfm4WjYrj8mvfE+cnJLn0aNHz8eUVFR+OyzzxAQEICGDRtiypQpmmvVJUlnKCYiIjMhCMCBz4GGLYHQJ8vtlGnXq+snOWWTmwoSBS1F+WL/FjtHsb7MtjTGkj4vhQ/6wJQoSWDKduht0AKQu1RvQr6M64BboHh+O8dqPW2ys7PD2JFDsGrdZrw9bQJkD45ZtyUWKpUaz48YjHVbduLNV8fD3c0FW3cdwAv/9y6aN22MHp2rHjijVqvx1MRZ8PNugKN//oSs7GxMn7dIp56biwtWffEeAv19EH/xX0x84wO4uTrjjVfHY9STj+Pc5WvYvvcQdsaIHbo93Fx1zpGbl4/IMVMQ3rUjjm/9Gan30vHy6+9j6tsfY9WXpasC7NmzBwEBAdizZw+uXr2KUaNGoVOnTpg4cWKV36c2mNwQERGQcBjYtUD8XNkSCyZPbsolCsps4NsBQN8HCyarVWLzUVE+sHJQ3cZVkRe3AfbVaxp66dlh+HTpT9h3+CT69e4GQGySGjH4UTRtHIhZk8dq6r720rPYsfcQfvsztlrJzc5/juLS1ZvYsXoJAv3FpRA+mj0Fg55/TaveO9Nf1nwODgrErOu3ELNpB954dTycnBzh6uIEO1tb+Pt6V3itNRu2oUBZiJ++eh8uzuJ3X/zBmxg6fjo+fvv/4OfTEADg5eWFxYsXw9bWFm3atMGQIUOwa9cuJjdERGQClT2t0HkqUcc9GiprllrSC1DLxL4whfmAYNhoI6m1aRGC3t3CsCJmE/r17oarNxLwz9HTWLDuFahUKnz09Qr8tiUWd5JTUVhYBGVhUbX71Fz89waCAv00iQ0AhHftqFNv7aYd+HpFDK7duo2c3DwUq1Rwd3Ux6Htc/PcGwtq20iQ2ANCnexjUajUuX7upSW7atWunNSlvQEAA4uPjDbpWTTC5ISIiA9RwPhdDlE1ujiwF+s0B/lkEXPoTUNwGXIPEfZk3AJ9m4mc7R/EJSm3JPQxfl8nOsPURJ4wehtfe+QRLPpqNlWs3o3lwY/QN74qPl6zCVz/8ii/fm4kObVrCxdkR0+d9hkIj9lE5fOIMxrz2Dt6b+R9E9usNDzdXxGzagUXLfzbaNcqyt7fX2pbJZFCra9lJvBqY3BARUcVDmwFo9bnZPgcY/KnYDGTvVPdNVPs/ARo2F+e90adkkUaZrNpNQ5VSFxrnPJV4ZujjmDb3M6zZsA0//b4Vr4wdCZlMhoPH4zAssi+eHzFEDEWtxpXrCQht1axa523bMgSJd1OQlJKGAD/x6c2RU9pPSQ6dOIumjQPw9rTSpqlbd5K06jjY20NVRQLStmUIVq37E7l5+ZqnNwePn4GNjQ1aNw+uVrx1iaOliIjqi5TzwIEvgWJl9Y8RBODQ16Xbx78DTq4CPgoAfnuh8qQoJw1Y/x/gyw7AqQdPBlRFQNyvQOaD2Xhz7wPf9gX2RGtfs6zjP1R8DcXt6n8XM+Hq4oxRTz6OOQsXIyn1HsY/I3bgbhnSBLH7j+LQ8TO4+O91/OfND5Fyr/pD2CMe7olWzZpg3PR5OHP+Cv45egpvf7xEq07LZk2QcCcZMZt24NrNRHz9w6/YsE17Yr7goEDcSLiDuHOXcS89A0pluUU8AYx5ahAc5Q4YN20uzl26ij0Hj+O1dz/BCyOGaJqkpMTkhoiovljaG9g5Dzj4IFnJva/95KOEqkwzyJUdwKUt2ufZOU98v/gnsGaU/jWPspOBz1oAZ2PE5Qo2TwW2zgT2fARsnAws7i7WW9YHSIoD9i2seC6a28cM/qrmbsKzw5CRqUBk33BNH5l3pr2MLh3aIHLMFPQbOQn+Pg0RFdmv2ue0sbHBhu8XIb+gAD2eeAEvz3ofH745RavOk4/3xX8nPoepb3+MTo+PxqETZ/BumQ7GADBi8AAM7Ncb/Z+ZBJ8OA/QOR3d2csKO1UuQnpmF7kNewMhJb2DAQ92x+MM3xQoy3cWvTUkmCJU+i7Q6CoUCHh4eyMrKgru7u9ThEBHVvZIRTvM9xO02TwCRHwJfhQE+bYEpR4DzG4B148X9DZoB/3da/HxkGbD9zaqv8XZyaXNOYR7wxwTg8l+VHzM7AVjYpHR75oPZeY9/B+z/VP8xAApcg3CjzyKENPKBox0X9TRLboGAm5/BhxUUFODGjRsICQmBo6N2XyZDfr/Z54aIyJrtWgCciQEm7dUuvyBOzIa0i7rHlJ3ErrodiNOvA37ttJOkqpRNbEr8OBS4d7l6x5P5kjjnZLMUEZG5u38N+HdnzY79ZxGguKPdbwbQnT+m/EP8W4f1l1eluomNXgITGzIKJjdERLVRrATi1gCKu3V3jf91AVaPABKN1PdEEFDlE5mVAw0/76UqmqGq8t2A2h1P9ACTGyKyfveuAjFjgDunjH/ufZ8AG18Bvn3E+Ocu726c9va/O4Gj39bsXOWf3MTO1a1z+hdUu1lKEICY0TWLpYQFjnwi88Q+N0Rk/X4dBdy/Ko76qWxpgZr4d4f4nptWWpZwBPAKqVGHykoJKuD0aiC4D+AVLD7NAYDAzkBQD936WrP7lukEcXkr4ORVup2TBmQl6h6/aYpuWUWUiurXra0HTWX1azhMDbj6VW+dLDNirDFOfHJDRNbv/rW6O7es3P9Gbx4AVkQCi1qJ27cOiaOUtr9V+2sd/x7Y9Ko4yqksxV3xl778D8Pa58vEWa6HZ9wvpZ+LclFrJlzXyV6ZDqgKkVf3i0tbLpkt4OipW+7oKSbGjh4mDqh6CgvFOXXKLtlQE3xyQ0TWJ/cekHETaNzNBBcrlzRc36u9XfKjf2QJMPCj2l3q/lX95fkZwKonxM/jt4iJzMUt2kOxD35V8XlT9YyYMmO2xXnwvLUNqQ4jAXjC2b7uJ0q2GN6tgfwsQO4KFCiB4nIJr3OA+F54X3efPjYOgNwNyL+vu89WDqgeTAgpswM8GgGZt8RtZRFgZ9i6X2q1GmlpaXB2doadXe3SEyY3RGR9FrUB1EXA+L/EJhyZrPZtGGfXARc3AVHLgDXPAO6NgBHf6alY5lfWkDV07pwSJ8d77H0gsFPV9b8ssyDilumlnxd3BzyDgGu7q3/tX5+tfl0z4f/vGgBAatNBgK2DxNGYkdySpSOyAFUhkF2mudTOEci98aDePaAor+rz2dgDrgAUadrl7o0AGxsg80G5jS2QhdLrORYBjnomd6zqcjY2aNKkCWS1zFaZ3BCR9VE/aK+4tltMbowx6cb6B7O4XvyztGzAu+LsuiV+HQ3kZ5ZuLyjTr6UqKwcBxQXi+9tJVdcv+Rdyeff/FV9WTgYBAf+uhu/19ShybGjdj256TBITuMOLq6479UTp57QrwPaZ4ufBi4DG7QGHB6t/b18OXI3VPV5mD7x6CFjyYAZpn1BgyGfAtnIJcMl1Fj8tvjv7AC9tAxY/J26Hvwa0HVe971eGg4MDbGxq32OGyQ0RWbEHT2tkspovZp12peJFG7/soL1d1Yy8AJB1GzjwBdDjP4DPg345uffFxAbQ/dd00lnD4q1nbFX5sM218lFWdjZA78nAwU+APD3NQ2WVndXX3gbIedBRvM1j4pOWEsVZpfvKCukLODkBT3wiLlo65EPAwU63bsl1SsplxWJZybaQpx2LibFDMRFZr5KmqLJNUoW5Fa9hpM9Pw4ALm4wX029jxY7BP0SUln2qZ9Xn3PvA9X3Atw8b79pk2eyqSBaGl5sWoOzTLJ0nW2W2hy8v/dz3DfG9zWBxVmvvltB58ukWoOfi5eo06V15rHWMyQ0RSS/1InBjf8X7r+0BNr6q3eRTLSXJTZkh0QubAJ+EAAXVHLqcbcTJ+Ta8Atw5KX4uyBJXu940VbeeWg0s7gr89KTxrk3mJ+QRADLAxbfyen6h4vvAByun935Ne/9rp4D/iwPCyvedqqSpruwov7BRpZ/tnPTULXOefnOA6ecqPu9/L4h93Zr0rLiOCbBZioik900v8f3/TouLNpb3c5T47uACDK54QUUA2k9pVHrGCquLxfdtbwCRHwHODQwOt8bOrNHe3jpDfz1D+uqQZbJzAsZuFme4trEF9nwoNleW1fl5MQlu/qi4HToMePOmOEfRof+V1vMKFs9RXtmy8k9uWgwA4n8DbMqlAXr7LpUp6zEJsK0kdfBoJL4kxuSGiMzH/Wv6k5sSmXr6CJRXdubd9BsV1zvzqzhc/KXt4nZBlthp095JXNV69UjAs2m1wiYL1KA5kF7J/Ef+HYHkcv2d2o8Azv1R82v6dQBS4h9sPFip3f5BU1PEfO3kJiAMGLZE9xxOehJffYkNAHi3AloNBJz0JPAdngHk7qUj80KjxDXIAsJ061ogNksRkfnQN1y7WFn6ufyEeVWdo6ompYQHi0PG//6guepBYhW3Grh1UPdJC1mGGZcq3//aKeD/DFyKw9YBeHIxMGCe4fE88QXQbQIwuszfp6qmJggdZvh1ypPJgOfWAsOX6u6zsRH71bgHitvP/Ai8vFN/olR2wr+S0VYlgh40P3V5ofbxGhGTGyIyH5te1S0r25m3OsN9yz65uXsaOKZvLpoylNnAHxPEzyUjlYryq74OmS93fR1ey2jYvBonKZd8TNwDODgDD1fQlFiRQZ8C3V4Cnvgc8GwCPPTg+EEf69ZtO7TM5Q2YI6mu2TsCU44BU44DdnLtfc//Ib4eeV2a2CrA5IaIzEduWrn1kKD95KYiN/4BYucBxYW6Pwp/zar82MXl1mRa0kv//B9Uv7Qbrr3t377087BvgF5TgJlXtOuUHyHk4gP0nKRdNmAuMOtfoNuLutd85ufSz+a2bpZP69KpC8qSuwEtIgBbe9PHVAkmN0RUO8e+A+J+Nd75ji4rV1Dm//KXtojrNF35WxzSDYj9cH58Ajj4JfDbC8CHBi5WWb7pKq2KkVskvQ7PVF1H36ifqtg8+IF+8n9A72kV1+s8RlxKw80PeOrBk8HIaGDcn8CsCpbIKCGTAa4VjI6SycR+PTZ2YmdiqjF2KCaimlPcLX0y0vGZijs2GiLuVyC8zGrU+vomrHkaaPEYMOpn4Msy/6K+sr321yfL1qy/+N51PHBUT1+TishsxJFIhbmGrebe8RmgzZDSviiuPmVPWv3zlBjxg7jEhx2XlKgNPrkhqs8MWftIH6Xha8do3PsX+PFJsUmpMpkJ+suvxvIJi7noOr76dZ+vxWgjjUrabEo64j72nmGnfHGbuNhk2cTm6R/F96jyTxPLKd/JtoR/B/3llZHJqpfYvLBRHFk4fqvh16gHJE1uli5dio4dO8Ld3R3u7u4IDw/Htm3bKqy/atUqyGQyrZejhNM7E1m063uBhUHAmbUV18nPAP75vOIEozY2TAZu7BOblMpSFZZ+vhsH/PNZxee4Y+CIF6obQ74AXP2rrhcaBTQfUPvrVTbSqKTTefmOr5Vp+TjQpJduebso4J00oNNog8LD5INA95eBKAOeHBmqeX9xXqjgh+ruGhZM0uSmcePGWLhwIU6ePIkTJ07g0UcfxbBhw3D+/PkKj3F3d0dSUpLmdetWBYvHEVHlVj8NFOYAGyZVXOfP6cCu94DvH6v6fIauup2bqr/83mWxX82FTVXPKbJvoWHXJON76jtxWPHIFZXXaz9SHG5s6AKXzt56Civ7u2bkBTRr0jzk3x4Yssiw5i0yKkmTm6FDh2Lw4MFo2bIlWrVqhQ8//BCurq44cuRIhcfIZDL4+/trXn5+/MtDVGeu7xXfc5KrUVko7eQLAAe/Ep/6AEDiMWDNKHGSvhJVPQ36bSyQb8AaUCQNzybie9lZadsNB/rO1q438ofSz426Vf/8s/SscF7pkxsDftZ82orvHUdVXo8sjtn0uVGpVIiJiUFubi7Cw8MrrJeTk4OmTZsiKCioyqc8RGRCR74BPgoE9n8mrtsUO1d86pOXDvzwmNjZN2aMWLf8cO+KpF2pug6Zjr6nMyXT93sFl5YJAtB/TsXn0TfLbolH3ih3fn0/U0JpYlKeIcnNyzvF+Wvaj6j+MWQRJE9u4uPj4erqCrlcjsmTJ2PDhg0IDQ3VW7d169ZYsWIFNm3ahF9++QVqtRq9e/fG7dsVL3evVCqhUCi0XkRkBLn3xSUKSsTOFd93v689N03JWk6AOMz65KrqT2F/+1itwyQj0pcE+LXXLStpNhrzB9CwhThEWmt3BR3ZfdsBj75ddRwPz0SFTVOGNHvJXYFGXQxvKiOzJ3ly07p1a8TFxeHo0aN45ZVXMG7cOFy4cEFv3fDwcIwdOxadOnVC3759sX79evj4+ODbb7/VWx8AoqOj4eHhoXkFBQXV1VchsizV6SNT9n/6aZeBq7uAnDSgqADYNb+SpqUy5y7/L+k/pwHrJxoaLVXFowb/b/PrIHaYranJB0rXRiqr5O9WywjgtZMPVr/WqlD5eZ/+EXBvDLy8W7u8zzTgrSRxFFJFTVtuFcxOPHFP5dckqyL5PDcODg5o0aIFAKBr1644fvw4vvrqq0oTlhL29vbo3Lkzrl6teNKkOXPmYMaM0umyFQoFExyybook8bG/vh+dquSlAwc+B8KeE9dXys8o3bek3Ey+3npmKy3xWcvSz4Y0E1DNPDwLGPAu8EV7IEvP4qKNuonJaWG5ofuCSrfD7ISdwA8RFV9r8GfAzQPAiO91Z6Xt/w5wZIm4CGRlqlpaoF2U+NLEFCt2MO/7prgEAiBOouceIHZU/qZnad1m/XTP5xYoPqGJ/Ei7+Yysltn9X0etVkOprMZ06xD76cTHxyMgoOJ1RORyuWaoecmLyGrdvwZ83gZYbECHzbK2/Bc49D9gaThweHHlde9Vsz/MYT0rG5NxlTxhq2gOGTs5MOFv3fI+03XLgrqLK2ZXpMdEcdSTvun2+74OvH696rWbyj41dPSsvC4ABPUAIj/Unk/G0QN49B3Atw0wOgawdxGXLyj7tPHlXUDzR0vvS/gUccI9snqSPrmZM2cOBg0ahCZNmiA7Oxtr1qzB3r17sWPHDgDA2LFj0ahRI0RHRwMAFixYgF69eqFFixbIzMzEp59+ilu3buHll1+W8msQmY+SGXr1/eu9vLI/Asps8cfh7mnjx1TZPDVkHCUT1/m01r/fzR/wK9OXcfhyMWFoEKK//ojvge/61ywWvR2Ayyn75ObNm+Lfkd0fiMOna6L1IGDObd1rN+4GvLChZuckiyZpcpOamoqxY8ciKSkJHh4e6NixI3bs2IHHHhPn1EhISIBNmb+sGRkZmDhxIpKTk+Hl5YWuXbvi0KFDFXZAJqJqim4MNH2IHSstUfsRVc+E23Oy9radQ8WJDSA24Tg1MM1QfJlMXFG697TaLTlQnaSK6g1Jk5sffvih0v179+7V2v7iiy/wxRdf1GFERPVI+Q7Ftw5IE0d9JLOput9JWfbOQFFe6fYrh4ClD1agrqwjcZsngIIs3c631ZlNOHyKOPKt7VCg31vi6s/Gom+5Aq6lREYkeYdiIqqhS3+JnSub9RMTlT8mABc2669bVCCOUmr1OOf0KK/Hf4BjVQ9g0Hj3PvB+Q93yZ38FLm4GzlRjhfROY4DTP1f/mg2aAb3/D8i8BfT8j9jfxKcNkHap8j/PZ1drbz/zM3D/X/1LDZT30AxxEUr/9oYtZVAdgz4BFHeAXlOqrktUA0xuiCxR4jEg5sF6N/OzxL4ylc0dc/w74GyM+FIkAV3GAuoi08Rq7iI/Miy5sbUD5O6AstycWW0GA1cqXhsPYc+Jo9juXxVHHBmS3ABAWLlZdP+zH8hNAzwaa5c/tw5YOwYYpqcjd+iTlV8jIKz0s40N0LirYTFWl1dTcRg5UR1hckNkzlTFQPJZ8UfHxlYsU2aLM/6WEATtSfP0yS6zfMLfb4svEtnW4H+Dj78vPgkrr7hQtwwQn/ZUdZ0GzYD06/r3tdSztpedXDexAcSnc28nl/59MUTww4YfQ2SGmNwQmbMt00v/hR+1TFydOLvcOk8V9d1YMQjo8gLQ6TnDF7Uk/eQPppKwraB/SEVPw/QlNoM/A/6aJX7u+iLQ/y1xLa/zG8VkptVAcd+13UCHkbrHV8bQxGbqCeDSVqBHJYuoElkQJjdE5qxs08XGyWJyU97f7wI39uuWJxwSX52eEydro4rZOQLFBdWo+GA0WbunxGUkEo9q7+7/dmnzoF8HICUeGF5Bk1ePiUDnF4Dsu+JTGwDo+Iz4KqvzmOp+i5rzbgk8NL3ur0NkIhw7R2RM+Rni4pAX/6y6rrEcWSL+iFZkvof4NKA+aPeUblnTh0o/2zsD47fq1nl5p/Z2aFTpZ30z2to7ipPiBXbRLm/YHJibAbx1F5j8j9gfKuzZiuO1dyxNbIjIaJjcEBnTno+AS1uAtc/X7PiK+mzUVk5y1XUsWePuwIvbxVWrx/wBNCrTEbbsUgBvXAeCHxKHUoeNBmY/mOzQvwPwxJel9cp2rC2r/DRAfR+sYN2xTAJjYyMOdeacQUSSYXJDZEw5qaWfD39j2LFHlgIf+IiLU6rVwJYZVR9Dopd3Ak3DxYSiZYQ4HT8ABHSC1iKNNg+WDPBrBwxfBjiWXY6lTL3wKUDf2eL0/ZVpPQiY9a94LiIyG0xuiIyp7CKRO+YYduz22eL7hv+IQ4pP6JnkUpmtf5ROfdBqkDgKSIsMGDBPt66rrzgd/8Td2h2uK+toW7aenRzoP0ecvr9Jb+3r6bsWn9IQmRUmN0TGZKwfuUMVLFq5+wPg1kHjXMPSDHgXsHfSbjJ6Nw14uIInXHI3MZkpO1Kssj+fDk8DLj7ie1mDFpb2uxn1S41CJyLT4mgpIqMyQnKTmya+9DlqZc0f4/8CVg2uXl3PJuK7rMzTF30rU5dX3WUOHD2AmZd1n+44egDTzgBqVc3mjiEik+OTGyJjYvOEYWwM+PdVjdc2MmCOn8qSFyY2RBaDyQ2RMcmq8Z9UfZhQzzcUCOqpW958gPa2WzUWcCxP36KLlakP95uItDC5ITKqck9uvmgPHCnTlHRhE/BxU3FElDV79TAwZp1ueUnTEiDOJePVFBi1GhhnwLxAQ78CfNsBT31XvfrugdU/NxFZBSY3ZJ3UanFkkamVb5bKSgS2v1m6/dtYoCAL+EXPZHPWxtEDeCdV/8R6ANB1nPje9gkg5JHqn7dhc+DVQ7oz+VZW/+kfxf49RFQvMLkh67TmaSC6MXD/mmHH5d4Hru0xrClDEIBfRwNrRsGgDsXZKYAyByjMFZMxc+XXXrds4u7qHWsnB55eqV026heg31tAs/61j6262kUBwX1Mdz0ikhSTG7JOVx9Mp192babq+KYn8HMUcPa36h/z67PA5b+AK9uBvHv66yQeB1aWGxW0qBUQ3Qj4KFBcINNcNeunvR3yiDgD8H/+AQZ+bNi5ZDKg7VCg35u6T7mahOvWdwsQ3529DbsOEdVrTG6IyioZgn1pS/XqK7PFpKaE4q7+ej9EVD4/zakfq3c9KZQf0VTShBTQUVxwsUTPyVWfq7LOwE+vAvpMA+ycSsvGbgbaj9S/HhQRUQU4zw2RPhXNjXL2N3EiuZM/Ag1CdEdHpZyr+9hMrfwTlrLfufmjYvLj6gdEfiS+Bz+se44hi8R79/DMiq/j5g88tgC4vB24d1ks82kFjNQzUzMRUSWY3FD9k3AESDkPdHvJsHlp7pwE1k+su7gshVdI6WeZDJh7v3S7otmCu78svqqFQ7eJqHbYLEX1z4pIYOsM4FolnWIvbQXuxokdfkuk36jz0MxSr1dLP8vdxSHcdcnBtW7PT0RWj8kN1V/3r1ayUwCW9wUWBombRQXAno9MEpbZcfUt/dxnGmBTx//bGP6tOELraTPuh0REZo3NUlR/VWe4t6AGdr4HHPi87uOxBK5+dX8Nn1bAK/V0cVAiMgo+uaF6rExyo7gLbJqqv5q1JDYzLwNho2t27DM/Az0m1fx4IiITYnJD9Ze6uPTzHxMNnxPHHDzzU9V1/DoA47aIo5F6TNLd/+i7wNspwNhNFZ8j9Elg8KeALR/2EpH5Y3JD1u3AF0Dug4n1ChTA4u6l+/5+R3xXFQO3Dpg+NmMIHaa/fOoJceHKbi8BrxwAQkqGZ+tpivNtC9g7ipP1NepWV5ESEZkM/xlG1m/nPGDYEnEW4HtXdPe/39DkIRnV5IPAsnJLC7h4AxP+rvy4ET8ASgXQuszMyc+uBk6sAJy8xORvBOeYISLLw+SGrF/Jk5tzf+juM2QNKSlN2AkUZAKx84CoJcCFzeITFwDwL7f2U5/pYnJSlQ4jdcvc/IH+b4mfu09kMxQRWST+n4usX2UJjFplujhqI+hBc1rLx8T3wM4V1+032zjXZGJDRBaKfW7IeiQeB06vNuyYpDN1E4uU7J2qrkNEZMX4TzOyPIV54vwz8nIz2f4QIb7b2pc7oJInN98/atTQ6sTgz6qu07g7cPt41fX8OgDujcTmJyIiK8XkhiyLWg1ENxKTm3dSATu5bp3y6z8JaqC40DTxGcLGHlAXaZfJ3cVOviVe3g007lr1uTwaVy+5sXMApp0FbGwNi5WIyIJI2iy1dOlSdOzYEe7u7nB3d0d4eDi2bdtW6THr1q1DmzZt4OjoiA4dOuCvv/4yUbRkFooLSlfsVtwtLT9VyXwv968BH/jUbVw14dNGe3t+FjAnEfBsUlrm0bh65xr0CdB2KPD8+qrr2toZtmAoEZGFkTS5ady4MRYuXIiTJ0/ixIkTePTRRzFs2DCcP39eb/1Dhw5h9OjRmDBhAk6fPo2oqChERUXh3LlzJo6cJHO5TDIrqMU5ao59B2x+reJjMsx0wUs7B/3lo9eK7y0iALdqLnfg6guM+gVoMcA4sRERWTCZIJjXWNgGDRrg008/xYQJE3T2jRo1Crm5udiyZYumrFevXujUqROWLVtWrfMrFAp4eHggKysL7u7uRoubTGS+R+nn104BV3cC296QLp7aGPYNsKnMitvzs6SLhYjIzBny+202o6VUKhViYmKQm5uL8PBwvXUOHz6MiIgIrbLIyEgcPny4wvMqlUooFAqtF1kJQQ3cPiF1FIbxCin97NwAaNBculiIiKyU5MlNfHw8XF1dIZfLMXnyZGzYsAGhoaF66yYnJ8PPT/sxvZ+fH5KTkys8f3R0NDw8PDSvoKAgo8ZPElKrgPwMqaOoOXsnIGopYOsARH4kdTRERFZD8tFSrVu3RlxcHLKysvD7779j3Lhx2LdvX4UJjqHmzJmDGTNmaLYVCgUTHGux/mUgOV7qKAwkAA/NANIuA8GPADY2wFtJnDCPiMiIJP8/qoODA1q0aAEA6Nq1K44fP46vvvoK3377rU5df39/pKSkaJWlpKTA37/iOTvkcjnkcj3DhcnyHPxKe1vKxKbnK8D59UBOStV1y4uYp73NxIaIyKgkb5YqT61WQ6lU6t0XHh6OXbt2aZXFxsZW2EeHrEzsXKkjKBXQEZhVZhFOV3+gaZnFKwPCgAmxpo+LiIikfXIzZ84cDBo0CE2aNEF2djbWrFmDvXv3YseOHQCAsWPHolGjRoiOjgYATJs2DX379sWiRYswZMgQxMTE4MSJE1i+fLmUX4PqgiCY+VwsD2IbvxW4sAmImA84uJSO5rJzBIJ6AJ1fAE7/XHpYUC+TR0pEVN9ImtykpqZi7NixSEpKgoeHBzp27IgdO3bgscfExQETEhJgY1P6cKl3795Ys2YN3nnnHbz11lto2bIlNm7ciPbt21d0CbJESWeAn58CBrwLdB0vlqnVkoako+mDp4XBD4mv8kpmWBj6NfDILLHzc/zvQM//mC5GIqJ6yuzmualrnOfGAiztA6Q8mJhxfhZweRvw67PSxOIVojsJYOvBwOhf9dcveXLTuAfwMpuliIiMxSLnuSHSEMo9pZEqsQGARl1KP7d8XHyPmF+NA+vVvxmIiMwKkxsyL4V5QOoF01zLs6lu2YyLgKzMopLujUo/P/cbMDsB8Gld8TltHqxIrq+pioiITIJjUMm8rB6pvf1JHc3g22kMMGwJEPOc9npV7oGAzAYQVOL2I68DuWlA+xFiB2dHD/3nKzH1OPBvLNDlhbqJm4iIqsQnN2Q6arW4Qre+bl556YCqCLh1sFz5vbqJJXyqmKzomxl4zG+AvTMQtQxwdAeGLwNaPla98zYIAXpOEmcfJiIiSTC5IdPZ+l/gf12AI0u1y7PuAJ+EAO97my4WvwczYDcIAVoN1N7X/FFgzm2g02jTxUNEREbD5IZM5+Qq8X3Pg6clhbnAyR+154GRQsMWumU2trplRERkEdjnhkzj+t7SzyWT8+14qzThkVLLx4HDi8WmKCIisnhMbqjuKbOBn4aVKXiQ3JxbL0k4Opr1BV7eLTZRERGRxWNyQ3WvQKG9LYPYqVip0Fu9TgxfLg7htnMUOwmX17ir6WIhIqI6xeSGJCADUs6b5lKvHC7tPExERPUCOxST6amLgV9GmOZadnLTXIeIiMwGkxuqe4W55bZzgJxk41+n7VDAI6h0u+crQMM6mgSQiIjMFpMbqlt56cCS7nV7jdExQMR7wMhV2utSDVpYt9clIiKzxOSGjOvCZuCXkUDug5mF41bX3bVG/QK8kwq0HgQ8NB2wtQOe+FLc1/+dursuERGZNXYoptrLzwCu7QFaDwZ+e7Cm0qfNgdev1e112w7VLWv1OPBWEuDAOWuIiOorJjdUez8PB+6eBnr8R7v80+aAT1vjX2/APKBpn4r3M7EhIqrXmNxQ7d09Lb4f+1Z3X9pF415rdIzYDEVERFQB9rkhy8LEhoiIqsDkhixH+FSpIyAiIgvAZimyDI++CzwyS+ooiIjIAjC5IfM3cQ8Q0EnqKIiIyEKwWYpqbs9HwP5P6+bc/d8W34MfBhp1AWz4V5WIiKqHT26oZgqygH0fG/ecocOAC5vEzw/PAkL6Av4djHsNIiKyevznMFWfIADZD9aEUhUb//zN+pd+trEBmvTknDVERGQwPrmhquWkApv/T5zPJidZfKLy1HLjnd8tQFxc06Ox8c5JRET1FpMbqtr22cCVbaXbN/YBx4yY3EyPFxe8vHXIeOckIqJ6i8kN6ScIgEwmfs5O0d3/zyLjXcvWXnxv1EV8d/Ex3rmJiKjeYXJDutRqYOVAwNEDeO43IPGoaa7r6AHMTgTs5Ka5HhERWSUmN6Qr/XppQnP6F0BdZLprO7qb7lpERGSVOFqKdJU0RwHAud+li4OIiKgGmNxQ5ewcjXu+5o8a93xERETlSJrcREdHo3v37nBzc4Ovry+ioqJw+fLlSo9ZtWoVZDKZ1svR0cg/wPVd2Sc3SWeNe+5nfzXu+YiIiMqRNLnZt28fpkyZgiNHjiA2NhZFRUV4/PHHkZubW+lx7u7uSEpK0rxu3bploojrizLJTfZd453Wxg6wZyJKRER1S9IOxdu3b9faXrVqFXx9fXHy5Ek88sgjFR4nk8ng7+9f1+HVXxc21v4cz/0GrBsPFOWVlslsdes9/WPtr0VERFSGWfW5ycrKAgA0aNCg0no5OTlo2rQpgoKCMGzYMJw/f77CukqlEgqFQutFVdg5v3bHP/RfoFUk8PwfgKOnuPilszfwwnrtesEPA+2ianctIiKicsxmKLharcb06dPRp08ftG/fvsJ6rVu3xooVK9CxY0dkZWXhs88+Q+/evXH+/Hk0bqw7fX90dDTee++9ugzdeqReBM5vrP157JzE96a9gTdvin14yk4K6NkUyLwFtBte+2sRERGVIxMEQZA6CAB45ZVXsG3bNhw4cEBvklKRoqIitG3bFqNHj8b777+vs1+pVEKpVGq2FQoFgoKCkJWVBXd3zqmiZb6Hcc4zO0GckK8ieelAUhwQ0k9cIJOIiKgKCoUCHh4e1fr9NosnN1OnTsWWLVuwf/9+gxIbALC3t0fnzp1x9epVvfvlcjnkcs54W6WS1b5rqkEzoPVgoNerlSc2AODcgEPCiYiozkia3AiCgNdeew0bNmzA3r17ERISYvA5VCoV4uPjMXjw4DqI0MqpigGVEkiOB079VLtzvXIIsHcyTlxERES1IGlyM2XKFKxZswabNm2Cm5sbkpPFpwceHh5wchJ/KMeOHYtGjRohOjoaALBgwQL06tULLVq0QGZmJj799FPcunULL7/8smTfwyLlZwJfdwby02t/rtaDmdgQEZHZkDS5Wbp0KQCgX79+WuUrV67E+PHjAQAJCQmwKdMvIyMjAxMnTkRycjK8vLzQtWtXHDp0CKGhoaYK2zpc2GicxAYAnvrOOOchIiIyArPpUGwqhnRIsmqnfwE2Tan9eWwdgHfTan8eIiKiShjy+82hKvWVjb1xzjNqtXHOQ0REZCRmMVqKJGCjZ7bg6npsgTgqCgBsjZQkERERGQmTm/qqNsmNiy+TGiIiMltslqq3ZFVXqQhHRhERkRljclNfyWqR3AR2Nl4cRERERsbkhgzn1VTqCIiIiCrEPjf1zZ2TwMU/Ae/WUkdCRERUJ5jcWDO1Gri8FWjUFXAPFMu+45pORERk3dgsZc3iVgNrnwe+7FD7c7WIqP05iIiITIBPbqzZtV3iu7q49ud6/ANxlJQn+9sQEZF5Y3JD1SOzAUb9InUUREREVapRs1RiYiJu376t2T527BimT5+O5cuXGy0wqgP/xkodARERUZ2rUXLz3HPPYc+ePQCA5ORkPPbYYzh27BjefvttLFiwwKgBUi2UXxP1yDeGHV+yxAIAeDSufTxEREQmUKPk5ty5c+jRowcA4LfffkP79u1x6NAhrF69GqtWrTJmfFQrZZIbtQpIjjfs8L5vArOuArP+BRxcjBsaERFRHalRn5uioiLI5XIAwM6dO/Hkk08CANq0aYOkpCTjRUfGs3okkJtWvbpv3ACKCwAnzzoNiYiIqC7U6MlNu3btsGzZMvzzzz+IjY3FwIEDAQB3795Fw4YNjRogGcm13dWv69ygdF4cIiIiC1Oj5Objjz/Gt99+i379+mH06NEICwsDAGzevFnTXEVEREQkhRo1S/Xr1w/37t2DQqGAl5eXpnzSpElwdnY2WnBUC/evARc2GX5c2U7EREREFqhGT27y8/OhVCo1ic2tW7fw5Zdf4vLly/D19TVqgFRD/+ti+DE+bYCB0caPhYiIyIRqlNwMGzYMP/30EwAgMzMTPXv2xKJFixAVFYWlS5caNUAywM75wNedgfzMmh3v3cqY0RAREUmiRsnNqVOn8PDDDwMAfv/9d/j5+eHWrVv46aef8PXXXxs1QDLAgS+A9OvA8e8MPzYgDBjyufFjIiIiMrEa9bnJy8uDm5sbAODvv//GU089BRsbG/Tq1Qu3bt0yaoBUA2q1YfWHfQN0HlM3sRAREZlYjZ7ctGjRAhs3bkRiYiJ27NiBxx9/HACQmpoKd3d3owZINbD3I8PqM7EhIiIrUqPkZu7cuZg1axaCg4PRo0cPhIeHAxCf4nTu3NmoAVIde+ZnqSMgIiIyqho1S40cORIPPfQQkpKSNHPcAMCAAQMwfPhwowVHJhD6pNQREBERGVWNkhsA8Pf3h7+/v2Z18MaNG3MCP0sRPhWwtQdcOGyfiIisT42apdRqNRYsWAAPDw80bdoUTZs2haenJ95//32oDe3MSqYnkwER84FwTthHRETWp0ZPbt5++2388MMPWLhwIfr06QMAOHDgAObPn4+CggJ8+OGHRg2SiIiIqLpqlNz8+OOP+P777zWrgQNAx44d0ahRI7z66qtMboiIiEgyNWqWSk9PR5s2bXTK27Rpg/T09FoHRURERFRTNUpuwsLCsHjxYp3yxYsXo2PHjrUOigykzAF2Lah+fVuHuouFiIhIYjVKbj755BOsWLECoaGhmDBhAiZMmIDQ0FCsWrUKn332WbXPEx0dje7du8PNzQ2+vr6IiorC5cuXqzxu3bp1aNOmDRwdHdGhQwf89ddfNfka1mP/J8A/i6qu5+QFuDcSR0sRERFZqRolN3379sWVK1cwfPhwZGZmIjMzE0899RTOnz+Pn3+u/qRw+/btw5QpU3DkyBHExsaiqKgIjz/+OHJzcys85tChQxg9ejQmTJiA06dPIyoqClFRUTh37lxNvop1uPJ31XVePQq8eROYcQFwblDnIREREUlFJgiCYKyTnTlzBl26dIFKparR8WlpafD19cW+ffvwyCOP6K0zatQo5ObmYsuWLZqyXr16oVOnTli2bFmV11AoFPDw8EBWVpb1LBUx36MadbLqPg4iIqI6Ysjvd42e3NSVrCzxB7hBg4qfLBw+fBgRERFaZZGRkTh8+HCdxmaRmvYBRq4Un9oQERHVEzWeodjY1Go1pk+fjj59+qB9+/YV1ktOToafn59WmZ+fH5KTk/XWVyqVUCqVmm2FQmGcgC3B+K3ihH1ERET1iNk8uZkyZQrOnTuHmJgYo543OjoaHh4emldQUJBRz2/WmNgQEVE9ZNCTm6eeeqrS/ZmZmTUKYurUqdiyZQv279+Pxo0bV1rX398fKSkpWmUpKSnw9/fXW3/OnDmYMWOGZluhUFhXglPAvjRERERlGZTceHhU3nHVw8MDY8eOrfb5BEHAa6+9hg0bNmDv3r0ICQmp8pjw8HDs2rUL06dP15TFxsYiPDxcb325XA65XF7tmCxOxk2pIyAiIjIrBiU3K1euNOrFp0yZgjVr1mDTpk1wc3PT9Jvx8PCAk5MTAGDs2LFo1KgRoqOjAQDTpk1D3759sWjRIgwZMgQxMTE4ceIEli9fbtTYLIIyG9j2ptRREBERmRVJ+9wsXboUWVlZ6NevHwICAjSvtWvXauokJCQgKSlJs927d2+sWbMGy5cvR1hYGH7//Xds3Lix0k7IVmtxDyCBo8SIiIjKMuo8N5bAqua5qWx+mye+ALq9ZLpYiIiI6pDFznNDRtKwBRD2nNRREBERScJs5rkhI5pyHLBh3kpERPUTkxtLIwhA7FzAu1XFdZjYEBFRPcbkxtIkHAYOfS11FERERGaL/8S3NPmZUkdARERk1pjcWBoZ/8iIiIgqw19KS8P1ooiIiCrFPjeWIi8dsHOs+smNrYNp4iEiIjJTTG4sQX4G8EmImNyMWq2/zrgtgIt35aOoiIiI6gEmN5Yg6az4XlwAFOborxP8EJusiIiIwD43lqFsU9S6cbr752YwsSEiInqAyY0lqCpx4aR9REREGvxVtAQc/k1ERFRt/NW0CGxyIiIiqi4mN5aAT26IiIiqjb+alqCyPjcOrqaLg4iIyAIwubEE59ZXvE9ma7o4iIiILACTG0twbXfF+x6ZZbo4iIiILACTG0tw73LF+7q9aLo4iIiILACTG3NXmFf5fht708RBRERkIZjcmLvigsr32zuaJg4iIiILweTG3AlCxfs6jTFdHERERBaCyY25u3Oy4n1cT4qIiEgHkxtzt+bpivdxGDgREZEOJjeWjDMXExER6eCvoyXzCpY6AiIiIrNjJ3UAVANugUDok0CvV6SOhIiIyOwwubFEzfsDgz6WOgoiIiKzxGYpc3brcAU7OEqKiIioIkxuzNm5P/SXN+9v2jiIiIgsCJulzNnx77S3O78AtB0KtHxcmniIiIgsAJMbS9J6MNAqUuooiIiIzJqkzVL79+/H0KFDERgYCJlMho0bN1Zaf+/evZDJZDqv5ORk0wRsSoW5umWc14aIiKhKkv5a5ubmIiwsDEuWLDHouMuXLyMpKUnz8vX1raMIJXTgS90yLrdARERUJUmbpQYNGoRBgwYZfJyvry88PT2NH5C5KMwD9n+iW+7d0vSxEBERWRiLbOfo1KkTAgIC8Nhjj+HgwYOV1lUqlVAoFFovs3dhk25Zn2lAg2amj4WIiMjCWFRyExAQgGXLluGPP/7AH3/8gaCgIPTr1w+nTp2q8Jjo6Gh4eHhoXkFBQSaMuKYE3aKmD5k+DCIiIgskEwRBzy+p6clkMmzYsAFRUVEGHde3b180adIEP//8s979SqUSSqVSs61QKBAUFISsrCy4u7vXJuS6874PoCrULhu9Fmg9UJp4iIiIJKZQKODh4VGt32+LHwreo0cPHDhwoML9crkccrnchBEZQfnEBgAadzd9HERERBbIopql9ImLi0NAQIDUYdStNk8ALg2ljoKIiMgiSPrkJicnB1evXtVs37hxA3FxcWjQoAGaNGmCOXPm4M6dO/jpp58AAF9++SVCQkLQrl07FBQU4Pvvv8fu3bvx999/S/UVTKPfbKkjICIishiSJjcnTpxA//6l6yTNmDEDADBu3DisWrUKSUlJSEhI0OwvLCzEzJkzcefOHTg7O6Njx47YuXOn1jksXnaKbpl/B9PHQUREZKHMpkOxqRjSIUkSy/sDd8uN/pqfJU0sREREZsKQ32+L73NjVc7E6CY27o2liYWIiMhCMbkxJxv+o1s2Pd70cRAREVkwJjfmzoZ/RERERIbgLycRERFZFSY35iyol9QREBERWRwmN+ZMJpM6AiIiIovD5MZcFObplsn4x0NERGQo/nqai0WtdcuY3BARERmMv57mQqnQLWOzFBERkcGY3Jg1JjdERESGYnJjDm6f0F/OZikiIiKD8dfTHPz+kv5yG1vTxkFERGQFmNyYg+wk/eX2zqaNg4iIyAowuTEHqkLt7UfeAHxDgciPpImHiIjIgtlJHQDp0XUc8OjbUkdBRERkkfjkRmpqlW4ZOxITERHVGH9Fpaa4q1vG5IaIiKjG+CsqNX0jouydTB8HERGRlWByIzVB0C3jKCkiIqIaY3IjtS9Ctbef/B9gay9NLERERFaAyY256fS81BEQERFZNCY3UrpzUnvbrz1gwz8SIiKi2uAvqZRy72lvd39ZmjiIiIisCJMbKd2/pr3duLs0cRAREVkRJjdS2jFHe9u/vTRxEBERWREmN0RERGRVmNyYi8hoqSMgIiKyCkxupOToUfo5/FXp4iAiIrIiTG6koswGCrLEz8OWSBsLERGRFWFyI5U9ZZqhlDnSxUFERGRlmNxIJe4XqSMgIiKySpImN/v378fQoUMRGBgImUyGjRs3VnnM3r170aVLF8jlcrRo0QKrVq2q8ziNLut2aZMUAEDP4plERERUI5ImN7m5uQgLC8OSJdXrc3Ljxg0MGTIE/fv3R1xcHKZPn46XX34ZO3bsqONIjezAl9rbXsFSREFERGSV7KS8+KBBgzBo0KBq11+2bBlCQkKwaNEiAEDbtm1x4MABfPHFF4iMjKyrMI2v/KrfrQZKEwcREZEVsqg+N4cPH0ZERIRWWWRkJA4fPlzhMUqlEgqFQuslKUEAjv+gXSaTSRMLERGRFbKo5CY5ORl+fn5aZX5+flAoFMjPz9d7THR0NDw8PDSvoKAgU4Rasc2vASpl6Xaf6ZKFQkREZI0sKrmpiTlz5iArK0vzSkxMlDag0z9rb/fi5H1ERETGJGmfG0P5+/sjJSVFqywlJQXu7u5wcnLSe4xcLodcLjdFeDXj6it1BERERFbFop7chIeHY9euXVplsbGxCA8PlyiiWuoylv1tiIiIjEzS5CYnJwdxcXGIi4sDIA71jouLQ0JCAgCxSWns2LGa+pMnT8b169fxxhtv4NKlS/jmm2/w22+/4b///a8U4dfew7OkjoCIiMjqSJrcnDhxAp07d0bnzp0BADNmzEDnzp0xd+5cAEBSUpIm0QGAkJAQbN26FbGxsQgLC8OiRYvw/fffW84wcLVae9tef1MaERER1ZxMEIR6NT2uQqGAh4cHsrKy4O7ubtqLn1wF/DmtdHvObUDuZtoYiIiILJAhv98W1efG4l3YrL1txyc3RERExsbkxpSuaXeGhq1FDVYjIiKyCExuTKWoQOoIiIiI6gUmN6ay+32pIyAiIqoXmNyYyuHF2ttjfpcmDiIiIivH5MYU1Crtbb/2QMvHpImFiIjIyjG5MYWicot6vrRdmjiIiIjqASY3pnDnhPY257YhIiKqM0xuTMHOsfSzb6h0cRAREdUDTG5MQVVY+jl8qnRxEBER1QNMbupa7n3gx6Gl27b20sVCRERUDzC5qWvrJ2pvu/hIEwcREVE9weSmLuVn6C650KyfJKEQERHVF0xu6tI34drbczMAmUyaWIiIiOoJJjd1KTtJe9uGt5uIiKiu8dfWVEbHSB0BERFRvcDkpq78s0h7u/UgaeIgIiKqZ5jcGJlKpRY/7FogbSBERET1lJ3UAViLi0f/hvuOaci3c0cLuzTtnWM3SxMUERFRPcTkxkgcXdzRSH0XKLwLlJmQGB2eAZr1lSwuIiKi+obNUkbStGUH3UK5BzDiO9MHQ0REVI8xuTESG7kL/u3+nmZ7W4MXIMy+JWFERERE9ROTGyNqOWQ6/hh6HsEFa/DK3UGY+NMJqUMiIiKqd5jcGNmIro0xuIM/AGDnxVT0/2yvtAERERHVM0xu6sDi0V00n2/cy8Xa4wkSRkNERFS/MLmpAzY2MsTPf1yz/eYf8chVFksYERERUf3B5KaOuDnaY9nzXTXb7ebtkDAaIiKi+oPJTR0a2N4fQzoGaLaDZ2+VMBoiIqL6gclNHVs8urPW9osrj0kUCRERUf3A5KaOyWQyXFgQqdneczkNPxy4IWFERERE1o3JjQk4O9hh54zSJRje33IBj3+xD4IgSBgVERGRdTKL5GbJkiUIDg6Go6MjevbsiWPHKm66WbVqFWQymdbL0dHRhNHWTAtfVzzVuZFm+0pKDu7nFlZyBBEREdWE5MnN2rVrMWPGDMybNw+nTp1CWFgYIiMjkZqaWuEx7u7uSEpK0rxu3bKMZQ4+H9UJPUMaaLa7fbATBUUqCSMiIiKyPpInN59//jkmTpyIF198EaGhoVi2bBmcnZ2xYsWKCo+RyWTw9/fXvPz8/EwYce2s/U+41nabd7dLFAkREZF1kjS5KSwsxMmTJxEREaEps7GxQUREBA4fPlzhcTk5OWjatCmCgoIwbNgwnD9/vsK6SqUSCoVC6yW1S+8P1NoOnr0VWXlFEkVDRERkXSRNbu7duweVSqXz5MXPzw/Jycl6j2ndujVWrFiBTZs24ZdffoFarUbv3r1x+/ZtvfWjo6Ph4eGheQUFBRn9exjK0d5WJ8EJW/A3LiVLn3gRERFZOsmbpQwVHh6OsWPHolOnTujbty/Wr18PHx8ffPvtt3rrz5kzB1lZWZpXYmKiiSPWz9HeFntn9dMqG/jlP7iTmS9NQERERFZC0uTG29sbtra2SElJ0SpPSUmBv79/tc5hb2+Pzp074+rVq3r3y+VyuLu7a73MRbC3i06C02fhbhy5fl+agIiIiKyApMmNg4MDunbtil27dmnK1Go1du3ahfDw8EqOLKVSqRAfH4+AgICqK5uhYG8XXFyg3UT17PIjYj+cfPbDISIiMpTkzVIzZszAd999hx9//BEXL17EK6+8gtzcXLz44osAgLFjx2LOnDma+gsWLMDff/+N69ev49SpU3j++edx69YtvPzyy1J9hVpzcrDF7pl9dcrD3vsbadlKCSIiIiKyXHZSBzBq1CikpaVh7ty5SE5ORqdOnbB9+3ZNJ+OEhATY2JTmYBkZGZg4cSKSk5Ph5eWFrl274tChQwgNDZXqKxhFMx9XXPtoMJq/9ZdWefcPd6JHSAP8OrEXbG1kEkVHRERkOWRCPVsDQKFQwMPDA1lZWWbV/6asDadv479rz+iUvz+sHZ7v1RQyGZMcIiKqXwz5/Za8WYp0De/cGIdmP6pT/u6m8wiZ8xdSFAUSREVERGQZmNyYqUBPJ8TPfxyezvY6+3p+tAvBs7ciNZtJDhERUXlslrIA2QVF6DD/7wr3j+8djLeHtEVhsRrODrZstiIiIqtjyO83kxsLIQgCnlx8EPF3sqqsu3PGI2jh62aCqIiIiEyDyU0lLDW5KSEIAu5k5iNqySHcy6l4mPjKF7ujc5AnPJ0dTBgdERFR3WByUwlLT27KOpOYiWFLDlZZb/XLPdG1qRfsbGRIyVaikaeTCaIjIiIyHiY3lbCm5KZEXGImoqqR5JRo5u2CIrUaPq5yTHqkGQa2t8zZnYmIqP5gclMJa0xuSlxLy8Hyfdex9oRhi4NO7tscy/Zdw3tPtkNBkQqDOwQgqIFzHUVJRERkOCY3lbDm5Kask7cyMGLpoRof36WJJ5r5uGJ6REsUqwT8eeYu+rb2QZFKQNemXkaMlIiIqGpMbipRX5KbstRqAfuupOHFVceNcr5PR3bEzfu5GNguAKuP3sLA9v7o19pXcy2bcstECIKAzLwieLnUvHNzanYBvJwdYG/LqZmIiOojJjeVqI/JTQlBEFCoUiMzrwhJWQUG9dMxhLODLfq38YWNTIY/z9xFKz9XXEnJwU8v9cAjrXxwL0eJI9fvQy0AA9r4wkVeusRZYbEaO84nI7x5Q3i7ygEA/6Zk47Ev9qNLE0+sf7VPncRMRETmjclNJepzclOZxPQ87DifjEPX7mP3pVSTXXdAG1/0b+OLxIw8vNqvBcLeEycrbO7jgl0z+6FIpcaY747i2M10AMDNhUM0x6ZlK3Ho2j0Mah+A/EIVlv9zDVGdGqGlnxsEQUBGXhGcHWzhYGuj8zSJiIgsC5ObSjC5qb5LyQp4ONnj5K0MXE3NwZc7/5U6JADAjMdaoXtwA7y1IR437uVi4sMhSFEosfnMXQDApfcHYvYfZ7ExTtxu6OKAw3MGYOfFFPQIaQBvVzkKi9W4nZGHRl5OuJiUjQ6NPFCsVkNuZ6v3moIgaGZ+nrrmFDLzivDTSz2YNFmIC3cV8HWXa54GEpHlYXJTCSY3tVPyI59dUISb9/LQ1NsZt+7l4ZlvDyO/SCV1eNWydlIvjFp+pNI6D7f0xpT+LWBva4PQAHe0nbsdALB7Zl88umgfAHGUWaCnI8aGB2sdq1YL2HYuGXM3ncPQsEDsvpSKiY80w/M9m2gtjRGXmAkPJ3uEeLugoEgFR3vtxCoxPQ9ODrb8Qa6lS8kKDPzyH8hkwI3oIVUfQERmiclNJZjc1D1BEHA7Ix9X03LQtakXJv54AkdvpGv2hwV54kxipnQBSqRpQ2c81bkx0nOVuJKSg8PX7wMA+rf2wZ7LaXhjYGuEN2uI62m5UKkFvPHHWQBAnxYN8WgbP5y/m4W3B7dFfpEK8zdfQN/WPugc5Il2ge6QyWRYdyIRn8dewffjuqFdoAeSsvJxJjEL/dv4VPhEyhAqtYAXVx1HM28XzH+yHQCgoEiF6TFxeLStL57pFlSj897LUeLPM3fxVOfG8HiwUGyqogA+bnKjrJP20+GbmLvpPADtZk0isixMbirB5EY6RSq1zminxPQ8eLk4wMXBFutO3AYAHL+ZjrjETAR4OiH+diYy8ooQ6OGIu1lcBb02Pn8mDK5yO0z6+SQAwMdNjrRsJXo3b4hD18RE68mwQE3z3rQBLZFdUIzMvEIMDQvEzfu5eO/PCwCANS/3RO8W3lqJw43owVh34jaOXL+PV/u3wImb6ThzOxO/HkvEoPb+WPp8VwDin/mha/cwoktj2Nna4Jllh3HsZjoi2/lhyXNdsGDLBfx0+BbG9w7GrMjW2HMpFdkFxXiuZxPNd1GrBaRmK+Hnrj8BKnnCePNeLv75Nw3vPohx3tBQdGzsgS5NvHSOEwQB3/9zA6GB7ujTwtuYt77OxCVm4m5mPgZ3ECfiXHs8AXcy8jHj8dYGnScxPQ97LqdiVPcgoyTC5iw5q6DCvzdk3pjcVILJjXU4ePUe3Bzt0MbfHQ52Njh49R7SspWQyYAURQHWn7oDL2cHuMhtsfOi6TpIU916MiwQPUIa4J2N5zRlY8Ob4qfDtzC6RxOkKAo0HeLfHNgGH2+/VOG5mvm4YP0rvTFnfTzu5xRiWOdAvL2h9LzPdGuMvq184exgi6SsAlxOVqBdIw+08HXF2mOJ8HJxwISHQuDjJodaLaBYLcDBzgY37uUiPVeJrk0b6L1uXmEx1hxNwPZzyfhiVCc09nJC7IUUtPJzw7f7r+PXYwlY/kJX9G7hjTnr4+HlbI8ADydM7tsMRSoBpxMy0LmJFxzsbBA8eysA4K//exihge6a7U1T+iAsyBMAUKxSo6BYDdcyoxLL67Tgb2TmFWFq/xaYFdkaBUUqjFp+BN2beuGdJ0Kr94dTiQt3Fdh2LgmT+zbXGh0J6J8+whBFKjVO3cpApyaeFSZmxSo1bG1k2BqfhKlrTuP5Xk3wQVSHGl/TGhWr1FAJglknt0xuKsHkhsoqLFbDwc4GymIVbmfk4+/zKTh8/T7iEjLw1bOd4Whvi3s5Sjja22Lr2bvo19oX09fGSR02Edwd7aAoKAYg9hHLzCtC/J0szf6nOjfC3KGhGPP9UZy/q8CcQW3g4ybH5eRsdA9ugDYBboi/nYVClRrTYuI0x/3xSjhm/HYGt+7nAQB2zeyLWevO4F6OEonp+Vg5vjuKVGr0CGmgszDvkj1Xsf1c8oNlXfxxP6cQz/9wFFdTcwAA43sHY/6T7XAxSYFFf1+Gu6M9dl1KxcYpfRDi7aI5z6VkBZKyCtArpCEuJGWhSxMvpGUr8fup2whr7Im7mfkIauCM/VfSkKJQ4o9T4lPf3yeHo1uwdlJZUKTCY1/sQ4i3K/ZfSdOUlzRRFharsfnMXTzW1k/TLFp2AAEAfLrjEpbsuYYtrz2E9o08qvyzOXT1Hv44dQdznwjVnLO6ilRqKKtIRuvCE//7B2nZSux/o3+FCU5WfhES7uehQ2MPZOUV4cDVexjQ1lenv2BdYXJTCSY3ZArHb6ajoYsDghu6wMZGpvmXvZ2NDCpBgL2tDTJyCxF3OxN9W/ogIV38IckrVOGHAzfwRMcAbD+XjLUnEvF4qB9eeigE+UUqvLhSnIhx3tBQXE3NweqjCZpr/jKhJ1755SSylcWSfGcic1G2Gfu5nk2wpsx/JyW2T38YZ29n4Y3fz+o9x2dPh+G7/ddxOSVbq/zQ7EeRnluIKWtO4db9PIwNb4pH2/jidkY+ilRqPNrGF30/3aupP7lvc7we2RrrTiRi9vp4TXlkOz8EejqhoEiFAA8n9Gvtg46NPfHUNwdxKiET/7zRHxl5hUhIz0MDZwdcTslG31Y+yMovgr+HIxb8eQHbziXjnSFtMbpHE+y6lAq1WsATHQNwIUmBVn5usLe1waa4O7h1Pw9PdgqErUwGub0NsguK0dLXFSdvZcDfwxG+bo5o9c42AMD0iJY4ej0dYUGeGNFFnFoDEBO+hz/Zg9sZ+Vj+Qlcs338dJ25l4D99m2HOoLY4k5gJHzc55HY2aFhHgyCY3FSCyQ3VN8UqNbILimEjk+n9V2R6biHWHL2F4V0aIykzHyq1AJVaQO8W3ricnI27Wfk4fiMdk/s1x5nETOy+lIpm3i5IyipAC19XPNTSG7svpsLJwRYtfF0RfzsLX+/6F/4ejni2RxMIgoA3/4jXE1mpEG8X3LiXq1Xm7GCLvELLGIFHRNqufjgIdkaeUZ7JTSWY3BBZjgt3FSgoVqFLk9L1zJTFKlxJzkFooDuy8otw8lYG+rf20fyPNC1bifl/nsfzPZuiha8r/opPQtemXlh/6g5ej2wNuZ0N7uUo4epoh6JiAQu3X0RkO38cuZ4OFwdbuDvZo0NjDzjY2mDFwRsY07MJbmfk44OtF6EsUkFRUIyoToF4qKUPVh26AS9nB/zz7z1M7d8CzX1dcPR6OmKOay9eOza8KcIae2LmujOaMkd7GxQUqU1zI4lMrIGLA069+5hRz8nkphJMbojIXAiCALUA2NawQ62yWIWcgmIcv5mBR1p5w8neFsVq8X/p9rY22HE+Ga393BDs7YIilRoqtaDpH1GkUsPORgaZTIZcZTHUgoBilYDr93Lg4+qIW+m5sLe1gaezPVIUStzLViLQ0wmf7rgERUExrqbmYHAHf+QXqtC1qRd6NmuIwmI1/jh5G/uupMHT2R5jw4NxL0eJX47cwvwn2+HW/Tw09nLC7Yx8fB57Reu7NGngjLAgT+y9lIpsZTGGdQrEpri76BHcQDNDeXkRbX1x634ecpTFSOJoSrNz+YOBRu2gzOSmEkxuiIjMg0ot1Dixq41ilbrKJpPCYjH5KxnJdfT6fcjtbdHcxwUuDnaavnQl+0s+37iXi1xlMVr6ucLOxgY5ymKo1AIauDjgdkYeAjyckFtYjIIiFdwd7ZGeW4g7mfkI8nLGh39dhKOdDSb3a47dF1PxXM8mSM1W4mKSAvmFKiQrxAWErzzoBzR7UBvcvJ+LBs4O8HV3xNazSZiy5hRkMuDZ7kEIa+yJKyk56NWsAVwd7bDiwE009nLCqkM3AQA9Qhpg0sPN8M3eqxjZNQg/Hb6Jq6k5kNvZILdMk/D0iJaIv52F4zfT8c6QULzxx1mM7x2MrfFJSMtWauo91aUR1p+6AwDYOaMvWvi6Gu3PDGByUykmN0RERJbHkN9v4/b2ISIiIpIYkxsiIiKyKkxuiIiIyKowuSEiIiKrwuSGiIiIrAqTGyIiIrIqTG6IiIjIqphFcrNkyRIEBwfD0dERPXv2xLFjxyqtv27dOrRp0waOjo7o0KED/vrrLxNFSkREROZO8uRm7dq1mDFjBubNm4dTp04hLCwMkZGRSE1N1Vv/0KFDGD16NCZMmIDTp08jKioKUVFROHfunIkjJyIiInMk+QzFPXv2RPfu3bF48WIAgFqtRlBQEF577TXMnj1bp/6oUaOQm5uLLVu2aMp69eqFTp06YdmyZVVejzMUExERWR6LmaG4sLAQJ0+eREREhKbMxsYGEREROHz4sN5jDh8+rFUfACIjIyusT0RERPWLnZQXv3fvHlQqFfz8/LTK/fz8cOnSJb3HJCcn662fnJyst75SqYRSWbqwl0KhqGXUREREZM4k73NT16Kjo+Hh4aF5BQUFSR0SERER1SFJkxtvb2/Y2toiJSVFqzwlJQX+/v56j/H39zeo/pw5c5CVlaV5JSYmGid4IiIiMkuSNks5ODiga9eu2LVrF6KiogCIHYp37dqFqVOn6j0mPDwcu3btwvTp0zVlsbGxCA8P11tfLpdDLpdrtkv6T7N5ioiIyHKU/G5XaxyUILGYmBhBLpcLq1atEi5cuCBMmjRJ8PT0FJKTkwVBEIQXXnhBmD17tqb+wYMHBTs7O+Gzzz4TLl68KMybN0+wt7cX4uPjq3W9xMREAQBffPHFF1988WWBr8TExCp/6yV9cgOIQ7vT0tIwd+5cJCcno1OnTti+fbum03BCQgJsbEpbz3r37o01a9bgnXfewVtvvYWWLVti48aNaN++fbWuFxgYiMTERLi5uUEmkxn1uygUCgQFBSExMZHDzOsQ77Np8D6bBu+z6fBem0Zd3WdBEJCdnY3AwMAq60o+z4014Rw6psH7bBq8z6bB+2w6vNemYQ732epHSxEREVH9wuSGiIiIrAqTGyOSy+WYN2+e1ugsMj7eZ9PgfTYN3mfT4b02DXO4z+xzQ0RERFaFT26IiIjIqjC5ISIiIqvC5IaIiIisCpMbIiIisipMboxkyZIlCA4OhqOjI3r27Iljx45JHZJZ279/P4YOHYrAwEDIZDJs3LhRa78gCJg7dy4CAgLg5OSEiIgI/Pvvv1p10tPTMWbMGLi7u8PT0xMTJkxATk6OVp2zZ8/i4YcfhqOjI4KCgvDJJ5/U9VczK9HR0ejevTvc3Nzg6+uLqKgoXL58WatOQUEBpkyZgoYNG8LV1RUjRozQWZw2ISEBQ4YMgbOzM3x9ffH666+juLhYq87evXvRpUsXyOVytGjRAqtWrarrr2c2li5dio4dO8Ld3R3u7u4IDw/Htm3bNPt5j+vGwoULIZPJtNYa5L2uvfnz50Mmk2m92rRpo9lvEffY0LWgSFdMTIzg4OAgrFixQjh//rwwceJEwdPTU0hJSZE6NLP1119/CW+//bawfv16AYCwYcMGrf0LFy4UPDw8hI0bNwpnzpwRnnzySSEkJETIz8/X1Bk4cKAQFhYmHDlyRPjnn3+EFi1aCKNHj9bsz8rKEvz8/IQxY8YI586dE3799VfByclJ+Pbbb031NSUXGRkprFy5Ujh37pwQFxcnDB48WGjSpImQk5OjqTN58mQhKChI2LVrl3DixAmhV69eQu/evTX7i4uLhfbt2wsRERHC6dOnhb/++kvw9vYW5syZo6lz/fp1wdnZWZgxY4Zw4cIF4X//+59ga2srbN++3aTfVyqbN28Wtm7dKly5ckW4fPmy8NZbbwn29vbCuXPnBEHgPa4Lx44dE4KDg4WOHTsK06ZN05TzXtfevHnzhHbt2glJSUmaV1pamma/JdxjJjdG0KNHD2HKlCmabZVKJQQGBgrR0dESRmU5yic3arVa8Pf3Fz799FNNWWZmpiCXy4Vff/1VEARBuHDhggBAOH78uKbOtm3bBJlMJty5c0cQBEH45ptvBC8vL0GpVGrqvPnmm0Lr1q3r+BuZr9TUVAGAsG/fPkEQxPtqb28vrFu3TlPn4sWLAgDh8OHDgiCIiaiNjY1mMVtBEISlS5cK7u7umnv7xhtvCO3atdO61qhRo4TIyMi6/kpmy8vLS/j+++95j+tAdna20LJlSyE2Nlbo27evJrnhvTaOefPmCWFhYXr3Wco9ZrNULRUWFuLkyZOIiIjQlNnY2CAiIgKHDx+WMDLLdePGDSQnJ2vdUw8PD/Ts2VNzTw8fPgxPT09069ZNUyciIgI2NjY4evSops4jjzwCBwcHTZ3IyEhcvnwZGRkZJvo25iUrKwsA0KBBAwDAyZMnUVRUpHWv27RpgyZNmmjd6w4dOmgWswXE+6hQKHD+/HlNnbLnKKlTH/8bUKlUiImJQW5uLsLDw3mP68CUKVMwZMgQnfvBe208//77LwIDA9GsWTOMGTMGCQkJACznHjO5qaV79+5BpVJp/SECgJ+fH5KTkyWKyrKV3LfK7mlycjJ8fX219tvZ2aFBgwZadfSdo+w16hO1Wo3p06ejT58+aN++PQDxPjg4OMDT01Orbvl7XdV9rKiOQqFAfn5+XXwdsxMfHw9XV1fI5XJMnjwZGzZsQGhoKO+xkcXExODUqVOIjo7W2cd7bRw9e/bEqlWrsH37dixduhQ3btzAww8/jOzsbIu5x3a1PgMRWYQpU6bg3LlzOHDggNShWKXWrVsjLi4OWVlZ+P333zFu3Djs27dP6rCsSmJiIqZNm4bY2Fg4OjpKHY7VGjRokOZzx44d0bNnTzRt2hS//fYbnJycJIys+vjkppa8vb1ha2ur01M8JSUF/v7+EkVl2UruW2X31N/fH6mpqVr7i4uLkZ6erlVH3znKXqO+mDp1KrZs2YI9e/agcePGmnJ/f38UFhYiMzNTq375e13Vfayojru7u8X8z7C2HBwc0KJFC3Tt2hXR0dEICwvDV199xXtsRCdPnkRqaiq6dOkCOzs72NnZYd++ffj6669hZ2cHPz8/3us64OnpiVatWuHq1asW8/eZyU0tOTg4oGvXrti1a5emTK1WY9euXQgPD5cwMssVEhICf39/rXuqUChw9OhRzT0NDw9HZmYmTp48qamze/duqNVq9OzZU1Nn//79KCoq0tSJjY1F69at4eXlZaJvIy1BEDB16lRs2LABu3fvRkhIiNb+rl27wt7eXuteX758GQkJCVr3Oj4+XiuZjI2Nhbu7O0JDQzV1yp6jpE59/m9ArVZDqVTyHhvRgAEDEB8fj7i4OM2rW7duGDNmjOYz77Xx5eTk4Nq1awgICLCcv89G6ZZcz8XExAhyuVxYtWqVcOHCBWHSpEmCp6enVk9x0padnS2cPn1aOH36tABA+Pzzz4XTp08Lt27dEgRBHAru6ekpbNq0STh79qwwbNgwvUPBO3fuLBw9elQ4cOCA0LJlS62h4JmZmYKfn5/wwgsvCOfOnRNiYmIEZ2fnejUU/JVXXhE8PDyEvXv3ag3rzMvL09SZPHmy0KRJE2H37t3CiRMnhPDwcCE8PFyzv2RY5+OPPy7ExcUJ27dvF3x8fPQO63z99deFixcvCkuWLKlXQ2dnz54t7Nu3T7hx44Zw9uxZYfbs2YJMJhP+/vtvQRB4j+tS2dFSgsB7bQwzZ84U9u7dK9y4cUM4ePCgEBERIXh7ewupqamCIFjGPWZyYyT/+9//hCZNmggODg5Cjx49hCNHjkgdklnbs2ePAEDnNW7cOEEQxOHg7777ruDn5yfI5XJhwIABwuXLl7XOcf/+fWH06NGCq6ur4O7uLrz44otCdna2Vp0zZ84IDz30kCCXy4VGjRoJCxcuNNVXNAv67jEAYeXKlZo6+fn5wquvvip4eXkJzs7OwvDhw4WkpCSt89y8eVMYNGiQ4OTkJHh7ewszZ84UioqKtOrs2bNH6NSpk+Dg4CA0a9ZM6xrW7qWXXhKaNm0qODg4CD4+PsKAAQM0iY0g8B7XpfLJDe917Y0aNUoICAgQHBwchEaNGgmjRo0Srl69qtlvCfdYJgiCYJxnQERERETSY58bIiIisipMboiIiMiqMLkhIiIiq8LkhoiIiKwKkxsiIiKyKkxuiIiIyKowuSEiIiKrwuSGiOo9mUyGjRs3Sh0GERkJkxsiktT48eMhk8l0XgMHDpQ6NCKyUHZSB0BENHDgQKxcuVKrTC6XSxQNEVk6PrkhIsnJ5XL4+/trvUpWbpfJZFi6dCkGDRoEJycnNGvWDL///rvW8fHx8Xj00Ufh5OSEhg0bYtKkScjJydGqs2LFCrRr1w5yuRwBAQGYOnWq1v579+5h+PDhcHZ2RsuWLbF58+a6/dJEVGeY3BCR2Xv33XcxYsQInDlzBmPGjMGzzz6LixcvAgByc3MRGRkJLy8vHD9+HOvWrcPOnTu1kpelS5diypQpmDRpEuLj47F582a0aNFC6xrvvfcennnmGZw9exaDBw/GmDFjkJ6ebtLvSURGYrQlOImIamDcuHGCra2t4OLiovX68MMPBUEQVzafPHmy1jE9e/YUXnnlFUEQBGH58uWCl5eXkJOTo9m/detWwcbGRkhOThYEQRACAwOFt99+u8IYAAjvvPOOZjsnJ0cAIGzbts1o35OITId9bohIcv3798fSpUu1yho0aKD5HB4errUvPDwccXFxAICLFy8iLCwMLi4umv19+vSBWq3G5cuXIZPJcPfuXQwYMKDSGDp27Kj57OLiAnd3d6Smptb0KxGRhJjcEJHkXFxcdJqJjMXJyala9ezt7bW2ZTIZ1Gp1XYRERHWMfW6IyOwdOXJEZ7tt27YAgLZt2+LMmTPIzc3V7D948CBsbGzQunVruLm5ITg4GLt27TJpzEQkHT65ISLJKZVKJCcna5XZ2dnB29sbALBu3Tp069YNDz30EFavXo1jx47hhx9+AACMGTMG8+bNw7hx4zB//nykpaXhtddewwsvvAA/Pz8AwPz58zF58mT4+vpi0KBByM7OxsGDB/Haa6+Z9osSkUkwuSEiyW3fvh0BAQFaZa1bt8alS5cAiCOZYmJi8OqrryIgIAC//vorQkNDAQDOzs7YsWMHpk2bhu7du8PZ2RkjRozA559/rjnXuHHjUFBQgC+++AKzZs2Ct7c3Ro4cabovSEQmJRMEQZA6CCKiishkMmzYsAFRUVFSh0JEFoJ9boiIiMiqMLkhIiIiq8I+N0Rk1thyTkSG4pMbIiIisipMboiIiMiqMLkhIiIiq8LkhoiIiKwKkxsiIiKyKkxuiIiIyKowuSEiIiKrwuSGiIiIrAqTGyIiIrIq/w94xUD25bwrHAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 3.8022 - accuracy: 0.5078\n",
            "[3.80216121673584, 0.5077844262123108]\n",
            "131/131 [==============================] - 1s 3ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.52      0.50      0.51      2133\n",
            "     class 0       0.50      0.51      0.51      2042\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.51      0.51      0.51      4175\n",
            "weighted avg       0.51      0.51      0.51      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "534/534 [==============================] - 2s 3ms/step - loss: 0.8964 - accuracy: 0.8478\n",
            "[0.8964493274688721, 0.8478260636329651]\n",
            "534/534 [==============================] - 2s 3ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.87      0.82      0.84      8533\n",
            "     class 0       0.83      0.87      0.85      8533\n",
            "\n",
            "    accuracy                           0.85     17066\n",
            "   macro avg       0.85      0.85      0.85     17066\n",
            "weighted avg       0.85      0.85      0.85     17066\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGxCAYAAABr1xxGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNNklEQVR4nO3de1hU5do/8O+ADOcZBDnGQRRRyTMdGDXTZIuFpYmZbktUtFS0xK2Se6d5KO21A2mmphZoP81DZTtlK6KmpWAqiVtJyAOECoNsEQYVGJhZvz+M0QkYGWcQ1vj9XNe63j1rPetZz+KV5ua+n/UsiSAIAoiIiIgskFVzD4CIiIioqTDQISIiIovFQIeIiIgsFgMdIiIislgMdIiIiMhiMdAhIiIii8VAh4iIiCwWAx0iIiKyWK2aewBUP61Wi4KCAjg7O0MikTT3cIiIyAiCIKC8vBw+Pj6wsmq6nEJlZSXUarVZ+pJKpbCzszNLXy0JA50WqqCgAH5+fs09DCIiMsGlS5fg6+vbJH1XVlYiMMAJyqsas/Tn5eWF3Nxciwt2GOi0UM7OzgCA1KOecHRihZEs0/yBQ5p7CERNokarxsHiDbr/ljcFtVoN5VUNcjMCIHM27XtCVa5FYOgfUKvVDHTowagtVzk6WcHJxH/ARC1VKytpcw+BqEk9iKkHMmcrkwMdS8ZAh4iISMQ0ghYaE1/PrRG05hlMC8RAh4iISMS0EKCFaZGOqee3ZMx1ERERkcViRoeIiEjEtNDC1MKT6T20XAx0iIiIREwjCNAIppWeTD2/JWPpioiIiCwWMzpEREQixsnIhjHQISIiEjEtBGgY6DSIpSsiIiKyWMzoEBERiRhLV4Yx0CEiIhIxPnVlGAMdIiIiEdP+uZnah6XiHB0iIiKyWMzoEBERiZjGDE9dmXp+S8ZAh4iISMQ0Aszw9nLzjKUlYumKiIiILBYzOkRERCLGyciGMdAhIiISMS0k0EBich+WiqUrIiIislgMdIiIiERMK5hna6y2bdtCIpHU2WJjYwEAlZWViI2NhZubG5ycnBAVFYWioiK9PvLz8xEZGQkHBwd4eHhg9uzZqKmp0Wtz8OBB9OrVC7a2tggKCkJSUtJ9/XwY6BAREYmY5s/SlalbYx0/fhyFhYW6LTU1FQDw0ksvAQDi4uKwc+dObN++HYcOHUJBQQGGDx9+Z7waDSIjI6FWq5GWloYNGzYgKSkJ8+fP17XJzc1FZGQkBgwYgMzMTMyYMQMTJ05ESkqK0T8fiSBY8LrPIqZSqSCXy5F2xhtOzoxHyTLNCRvW3EMgahI1WjX2Fa1DWVkZZDJZk1yj9nvilywvk78nbpRr8eSjyvsa74wZM7Br1y6cO3cOKpUK7u7u2Lx5M0aMGAEAyM7ORufOnZGeno6wsDDs3r0bQ4YMQUFBATw9PQEAa9asQXx8PIqLiyGVShEfH4/k5GScOXNGd51Ro0ahtLQUe/bsMWp8/AYlIiISsQed0bmbWq3G//t//w8TJkyARCJBRkYGqqurER4ermvTqVMn+Pv7Iz09HQCQnp6Orl276oIcAIiIiIBKpUJWVpauzd191Lap7cMYfOqKiIhIxLSCBFrBxKeu/jxfpVLp7be1tYWtrW2D533//fcoLS3FuHHjAABKpRJSqRQuLi567Tw9PaFUKnVt7g5yao/XHjPURqVSoaKiAvb29o2+N2Z0iIiIRMycGR0/Pz/I5XLdtnTpUoPX/uKLL/Dss8/Cx8fnQdzqfWFGh4iIiAAAly5d0pujYyib88cff2Dfvn347rvvdPu8vLygVqtRWlqql9UpKiqCl5eXrs2xY8f0+qp9KuvuNn99UquoqAgymcyobA7AjA4REZGoaWBllg0AZDKZ3mYo0ElMTISHhwciIyN1+0JDQ2FjY4P9+/fr9uXk5CA/Px8KhQIAoFAocPr0aVy9elXXJjU1FTKZDCEhIbo2d/dR26a2D2Mwo0NERCRighnm6AhGnq/VapGYmIjo6Gi0anUnlJDL5YiJicHMmTPh6uoKmUyG6dOnQ6FQICwsDAAwaNAghISE4NVXX8WyZcugVCrx9ttvIzY2VhdYTZ48GStXrsScOXMwYcIEHDhwANu2bUNycrLR98ZAh4iIiIyyb98+5OfnY8KECXWOJSQkwMrKClFRUaiqqkJERARWrVqlO25tbY1du3ZhypQpUCgUcHR0RHR0NBYtWqRrExgYiOTkZMTFxWH58uXw9fXF+vXrERERYfRYuY5OC8V1dOhhwHV0yFI9yHV09p4OgKOJ3xM3y7UY1PWPJh1vc2FGh4iISMQ0ghU0gmmBjsaCUx5MFRAREZHFYkaHiIhIxLSQQGti3kILy03pMNAhIiISMVNe4XB3H5aKpSsiIiKyWMzoEBERiZh5JiOzdEVEREQt0O05Oia+1NOCS1cMdIiIiERMe9crHO6/D8vN6HCODhEREVksZnSIiIhEjHN0DGOgQ0REJGJaWHEdHQNYuiIiIiKLxYwOERGRiGkECTSCiQsGmnh+S8ZAh4iISMQ0ZnjqSsPSFREREZH4MKNDREQkYlrBCloTn7rS8qkrIiIiaolYujKMpSsiIiKyWMzoEBERiZgWpj81pTXPUFokBjpEREQiZp4FAy23wMNAh4iISMTM8woIyw10LPfOiIiI6KHHjA4REZGIaSGBFqbO0eHKyERERNQCsXRlmOXeGRERET30mNEhIiISMfMsGGi5eQ8GOkRERCKmFSTQmrqOjgW/vdxyQzgiIiJ66DGjQ0REJGJaM5SuuGAgERERtUjmeXu55QY6lntnRERE9NBjRoeIiEjENJBAY+KCf6ae35Ix0CEiIhIxlq4MY6BDREQkYhqYnpHRmGcoLZLlhnBERET00GNGh4iISMRYujKMgQ4REZGI8aWehlnunREREdFDjxkdIiIiERMggdbEycgCHy8nIiKiloilK8Ms986IiIjooceMDhERkYhpBQm0gmmlJ1PPb8kY6BAREYmYxgxvLzf1/JbMcu+MiIiIHnrM6BAREYkYS1eGMdAhIiISMS2soDWxQGPq+S0ZAx0iIiIR0wgSaEzMyJh6fktmuSEcERERNYkrV67glVdegZubG+zt7dG1a1ecOHFCd1wQBMyfPx/e3t6wt7dHeHg4zp07p9dHSUkJxowZA5lMBhcXF8TExODGjRt6bf773//iqaeegp2dHfz8/LBs2TKjx8pAh4iISMRq5+iYujXW9evX0adPH9jY2GD37t347bff8NFHH6F169a6NsuWLcOKFSuwZs0a/PLLL3B0dERERAQqKyt1bcaMGYOsrCykpqZi165d+Omnn/Daa6/pjqtUKgwaNAgBAQHIyMjABx98gAULFmDt2rVG/XxYuiIiIhIxwQxvLxeMOP///u//4Ofnh8TERN2+wMDAu/oS8Mknn+Dtt9/G0KFDAQAbN26Ep6cnvv/+e4waNQpnz57Fnj17cPz4cTz22GMAgE8//RTPPfccPvzwQ/j4+GDTpk1Qq9X48ssvIZVK8eijjyIzMxMff/yxXkB0L8zoEBEREYDbWZS7t6qqqjptfvjhBzz22GN46aWX4OHhgZ49e2LdunW647m5uVAqlQgPD9ftk8vlePLJJ5Geng4ASE9Ph4uLiy7IAYDw8HBYWVnhl19+0bXp168fpFKprk1ERARycnJw/fr1Rt8TAx0iIiIR00Bilg0A/Pz8IJfLddvSpUvrXO/ixYtYvXo1OnTogJSUFEyZMgVvvPEGNmzYAABQKpUAAE9PT73zPD09dceUSiU8PDz0jrdq1Qqurq56berr4+5rNAZLV0RERCKmFUxfB0cr3P6/ly5dgkwm0+23tbWt21arxWOPPYYlS5YAAHr27IkzZ85gzZo1iI6ONmkcTYGBDlmMxX164voVuzr7+7yqRNTiXFRXSvDDe21xcqcbatRW6NivFCMW58LZvVrX9rsFbZF3whmFvzvAs30FZu3+r15f1ZUSfPOvdrh0xglXz9sj5JnrmLAup8nvjQgAHu1VgqixeQjqXA439yosntkDRw/e+as4bsEZhL9QoHdORpob5k8LBQB4eFdg9KSL6Pb4NbR2U6Ok2BY/7vbG1vXtUFNzO8HfNbQEw8b8geBHy+DgVIOCfEd8u7EtDu72fnA3Ss1GJpPpBTr18fb2RkhIiN6+zp0749tvvwUAeHl5AQCKiorg7X3n301RURF69Oiha3P16lW9PmpqalBSUqI738vLC0VFRXptaj/XtmmMFhvo5OXlITAwECdPntT9YIgMifvhNLSaO3/VKH93wJpXQtD9uWsAgH8vbovffmyN6FW/w85Zg+/mByJxcjDe+DZLr58nRl5FfqYzCs461LmGViuBjZ0WT40rxH93uzXtDRH9hZ2dBrm/OyP134/g7Y9O1dvmxBE3fLKgi+5ztfrODAW/wJuQWAlY+V4ICi85IKD9DUyf9xvs7DT44pOOAIDO3UuRe84Z3yQF4nqJFE88VYyZi07j5o1WOP6ze9PeIN0XrRkmIxtzfp8+fZCTo/8H3u+//46AgAAAtycme3l5Yf/+/brvb5VKhV9++QVTpkwBACgUCpSWliIjIwOhobcD8QMHDkCr1eLJJ5/UtfnXv/6F6upq2NjYAABSU1PRsWNHvSe87oVzdBqwdu1a9O/fHzKZDBKJBKWlpY0+Ny8vDzExMQgMDIS9vT3at2+Pd955B2q1uukGTHByq4HMo1q3Ze1vDbeASrQPU6FCZY1ftnlg6Nt56NBbBb+uNzHqg/PIy5Ah71cnXR/DF+Sh79giuPpV1nsNWwctRryXC8Xoq5C58/+f9GBlpLnjq1UdkP6jZ4NtqtVWuH7NVrfdKLe56/w2+GRBF5w82gbKKw745ScPfPdVAHo/c+cv621ftsP/Wx2Es/91gfKyA374OgC/prVB72eK6rsctQBaSMyyNVZcXByOHj2KJUuW4Pz589i8eTPWrl2L2NhYAIBEIsGMGTPw7rvv4ocffsDp06cxduxY+Pj4YNiwYQBuZ4AGDx6MSZMm4dixYzhy5AimTZuGUaNGwcfHBwDw97//HVKpFDExMcjKysLWrVuxfPlyzJw506ifT4vN6DS3W7duYfDgwRg8eDDmzp1r1LnZ2dnQarX4/PPPERQUhDNnzmDSpEm4efMmPvzwwyYaMd2tRi3Br9+3wdMTCyGRAJfPOEJTbYXgPmW6Np5BlWj9SBX++NUZbXvdMNAbkXh0few6Nu37ETdUNjh13BVfrQpCeZm0wfaOTjUoV9k0eBwAHJxqcCnX0dxDJTN50CsjP/7449ixYwfmzp2LRYsWITAwEJ988gnGjBmjazNnzhzcvHkTr732GkpLS9G3b1/s2bMHdnZ3phds2rQJ06ZNw8CBA2FlZYWoqCisWLFCd1wul2Pv3r2IjY1FaGgo2rRpg/nz5xv1aDnQzBkdrVaLZcuWISgoCLa2tvD398d7771Xb1uNRqOXJenYsSOWL1+u1+bgwYN44okn4OjoCBcXF/Tp0wd//PEHAODUqVMYMGAAnJ2dIZPJEBoaqreK41/NmDEDb731FsLCwoy+r8GDByMxMRGDBg1Cu3bt8MILL2DWrFn47rvvjO6L7s+Zva6oULXC4yNu/6WqKpbCWqqFvVyj186pTTVUxYb/I08kFhlpbvh4Xhf8c/JjSFwRjK6h17Hw019hZSXU297b7xaef/kSdn/r22Cfff+mRPCjZUj94ZGmGjaJ0JAhQ3D69GlUVlbi7NmzmDRpkt5xiUSCRYsWQalUorKyEvv27UNwcLBeG1dXV2zevBnl5eUoKyvDl19+CScnJ7023bp1w88//4zKykpcvnwZ8fHxRo+1WTM6c+fOxbp165CQkIC+ffuisLAQ2dnZ9bbVarXw9fXF9u3b4ebmhrS0NLz22mvw9vbGyJEjUVNTg2HDhmHSpEn4+uuvoVarcezYMUgkt6PUMWPGoGfPnli9ejWsra2RmZmpq/k9CGVlZXB1dW3weFVVld56BSqV6kEMy2L9stUDnfpfh9yz+t6NiSzET3vvTPz847wz8s454Yudh9H1sRKcOqY/p8zNvRKLVmbg8D5PpOyoP9Dp9lgJ4hacwYrFjyL/olO9baj5Peg5OmLTbIFOeXk5li9fjpUrV+oeR2vfvj369u1bb3sbGxssXLhQ9zkwMBDp6enYtm0bRo4cCZVKhbKyMgwZMgTt27cHcLsGWCs/Px+zZ89Gp06dAAAdOnRoqlur4/z58/j0008Nlq2WLl2qd390/0ouS/H7ETnGr7kzWU7mroZGbYWKMmu9rM6N/9lA5s5giCyT8ooDyq7bwNvvll6g49qmEkvXnsDZUy749N2Qes/t0qsE8z85iXUfdcKBZJ8HNWS6D1oY9wqHhvqwVM0Wwp09exZVVVUYOHBgo8/57LPPEBoaCnd3dzg5OWHt2rXIz88HcDsFNm7cOEREROD555/H8uXLUVhYqDt35syZmDhxIsLDw/H+++/jwoULZr+n+ly5cgWDBw/GSy+9VCe1d7e5c+eirKxMt126dOmBjM8SHdvuASe3anR+5s7Kmb5dbsLaRovf0+S6fVcv2OH6FVsE9CpvjmESNTk3j0o4y6txvfjOWihu7pV4f90JnD8rwycLukCo5wuya2gJFqw4icQVHbDnu4bLWkRi0GyBjr29vVHtt2zZglmzZiEmJgZ79+5FZmYmxo8fr/ckU2JiItLT09G7d29s3boVwcHBOHr0KABgwYIFyMrKQmRkJA4cOICQkBDs2LHDrPf0VwUFBRgwYAB69+59z5eQ2dra6tYvaMw6BlQ/rRY4/o0HHo8qhvVd+Up7mQZPjryKH95ti3NpMlw67Ygts4PQtle53kTk4jw7XMlyQHmxDaqrrHAlywFXshxQo77rsfVz9riS5YBbZa1QUW6ta0PU1Ozsa9AuWIV2wbdL216PVKBdsAruXhWws6/BhBk56Ni1FB7eFej+xDXMTziJwksOyEhvA+B2kLN03QkUK+3wRUIw5K3VaO1WhdZud8rm3R4rwYIVv+KHLf5I2++pO+4kY+azpRLM8MSVYMEZnWYrXXXo0AH29vbYv38/Jk6ceM/2R44cQe/evTF16lTdvvqyMj179kTPnj0xd+5cKBQKbN68WTehODg4GMHBwYiLi8Po0aORmJiIF1980Xw3dZcrV65gwIABCA0NRWJiIqysLLf+2ZKcOyzH9Su2eGLk1TrHhs7Lg8QKSJrSERq1BB37lSJqca5em23x7XDhlztZn48iuwMA3v75V7j63f4yWDeuk97ChLVtPs5LN/v9EN2tQ4gK76+78xDFpH/cLs/u+8EHny3tjLYdbmDgkAI4OtegpNgWJ4+64atVQaipvv3fn55h1/CI/y084n8LG1N+0us7stcgAMDAIQWws9fi5Qm5eHnCnd+P/55ojbmvPd7Ut0j3wdi3jzfUh6VqtkDHzs4O8fHxmDNnDqRSKfr06YPi4mJkZWUhJiamTvsOHTpg48aNSElJQWBgIL766iscP35c98bU3NxcrF27Fi+88AJ8fHyQk5ODc+fOYezYsaioqMDs2bMxYsQIBAYG4vLlyzh+/DiioqIaHJ9SqYRSqcT58+cBAKdPn4azszP8/f0NTioGbgc5/fv3R0BAAD788EMUFxfrjhmzmiMZr2O/sgYDDhs7AVGLc+sEN3eL3frbPa8x78jJ+x4fkSlOZ7jqApL6zI8NNXj+vp2PYN9Ow09PJSzogoS7FhwkErtmfepq3rx5aNWqFebPn4+CggJ4e3tj8uTJ9bZ9/fXXcfLkSbz88suQSCQYPXo0pk6dit27dwMAHBwckJ2djQ0bNuDatWvw9vZGbGwsXn/9ddTU1ODatWsYO3YsioqK0KZNGwwfPtzg5N81a9boHe/Xrx+A2+WxcePGGbyv1NRUnD9/HufPn4evr359WxDqf8yTiIjofvCpK8MkAr95WySVSgW5XI60M95wcrbcf4D0cJsTNqy5h0DUJGq0auwrWoeysrImm3NZ+z0xdO8E2Dg2vChkY1TfVOPfg75s0vE2F36DEhERkcVioHMflixZAicnp3q3Z599trmHR0RED5EH/a4rseG7ru7D5MmTMXLkyHqPGfvYPBERkSn41JVhDHTug6ur6z2fvCIiInoQGOgYxtIVERERWSxmdIiIiESMGR3DGOgQERGJGAMdw1i6IiIiIovFjA4REZGICYDJj4db8srBDHSIiIhEjKUrw1i6IiIiIovFjA4REZGIMaNjGAMdIiIiEWOgYxhLV0RERGSxmNEhIiISMWZ0DGOgQ0REJGKCIIFgYqBi6vktGQMdIiIiEdNCYvI6Oqae35Jxjg4RERFZLGZ0iIiIRIxzdAxjoENERCRinKNjGEtXREREZLGY0SEiIhIxlq4MY6BDREQkYixdGcbSFREREVksZnSIiIhETDBD6cqSMzoMdIiIiERMACAIpvdhqVi6IiIiIovFjA4REZGIaSGBhK+AaBADHSIiIhHjU1eGMdAhIiISMa0ggYTr6DSIc3SIiIjIYjGjQ0REJGKCYIanriz4sSsGOkRERCLGOTqGsXRFREREFosZHSIiIhFjRscwBjpEREQixqeuDGPpioiIiCwWMzpEREQixqeuDGOgQ0REJGK3Ax1T5+iYaTAtEEtXRERE1GgLFiyARCLR2zp16qQ7XllZidjYWLi5ucHJyQlRUVEoKirS6yM/Px+RkZFwcHCAh4cHZs+ejZqaGr02Bw8eRK9evWBra4ugoCAkJSXd13gZ6BAREYlY7VNXpm7GePTRR1FYWKjbDh8+rDsWFxeHnTt3Yvv27Th06BAKCgowfPhw3XGNRoPIyEio1WqkpaVhw4YNSEpKwvz583VtcnNzERkZiQEDBiAzMxMzZszAxIkTkZKSYvTPh6UrIiIiERP+3EztwxitWrWCl5dXnf1lZWX44osvsHnzZjzzzDMAgMTERHTu3BlHjx5FWFgY9u7di99++w379u2Dp6cnevTogcWLFyM+Ph4LFiyAVCrFmjVrEBgYiI8++ggA0LlzZxw+fBgJCQmIiIgwaqzM6BAREYmYOTM6KpVKb6uqqqr3mufOnYOPjw/atWuHMWPGID8/HwCQkZGB6upqhIeH69p26tQJ/v7+SE9PBwCkp6eja9eu8PT01LWJiIiASqVCVlaWrs3dfdS2qe3DGAx0iIiICADg5+cHuVyu25YuXVqnzZNPPomkpCTs2bMHq1evRm5uLp566imUl5dDqVRCKpXCxcVF7xxPT08olUoAgFKp1Atyao/XHjPURqVSoaKiwqh7YumKiIhIzMxYu7p06RJkMplut62tbZ2mzz77rO5/d+vWDU8++SQCAgKwbds22NvbmzgQ82NGh4iISMzMUbb6s3Qlk8n0tvoCnb9ycXFBcHAwzp8/Dy8vL6jVapSWluq1KSoq0s3p8fLyqvMUVu3ne7WRyWRGB1MMdIiIiOi+3bhxAxcuXIC3tzdCQ0NhY2OD/fv3647n5OQgPz8fCoUCAKBQKHD69GlcvXpV1yY1NRUymQwhISG6Nnf3Udumtg9jMNAhIiISsdqVkU3dGmvWrFk4dOgQ8vLykJaWhhdffBHW1tYYPXo05HI5YmJiMHPmTPz444/IyMjA+PHjoVAoEBYWBgAYNGgQQkJC8Oqrr+LUqVNISUnB22+/jdjYWF0GafLkybh48SLmzJmD7OxsrFq1Ctu2bUNcXJzRPx/O0SEiIhKxB/328suXL2P06NG4du0a3N3d0bdvXxw9ehTu7u4AgISEBFhZWSEqKgpVVVWIiIjAqlWrdOdbW1tj165dmDJlChQKBRwdHREdHY1Fixbp2gQGBiI5ORlxcXFYvnw5fH19sX79eqMfLQcAiSBY8sLP4qVSqSCXy5F2xhtOzky8kWWaEzasuYdA1CRqtGrsK1qHsrIyvcm95lT7PdH2y7dh5WBnUl/aW5XIm/Buk463uTCjQ0REJGZ3TSY2qQ8LxUCHiIhIxPj2csNYEyEiIiKLxYwOERGRmDXHy65EpFGBzg8//NDoDl944YX7HgwREREZ50E/dSU2jQp0hg0b1qjOJBIJNBqNKeMhIiIiY1lwRsZUjQp0tFptU4+DiIiIyOxMmqNTWVkJOzvTnt0nIiKi+8fSlWFGP3Wl0WiwePFiPPLII3BycsLFixcBAPPmzcMXX3xh9gESERGRAYKZNgtldKDz3nvvISkpCcuWLYNUKtXt79KlC9avX2/WwRERERGZwuhAZ+PGjVi7di3GjBkDa2tr3f7u3bsjOzvbrIMjIiKie5GYabNMRs/RuXLlCoKCgurs12q1qK6uNsugiIiIqJG4jo5BRmd0QkJC8PPPP9fZ/80336Bnz55mGRQRERGRORid0Zk/fz6io6Nx5coVaLVafPfdd8jJycHGjRuxa9euphgjERERNYQZHYOMzugMHToUO3fuxL59++Do6Ij58+fj7Nmz2LlzJ/72t781xRiJiIioIbVvLzd1s1D3tY7OU089hdTUVHOPhYiIiMis7nvBwBMnTuDs2bMAbs/bCQ0NNdugiIiIqHEE4fZmah+WyuhA5/Llyxg9ejSOHDkCFxcXAEBpaSl69+6NLVu2wNfX19xjJCIiooZwjo5BRs/RmThxIqqrq3H27FmUlJSgpKQEZ8+ehVarxcSJE5tijERERNQQztExyOiMzqFDh5CWloaOHTvq9nXs2BGffvopnnrqKbMOjoiIiMgURgc6fn5+9S4MqNFo4OPjY5ZBERERUeNIhNubqX1YKqNLVx988AGmT5+OEydO6PadOHECb775Jj788EOzDo6IiIjugS/1NKhRGZ3WrVtDIrlTv7t58yaefPJJtGp1+/Samhq0atUKEyZMwLBhw5pkoERERETGalSg88knnzTxMIiIiOi+mGMy8cM+GTk6Orqpx0FERET3g4+XG3TfCwYCQGVlJdRqtd4+mUxm0oCIiIiIzMXoycg3b97EtGnT4OHhAUdHR7Ru3VpvIyIiogeIk5ENMjrQmTNnDg4cOIDVq1fD1tYW69evx8KFC+Hj44ONGzc2xRiJiIioIQx0DDK6dLVz505s3LgR/fv3x/jx4/HUU08hKCgIAQEB2LRpE8aMGdMU4yQiIiIymtEZnZKSErRr1w7A7fk4JSUlAIC+ffvip59+Mu/oiIiIyDC+AsIgowOddu3aITc3FwDQqVMnbNu2DcDtTE/tSz6JiIjowahdGdnUzVIZHeiMHz8ep06dAgC89dZb+Oyzz2BnZ4e4uDjMnj3b7AMkIiIiAzhHxyCj5+jExcXp/nd4eDiys7ORkZGBoKAgdOvWzayDIyIiIjKFSevoAEBAQAACAgLMMRYiIiIis2pUoLNixYpGd/jGG2/c92CIiIjIOBKY4e3lZhlJy9SoQCchIaFRnUkkEgY6RERE1GI0KtCpfcqKHrx/dnkCrSQ2zT0MoiaRUpDS3EMgahKqci1aBz+gi/GlngaZPEeHiIiImhFf6mmQ0Y+XExEREYkFMzpERERixoyOQQx0iIiIRMwcKxtzZWQiIiIiEbqvQOfnn3/GK6+8AoVCgStXrgAAvvrqKxw+fNisgyMiIqJ74CsgDDI60Pn2228REREBe3t7nDx5ElVVVQCAsrIyLFmyxOwDJCIiIgMY6BhkdKDz7rvvYs2aNVi3bh1sbO6s79KnTx/8+uuvZh0cERERGca3lxtmdKCTk5ODfv361dkvl8tRWlpqjjERERERmYXRgY6XlxfOnz9fZ//hw4fRrl07swyKiIiIGql2ZWRTNwtldKAzadIkvPnmm/jll18gkUhQUFCATZs2YdasWZgyZUpTjJGIiIga0sxzdN5//31IJBLMmDFDt6+yshKxsbFwc3ODk5MToqKiUFRUpHdefn4+IiMj4eDgAA8PD8yePRs1NTV6bQ4ePIhevXrB1tYWQUFBSEpKMnp8Rq+j89Zbb0Gr1WLgwIG4desW+vXrB1tbW8yaNQvTp083egBEREQkTsePH8fnn3+Obt266e2Pi4tDcnIytm/fDrlcjmnTpmH48OE4cuQIAECj0SAyMhJeXl5IS0tDYWEhxo4dCxsbG92DTbm5uYiMjMTkyZOxadMm7N+/HxMnToS3tzciIiIaPUaJIAj3Fcep1WqcP38eN27cQEhICJycnO6nG2qASqWCXC5HfwzlSz3JYqUUZDb3EIiaxO2Xel5EWVkZZDJZ01zjz++Jdu8sgZWdnUl9aSsrcXHhP40a740bN9CrVy+sWrUK7777Lnr06IFPPvkEZWVlcHd3x+bNmzFixAgAQHZ2Njp37oz09HSEhYVh9+7dGDJkCAoKCuDp6QkAWLNmDeLj41FcXAypVIr4+HgkJyfjzJkzumuOGjUKpaWl2LNnT6Pv7b4XDJRKpQgJCcETTzzBIIeIiKi5mLF0pVKp9LbaJWTqExsbi8jISISHh+vtz8jIQHV1td7+Tp06wd/fH+np6QCA9PR0dO3aVRfkAEBERARUKhWysrJ0bf7ad0REhK6PxjK6dDVgwABIJA1PWjpw4ICxXRIREVEL4Ofnp/f5nXfewYIFC+q027JlC3799VccP368zjGlUgmpVAoXFxe9/Z6enlAqlbo2dwc5tcdrjxlqo1KpUFFRAXt7+0bdk9GBTo8ePfQ+V1dXIzMzE2fOnEF0dLSx3REREZEpzLEOzp/nX7p0Sa90ZWtrW6fppUuX8OabbyI1NRV2JpbMHgSjA52EhIR69y9YsAA3btwweUBERERkBDO+vVwmk91zjk5GRgauXr2KXr166fZpNBr89NNPWLlyJVJSUqBWq1FaWqqX1SkqKoKXlxeA20vVHDt2TK/f2qey7m7z1ye1ioqKIJPJGp3NAcz4Us9XXnkFX375pbm6IyIiohZo4MCBOH36NDIzM3XbY489hjFjxuj+t42NDfbv3687JycnB/n5+VAoFAAAhUKB06dP4+rVq7o2qampkMlkCAkJ0bW5u4/aNrV9NJbRGZ2GpKeniyKFRUREZFHMmNFpDGdnZ3Tp0kVvn6OjI9zc3HT7Y2JiMHPmTLi6ukImk2H69OlQKBQICwsDAAwaNAghISF49dVXsWzZMiiVSrz99tuIjY3VlcsmT56MlStXYs6cOZgwYQIOHDiAbdu2ITk52ahbMzrQGT58uN5nQRBQWFiIEydOYN68ecZ2R0RERCYwx7uqzP2uq4SEBFhZWSEqKgpVVVWIiIjAqlWrdMetra2xa9cuTJkyBQqFAo6OjoiOjsaiRYt0bQIDA5GcnIy4uDgsX74cvr6+WL9+vVFr6AD3sY7O+PHj9T5bWVnB3d0dzzzzDAYNGmTUxalhXEeHHgZcR4cs1YNcR6f9P5fA2sSKiqayEheWGLeOjlgYldHRaDQYP348unbtitatWzfVmIiIiIjMwqjJyNbW1hg0aBDfUk5ERNRSNPO7rlo6o5+66tKlCy5evNgUYyEiIiIj1c7RMXWzVEYHOu+++y5mzZqFXbt2obCwsM5y0UREREQtRaPn6CxatAj/+Mc/8NxzzwEAXnjhBb1XQQiCAIlEAo1GY/5REhERUcMsOCNjqkYHOgsXLsTkyZPx448/NuV4iIiIyBgPeB0dsWl0oFP7FPrTTz/dZIMhIiIiMiejHi839NZyIiIievBa4oKBLYlRgU5wcPA9g52SkhKTBkRERERGYOnKIKMCnYULF0IulzfVWIiIiIjMyqhAZ9SoUfDw8GiqsRAREZGRWLoyrNGBDufnEBERtUAsXRlk9FNXRERE1IIw0DGo0YGOVqttynEQERERmZ1Rc3SIiIioZeEcHcMY6BAREYkZS1cGGf1STyIiIiKxYEaHiIhIzJjRMYiBDhERkYhxjo5hLF0RERGRxWJGh4iISMxYujKIgQ4REZGIsXRlGEtXREREZLGY0SEiIhIzlq4MYqBDREQkZgx0DGKgQ0REJGKSPzdT+7BUnKNDREREFosZHSIiIjFj6cogBjpEREQixsfLDWPpioiIiCwWMzpERERixtKVQQx0iIiIxM6CAxVTsXRFREREFosZHSIiIhHjZGTDGOgQERGJGefoGMTSFREREVksZnSIiIhEjKUrwxjoEBERiRlLVwYx0CEiIhIxZnQM4xwdIiIisljM6BAREYkZS1cGMdAhIiISMwY6BrF0RURERBaLGR0iIiIR42RkwxjoEBERiRlLVwaxdEVEREQWixkdIiIiEZMIAiSCaSkZU89vyZjRISIiEjPBTFsjrV69Gt26dYNMJoNMJoNCocDu3bt1xysrKxEbGws3Nzc4OTkhKioKRUVFen3k5+cjMjISDg4O8PDwwOzZs1FTU6PX5uDBg+jVqxdsbW0RFBSEpKQkI34odzDQISIiokbz9fXF+++/j4yMDJw4cQLPPPMMhg4diqysLABAXFwcdu7cie3bt+PQoUMoKCjA8OHDdedrNBpERkZCrVYjLS0NGzZsQFJSEubPn69rk5ubi8jISAwYMACZmZmYMWMGJk6ciJSUFKPHKxEEC85XiZhKpYJcLkd/DEUriU1zD4eoSaQUZDb3EIiahKpci9bBF1FWVgaZTNY01/jze6LnmPdgLbUzqS+NuhInN/3rvsfr6uqKDz74ACNGjIC7uzs2b96MESNGAACys7PRuXNnpKenIywsDLt378aQIUNQUFAAT09PAMCaNWsQHx+P4uJiSKVSxMfHIzk5GWfOnNFdY9SoUSgtLcWePXuMGhszOkRERGJmxtKVSqXS26qqqgxeWqPRYMuWLbh58yYUCgUyMjJQXV2N8PBwXZtOnTrB398f6enpAID09HR07dpVF+QAQEREBFQqlS4rlJ6ertdHbZvaPozBQIeIiEjEatfRMXUDAD8/P8jlct22dOnSeq95+vRpODk5wdbWFpMnT8aOHTsQEhICpVIJqVQKFxcXvfaenp5QKpUAAKVSqRfk1B6vPWaojUqlQkVFhVE/Hz51RURERACAS5cu6ZWubG1t623XsWNHZGZmoqysDN988w2io6Nx6NChBzVMozDQISIiEjMzLhhY+yTVvUilUgQFBQEAQkNDcfz4cSxfvhwvv/wy1Go1SktL9bI6RUVF8PLyAgB4eXnh2LFjev3VPpV1d5u/PqlVVFQEmUwGe3t7o26NpSsiIiIRM2fp6n5ptVpUVVUhNDQUNjY22L9/v+5YTk4O8vPzoVAoAAAKhQKnT5/G1atXdW1SU1Mhk8kQEhKia3N3H7VtavswBjM6RERE1Ghz587Fs88+C39/f5SXl2Pz5s04ePAgUlJSIJfLERMTg5kzZ8LV1RUymQzTp0+HQqFAWFgYAGDQoEEICQnBq6++imXLlkGpVOLtt99GbGysrlQ2efJkrFy5EnPmzMGECRNw4MABbNu2DcnJyUaPl4EOERGRmD3gd11dvXoVY8eORWFhIeRyObp164aUlBT87W9/AwAkJCTAysoKUVFRqKqqQkREBFatWqU739raGrt27cKUKVOgUCjg6OiI6OhoLFq0SNcmMDAQycnJiIuLw/Lly+Hr64v169cjIiLC6FvjOjotFNfRoYcB19EhS/Ug19EJHfkeWtmYto5OTXUlMrbd/zo6LRnn6BAREZHFYumKiIhIzATh9mZqHxaKgQ4REZGImeOpKVPPb8lYuiIiIiKLxYwOERGRmD3gp67EhoEOERGRiEm0tzdT+7BUDHTIYnR58gZemlqMDl1vwc2rBgsmtEX6HvldLQSMnV2EwX+/BieZBr+dcMSKt3xRkHt7gapuihv44NsL9fY9/dkO+P2UAzx91dh47Gyd428OCUL2r45NcVtEAICxT4Sg6LK0zv7no4sxbekV3WdBAN5+pR1O/CjDO1/kovezZbpjET496pw/d1Ue+g8r1X0+8F1rbFvlgYKLtnCUafDYABUmzSuAzFVj1vshM2JGxyCLDHTy8vIQGBiIkydPokePHs09HHpA7By0uJhlh5SvXfHOl3l1jo+MLcbQCcX4cIY/lPlSRM9RYsnmi5jUvyOqq6zw2wkHjOoeondO9BwlevS9gd9P6b9bJX5kO/yRc2fdCtV1i/xVohZkxe4caDUS3ee8bDvMHRWEp54v02u3Y507JJK/nn3HPxLy8dgAle6zk+xOAJN1zBEfvOGP1xdcQdggFf5XaIMVb/nik9l+mP9FntnuhehB4mTkJlBZWYnY2Fi4ubnByckJUVFRdV5ORuZ34kcZNizzRppeFqeWgGETi/H1ck+kp8iRe9Yey97wh5tnNXoPvv1FUVNthevFNrpNdb0VFBEq7N3qCkD/m0N1vZVeW02NgW8WIjNwcdPA1aNGt/2yTw7vtlXopriha3PhjD2+/dwdMz/Ob7AfJ5l+P1K7O3/K/5bhAE8/NYZN/B+8/NXo8uRNRL5yDTmZDk16b2SalvCuq5aMgU4TiIuLw86dO7F9+3YcOnQIBQUFGD58eHMP66Hm5a+Gm2cNfv3ZWbfvVrk1sk86oHPorXrPUQwqg3PrGuzd2rrOsYVJudj63yx89P05hA0qq+dsoqZTrZbgwLetETHqmi57U3lLgvdjAxD73mW4etQ0eO7Kfz2Clx7tgunPdUDK1656y6eEhN5CcYENju13hiAA14tb4edkFzz+jKrB/qgFqF1Hx9TNQok20NFqtVi2bBmCgoJga2sLf39/vPfee/W21Wg0iImJQWBgIOzt7dGxY0csX75cr83BgwfxxBNPwNHRES4uLujTpw/++OMPAMCpU6cwYMAAODs7QyaTITQ0FCdOnKj3WmVlZfjiiy/w8ccf45lnnkFoaCgSExORlpaGo0ePmveHQI1W+x/+0mL9ElNpcSu4elTXe07E6BJkHHTG/wrvzIuouGWFzxf44N3XAjDv1UBkHXPEO1/mMdihByptjxw3VNYYNLJEt+/zBY8g5LGb6D244aBk7OxC/GvNH1i65QL6PleGT//pi39/0UZ3/NEnbiJ+5R9YMrktIgO6Y1T3LnB01mDakstNej9ETUm0Ewvmzp2LdevWISEhAX379kVhYSGys7PrbavVauHr64vt27fDzc0NaWlpeO211+Dt7Y2RI0eipqYGw4YNw6RJk/D1119DrVbj2LFjkPz5p9KYMWPQs2dPrF69GtbW1sjMzISNTf3vn8rIyEB1dTXCw8N1+zp16gR/f3+kp6fr3t76V1VVVaiqqtJ9Vqn4F1RzauOtRmj/cix5PUBvv6qkFb5b6677/PspB7h51uClKcU4ure+khmR+aV87YrHB6jg5nU7gE9PkSHziDNW7c0xeN6YuDsl9KCuFai8ZYXtqz0wbOL/AAB//G6L1fN9MSZOidD+5Si5aoP1i32wIt4PMz++1HQ3RCbhgoGGiTLQKS8vx/Lly7Fy5UpER0cDANq3b4++ffvW297GxgYLFy7UfQ4MDER6ejq2bduGkSNHQqVSoaysDEOGDEH79u0BAJ07d9a1z8/Px+zZs9GpUycAQIcOHRocm1KphFQqhYuLi95+T09PKJXKBs9bunSp3hjJvEqu3v6n7uJeg5Krd4JUF/caXMiyr9N+0MvXUX69FdIbEbxkn3RAz37l5hsskQFFl21w8mdnzFufq9uXecQZhXlSDO/UVa/t4klt0eXJm/jg2/P19tWp1y1s/sQL6ioJpLYCtn7qiUcfv4mXphYDANqFVMLO/jL+8WIHRMcXws2z4ZIYNSM+dWWQKEtXZ8+eRVVVFQYOHNjocz777DOEhobC3d0dTk5OWLt2LfLzb0/Yc3V1xbhx4xAREYHnn38ey5cvR2Fhoe7cmTNnYuLEiQgPD8f777+PCxfqfwTZFHPnzkVZWZluu3SJfz2ZkzJfimtFrdCz752AxMFJg049b+Fsxl8nWgoY9HIJ9n3TulGTjNs/WqEXPBE1pb1b3ODSpgZPht/J+r48rQhr9udgdeqdDQBeX3AF/0hoeGLyhSx7OLnUQGp7+1uussIKkr/8aW9l/ednC/4iJMsmykDH3r7uX+CGbNmyBbNmzUJMTAz27t2LzMxMjB8/Hmq1WtcmMTER6enp6N27N7Zu3Yrg4GDdnJoFCxYgKysLkZGROHDgAEJCQrBjx456r+Xl5QW1Wo3S0lK9/UVFRfDy8mpwjLa2tpDJZHobGcfOQYN2j1ag3aMVAAAvPzXaPVoB90fUACT4fr07Rr95FWGDytC2UwVmr8jHtSKbOk9p9eh7A94BauzZ7FrnGuEvlaD/sOvwC6qEX1AlRk0vwqBRJfjhyzZ12hKZm1YL7N3qivCXSmB9Vz7e1aMGbTtV6m0A4PFINbz8b/937uheGXZvckVeth2u5Eqxc4MbtqzwwNDx/9P1E/Y3FY7sdsHODW4o/EOKrGOOWD3PFx173tSVyajl4VNXhomydNWhQwfY29tj//79mDhx4j3bHzlyBL1798bUqVN1++rLyvTs2RM9e/bE3LlzoVAosHnzZt2cmuDgYAQHByMuLg6jR49GYmIiXnzxxTp9hIaGwsbGBvv370dUVBQAICcnB/n5+VAoFPd7y9QIwd0r9Bb8m7ywAACwd2trfBTnj22fucPOQYs3l12Gk0yDrOOO+NeYdqiu0o/3B48uQdZxB1w6b4f6/H1GETx9q6GpAS6dt8OSyQE4nOzSZPdFVOvkT864ekWKiFEl9278F9Y2AnYmtcHnC2whCIBPWzVeX1CAZ8dc07UZ9HIJKm5Y4YfENli38BE4yjXo0accMf8qNNAzNTu+vdwgUQY6dnZ2iI+Px5w5cyCVStGnTx8UFxcjKysLMTExddp36NABGzduREpKCgIDA/HVV1/h+PHjCAwMBADk5uZi7dq1eOGFF+Dj44OcnBycO3cOY8eORUVFBWbPno0RI0YgMDAQly9fxvHjx3VBzF/J5XLExMRg5syZcHV1hUwmw/Tp06FQKBqciEzm8d90J0T4dDfQQoKNH3hh4wcNZ9YA4P3YgAaP7dvuin3b62Z6iB6E0P7lSCnIbFTbv7Z7fEA5Hh9w77lkQ2P+h6Ex/7tnOyKxEGWgAwDz5s1Dq1atMH/+fBQUFMDb2xuTJ0+ut+3rr7+OkydP4uWXX4ZEIsHo0aMxdepU7N69GwDg4OCA7OxsbNiwAdeuXYO3tzdiY2Px+uuvo6amBteuXcPYsWNRVFSENm3aYPjw4QYnDickJMDKygpRUVGoqqpCREQEVq1a1SQ/ByIierjxqSvDJIJgwfkqEVOpVJDL5eiPoWgl4URXskyNzU4QiY2qXIvWwRdRVlbWZHMua78nFIMXoZVN/aX2xqqprkT6nvlNOt7mItqMDhERETGjcy+ifOqKiIiIqDGY0SEiIhIzrXB7M7UPC8VAh4iISMy4MrJBLF0RERGRxWJGh4iISMQkMMNkZLOMpGVioENERCRmXBnZIJauiIiIyGIxo0NERCRiXEfHMAY6REREYsanrgxi6YqIiIgsFjM6REREIiYRBEhMnExs6vktGQMdIiIiMdP+uZnah4VioENERCRizOgYxjk6REREZLGY0SEiIhIzPnVlEAMdIiIiMePKyAaxdEVEREQWixkdIiIiEePKyIYx0CEiIhIzlq4MYumKiIiILBYzOkRERCIm0d7eTO3DUjHQISIiEjOWrgxi6YqIiIgsFjM6REREYsYFAw1ioENERCRifNeVYQx0iIiIxIxzdAziHB0iIiKyWAx0iIiIxEwAoDVxMyKhs3TpUjz++ONwdnaGh4cHhg0bhpycHL02lZWViI2NhZubG5ycnBAVFYWioiK9Nvn5+YiMjISDgwM8PDwwe/Zs1NTU6LU5ePAgevXqBVtbWwQFBSEpKanxA/0TAx0iIiIRq52jY+rWWIcOHUJsbCyOHj2K1NRUVFdXY9CgQbh586auTVxcHHbu3Int27fj0KFDKCgowPDhw3XHNRoNIiMjoVarkZaWhg0bNiApKQnz58/XtcnNzUVkZCQGDBiAzMxMzJgxAxMnTkRKSoqxPx8LLsyJmEqlglwuR38MRSuJTXMPh6hJpBRkNvcQiJqEqlyL1sEXUVZWBplM1jTX+PN74pmeb6GVtZ1JfdVoKnHg5Pv3Nd7i4mJ4eHjg0KFD6NevH8rKyuDu7o7NmzdjxIgRAIDs7Gx07twZ6enpCAsLw+7duzFkyBAUFBTA09MTALBmzRrEx8ejuLgYUqkU8fHxSE5OxpkzZ3TXGjVqFEpLS7Fnz55Gj48ZHSIiIjETcGdC8n1vt7tSqVR6W1VV1T0vX1ZWBgBwdXUFAGRkZKC6uhrh4eG6Np06dYK/vz/S09MBAOnp6ejatasuyAGAiIgIqFQqZGVl6drc3Udtm9o+GouBDhERkZiZHOTceWrLz88Pcrlcty1dutTgpbVaLWbMmIE+ffqgS5cuAAClUgmpVAoXFxe9tp6enlAqlbo2dwc5tcdrjxlqo1KpUFFR0egfDx8vJyIiIgDApUuX9EpXtra2BtvHxsbizJkzOHz4cFMP7b4x0CEiIhIzLQCJGfoAIJPJGj1HZ9q0adi1axd++ukn+Pr66vZ7eXlBrVajtLRUL6tTVFQELy8vXZtjx47p9Vf7VNbdbf76pFZRURFkMhns7e0bfWssXREREYnYg37qShAETJs2DTt27MCBAwcQGBiodzw0NBQ2NjbYv3+/bl9OTg7y8/OhUCgAAAqFAqdPn8bVq1d1bVJTUyGTyRASEqJrc3cftW1q+2gsZnSIiIjE7AGvjBwbG4vNmzfj3//+N5ydnXVzauRyOezt7SGXyxETE4OZM2fC1dUVMpkM06dPh0KhQFhYGABg0KBBCAkJwauvvoply5ZBqVTi7bffRmxsrK5cNnnyZKxcuRJz5szBhAkTcODAAWzbtg3JyclG3RozOkRERNRoq1evRllZGfr37w9vb2/dtnXrVl2bhIQEDBkyBFFRUejXrx+8vLzw3Xff6Y5bW1tj165dsLa2hkKhwCuvvIKxY8di0aJFujaBgYFITk5Gamoqunfvjo8++gjr169HRESEUePlOjotFNfRoYcB19EhS/Ug19EZGDILrawNTxq+lxpNFfb/9mGTjre5sHRFREQkZnypp0EsXREREZHFYkaHiIhIzMz4eLklYqBDREQkYsY+Ht5QH5aKpSsiIiKyWMzoEBERiRknIxvEQIeIiEjMtAIgMTFQ0VpuoMPSFREREVksZnSIiIjEjKUrgxjoEBERiZoZAh0w0CEiIqKWiBkdgzhHh4iIiCwWMzpERERiphVgcunJgp+6YqBDREQkZoL29mZqHxaKpSsiIiKyWMzoEBERiRknIxvEQIeIiEjMOEfHIJauiIiIyGIxo0NERCRmLF0ZxECHiIhIzASYIdAxy0haJJauiIiIyGIxo0NERCRmLF0ZxECHiIhIzLRaACYu+Ke13AUDGegQERGJGTM6BnGODhEREVksZnSIiIjEjBkdgxjoEBERiRlXRjaIpSsiIiKyWMzoEBERiZggaCEIpj01Zer5LRkDHSIiIjETBNNLTxY8R4elKyIiIrJYzOgQERGJmWCGycgWnNFhoENERCRmWi0gMXGOjQXP0WHpioiIiCwWMzpERERixtKVQQx0iIiIREzQaiGYWLri4+VERETUMjGjYxDn6BAREZHFYkaHiIhIzLQCIGFGpyEMdIiIiMRMEACY+ni55QY6LF0RERGRxWJGh4iISMQErQDBxNKVYMEZHQY6REREYiZoYXrpynIfL2fpioiIiCwWMzpEREQixtKVYQx0iIiIxIylK4MY6LRQtdF1DapNXvCSqKVSlVvuf1zp4aa6cfvf9oPIlJjje6IG1eYZTAvEQKeFKi8vBwAcxn+aeSRETad1cHOPgKhplZeXQy6XN0nfUqkUXl5eOKw0z/eEl5cXpFKpWfpqSSSCJRfmREyr1aKgoADOzs6QSCTNPRyLp1Kp4Ofnh0uXLkEmkzX3cIjMjv/GHyxBEFBeXg4fHx9YWTXdcz+VlZVQq9Vm6UsqlcLOzs4sfbUkzOi0UFZWVvD19W3uYTx0ZDIZvwTIovHf+IPTVJmcu9nZ2VlkcGJOfLyciIiILBYDHSIiIrJYDHSIANja2uKdd96Bra1tcw+FqEnw3zg9rDgZmYiIiCwWMzpERERksRjoEBERkcVioEOikZeXB4lEgszMzOYeClGz4O8AkfEY6BA10tq1a9G/f3/IZDJIJBKUlpY2+ty8vDzExMQgMDAQ9vb2aN++Pd555x2zLfRF9CBUVlYiNjYWbm5ucHJyQlRUFIqKipp7WEQGMdAhaqRbt25h8ODB+Oc//2n0udnZ2dBqtfj888+RlZWFhIQErFmz5r76ImoucXFx2LlzJ7Zv345Dhw6hoKAAw4cPb+5hERkmELUgGo1G+L//+z+hffv2glQqFfz8/IR3331XEARByM3NFQAIJ0+eFARBEGpqaoQJEyYIbdu2Fezs7ITg4GDhk08+0evvxx9/FB5//HHBwcFBkMvlQu/evYW8vDxBEAQhMzNT6N+/v+Dk5CQ4OzsLvXr1Eo4fP37PMf74448CAOH69esm3euyZcuEwMBAk/ogy9NSfwdKS0sFGxsbYfv27bp9Z8+eFQAI6enpTfCTIDIPvgKCWpS5c+di3bp1SEhIQN++fVFYWIjs7Ox622q1Wvj6+mL79u1wc3NDWloaXnvtNXh7e2PkyJGoqanBsGHDMGnSJHz99ddQq9U4duyY7t1hY8aMQc+ePbF69WpYW1sjMzMTNjY2D+xey8rK4Orq+sCuR+LQUn8HMjIyUF1djfDwcN2+Tp06wd/fH+np6QgLCzP/D4PIHJo70iKqpVKpBFtbW2HdunX1Hv/rX7P1iY2NFaKiogRBEIRr164JAISDBw/W29bZ2VlISkoyepzmyOicO3dOkMlkwtq1a++7D7I8Lfl3YNOmTYJUKq2z//HHHxfmzJnTqD6ImgPn6FCLcfbsWVRVVWHgwIGNPuezzz5DaGgo3N3d4eTkhLVr1yI/Px8A4OrqinHjxiEiIgLPP/88li9fjsLCQt25M2fOxMSJExEeHo73338fFy5cMPs91efKlSsYPHgwXnrpJUyaNOmBXJPE4WH5HSB6kBjoUIthb29vVPstW7Zg1qxZiImJwd69e5GZmYnx48frPcmUmJiI9PR09O7dG1u3bkVwcDCOHj0KAFiwYAGysrIQGRmJAwcOICQkBDt27DDrPf1VQUEBBgwYgN69e2Pt2rVNei0Sn5b8O+Dl5QW1Wl3nacOioiJ4eXkZd6NED1Jzp5SIalVUVAj29vaNTttPmzZNeOaZZ/TaDBw4UOjevXuD1wgLCxOmT59e77FRo0YJzz///D3Heb+lq8uXLwsdOnQQRo0aJdTU1Bh1Lj0cWvLvQO1k5G+++Ua3Lzs7m5ORqcXjZGRqMezs7BAfH485c+ZAKpWiT58+KC4uRlZWFmJiYuq079ChAzZu3IiUlBQEBgbiq6++wvHjxxEYGAgAyM3Nxdq1a/HCCy/Ax8cHOTk5OHfuHMaOHYuKigrMnj0bI0aMQGBgIC5fvozjx48jKiqqwfEplUoolUqcP38eAHD69Gk4OzvD39//npOKr1y5gv79+yMgIAAffvghiouLdcf41zDVasm/A3K5HDExMZg5cyZcXV0hk8kwffp0KBQKTkSmlq25Iy2iu2k0GuHdd98VAgICBBsbG8Hf319YsmSJIAh1/5qtrKwUxo0bJ8jlcsHFxUWYMmWK8NZbb+n+mlUqlcKwYcMEb29vQSqVCgEBAcL8+fMFjUYjVFVVCaNGjRL8/PwEqVQq+Pj4CNOmTRMqKioaHNs777wjAKizJSYm3vO+EhMT6z2Xv4L0Vy35d6CiokKYOnWq0Lp1a8HBwUF48cUXhcLCwqb+kRCZhG8vJyIiIovFychERERksRjoEJnBkiVL4OTkVO/27LPPNvfwiIgeWixdEZlBSUkJSkpK6j1mb2+PRx555AGPiIiIAAY6REREZMFYuiIiIiKLxUCHiIiILBYDHSIiIrJYDHSIiIjIYjHQIaIGjRs3DsOGDdN97t+/P2bMmPHAx3Hw4EFIJJI6L5S8m0Qiwffff9/oPhcsWIAePXqYNK68vDxIJBJkZmaa1A8RNR0GOkQiM27cOEgkEkgkEkilUgQFBWHRokWoqalp8mt/9913WLx4caPaNiY4ISJqanypJ5EIDR48GImJiaiqqsJ//vMfxMbGwsbGBnPnzq3TVq1WQyqVmuW693p5KRFRS8OMDpEI2drawsvLCwEBAZgyZQrCw8Pxww8/ALhTbnrvvffg4+ODjh07AgAuXbqEkSNHwsXFBa6urhg6dCjy8vJ0fWo0GsycORMuLi5wc3PDnDlz8Ndltv5auqqqqkJ8fDz8/Pxga2uLoKAgfPHFF8jLy8OAAQMAAK1bt4ZEIsG4ceMAAFqtFkuXLkVgYCDs7e3RvXt3fPPNN3rX+c9//oPg4GDY29tjwIABeuNsrPj4eAQHB8PBwQHt2rXDvHnzUF1dXafd559/Dj8/Pzg4OGDkyJEoKyvTO75+/Xp07twZdnZ26NSpE1atWmX0WIio+TDQIbIA9vb2UKvVus/79+9HTk4OUlNTsWvXLlRXVyMiIgLOzs74+eefceTIETg5OWHw4MG68z766CMkJSXhyy+/xOHDh1FSUoIdO3YYvO7YsWPx9ddfY8WKFTh79iw+//xzODk5wc/PD99++y0AICcnB4WFhVi+fDkAYOnSpdi4cSPWrFmDrKwsxMXF4ZVXXsGhQ4cA3A7Ihg8fjueffx6ZmZmYOHEi3nrrLaN/Js7OzkhKSsJvv/2G5cuXY926dUhISNBrc/78eWzbtg07d+7Enj17cPLkSUydOlV3fNOmTZg/fz7ee+89nD17FkuWLMG8efOwYcMGo8dDRM2kGd+cTkT3ITo6Whg6dKggCIKg1WqF1NRUwdbWVpg1a5buuKenp1BVVaU756uvvhI6duwoaLVa3b6qqirB3t5eSElJEQRBELy9vYVly5bpjldXVwu+vr66awmCIDz99NPCm2++KQiCIOTk5AgAhNTU1HrH+eOPPwoAhOvXr+v2VVZWCg4ODkJaWppe25iYGGH06NGCIAjC3LlzhZCQEL3j8fHxdfr6KwDCjh07Gjz+wQcfCKGhobrP77zzjmBtbS1cvnxZt2/37t2ClZWVUFhYKAiCILRv317YvHmzXj+LFy8WFAqFIAiCkJubKwAQTp482eB1iah5cY4OkQjt2rULTk5OqK6uhlarxd///ncsWLBAd7xr165683JOnTqF8+fPw9nZWa+fyspKXLhwAWVlZSgsLMSTTz6pO9aqVSs89thjdcpXtTIzM2FtbY2nn3660eM+f/48bt26hb/97W96+9VqNXr27AkAOHv2rN44AEChUDT6GrW2bt2KFStW4MKFC7hx4wZqamogk8n02vj7++u9h0yhUECr1SInJwfOzs64cOECYmJiMGnSJF2bmpoayOVyo8dDRM2DgQ6RCA0YMACrV6+GVCqFj48PWrXS/1V2dHTU+3zjxg2EhoZi06ZNdfpyd3e/rzHY29sbfc6NGzcAAMnJyXVedGpra3tf46hPeno6xowZg4ULFyIiIgJyuRxbtmzBRx99ZPRY161bVyfwsra2NttYiahpMdAhEiFHR0cEBQU1un2vXr2wdetWeHh41Mlq1PL29sYvv/yCfv36AbiducjIyECvXr3qbd+1a1dotVocOnQI4eHhdY7XZpQ0Go1uX0hICGxtbZGfn99gJqhz5866idW1jh49eu+bvEtaWhoCAgLwr3/9S7fvjz/+qNMuPz8fBQUF8PHx0V3HysoKHTt2hKenJ3x8fHDx4kWMGTPGqOsTUcvBychED4ExY8agTZs2GDp0KH7++Wfk5ubi4MGDeOONN3D58mUAwJtvvon3338f33//PbKzszF16lSDa+C0bdsW0dHRmDBhAr7//ntdn9u2bQMABAQEQCKRYNeuXSguLsaNGzfg7OyMWbNmIS4uDhs2bMCFCxfw66+/4tNPP9VN8J08eTLOnTuH2bNnIycnB5s3b0ZSUpJR99uhQwfk5+djy5YtuHDhAlasWFHvxGo7OztER0fj1KlT+Pnnn/HGG29g5MiR8PLyAgAsXLgQS5cuxYoVK/D777/j9OnTSExMxMcff2zUeIio+TDQIXoIODg44KeffoK/vz+GDx+Ozp07IyYmBpWVlboMzz/+8Q+8+uqriI6OhkKhgLOzM1588UWD/a5evRojRozA1KlT0alTJ0yaNAk3b94EADzyyCNYuHAh3nrrLXh6emLatGkAgMWLF2PevHlYunQpOnfujMGDByM5ORmBgYEAbs+b+fbbb/H999+je/fuWLNmDZYsWWLU/b7wwguIi4vDtGnT0KNHD6SlpWHevHl12gUFBWH48OF47rnnMGjQIHTr1k3v8fGJEydi/fr1SExMRNeuXfH0008jKSlJN1YiavkkQkMzDYmIiIhEjhkdIiIislgMdIiIiMhiMdAhIiIii8VAh4iIiCwWAx0iIiKyWAx0iIiIyGIx0CEiIiKLxUCHiIiILBYDHSIiIrJYDHSIiIjIYjHQISIiIovFQIeIiIgs1v8HqdMlFDR+6dYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Model Continue Learning***#\n",
        "#------------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 8224, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 1_2', 'class 0']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wvSnB521BB_z",
        "outputId": "34aec711-bc05-4538-d4bc-8b2c354dccae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9625\n",
            "Epoch 3752: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0757 - accuracy: 0.9625 - val_loss: 7.2976 - val_accuracy: 0.5357\n",
            "Epoch 3753/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9649\n",
            "Epoch 3753: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0748 - accuracy: 0.9649 - val_loss: 7.5354 - val_accuracy: 0.5264\n",
            "Epoch 3754/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9596\n",
            "Epoch 3754: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0789 - accuracy: 0.9596 - val_loss: 7.2563 - val_accuracy: 0.5384\n",
            "Epoch 3755/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9610\n",
            "Epoch 3755: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 0.0778 - accuracy: 0.9610 - val_loss: 7.4740 - val_accuracy: 0.5313\n",
            "Epoch 3756/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9612\n",
            "Epoch 3756: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0783 - accuracy: 0.9612 - val_loss: 7.3907 - val_accuracy: 0.5313\n",
            "Epoch 3757/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9651\n",
            "Epoch 3757: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 0.0752 - accuracy: 0.9651 - val_loss: 7.3418 - val_accuracy: 0.5328\n",
            "Epoch 3758/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9624\n",
            "Epoch 3758: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0759 - accuracy: 0.9624 - val_loss: 7.5390 - val_accuracy: 0.5296\n",
            "Epoch 3759/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9648\n",
            "Epoch 3759: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0741 - accuracy: 0.9648 - val_loss: 7.2354 - val_accuracy: 0.5360\n",
            "Epoch 3760/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9632\n",
            "Epoch 3760: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0762 - accuracy: 0.9632 - val_loss: 7.5510 - val_accuracy: 0.5302\n",
            "Epoch 3761/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9617\n",
            "Epoch 3761: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0773 - accuracy: 0.9617 - val_loss: 7.2910 - val_accuracy: 0.5322\n",
            "Epoch 3762/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9630\n",
            "Epoch 3762: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0776 - accuracy: 0.9630 - val_loss: 7.4075 - val_accuracy: 0.5369\n",
            "Epoch 3763/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9636\n",
            "Epoch 3763: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0766 - accuracy: 0.9636 - val_loss: 7.5100 - val_accuracy: 0.5272\n",
            "Epoch 3764/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9596\n",
            "Epoch 3764: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0794 - accuracy: 0.9596 - val_loss: 7.2369 - val_accuracy: 0.5381\n",
            "Epoch 3765/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9560\n",
            "Epoch 3765: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0847 - accuracy: 0.9560 - val_loss: 7.5329 - val_accuracy: 0.5299\n",
            "Epoch 3766/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9574\n",
            "Epoch 3766: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0832 - accuracy: 0.9574 - val_loss: 7.3549 - val_accuracy: 0.5331\n",
            "Epoch 3767/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9628\n",
            "Epoch 3767: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0756 - accuracy: 0.9628 - val_loss: 7.3119 - val_accuracy: 0.5357\n",
            "Epoch 3768/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9617\n",
            "Epoch 3768: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0756 - accuracy: 0.9617 - val_loss: 7.5368 - val_accuracy: 0.5258\n",
            "Epoch 3769/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9574\n",
            "Epoch 3769: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0820 - accuracy: 0.9574 - val_loss: 7.2836 - val_accuracy: 0.5357\n",
            "Epoch 3770/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9640\n",
            "Epoch 3770: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0746 - accuracy: 0.9640 - val_loss: 7.3477 - val_accuracy: 0.5331\n",
            "Epoch 3771/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9666\n",
            "Epoch 3771: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0726 - accuracy: 0.9666 - val_loss: 7.4121 - val_accuracy: 0.5319\n",
            "Epoch 3772/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9676\n",
            "Epoch 3772: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0720 - accuracy: 0.9676 - val_loss: 7.3045 - val_accuracy: 0.5395\n",
            "Epoch 3773/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9628\n",
            "Epoch 3773: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0743 - accuracy: 0.9628 - val_loss: 7.4941 - val_accuracy: 0.5305\n",
            "Epoch 3774/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9644\n",
            "Epoch 3774: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0740 - accuracy: 0.9644 - val_loss: 7.3013 - val_accuracy: 0.5390\n",
            "Epoch 3775/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9633\n",
            "Epoch 3775: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0774 - accuracy: 0.9633 - val_loss: 7.3310 - val_accuracy: 0.5363\n",
            "Epoch 3776/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9656\n",
            "Epoch 3776: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0732 - accuracy: 0.9656 - val_loss: 7.5299 - val_accuracy: 0.5290\n",
            "Epoch 3777/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9577\n",
            "Epoch 3777: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0800 - accuracy: 0.9577 - val_loss: 7.3609 - val_accuracy: 0.5343\n",
            "Epoch 3778/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9675\n",
            "Epoch 3778: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0711 - accuracy: 0.9675 - val_loss: 7.4172 - val_accuracy: 0.5346\n",
            "Epoch 3779/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9681\n",
            "Epoch 3779: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0711 - accuracy: 0.9681 - val_loss: 7.3771 - val_accuracy: 0.5349\n",
            "Epoch 3780/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9670\n",
            "Epoch 3780: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0703 - accuracy: 0.9670 - val_loss: 7.4153 - val_accuracy: 0.5340\n",
            "Epoch 3781/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9690\n",
            "Epoch 3781: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0699 - accuracy: 0.9690 - val_loss: 7.4064 - val_accuracy: 0.5349\n",
            "Epoch 3782/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9687\n",
            "Epoch 3782: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0694 - accuracy: 0.9687 - val_loss: 7.3616 - val_accuracy: 0.5357\n",
            "Epoch 3783/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9692\n",
            "Epoch 3783: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0704 - accuracy: 0.9692 - val_loss: 7.3871 - val_accuracy: 0.5357\n",
            "Epoch 3784/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9702\n",
            "Epoch 3784: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.0689 - accuracy: 0.9702 - val_loss: 7.4298 - val_accuracy: 0.5343\n",
            "Epoch 3785/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9699\n",
            "Epoch 3785: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0688 - accuracy: 0.9699 - val_loss: 7.3854 - val_accuracy: 0.5357\n",
            "Epoch 3786/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9679\n",
            "Epoch 3786: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 242ms/step - loss: 0.0698 - accuracy: 0.9679 - val_loss: 7.4312 - val_accuracy: 0.5316\n",
            "Epoch 3787/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9690\n",
            "Epoch 3787: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0697 - accuracy: 0.9690 - val_loss: 7.4275 - val_accuracy: 0.5349\n",
            "Epoch 3788/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9696\n",
            "Epoch 3788: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0694 - accuracy: 0.9696 - val_loss: 7.3773 - val_accuracy: 0.5346\n",
            "Epoch 3789/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9666\n",
            "Epoch 3789: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0706 - accuracy: 0.9666 - val_loss: 7.5342 - val_accuracy: 0.5302\n",
            "Epoch 3790/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9661\n",
            "Epoch 3790: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0708 - accuracy: 0.9661 - val_loss: 7.3382 - val_accuracy: 0.5372\n",
            "Epoch 3791/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9659\n",
            "Epoch 3791: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 176ms/step - loss: 0.0725 - accuracy: 0.9659 - val_loss: 7.5223 - val_accuracy: 0.5284\n",
            "Epoch 3792/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9632\n",
            "Epoch 3792: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0752 - accuracy: 0.9632 - val_loss: 7.4616 - val_accuracy: 0.5313\n",
            "Epoch 3793/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9677\n",
            "Epoch 3793: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0713 - accuracy: 0.9677 - val_loss: 7.3370 - val_accuracy: 0.5360\n",
            "Epoch 3794/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9662\n",
            "Epoch 3794: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.0710 - accuracy: 0.9662 - val_loss: 7.4946 - val_accuracy: 0.5319\n",
            "Epoch 3795/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9686\n",
            "Epoch 3795: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0703 - accuracy: 0.9686 - val_loss: 7.3264 - val_accuracy: 0.5351\n",
            "Epoch 3796/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9639\n",
            "Epoch 3796: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0734 - accuracy: 0.9639 - val_loss: 7.4942 - val_accuracy: 0.5331\n",
            "Epoch 3797/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9675\n",
            "Epoch 3797: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.0712 - accuracy: 0.9675 - val_loss: 7.4144 - val_accuracy: 0.5340\n",
            "Epoch 3798/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9688\n",
            "Epoch 3798: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0700 - accuracy: 0.9688 - val_loss: 7.3982 - val_accuracy: 0.5369\n",
            "Epoch 3799/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9684\n",
            "Epoch 3799: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0706 - accuracy: 0.9684 - val_loss: 7.4602 - val_accuracy: 0.5284\n",
            "Epoch 3800/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9670\n",
            "Epoch 3800: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0713 - accuracy: 0.9670 - val_loss: 7.3250 - val_accuracy: 0.5375\n",
            "Epoch 3801/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9649\n",
            "Epoch 3801: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0731 - accuracy: 0.9649 - val_loss: 7.4242 - val_accuracy: 0.5337\n",
            "Epoch 3802/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9681\n",
            "Epoch 3802: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0706 - accuracy: 0.9681 - val_loss: 7.4485 - val_accuracy: 0.5363\n",
            "Epoch 3803/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9678\n",
            "Epoch 3803: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0706 - accuracy: 0.9678 - val_loss: 7.3662 - val_accuracy: 0.5351\n",
            "Epoch 3804/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9669\n",
            "Epoch 3804: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0721 - accuracy: 0.9669 - val_loss: 7.5079 - val_accuracy: 0.5319\n",
            "Epoch 3805/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9656\n",
            "Epoch 3805: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0719 - accuracy: 0.9656 - val_loss: 7.3609 - val_accuracy: 0.5372\n",
            "Epoch 3806/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9674\n",
            "Epoch 3806: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0719 - accuracy: 0.9674 - val_loss: 7.4300 - val_accuracy: 0.5337\n",
            "Epoch 3807/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9683\n",
            "Epoch 3807: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 184ms/step - loss: 0.0706 - accuracy: 0.9683 - val_loss: 7.4981 - val_accuracy: 0.5319\n",
            "Epoch 3808/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9675\n",
            "Epoch 3808: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0709 - accuracy: 0.9675 - val_loss: 7.3612 - val_accuracy: 0.5357\n",
            "Epoch 3809/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9659\n",
            "Epoch 3809: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0724 - accuracy: 0.9659 - val_loss: 7.4643 - val_accuracy: 0.5316\n",
            "Epoch 3810/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9650\n",
            "Epoch 3810: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0729 - accuracy: 0.9650 - val_loss: 7.3958 - val_accuracy: 0.5354\n",
            "Epoch 3811/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9667\n",
            "Epoch 3811: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0729 - accuracy: 0.9667 - val_loss: 7.3603 - val_accuracy: 0.5346\n",
            "Epoch 3812/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9665\n",
            "Epoch 3812: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.0715 - accuracy: 0.9665 - val_loss: 7.4725 - val_accuracy: 0.5296\n",
            "Epoch 3813/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9657\n",
            "Epoch 3813: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0730 - accuracy: 0.9657 - val_loss: 7.4184 - val_accuracy: 0.5372\n",
            "Epoch 3814/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9659\n",
            "Epoch 3814: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0716 - accuracy: 0.9659 - val_loss: 7.4049 - val_accuracy: 0.5337\n",
            "Epoch 3815/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9689\n",
            "Epoch 3815: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0702 - accuracy: 0.9689 - val_loss: 7.4516 - val_accuracy: 0.5328\n",
            "Epoch 3816/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9681\n",
            "Epoch 3816: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0704 - accuracy: 0.9681 - val_loss: 7.3865 - val_accuracy: 0.5351\n",
            "Epoch 3817/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9676\n",
            "Epoch 3817: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0697 - accuracy: 0.9676 - val_loss: 7.4519 - val_accuracy: 0.5313\n",
            "Epoch 3818/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9697\n",
            "Epoch 3818: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0694 - accuracy: 0.9697 - val_loss: 7.3921 - val_accuracy: 0.5387\n",
            "Epoch 3819/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9687\n",
            "Epoch 3819: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0694 - accuracy: 0.9687 - val_loss: 7.4752 - val_accuracy: 0.5334\n",
            "Epoch 3820/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9683\n",
            "Epoch 3820: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0704 - accuracy: 0.9683 - val_loss: 7.4927 - val_accuracy: 0.5349\n",
            "Epoch 3821/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9688\n",
            "Epoch 3821: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0696 - accuracy: 0.9688 - val_loss: 7.3862 - val_accuracy: 0.5325\n",
            "Epoch 3822/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9684\n",
            "Epoch 3822: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0686 - accuracy: 0.9684 - val_loss: 7.4588 - val_accuracy: 0.5290\n",
            "Epoch 3823/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9706\n",
            "Epoch 3823: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0686 - accuracy: 0.9706 - val_loss: 7.4682 - val_accuracy: 0.5340\n",
            "Epoch 3824/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9696\n",
            "Epoch 3824: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0688 - accuracy: 0.9696 - val_loss: 7.3712 - val_accuracy: 0.5378\n",
            "Epoch 3825/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9706\n",
            "Epoch 3825: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0687 - accuracy: 0.9706 - val_loss: 7.5804 - val_accuracy: 0.5322\n",
            "Epoch 3826/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9667\n",
            "Epoch 3826: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0704 - accuracy: 0.9667 - val_loss: 7.3611 - val_accuracy: 0.5331\n",
            "Epoch 3827/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9681\n",
            "Epoch 3827: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0706 - accuracy: 0.9681 - val_loss: 7.5180 - val_accuracy: 0.5343\n",
            "Epoch 3828/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9666\n",
            "Epoch 3828: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0723 - accuracy: 0.9666 - val_loss: 7.4409 - val_accuracy: 0.5340\n",
            "Epoch 3829/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9689\n",
            "Epoch 3829: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0704 - accuracy: 0.9689 - val_loss: 7.3498 - val_accuracy: 0.5363\n",
            "Epoch 3830/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9648\n",
            "Epoch 3830: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0721 - accuracy: 0.9648 - val_loss: 7.5419 - val_accuracy: 0.5272\n",
            "Epoch 3831/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9665\n",
            "Epoch 3831: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0713 - accuracy: 0.9665 - val_loss: 7.3786 - val_accuracy: 0.5357\n",
            "Epoch 3832/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9645\n",
            "Epoch 3832: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0730 - accuracy: 0.9645 - val_loss: 7.4669 - val_accuracy: 0.5346\n",
            "Epoch 3833/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9673\n",
            "Epoch 3833: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0704 - accuracy: 0.9673 - val_loss: 7.4552 - val_accuracy: 0.5331\n",
            "Epoch 3834/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9685\n",
            "Epoch 3834: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0695 - accuracy: 0.9685 - val_loss: 7.4626 - val_accuracy: 0.5369\n",
            "Epoch 3835/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9707\n",
            "Epoch 3835: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0684 - accuracy: 0.9707 - val_loss: 7.4117 - val_accuracy: 0.5343\n",
            "Epoch 3836/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9686\n",
            "Epoch 3836: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 254ms/step - loss: 0.0695 - accuracy: 0.9686 - val_loss: 7.4353 - val_accuracy: 0.5340\n",
            "Epoch 3837/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9696\n",
            "Epoch 3837: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0686 - accuracy: 0.9696 - val_loss: 7.4969 - val_accuracy: 0.5322\n",
            "Epoch 3838/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9693\n",
            "Epoch 3838: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0686 - accuracy: 0.9693 - val_loss: 7.3900 - val_accuracy: 0.5384\n",
            "Epoch 3839/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9698\n",
            "Epoch 3839: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0693 - accuracy: 0.9698 - val_loss: 7.4494 - val_accuracy: 0.5369\n",
            "Epoch 3840/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9676\n",
            "Epoch 3840: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0698 - accuracy: 0.9676 - val_loss: 7.4997 - val_accuracy: 0.5322\n",
            "Epoch 3841/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9682\n",
            "Epoch 3841: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0705 - accuracy: 0.9682 - val_loss: 7.4476 - val_accuracy: 0.5369\n",
            "Epoch 3842/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9695\n",
            "Epoch 3842: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0711 - accuracy: 0.9695 - val_loss: 7.4710 - val_accuracy: 0.5319\n",
            "Epoch 3843/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9684\n",
            "Epoch 3843: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0703 - accuracy: 0.9684 - val_loss: 7.4370 - val_accuracy: 0.5322\n",
            "Epoch 3844/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9694\n",
            "Epoch 3844: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 269ms/step - loss: 0.0700 - accuracy: 0.9694 - val_loss: 7.4250 - val_accuracy: 0.5375\n",
            "Epoch 3845/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9680\n",
            "Epoch 3845: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 0.0702 - accuracy: 0.9680 - val_loss: 7.4870 - val_accuracy: 0.5313\n",
            "Epoch 3846/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9659\n",
            "Epoch 3846: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0719 - accuracy: 0.9659 - val_loss: 7.5132 - val_accuracy: 0.5334\n",
            "Epoch 3847/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9677\n",
            "Epoch 3847: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0698 - accuracy: 0.9677 - val_loss: 7.3377 - val_accuracy: 0.5372\n",
            "Epoch 3848/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9672\n",
            "Epoch 3848: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0713 - accuracy: 0.9672 - val_loss: 7.5916 - val_accuracy: 0.5325\n",
            "Epoch 3849/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9641\n",
            "Epoch 3849: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 238ms/step - loss: 0.0737 - accuracy: 0.9641 - val_loss: 7.3794 - val_accuracy: 0.5351\n",
            "Epoch 3850/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9668\n",
            "Epoch 3850: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0725 - accuracy: 0.9668 - val_loss: 7.4377 - val_accuracy: 0.5346\n",
            "Epoch 3851/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9694\n",
            "Epoch 3851: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0701 - accuracy: 0.9694 - val_loss: 7.5708 - val_accuracy: 0.5302\n",
            "Epoch 3852/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9658\n",
            "Epoch 3852: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 170ms/step - loss: 0.0724 - accuracy: 0.9658 - val_loss: 7.4073 - val_accuracy: 0.5360\n",
            "Epoch 3853/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9696\n",
            "Epoch 3853: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0687 - accuracy: 0.9696 - val_loss: 7.4916 - val_accuracy: 0.5349\n",
            "Epoch 3854/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9691\n",
            "Epoch 3854: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0684 - accuracy: 0.9691 - val_loss: 7.3671 - val_accuracy: 0.5360\n",
            "Epoch 3855/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9661\n",
            "Epoch 3855: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0718 - accuracy: 0.9661 - val_loss: 7.4956 - val_accuracy: 0.5354\n",
            "Epoch 3856/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9702\n",
            "Epoch 3856: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0688 - accuracy: 0.9702 - val_loss: 7.4898 - val_accuracy: 0.5322\n",
            "Epoch 3857/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9692\n",
            "Epoch 3857: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0693 - accuracy: 0.9692 - val_loss: 7.4467 - val_accuracy: 0.5354\n",
            "Epoch 3858/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9699\n",
            "Epoch 3858: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0686 - accuracy: 0.9699 - val_loss: 7.4679 - val_accuracy: 0.5343\n",
            "Epoch 3859/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9704\n",
            "Epoch 3859: loss did not improve from 0.06759\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0685 - accuracy: 0.9704 - val_loss: 7.4744 - val_accuracy: 0.5325\n",
            "Epoch 3860/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9709\n",
            "Epoch 3860: loss improved from 0.06759 to 0.06753, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 2s 2s/step - loss: 0.0675 - accuracy: 0.9709 - val_loss: 7.4754 - val_accuracy: 0.5360\n",
            "Epoch 3861/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9704\n",
            "Epoch 3861: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0684 - accuracy: 0.9704 - val_loss: 7.4645 - val_accuracy: 0.5343\n",
            "Epoch 3862/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9711\n",
            "Epoch 3862: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0680 - accuracy: 0.9711 - val_loss: 7.4724 - val_accuracy: 0.5360\n",
            "Epoch 3863/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9708\n",
            "Epoch 3863: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0681 - accuracy: 0.9708 - val_loss: 7.4444 - val_accuracy: 0.5360\n",
            "Epoch 3864/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9717\n",
            "Epoch 3864: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0683 - accuracy: 0.9717 - val_loss: 7.5383 - val_accuracy: 0.5308\n",
            "Epoch 3865/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9700\n",
            "Epoch 3865: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0682 - accuracy: 0.9700 - val_loss: 7.3850 - val_accuracy: 0.5340\n",
            "Epoch 3866/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9659\n",
            "Epoch 3866: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0720 - accuracy: 0.9659 - val_loss: 7.5271 - val_accuracy: 0.5328\n",
            "Epoch 3867/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9685\n",
            "Epoch 3867: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.0698 - accuracy: 0.9685 - val_loss: 7.5199 - val_accuracy: 0.5308\n",
            "Epoch 3868/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9685\n",
            "Epoch 3868: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0696 - accuracy: 0.9685 - val_loss: 7.3922 - val_accuracy: 0.5325\n",
            "Epoch 3869/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9705\n",
            "Epoch 3869: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0686 - accuracy: 0.9705 - val_loss: 7.5517 - val_accuracy: 0.5313\n",
            "Epoch 3870/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9669\n",
            "Epoch 3870: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0708 - accuracy: 0.9669 - val_loss: 7.5070 - val_accuracy: 0.5372\n",
            "Epoch 3871/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9698\n",
            "Epoch 3871: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0684 - accuracy: 0.9698 - val_loss: 7.4474 - val_accuracy: 0.5331\n",
            "Epoch 3872/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9701\n",
            "Epoch 3872: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0684 - accuracy: 0.9701 - val_loss: 7.5059 - val_accuracy: 0.5322\n",
            "Epoch 3873/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9699\n",
            "Epoch 3873: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0685 - accuracy: 0.9699 - val_loss: 7.3895 - val_accuracy: 0.5351\n",
            "Epoch 3874/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9651\n",
            "Epoch 3874: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0732 - accuracy: 0.9651 - val_loss: 7.5218 - val_accuracy: 0.5328\n",
            "Epoch 3875/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9642\n",
            "Epoch 3875: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0738 - accuracy: 0.9642 - val_loss: 7.5244 - val_accuracy: 0.5313\n",
            "Epoch 3876/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9664\n",
            "Epoch 3876: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0711 - accuracy: 0.9664 - val_loss: 7.3913 - val_accuracy: 0.5372\n",
            "Epoch 3877/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9625\n",
            "Epoch 3877: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0753 - accuracy: 0.9625 - val_loss: 7.5496 - val_accuracy: 0.5305\n",
            "Epoch 3878/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9608\n",
            "Epoch 3878: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0770 - accuracy: 0.9608 - val_loss: 7.5418 - val_accuracy: 0.5316\n",
            "Epoch 3879/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9660\n",
            "Epoch 3879: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0714 - accuracy: 0.9660 - val_loss: 7.3233 - val_accuracy: 0.5354\n",
            "Epoch 3880/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9618\n",
            "Epoch 3880: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0769 - accuracy: 0.9618 - val_loss: 7.5495 - val_accuracy: 0.5261\n",
            "Epoch 3881/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9628\n",
            "Epoch 3881: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0768 - accuracy: 0.9628 - val_loss: 7.4657 - val_accuracy: 0.5331\n",
            "Epoch 3882/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9665\n",
            "Epoch 3882: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0717 - accuracy: 0.9665 - val_loss: 7.3734 - val_accuracy: 0.5363\n",
            "Epoch 3883/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9667\n",
            "Epoch 3883: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0722 - accuracy: 0.9667 - val_loss: 7.6178 - val_accuracy: 0.5275\n",
            "Epoch 3884/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9599\n",
            "Epoch 3884: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0796 - accuracy: 0.9599 - val_loss: 7.3112 - val_accuracy: 0.5387\n",
            "Epoch 3885/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9604\n",
            "Epoch 3885: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0779 - accuracy: 0.9604 - val_loss: 7.4242 - val_accuracy: 0.5369\n",
            "Epoch 3886/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9667\n",
            "Epoch 3886: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.0726 - accuracy: 0.9667 - val_loss: 7.5181 - val_accuracy: 0.5305\n",
            "Epoch 3887/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9654\n",
            "Epoch 3887: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.0728 - accuracy: 0.9654 - val_loss: 7.3065 - val_accuracy: 0.5372\n",
            "Epoch 3888/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9605\n",
            "Epoch 3888: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0774 - accuracy: 0.9605 - val_loss: 7.4956 - val_accuracy: 0.5313\n",
            "Epoch 3889/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9632\n",
            "Epoch 3889: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0743 - accuracy: 0.9632 - val_loss: 7.4776 - val_accuracy: 0.5328\n",
            "Epoch 3890/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9666\n",
            "Epoch 3890: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0707 - accuracy: 0.9666 - val_loss: 7.4027 - val_accuracy: 0.5357\n",
            "Epoch 3891/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9661\n",
            "Epoch 3891: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0719 - accuracy: 0.9661 - val_loss: 7.5679 - val_accuracy: 0.5243\n",
            "Epoch 3892/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9648\n",
            "Epoch 3892: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0737 - accuracy: 0.9648 - val_loss: 7.3534 - val_accuracy: 0.5331\n",
            "Epoch 3893/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9650\n",
            "Epoch 3893: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0724 - accuracy: 0.9650 - val_loss: 7.4501 - val_accuracy: 0.5308\n",
            "Epoch 3894/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9667\n",
            "Epoch 3894: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 252ms/step - loss: 0.0710 - accuracy: 0.9667 - val_loss: 7.5468 - val_accuracy: 0.5302\n",
            "Epoch 3895/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9673\n",
            "Epoch 3895: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0714 - accuracy: 0.9673 - val_loss: 7.3463 - val_accuracy: 0.5354\n",
            "Epoch 3896/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9655\n",
            "Epoch 3896: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0712 - accuracy: 0.9655 - val_loss: 7.5290 - val_accuracy: 0.5305\n",
            "Epoch 3897/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9612\n",
            "Epoch 3897: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0760 - accuracy: 0.9612 - val_loss: 7.4086 - val_accuracy: 0.5360\n",
            "Epoch 3898/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9665\n",
            "Epoch 3898: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0719 - accuracy: 0.9665 - val_loss: 7.4438 - val_accuracy: 0.5351\n",
            "Epoch 3899/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9665\n",
            "Epoch 3899: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0726 - accuracy: 0.9665 - val_loss: 7.5583 - val_accuracy: 0.5305\n",
            "Epoch 3900/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9619\n",
            "Epoch 3900: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0765 - accuracy: 0.9619 - val_loss: 7.2910 - val_accuracy: 0.5340\n",
            "Epoch 3901/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9568\n",
            "Epoch 3901: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0821 - accuracy: 0.9568 - val_loss: 7.4212 - val_accuracy: 0.5360\n",
            "Epoch 3902/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9678\n",
            "Epoch 3902: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0717 - accuracy: 0.9678 - val_loss: 7.4443 - val_accuracy: 0.5310\n",
            "Epoch 3903/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9648\n",
            "Epoch 3903: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0724 - accuracy: 0.9648 - val_loss: 7.3608 - val_accuracy: 0.5375\n",
            "Epoch 3904/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9635\n",
            "Epoch 3904: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0743 - accuracy: 0.9635 - val_loss: 7.4422 - val_accuracy: 0.5308\n",
            "Epoch 3905/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9672\n",
            "Epoch 3905: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0706 - accuracy: 0.9672 - val_loss: 7.3855 - val_accuracy: 0.5366\n",
            "Epoch 3906/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9670\n",
            "Epoch 3906: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 168ms/step - loss: 0.0716 - accuracy: 0.9670 - val_loss: 7.4801 - val_accuracy: 0.5357\n",
            "Epoch 3907/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9684\n",
            "Epoch 3907: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 171ms/step - loss: 0.0701 - accuracy: 0.9684 - val_loss: 7.4040 - val_accuracy: 0.5328\n",
            "Epoch 3908/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9685\n",
            "Epoch 3908: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0701 - accuracy: 0.9685 - val_loss: 7.4494 - val_accuracy: 0.5328\n",
            "Epoch 3909/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9679\n",
            "Epoch 3909: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 168ms/step - loss: 0.0690 - accuracy: 0.9679 - val_loss: 7.4143 - val_accuracy: 0.5331\n",
            "Epoch 3910/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9682\n",
            "Epoch 3910: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 168ms/step - loss: 0.0698 - accuracy: 0.9682 - val_loss: 7.4698 - val_accuracy: 0.5308\n",
            "Epoch 3911/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9697\n",
            "Epoch 3911: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0682 - accuracy: 0.9697 - val_loss: 7.3961 - val_accuracy: 0.5337\n",
            "Epoch 3912/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9688\n",
            "Epoch 3912: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0695 - accuracy: 0.9688 - val_loss: 7.4117 - val_accuracy: 0.5325\n",
            "Epoch 3913/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9703\n",
            "Epoch 3913: loss did not improve from 0.06753\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0686 - accuracy: 0.9703 - val_loss: 7.4390 - val_accuracy: 0.5337\n",
            "Epoch 3914/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9705\n",
            "Epoch 3914: loss improved from 0.06753 to 0.06752, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 695ms/step - loss: 0.0675 - accuracy: 0.9705 - val_loss: 7.4544 - val_accuracy: 0.5343\n",
            "Epoch 3915/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9695\n",
            "Epoch 3915: loss did not improve from 0.06752\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 0.0682 - accuracy: 0.9695 - val_loss: 7.4746 - val_accuracy: 0.5351\n",
            "Epoch 3916/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9697\n",
            "Epoch 3916: loss did not improve from 0.06752\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0680 - accuracy: 0.9697 - val_loss: 7.3967 - val_accuracy: 0.5363\n",
            "Epoch 3917/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9717\n",
            "Epoch 3917: loss did not improve from 0.06752\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0675 - accuracy: 0.9717 - val_loss: 7.5017 - val_accuracy: 0.5349\n",
            "Epoch 3918/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9684\n",
            "Epoch 3918: loss did not improve from 0.06752\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0692 - accuracy: 0.9684 - val_loss: 7.4362 - val_accuracy: 0.5325\n",
            "Epoch 3919/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9706\n",
            "Epoch 3919: loss improved from 0.06752 to 0.06744, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0674 - accuracy: 0.9706 - val_loss: 7.4503 - val_accuracy: 0.5372\n",
            "Epoch 3920/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9718\n",
            "Epoch 3920: loss improved from 0.06744 to 0.06716, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0672 - accuracy: 0.9718 - val_loss: 7.5107 - val_accuracy: 0.5340\n",
            "Epoch 3921/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9695\n",
            "Epoch 3921: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0680 - accuracy: 0.9695 - val_loss: 7.4215 - val_accuracy: 0.5343\n",
            "Epoch 3922/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9691\n",
            "Epoch 3922: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0680 - accuracy: 0.9691 - val_loss: 7.4885 - val_accuracy: 0.5340\n",
            "Epoch 3923/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9712\n",
            "Epoch 3923: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0678 - accuracy: 0.9712 - val_loss: 7.5273 - val_accuracy: 0.5331\n",
            "Epoch 3924/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9708\n",
            "Epoch 3924: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0679 - accuracy: 0.9708 - val_loss: 7.3865 - val_accuracy: 0.5398\n",
            "Epoch 3925/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9672\n",
            "Epoch 3925: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0717 - accuracy: 0.9672 - val_loss: 7.5501 - val_accuracy: 0.5269\n",
            "Epoch 3926/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9616\n",
            "Epoch 3926: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0766 - accuracy: 0.9616 - val_loss: 7.5280 - val_accuracy: 0.5299\n",
            "Epoch 3927/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9661\n",
            "Epoch 3927: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0720 - accuracy: 0.9661 - val_loss: 7.3106 - val_accuracy: 0.5360\n",
            "Epoch 3928/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9607\n",
            "Epoch 3928: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0770 - accuracy: 0.9607 - val_loss: 7.6238 - val_accuracy: 0.5255\n",
            "Epoch 3929/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9587\n",
            "Epoch 3929: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0802 - accuracy: 0.9587 - val_loss: 7.3696 - val_accuracy: 0.5325\n",
            "Epoch 3930/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9613\n",
            "Epoch 3930: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0781 - accuracy: 0.9613 - val_loss: 7.4185 - val_accuracy: 0.5378\n",
            "Epoch 3931/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9629\n",
            "Epoch 3931: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0745 - accuracy: 0.9629 - val_loss: 7.5674 - val_accuracy: 0.5296\n",
            "Epoch 3932/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9599\n",
            "Epoch 3932: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 172ms/step - loss: 0.0787 - accuracy: 0.9599 - val_loss: 7.3360 - val_accuracy: 0.5363\n",
            "Epoch 3933/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9623\n",
            "Epoch 3933: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0754 - accuracy: 0.9623 - val_loss: 7.5000 - val_accuracy: 0.5328\n",
            "Epoch 3934/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9676\n",
            "Epoch 3934: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0714 - accuracy: 0.9676 - val_loss: 7.3831 - val_accuracy: 0.5343\n",
            "Epoch 3935/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9652\n",
            "Epoch 3935: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0722 - accuracy: 0.9652 - val_loss: 7.4228 - val_accuracy: 0.5346\n",
            "Epoch 3936/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9670\n",
            "Epoch 3936: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.0709 - accuracy: 0.9670 - val_loss: 7.4767 - val_accuracy: 0.5319\n",
            "Epoch 3937/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9683\n",
            "Epoch 3937: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0701 - accuracy: 0.9683 - val_loss: 7.4083 - val_accuracy: 0.5334\n",
            "Epoch 3938/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9689\n",
            "Epoch 3938: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0691 - accuracy: 0.9689 - val_loss: 7.4722 - val_accuracy: 0.5322\n",
            "Epoch 3939/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9678\n",
            "Epoch 3939: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 168ms/step - loss: 0.0699 - accuracy: 0.9678 - val_loss: 7.4536 - val_accuracy: 0.5334\n",
            "Epoch 3940/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9714\n",
            "Epoch 3940: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0682 - accuracy: 0.9714 - val_loss: 7.4285 - val_accuracy: 0.5360\n",
            "Epoch 3941/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9695\n",
            "Epoch 3941: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 247ms/step - loss: 0.0684 - accuracy: 0.9695 - val_loss: 7.5070 - val_accuracy: 0.5343\n",
            "Epoch 3942/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9676\n",
            "Epoch 3942: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0700 - accuracy: 0.9676 - val_loss: 7.4324 - val_accuracy: 0.5363\n",
            "Epoch 3943/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9699\n",
            "Epoch 3943: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0683 - accuracy: 0.9699 - val_loss: 7.4384 - val_accuracy: 0.5354\n",
            "Epoch 3944/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9708\n",
            "Epoch 3944: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0682 - accuracy: 0.9708 - val_loss: 7.5125 - val_accuracy: 0.5302\n",
            "Epoch 3945/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9678\n",
            "Epoch 3945: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0693 - accuracy: 0.9678 - val_loss: 7.4034 - val_accuracy: 0.5346\n",
            "Epoch 3946/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9664\n",
            "Epoch 3946: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0716 - accuracy: 0.9664 - val_loss: 7.4759 - val_accuracy: 0.5346\n",
            "Epoch 3947/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9675\n",
            "Epoch 3947: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0713 - accuracy: 0.9675 - val_loss: 7.5866 - val_accuracy: 0.5278\n",
            "Epoch 3948/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9636\n",
            "Epoch 3948: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0733 - accuracy: 0.9636 - val_loss: 7.4144 - val_accuracy: 0.5354\n",
            "Epoch 3949/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9656\n",
            "Epoch 3949: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0711 - accuracy: 0.9656 - val_loss: 7.4666 - val_accuracy: 0.5316\n",
            "Epoch 3950/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9682\n",
            "Epoch 3950: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0696 - accuracy: 0.9682 - val_loss: 7.4736 - val_accuracy: 0.5319\n",
            "Epoch 3951/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9686\n",
            "Epoch 3951: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 0.0693 - accuracy: 0.9686 - val_loss: 7.3696 - val_accuracy: 0.5346\n",
            "Epoch 3952/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9667\n",
            "Epoch 3952: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0711 - accuracy: 0.9667 - val_loss: 7.5878 - val_accuracy: 0.5281\n",
            "Epoch 3953/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9651\n",
            "Epoch 3953: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0717 - accuracy: 0.9651 - val_loss: 7.4118 - val_accuracy: 0.5349\n",
            "Epoch 3954/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9667\n",
            "Epoch 3954: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 0.0725 - accuracy: 0.9667 - val_loss: 7.3819 - val_accuracy: 0.5363\n",
            "Epoch 3955/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9663\n",
            "Epoch 3955: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0719 - accuracy: 0.9663 - val_loss: 7.6094 - val_accuracy: 0.5261\n",
            "Epoch 3956/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9607\n",
            "Epoch 3956: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0771 - accuracy: 0.9607 - val_loss: 7.3394 - val_accuracy: 0.5360\n",
            "Epoch 3957/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9607\n",
            "Epoch 3957: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0763 - accuracy: 0.9607 - val_loss: 7.4512 - val_accuracy: 0.5354\n",
            "Epoch 3958/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9653\n",
            "Epoch 3958: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0738 - accuracy: 0.9653 - val_loss: 7.6693 - val_accuracy: 0.5267\n",
            "Epoch 3959/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9600\n",
            "Epoch 3959: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0790 - accuracy: 0.9600 - val_loss: 7.2052 - val_accuracy: 0.5384\n",
            "Epoch 3960/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9480\n",
            "Epoch 3960: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0972 - accuracy: 0.9480 - val_loss: 7.5266 - val_accuracy: 0.5325\n",
            "Epoch 3961/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9582\n",
            "Epoch 3961: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0862 - accuracy: 0.9582 - val_loss: 7.5714 - val_accuracy: 0.5281\n",
            "Epoch 3962/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9588\n",
            "Epoch 3962: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0827 - accuracy: 0.9588 - val_loss: 7.1447 - val_accuracy: 0.5393\n",
            "Epoch 3963/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9525\n",
            "Epoch 3963: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0940 - accuracy: 0.9525 - val_loss: 7.5795 - val_accuracy: 0.5267\n",
            "Epoch 3964/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9525\n",
            "Epoch 3964: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0926 - accuracy: 0.9525 - val_loss: 7.2967 - val_accuracy: 0.5387\n",
            "Epoch 3965/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9607\n",
            "Epoch 3965: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0798 - accuracy: 0.9607 - val_loss: 7.3778 - val_accuracy: 0.5354\n",
            "Epoch 3966/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9610\n",
            "Epoch 3966: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0789 - accuracy: 0.9610 - val_loss: 7.4940 - val_accuracy: 0.5269\n",
            "Epoch 3967/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9585\n",
            "Epoch 3967: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0805 - accuracy: 0.9585 - val_loss: 7.2761 - val_accuracy: 0.5401\n",
            "Epoch 3968/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9603\n",
            "Epoch 3968: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0813 - accuracy: 0.9603 - val_loss: 7.5296 - val_accuracy: 0.5310\n",
            "Epoch 3969/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9577\n",
            "Epoch 3969: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0840 - accuracy: 0.9577 - val_loss: 7.2600 - val_accuracy: 0.5354\n",
            "Epoch 3970/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9605\n",
            "Epoch 3970: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0794 - accuracy: 0.9605 - val_loss: 7.3952 - val_accuracy: 0.5325\n",
            "Epoch 3971/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9640\n",
            "Epoch 3971: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0770 - accuracy: 0.9640 - val_loss: 7.3954 - val_accuracy: 0.5316\n",
            "Epoch 3972/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9645\n",
            "Epoch 3972: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0749 - accuracy: 0.9645 - val_loss: 7.3508 - val_accuracy: 0.5357\n",
            "Epoch 3973/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9627\n",
            "Epoch 3973: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0748 - accuracy: 0.9627 - val_loss: 7.4387 - val_accuracy: 0.5322\n",
            "Epoch 3974/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9632\n",
            "Epoch 3974: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0752 - accuracy: 0.9632 - val_loss: 7.4024 - val_accuracy: 0.5346\n",
            "Epoch 3975/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9673\n",
            "Epoch 3975: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0709 - accuracy: 0.9673 - val_loss: 7.3374 - val_accuracy: 0.5354\n",
            "Epoch 3976/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9687\n",
            "Epoch 3976: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0702 - accuracy: 0.9687 - val_loss: 7.4556 - val_accuracy: 0.5369\n",
            "Epoch 3977/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9687\n",
            "Epoch 3977: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0698 - accuracy: 0.9687 - val_loss: 7.3655 - val_accuracy: 0.5357\n",
            "Epoch 3978/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9697\n",
            "Epoch 3978: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0699 - accuracy: 0.9697 - val_loss: 7.4831 - val_accuracy: 0.5275\n",
            "Epoch 3979/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9660\n",
            "Epoch 3979: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0712 - accuracy: 0.9660 - val_loss: 7.3890 - val_accuracy: 0.5349\n",
            "Epoch 3980/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9675\n",
            "Epoch 3980: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0696 - accuracy: 0.9675 - val_loss: 7.3753 - val_accuracy: 0.5328\n",
            "Epoch 3981/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9703\n",
            "Epoch 3981: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0683 - accuracy: 0.9703 - val_loss: 7.4443 - val_accuracy: 0.5351\n",
            "Epoch 3982/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9696\n",
            "Epoch 3982: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0688 - accuracy: 0.9696 - val_loss: 7.3896 - val_accuracy: 0.5357\n",
            "Epoch 3983/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9706\n",
            "Epoch 3983: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0680 - accuracy: 0.9706 - val_loss: 7.4398 - val_accuracy: 0.5334\n",
            "Epoch 3984/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9711\n",
            "Epoch 3984: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0677 - accuracy: 0.9711 - val_loss: 7.3435 - val_accuracy: 0.5372\n",
            "Epoch 3985/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9700\n",
            "Epoch 3985: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0692 - accuracy: 0.9700 - val_loss: 7.4584 - val_accuracy: 0.5349\n",
            "Epoch 3986/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9711\n",
            "Epoch 3986: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0674 - accuracy: 0.9711 - val_loss: 7.3967 - val_accuracy: 0.5369\n",
            "Epoch 3987/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9692\n",
            "Epoch 3987: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0685 - accuracy: 0.9692 - val_loss: 7.4473 - val_accuracy: 0.5322\n",
            "Epoch 3988/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9697\n",
            "Epoch 3988: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0680 - accuracy: 0.9697 - val_loss: 7.4594 - val_accuracy: 0.5334\n",
            "Epoch 3989/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9697\n",
            "Epoch 3989: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0685 - accuracy: 0.9697 - val_loss: 7.3819 - val_accuracy: 0.5360\n",
            "Epoch 3990/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9706\n",
            "Epoch 3990: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0680 - accuracy: 0.9706 - val_loss: 7.4984 - val_accuracy: 0.5313\n",
            "Epoch 3991/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9701\n",
            "Epoch 3991: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0682 - accuracy: 0.9701 - val_loss: 7.3577 - val_accuracy: 0.5357\n",
            "Epoch 3992/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9671\n",
            "Epoch 3992: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0702 - accuracy: 0.9671 - val_loss: 7.5078 - val_accuracy: 0.5334\n",
            "Epoch 3993/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9687\n",
            "Epoch 3993: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0686 - accuracy: 0.9687 - val_loss: 7.4210 - val_accuracy: 0.5351\n",
            "Epoch 3994/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9689\n",
            "Epoch 3994: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0686 - accuracy: 0.9689 - val_loss: 7.4275 - val_accuracy: 0.5337\n",
            "Epoch 3995/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9714\n",
            "Epoch 3995: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0675 - accuracy: 0.9714 - val_loss: 7.5007 - val_accuracy: 0.5337\n",
            "Epoch 3996/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9702\n",
            "Epoch 3996: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0678 - accuracy: 0.9702 - val_loss: 7.3841 - val_accuracy: 0.5357\n",
            "Epoch 3997/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9696\n",
            "Epoch 3997: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0686 - accuracy: 0.9696 - val_loss: 7.5344 - val_accuracy: 0.5334\n",
            "Epoch 3998/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9675\n",
            "Epoch 3998: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0688 - accuracy: 0.9675 - val_loss: 7.4357 - val_accuracy: 0.5357\n",
            "Epoch 3999/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9683\n",
            "Epoch 3999: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0693 - accuracy: 0.9683 - val_loss: 7.4337 - val_accuracy: 0.5343\n",
            "Epoch 4000/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9687\n",
            "Epoch 4000: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0685 - accuracy: 0.9687 - val_loss: 7.5246 - val_accuracy: 0.5296\n",
            "Epoch 4001/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9685\n",
            "Epoch 4001: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0691 - accuracy: 0.9685 - val_loss: 7.3337 - val_accuracy: 0.5375\n",
            "Epoch 4002/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9630\n",
            "Epoch 4002: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0742 - accuracy: 0.9630 - val_loss: 7.5432 - val_accuracy: 0.5354\n",
            "Epoch 4003/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9670\n",
            "Epoch 4003: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0699 - accuracy: 0.9670 - val_loss: 7.4229 - val_accuracy: 0.5378\n",
            "Epoch 4004/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9664\n",
            "Epoch 4004: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0717 - accuracy: 0.9664 - val_loss: 7.4062 - val_accuracy: 0.5354\n",
            "Epoch 4005/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9662\n",
            "Epoch 4005: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0730 - accuracy: 0.9662 - val_loss: 7.5883 - val_accuracy: 0.5284\n",
            "Epoch 4006/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9624\n",
            "Epoch 4006: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.0771 - accuracy: 0.9624 - val_loss: 7.3245 - val_accuracy: 0.5369\n",
            "Epoch 4007/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9591\n",
            "Epoch 4007: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0805 - accuracy: 0.9591 - val_loss: 7.4173 - val_accuracy: 0.5349\n",
            "Epoch 4008/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9665\n",
            "Epoch 4008: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0728 - accuracy: 0.9665 - val_loss: 7.5266 - val_accuracy: 0.5272\n",
            "Epoch 4009/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9669\n",
            "Epoch 4009: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0714 - accuracy: 0.9669 - val_loss: 7.3749 - val_accuracy: 0.5363\n",
            "Epoch 4010/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9633\n",
            "Epoch 4010: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0751 - accuracy: 0.9633 - val_loss: 7.4278 - val_accuracy: 0.5310\n",
            "Epoch 4011/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9655\n",
            "Epoch 4011: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0728 - accuracy: 0.9655 - val_loss: 7.3893 - val_accuracy: 0.5337\n",
            "Epoch 4012/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9650\n",
            "Epoch 4012: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0744 - accuracy: 0.9650 - val_loss: 7.3542 - val_accuracy: 0.5369\n",
            "Epoch 4013/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9656\n",
            "Epoch 4013: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0723 - accuracy: 0.9656 - val_loss: 7.5512 - val_accuracy: 0.5278\n",
            "Epoch 4014/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9640\n",
            "Epoch 4014: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0747 - accuracy: 0.9640 - val_loss: 7.4147 - val_accuracy: 0.5372\n",
            "Epoch 4015/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9689\n",
            "Epoch 4015: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0701 - accuracy: 0.9689 - val_loss: 7.4116 - val_accuracy: 0.5328\n",
            "Epoch 4016/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9670\n",
            "Epoch 4016: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0703 - accuracy: 0.9670 - val_loss: 7.4739 - val_accuracy: 0.5319\n",
            "Epoch 4017/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9674\n",
            "Epoch 4017: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0700 - accuracy: 0.9674 - val_loss: 7.3881 - val_accuracy: 0.5354\n",
            "Epoch 4018/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9694\n",
            "Epoch 4018: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0698 - accuracy: 0.9694 - val_loss: 7.4642 - val_accuracy: 0.5313\n",
            "Epoch 4019/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9693\n",
            "Epoch 4019: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0683 - accuracy: 0.9693 - val_loss: 7.4383 - val_accuracy: 0.5346\n",
            "Epoch 4020/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9700\n",
            "Epoch 4020: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0682 - accuracy: 0.9700 - val_loss: 7.4613 - val_accuracy: 0.5310\n",
            "Epoch 4021/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9699\n",
            "Epoch 4021: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0696 - accuracy: 0.9699 - val_loss: 7.4618 - val_accuracy: 0.5340\n",
            "Epoch 4022/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9711\n",
            "Epoch 4022: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0685 - accuracy: 0.9711 - val_loss: 7.4212 - val_accuracy: 0.5372\n",
            "Epoch 4023/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9702\n",
            "Epoch 4023: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.0685 - accuracy: 0.9702 - val_loss: 7.5152 - val_accuracy: 0.5299\n",
            "Epoch 4024/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9656\n",
            "Epoch 4024: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0699 - accuracy: 0.9656 - val_loss: 7.4414 - val_accuracy: 0.5387\n",
            "Epoch 4025/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9683\n",
            "Epoch 4025: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.0699 - accuracy: 0.9683 - val_loss: 7.4096 - val_accuracy: 0.5319\n",
            "Epoch 4026/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9681\n",
            "Epoch 4026: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0698 - accuracy: 0.9681 - val_loss: 7.5400 - val_accuracy: 0.5319\n",
            "Epoch 4027/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9674\n",
            "Epoch 4027: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0704 - accuracy: 0.9674 - val_loss: 7.3404 - val_accuracy: 0.5343\n",
            "Epoch 4028/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9643\n",
            "Epoch 4028: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0739 - accuracy: 0.9643 - val_loss: 7.5355 - val_accuracy: 0.5293\n",
            "Epoch 4029/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9640\n",
            "Epoch 4029: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0732 - accuracy: 0.9640 - val_loss: 7.4580 - val_accuracy: 0.5363\n",
            "Epoch 4030/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9657\n",
            "Epoch 4030: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0714 - accuracy: 0.9657 - val_loss: 7.3329 - val_accuracy: 0.5357\n",
            "Epoch 4031/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9657\n",
            "Epoch 4031: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0717 - accuracy: 0.9657 - val_loss: 7.5891 - val_accuracy: 0.5293\n",
            "Epoch 4032/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9631\n",
            "Epoch 4032: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0739 - accuracy: 0.9631 - val_loss: 7.3506 - val_accuracy: 0.5340\n",
            "Epoch 4033/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9671\n",
            "Epoch 4033: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0717 - accuracy: 0.9671 - val_loss: 7.5334 - val_accuracy: 0.5351\n",
            "Epoch 4034/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9672\n",
            "Epoch 4034: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0710 - accuracy: 0.9672 - val_loss: 7.4916 - val_accuracy: 0.5334\n",
            "Epoch 4035/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9676\n",
            "Epoch 4035: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0698 - accuracy: 0.9676 - val_loss: 7.3594 - val_accuracy: 0.5349\n",
            "Epoch 4036/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9675\n",
            "Epoch 4036: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0706 - accuracy: 0.9675 - val_loss: 7.5944 - val_accuracy: 0.5299\n",
            "Epoch 4037/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9659\n",
            "Epoch 4037: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0720 - accuracy: 0.9659 - val_loss: 7.3430 - val_accuracy: 0.5360\n",
            "Epoch 4038/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9603\n",
            "Epoch 4038: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0781 - accuracy: 0.9603 - val_loss: 7.4351 - val_accuracy: 0.5372\n",
            "Epoch 4039/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9651\n",
            "Epoch 4039: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 0.0750 - accuracy: 0.9651 - val_loss: 7.5208 - val_accuracy: 0.5284\n",
            "Epoch 4040/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9604\n",
            "Epoch 4040: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0797 - accuracy: 0.9604 - val_loss: 7.2924 - val_accuracy: 0.5366\n",
            "Epoch 4041/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9528\n",
            "Epoch 4041: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0873 - accuracy: 0.9528 - val_loss: 7.5445 - val_accuracy: 0.5269\n",
            "Epoch 4042/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9553\n",
            "Epoch 4042: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0852 - accuracy: 0.9553 - val_loss: 7.4327 - val_accuracy: 0.5325\n",
            "Epoch 4043/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9629\n",
            "Epoch 4043: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0763 - accuracy: 0.9629 - val_loss: 7.3476 - val_accuracy: 0.5387\n",
            "Epoch 4044/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9622\n",
            "Epoch 4044: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0778 - accuracy: 0.9622 - val_loss: 7.4871 - val_accuracy: 0.5264\n",
            "Epoch 4045/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9632\n",
            "Epoch 4045: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0753 - accuracy: 0.9632 - val_loss: 7.2839 - val_accuracy: 0.5381\n",
            "Epoch 4046/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9629\n",
            "Epoch 4046: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0756 - accuracy: 0.9629 - val_loss: 7.4953 - val_accuracy: 0.5316\n",
            "Epoch 4047/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9632\n",
            "Epoch 4047: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0748 - accuracy: 0.9632 - val_loss: 7.3994 - val_accuracy: 0.5366\n",
            "Epoch 4048/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9660\n",
            "Epoch 4048: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0728 - accuracy: 0.9660 - val_loss: 7.3366 - val_accuracy: 0.5366\n",
            "Epoch 4049/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9658\n",
            "Epoch 4049: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0722 - accuracy: 0.9658 - val_loss: 7.5215 - val_accuracy: 0.5287\n",
            "Epoch 4050/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9629\n",
            "Epoch 4050: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0756 - accuracy: 0.9629 - val_loss: 7.3543 - val_accuracy: 0.5372\n",
            "Epoch 4051/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9595\n",
            "Epoch 4051: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0774 - accuracy: 0.9595 - val_loss: 7.4466 - val_accuracy: 0.5337\n",
            "Epoch 4052/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9646\n",
            "Epoch 4052: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0745 - accuracy: 0.9646 - val_loss: 7.4769 - val_accuracy: 0.5316\n",
            "Epoch 4053/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9640\n",
            "Epoch 4053: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0737 - accuracy: 0.9640 - val_loss: 7.2732 - val_accuracy: 0.5357\n",
            "Epoch 4054/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9594\n",
            "Epoch 4054: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0790 - accuracy: 0.9594 - val_loss: 7.5748 - val_accuracy: 0.5272\n",
            "Epoch 4055/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9563\n",
            "Epoch 4055: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0821 - accuracy: 0.9563 - val_loss: 7.3492 - val_accuracy: 0.5372\n",
            "Epoch 4056/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9654\n",
            "Epoch 4056: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0756 - accuracy: 0.9654 - val_loss: 7.3126 - val_accuracy: 0.5351\n",
            "Epoch 4057/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9631\n",
            "Epoch 4057: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0746 - accuracy: 0.9631 - val_loss: 7.6008 - val_accuracy: 0.5296\n",
            "Epoch 4058/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9600\n",
            "Epoch 4058: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0793 - accuracy: 0.9600 - val_loss: 7.2384 - val_accuracy: 0.5363\n",
            "Epoch 4059/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9588\n",
            "Epoch 4059: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0803 - accuracy: 0.9588 - val_loss: 7.5294 - val_accuracy: 0.5337\n",
            "Epoch 4060/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9629\n",
            "Epoch 4060: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0766 - accuracy: 0.9629 - val_loss: 7.3876 - val_accuracy: 0.5316\n",
            "Epoch 4061/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9625\n",
            "Epoch 4061: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 248ms/step - loss: 0.0767 - accuracy: 0.9625 - val_loss: 7.3341 - val_accuracy: 0.5390\n",
            "Epoch 4062/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9602\n",
            "Epoch 4062: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0792 - accuracy: 0.9602 - val_loss: 7.5540 - val_accuracy: 0.5246\n",
            "Epoch 4063/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9612\n",
            "Epoch 4063: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 270ms/step - loss: 0.0767 - accuracy: 0.9612 - val_loss: 7.2635 - val_accuracy: 0.5363\n",
            "Epoch 4064/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9581\n",
            "Epoch 4064: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0828 - accuracy: 0.9581 - val_loss: 7.4538 - val_accuracy: 0.5351\n",
            "Epoch 4065/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9610\n",
            "Epoch 4065: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 0.0779 - accuracy: 0.9610 - val_loss: 7.4185 - val_accuracy: 0.5308\n",
            "Epoch 4066/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9615\n",
            "Epoch 4066: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0770 - accuracy: 0.9615 - val_loss: 7.3148 - val_accuracy: 0.5369\n",
            "Epoch 4067/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9599\n",
            "Epoch 4067: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.0779 - accuracy: 0.9599 - val_loss: 7.4645 - val_accuracy: 0.5284\n",
            "Epoch 4068/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9618\n",
            "Epoch 4068: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0776 - accuracy: 0.9618 - val_loss: 7.2760 - val_accuracy: 0.5363\n",
            "Epoch 4069/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9595\n",
            "Epoch 4069: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0793 - accuracy: 0.9595 - val_loss: 7.4029 - val_accuracy: 0.5363\n",
            "Epoch 4070/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9670\n",
            "Epoch 4070: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0724 - accuracy: 0.9670 - val_loss: 7.3523 - val_accuracy: 0.5316\n",
            "Epoch 4071/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9662\n",
            "Epoch 4071: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0732 - accuracy: 0.9662 - val_loss: 7.2867 - val_accuracy: 0.5416\n",
            "Epoch 4072/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9644\n",
            "Epoch 4072: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0757 - accuracy: 0.9644 - val_loss: 7.5474 - val_accuracy: 0.5255\n",
            "Epoch 4073/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9574\n",
            "Epoch 4073: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0807 - accuracy: 0.9574 - val_loss: 7.3525 - val_accuracy: 0.5366\n",
            "Epoch 4074/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9660\n",
            "Epoch 4074: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0725 - accuracy: 0.9660 - val_loss: 7.3761 - val_accuracy: 0.5381\n",
            "Epoch 4075/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9670\n",
            "Epoch 4075: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0707 - accuracy: 0.9670 - val_loss: 7.4800 - val_accuracy: 0.5313\n",
            "Epoch 4076/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9643\n",
            "Epoch 4076: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0733 - accuracy: 0.9643 - val_loss: 7.2678 - val_accuracy: 0.5390\n",
            "Epoch 4077/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9577\n",
            "Epoch 4077: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0817 - accuracy: 0.9577 - val_loss: 7.5221 - val_accuracy: 0.5278\n",
            "Epoch 4078/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9569\n",
            "Epoch 4078: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0820 - accuracy: 0.9569 - val_loss: 7.3872 - val_accuracy: 0.5343\n",
            "Epoch 4079/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9643\n",
            "Epoch 4079: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0750 - accuracy: 0.9643 - val_loss: 7.3606 - val_accuracy: 0.5387\n",
            "Epoch 4080/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9642\n",
            "Epoch 4080: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0757 - accuracy: 0.9642 - val_loss: 7.5357 - val_accuracy: 0.5287\n",
            "Epoch 4081/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9614\n",
            "Epoch 4081: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0774 - accuracy: 0.9614 - val_loss: 7.2377 - val_accuracy: 0.5340\n",
            "Epoch 4082/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9587\n",
            "Epoch 4082: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.0808 - accuracy: 0.9587 - val_loss: 7.4604 - val_accuracy: 0.5278\n",
            "Epoch 4083/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9593\n",
            "Epoch 4083: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0784 - accuracy: 0.9593 - val_loss: 7.2996 - val_accuracy: 0.5372\n",
            "Epoch 4084/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9649\n",
            "Epoch 4084: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0736 - accuracy: 0.9649 - val_loss: 7.4332 - val_accuracy: 0.5334\n",
            "Epoch 4085/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9650\n",
            "Epoch 4085: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0744 - accuracy: 0.9650 - val_loss: 7.3640 - val_accuracy: 0.5305\n",
            "Epoch 4086/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9642\n",
            "Epoch 4086: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0735 - accuracy: 0.9642 - val_loss: 7.3100 - val_accuracy: 0.5346\n",
            "Epoch 4087/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9636\n",
            "Epoch 4087: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 184ms/step - loss: 0.0738 - accuracy: 0.9636 - val_loss: 7.4931 - val_accuracy: 0.5287\n",
            "Epoch 4088/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9625\n",
            "Epoch 4088: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0757 - accuracy: 0.9625 - val_loss: 7.2998 - val_accuracy: 0.5375\n",
            "Epoch 4089/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9654\n",
            "Epoch 4089: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0725 - accuracy: 0.9654 - val_loss: 7.4225 - val_accuracy: 0.5357\n",
            "Epoch 4090/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9668\n",
            "Epoch 4090: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0719 - accuracy: 0.9668 - val_loss: 7.4134 - val_accuracy: 0.5334\n",
            "Epoch 4091/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9687\n",
            "Epoch 4091: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.0693 - accuracy: 0.9687 - val_loss: 7.4262 - val_accuracy: 0.5343\n",
            "Epoch 4092/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9684\n",
            "Epoch 4092: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0700 - accuracy: 0.9684 - val_loss: 7.4490 - val_accuracy: 0.5319\n",
            "Epoch 4093/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9678\n",
            "Epoch 4093: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0706 - accuracy: 0.9678 - val_loss: 7.4270 - val_accuracy: 0.5366\n",
            "Epoch 4094/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9708\n",
            "Epoch 4094: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0686 - accuracy: 0.9708 - val_loss: 7.3768 - val_accuracy: 0.5357\n",
            "Epoch 4095/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9697\n",
            "Epoch 4095: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0677 - accuracy: 0.9697 - val_loss: 7.4233 - val_accuracy: 0.5366\n",
            "Epoch 4096/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9717\n",
            "Epoch 4096: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0675 - accuracy: 0.9717 - val_loss: 7.4542 - val_accuracy: 0.5349\n",
            "Epoch 4097/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9707\n",
            "Epoch 4097: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0674 - accuracy: 0.9707 - val_loss: 7.3791 - val_accuracy: 0.5357\n",
            "Epoch 4098/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9692\n",
            "Epoch 4098: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0682 - accuracy: 0.9692 - val_loss: 7.4497 - val_accuracy: 0.5337\n",
            "Epoch 4099/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9714\n",
            "Epoch 4099: loss did not improve from 0.06716\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0673 - accuracy: 0.9714 - val_loss: 7.4286 - val_accuracy: 0.5360\n",
            "Epoch 4100/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9722\n",
            "Epoch 4100: loss improved from 0.06716 to 0.06672, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.0667 - accuracy: 0.9722 - val_loss: 7.3816 - val_accuracy: 0.5351\n",
            "Epoch 4101/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9711\n",
            "Epoch 4101: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0678 - accuracy: 0.9711 - val_loss: 7.4403 - val_accuracy: 0.5331\n",
            "Epoch 4102/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9708\n",
            "Epoch 4102: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0674 - accuracy: 0.9708 - val_loss: 7.5012 - val_accuracy: 0.5360\n",
            "Epoch 4103/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9711\n",
            "Epoch 4103: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0673 - accuracy: 0.9711 - val_loss: 7.4050 - val_accuracy: 0.5366\n",
            "Epoch 4104/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9712\n",
            "Epoch 4104: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0679 - accuracy: 0.9712 - val_loss: 7.4031 - val_accuracy: 0.5343\n",
            "Epoch 4105/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9703\n",
            "Epoch 4105: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0682 - accuracy: 0.9703 - val_loss: 7.4508 - val_accuracy: 0.5357\n",
            "Epoch 4106/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9720\n",
            "Epoch 4106: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0671 - accuracy: 0.9720 - val_loss: 7.4505 - val_accuracy: 0.5328\n",
            "Epoch 4107/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9697\n",
            "Epoch 4107: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0677 - accuracy: 0.9697 - val_loss: 7.4731 - val_accuracy: 0.5360\n",
            "Epoch 4108/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9706\n",
            "Epoch 4108: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0675 - accuracy: 0.9706 - val_loss: 7.4750 - val_accuracy: 0.5316\n",
            "Epoch 4109/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9713\n",
            "Epoch 4109: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0670 - accuracy: 0.9713 - val_loss: 7.4451 - val_accuracy: 0.5343\n",
            "Epoch 4110/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9712\n",
            "Epoch 4110: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0676 - accuracy: 0.9712 - val_loss: 7.4558 - val_accuracy: 0.5337\n",
            "Epoch 4111/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9711\n",
            "Epoch 4111: loss did not improve from 0.06672\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.0668 - accuracy: 0.9711 - val_loss: 7.4535 - val_accuracy: 0.5357\n",
            "Epoch 4112/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9705\n",
            "Epoch 4112: loss improved from 0.06672 to 0.06656, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 280ms/step - loss: 0.0666 - accuracy: 0.9705 - val_loss: 7.4818 - val_accuracy: 0.5378\n",
            "Epoch 4113/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9700\n",
            "Epoch 4113: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 240ms/step - loss: 0.0678 - accuracy: 0.9700 - val_loss: 7.4865 - val_accuracy: 0.5349\n",
            "Epoch 4114/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9703\n",
            "Epoch 4114: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0674 - accuracy: 0.9703 - val_loss: 7.4901 - val_accuracy: 0.5366\n",
            "Epoch 4115/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9709\n",
            "Epoch 4115: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 235ms/step - loss: 0.0679 - accuracy: 0.9709 - val_loss: 7.4433 - val_accuracy: 0.5308\n",
            "Epoch 4116/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9706\n",
            "Epoch 4116: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 250ms/step - loss: 0.0676 - accuracy: 0.9706 - val_loss: 7.4642 - val_accuracy: 0.5310\n",
            "Epoch 4117/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9706\n",
            "Epoch 4117: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.0669 - accuracy: 0.9706 - val_loss: 7.4763 - val_accuracy: 0.5331\n",
            "Epoch 4118/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9718\n",
            "Epoch 4118: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.0671 - accuracy: 0.9718 - val_loss: 7.4577 - val_accuracy: 0.5349\n",
            "Epoch 4119/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9698\n",
            "Epoch 4119: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0679 - accuracy: 0.9698 - val_loss: 7.4579 - val_accuracy: 0.5340\n",
            "Epoch 4120/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9728\n",
            "Epoch 4120: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0667 - accuracy: 0.9728 - val_loss: 7.4008 - val_accuracy: 0.5366\n",
            "Epoch 4121/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9701\n",
            "Epoch 4121: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0673 - accuracy: 0.9701 - val_loss: 7.5217 - val_accuracy: 0.5334\n",
            "Epoch 4122/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9708\n",
            "Epoch 4122: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0679 - accuracy: 0.9708 - val_loss: 7.4209 - val_accuracy: 0.5357\n",
            "Epoch 4123/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9684\n",
            "Epoch 4123: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0692 - accuracy: 0.9684 - val_loss: 7.4044 - val_accuracy: 0.5363\n",
            "Epoch 4124/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9683\n",
            "Epoch 4124: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0688 - accuracy: 0.9683 - val_loss: 7.5377 - val_accuracy: 0.5310\n",
            "Epoch 4125/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9674\n",
            "Epoch 4125: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0697 - accuracy: 0.9674 - val_loss: 7.4298 - val_accuracy: 0.5369\n",
            "Epoch 4126/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9670\n",
            "Epoch 4126: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0695 - accuracy: 0.9670 - val_loss: 7.4625 - val_accuracy: 0.5369\n",
            "Epoch 4127/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9703\n",
            "Epoch 4127: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0681 - accuracy: 0.9703 - val_loss: 7.4828 - val_accuracy: 0.5299\n",
            "Epoch 4128/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9683\n",
            "Epoch 4128: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 0.0682 - accuracy: 0.9683 - val_loss: 7.3933 - val_accuracy: 0.5360\n",
            "Epoch 4129/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9659\n",
            "Epoch 4129: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0714 - accuracy: 0.9659 - val_loss: 7.5580 - val_accuracy: 0.5287\n",
            "Epoch 4130/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9646\n",
            "Epoch 4130: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 244ms/step - loss: 0.0718 - accuracy: 0.9646 - val_loss: 7.4138 - val_accuracy: 0.5346\n",
            "Epoch 4131/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9684\n",
            "Epoch 4131: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0695 - accuracy: 0.9684 - val_loss: 7.4507 - val_accuracy: 0.5354\n",
            "Epoch 4132/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9689\n",
            "Epoch 4132: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0688 - accuracy: 0.9689 - val_loss: 7.5383 - val_accuracy: 0.5316\n",
            "Epoch 4133/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9684\n",
            "Epoch 4133: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0689 - accuracy: 0.9684 - val_loss: 7.4148 - val_accuracy: 0.5393\n",
            "Epoch 4134/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9662\n",
            "Epoch 4134: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0712 - accuracy: 0.9662 - val_loss: 7.5178 - val_accuracy: 0.5319\n",
            "Epoch 4135/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9662\n",
            "Epoch 4135: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0710 - accuracy: 0.9662 - val_loss: 7.4814 - val_accuracy: 0.5369\n",
            "Epoch 4136/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9678\n",
            "Epoch 4136: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0696 - accuracy: 0.9678 - val_loss: 7.4147 - val_accuracy: 0.5398\n",
            "Epoch 4137/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9676\n",
            "Epoch 4137: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0706 - accuracy: 0.9676 - val_loss: 7.6181 - val_accuracy: 0.5296\n",
            "Epoch 4138/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9625\n",
            "Epoch 4138: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0744 - accuracy: 0.9625 - val_loss: 7.3602 - val_accuracy: 0.5354\n",
            "Epoch 4139/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9632\n",
            "Epoch 4139: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0756 - accuracy: 0.9632 - val_loss: 7.4822 - val_accuracy: 0.5299\n",
            "Epoch 4140/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9657\n",
            "Epoch 4140: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0726 - accuracy: 0.9657 - val_loss: 7.4470 - val_accuracy: 0.5354\n",
            "Epoch 4141/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9661\n",
            "Epoch 4141: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0722 - accuracy: 0.9661 - val_loss: 7.3153 - val_accuracy: 0.5375\n",
            "Epoch 4142/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9640\n",
            "Epoch 4142: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0726 - accuracy: 0.9640 - val_loss: 7.6083 - val_accuracy: 0.5290\n",
            "Epoch 4143/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9627\n",
            "Epoch 4143: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0750 - accuracy: 0.9627 - val_loss: 7.2834 - val_accuracy: 0.5395\n",
            "Epoch 4144/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9656\n",
            "Epoch 4144: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0745 - accuracy: 0.9656 - val_loss: 7.4810 - val_accuracy: 0.5308\n",
            "Epoch 4145/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9649\n",
            "Epoch 4145: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0739 - accuracy: 0.9649 - val_loss: 7.5016 - val_accuracy: 0.5290\n",
            "Epoch 4146/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9639\n",
            "Epoch 4146: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0726 - accuracy: 0.9639 - val_loss: 7.2811 - val_accuracy: 0.5346\n",
            "Epoch 4147/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9624\n",
            "Epoch 4147: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0743 - accuracy: 0.9624 - val_loss: 7.6240 - val_accuracy: 0.5272\n",
            "Epoch 4148/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9594\n",
            "Epoch 4148: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0805 - accuracy: 0.9594 - val_loss: 7.3101 - val_accuracy: 0.5351\n",
            "Epoch 4149/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9610\n",
            "Epoch 4149: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0782 - accuracy: 0.9610 - val_loss: 7.3751 - val_accuracy: 0.5349\n",
            "Epoch 4150/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9653\n",
            "Epoch 4150: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0724 - accuracy: 0.9653 - val_loss: 7.4974 - val_accuracy: 0.5293\n",
            "Epoch 4151/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9648\n",
            "Epoch 4151: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0725 - accuracy: 0.9648 - val_loss: 7.3475 - val_accuracy: 0.5372\n",
            "Epoch 4152/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9648\n",
            "Epoch 4152: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0733 - accuracy: 0.9648 - val_loss: 7.5648 - val_accuracy: 0.5302\n",
            "Epoch 4153/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9611\n",
            "Epoch 4153: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0771 - accuracy: 0.9611 - val_loss: 7.3052 - val_accuracy: 0.5375\n",
            "Epoch 4154/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9605\n",
            "Epoch 4154: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0785 - accuracy: 0.9605 - val_loss: 7.4110 - val_accuracy: 0.5366\n",
            "Epoch 4155/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9662\n",
            "Epoch 4155: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0729 - accuracy: 0.9662 - val_loss: 7.4736 - val_accuracy: 0.5325\n",
            "Epoch 4156/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9646\n",
            "Epoch 4156: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0731 - accuracy: 0.9646 - val_loss: 7.3343 - val_accuracy: 0.5387\n",
            "Epoch 4157/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9642\n",
            "Epoch 4157: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0743 - accuracy: 0.9642 - val_loss: 7.5616 - val_accuracy: 0.5296\n",
            "Epoch 4158/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9620\n",
            "Epoch 4158: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0751 - accuracy: 0.9620 - val_loss: 7.3803 - val_accuracy: 0.5360\n",
            "Epoch 4159/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9675\n",
            "Epoch 4159: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0713 - accuracy: 0.9675 - val_loss: 7.4887 - val_accuracy: 0.5316\n",
            "Epoch 4160/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9675\n",
            "Epoch 4160: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0709 - accuracy: 0.9675 - val_loss: 7.4044 - val_accuracy: 0.5351\n",
            "Epoch 4161/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9676\n",
            "Epoch 4161: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0712 - accuracy: 0.9676 - val_loss: 7.3881 - val_accuracy: 0.5357\n",
            "Epoch 4162/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9680\n",
            "Epoch 4162: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0704 - accuracy: 0.9680 - val_loss: 7.4986 - val_accuracy: 0.5310\n",
            "Epoch 4163/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9663\n",
            "Epoch 4163: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0710 - accuracy: 0.9663 - val_loss: 7.4010 - val_accuracy: 0.5375\n",
            "Epoch 4164/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9676\n",
            "Epoch 4164: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0710 - accuracy: 0.9676 - val_loss: 7.4867 - val_accuracy: 0.5328\n",
            "Epoch 4165/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9692\n",
            "Epoch 4165: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0687 - accuracy: 0.9692 - val_loss: 7.4076 - val_accuracy: 0.5357\n",
            "Epoch 4166/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9698\n",
            "Epoch 4166: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0691 - accuracy: 0.9698 - val_loss: 7.4431 - val_accuracy: 0.5343\n",
            "Epoch 4167/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9690\n",
            "Epoch 4167: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.0686 - accuracy: 0.9690 - val_loss: 7.4383 - val_accuracy: 0.5313\n",
            "Epoch 4168/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9700\n",
            "Epoch 4168: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0680 - accuracy: 0.9700 - val_loss: 7.3931 - val_accuracy: 0.5340\n",
            "Epoch 4169/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9681\n",
            "Epoch 4169: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0694 - accuracy: 0.9681 - val_loss: 7.5072 - val_accuracy: 0.5293\n",
            "Epoch 4170/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9681\n",
            "Epoch 4170: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0695 - accuracy: 0.9681 - val_loss: 7.4240 - val_accuracy: 0.5369\n",
            "Epoch 4171/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9680\n",
            "Epoch 4171: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0690 - accuracy: 0.9680 - val_loss: 7.4687 - val_accuracy: 0.5369\n",
            "Epoch 4172/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9692\n",
            "Epoch 4172: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0692 - accuracy: 0.9692 - val_loss: 7.4710 - val_accuracy: 0.5319\n",
            "Epoch 4173/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9694\n",
            "Epoch 4173: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0691 - accuracy: 0.9694 - val_loss: 7.4214 - val_accuracy: 0.5346\n",
            "Epoch 4174/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9681\n",
            "Epoch 4174: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0689 - accuracy: 0.9681 - val_loss: 7.5522 - val_accuracy: 0.5296\n",
            "Epoch 4175/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9667\n",
            "Epoch 4175: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0708 - accuracy: 0.9667 - val_loss: 7.4128 - val_accuracy: 0.5375\n",
            "Epoch 4176/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9660\n",
            "Epoch 4176: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.0720 - accuracy: 0.9660 - val_loss: 7.4445 - val_accuracy: 0.5363\n",
            "Epoch 4177/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9706\n",
            "Epoch 4177: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0684 - accuracy: 0.9706 - val_loss: 7.4665 - val_accuracy: 0.5331\n",
            "Epoch 4178/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9702\n",
            "Epoch 4178: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0685 - accuracy: 0.9702 - val_loss: 7.4280 - val_accuracy: 0.5366\n",
            "Epoch 4179/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9692\n",
            "Epoch 4179: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0689 - accuracy: 0.9692 - val_loss: 7.5240 - val_accuracy: 0.5308\n",
            "Epoch 4180/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9689\n",
            "Epoch 4180: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0692 - accuracy: 0.9689 - val_loss: 7.4451 - val_accuracy: 0.5378\n",
            "Epoch 4181/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9709\n",
            "Epoch 4181: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.0692 - accuracy: 0.9709 - val_loss: 7.3921 - val_accuracy: 0.5393\n",
            "Epoch 4182/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9702\n",
            "Epoch 4182: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0687 - accuracy: 0.9702 - val_loss: 7.5433 - val_accuracy: 0.5316\n",
            "Epoch 4183/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9678\n",
            "Epoch 4183: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0692 - accuracy: 0.9678 - val_loss: 7.4130 - val_accuracy: 0.5349\n",
            "Epoch 4184/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9689\n",
            "Epoch 4184: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0688 - accuracy: 0.9689 - val_loss: 7.4757 - val_accuracy: 0.5337\n",
            "Epoch 4185/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9717\n",
            "Epoch 4185: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0678 - accuracy: 0.9717 - val_loss: 7.4876 - val_accuracy: 0.5328\n",
            "Epoch 4186/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9715\n",
            "Epoch 4186: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0667 - accuracy: 0.9715 - val_loss: 7.4332 - val_accuracy: 0.5337\n",
            "Epoch 4187/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9708\n",
            "Epoch 4187: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.0671 - accuracy: 0.9708 - val_loss: 7.4942 - val_accuracy: 0.5331\n",
            "Epoch 4188/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9706\n",
            "Epoch 4188: loss did not improve from 0.06656\n",
            "2/2 [==============================] - 0s 171ms/step - loss: 0.0672 - accuracy: 0.9706 - val_loss: 7.4842 - val_accuracy: 0.5331\n",
            "Epoch 4189/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9724\n",
            "Epoch 4189: loss improved from 0.06656 to 0.06650, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 782ms/step - loss: 0.0665 - accuracy: 0.9724 - val_loss: 7.4796 - val_accuracy: 0.5360\n",
            "Epoch 4190/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9694\n",
            "Epoch 4190: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0678 - accuracy: 0.9694 - val_loss: 7.4651 - val_accuracy: 0.5346\n",
            "Epoch 4191/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9710\n",
            "Epoch 4191: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0676 - accuracy: 0.9710 - val_loss: 7.5375 - val_accuracy: 0.5340\n",
            "Epoch 4192/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9723\n",
            "Epoch 4192: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0671 - accuracy: 0.9723 - val_loss: 7.4227 - val_accuracy: 0.5375\n",
            "Epoch 4193/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9676\n",
            "Epoch 4193: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0692 - accuracy: 0.9676 - val_loss: 7.5184 - val_accuracy: 0.5319\n",
            "Epoch 4194/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9701\n",
            "Epoch 4194: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0678 - accuracy: 0.9701 - val_loss: 7.4998 - val_accuracy: 0.5384\n",
            "Epoch 4195/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9666\n",
            "Epoch 4195: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0708 - accuracy: 0.9666 - val_loss: 7.4089 - val_accuracy: 0.5351\n",
            "Epoch 4196/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9692\n",
            "Epoch 4196: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0686 - accuracy: 0.9692 - val_loss: 7.5646 - val_accuracy: 0.5346\n",
            "Epoch 4197/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9689\n",
            "Epoch 4197: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0684 - accuracy: 0.9689 - val_loss: 7.4362 - val_accuracy: 0.5357\n",
            "Epoch 4198/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9706\n",
            "Epoch 4198: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0681 - accuracy: 0.9706 - val_loss: 7.4975 - val_accuracy: 0.5328\n",
            "Epoch 4199/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9693\n",
            "Epoch 4199: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0688 - accuracy: 0.9693 - val_loss: 7.5654 - val_accuracy: 0.5322\n",
            "Epoch 4200/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9703\n",
            "Epoch 4200: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0674 - accuracy: 0.9703 - val_loss: 7.4108 - val_accuracy: 0.5363\n",
            "Epoch 4201/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9701\n",
            "Epoch 4201: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0681 - accuracy: 0.9701 - val_loss: 7.5507 - val_accuracy: 0.5308\n",
            "Epoch 4202/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9672\n",
            "Epoch 4202: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0698 - accuracy: 0.9672 - val_loss: 7.4509 - val_accuracy: 0.5381\n",
            "Epoch 4203/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9694\n",
            "Epoch 4203: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0676 - accuracy: 0.9694 - val_loss: 7.4696 - val_accuracy: 0.5375\n",
            "Epoch 4204/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9701\n",
            "Epoch 4204: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0667 - accuracy: 0.9701 - val_loss: 7.5332 - val_accuracy: 0.5340\n",
            "Epoch 4205/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9712\n",
            "Epoch 4205: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0672 - accuracy: 0.9712 - val_loss: 7.4236 - val_accuracy: 0.5354\n",
            "Epoch 4206/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9708\n",
            "Epoch 4206: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0676 - accuracy: 0.9708 - val_loss: 7.5740 - val_accuracy: 0.5325\n",
            "Epoch 4207/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9678\n",
            "Epoch 4207: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0692 - accuracy: 0.9678 - val_loss: 7.4921 - val_accuracy: 0.5331\n",
            "Epoch 4208/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9703\n",
            "Epoch 4208: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0679 - accuracy: 0.9703 - val_loss: 7.3924 - val_accuracy: 0.5360\n",
            "Epoch 4209/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9685\n",
            "Epoch 4209: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 0.0691 - accuracy: 0.9685 - val_loss: 7.6087 - val_accuracy: 0.5296\n",
            "Epoch 4210/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9655\n",
            "Epoch 4210: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 0.0723 - accuracy: 0.9655 - val_loss: 7.4354 - val_accuracy: 0.5351\n",
            "Epoch 4211/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9676\n",
            "Epoch 4211: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0695 - accuracy: 0.9676 - val_loss: 7.4322 - val_accuracy: 0.5366\n",
            "Epoch 4212/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9689\n",
            "Epoch 4212: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0689 - accuracy: 0.9689 - val_loss: 7.5902 - val_accuracy: 0.5284\n",
            "Epoch 4213/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9659\n",
            "Epoch 4213: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0713 - accuracy: 0.9659 - val_loss: 7.4141 - val_accuracy: 0.5375\n",
            "Epoch 4214/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9686\n",
            "Epoch 4214: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0688 - accuracy: 0.9686 - val_loss: 7.5221 - val_accuracy: 0.5340\n",
            "Epoch 4215/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9666\n",
            "Epoch 4215: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0713 - accuracy: 0.9666 - val_loss: 7.5457 - val_accuracy: 0.5349\n",
            "Epoch 4216/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9671\n",
            "Epoch 4216: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0690 - accuracy: 0.9671 - val_loss: 7.4102 - val_accuracy: 0.5369\n",
            "Epoch 4217/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9664\n",
            "Epoch 4217: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.0703 - accuracy: 0.9664 - val_loss: 7.6155 - val_accuracy: 0.5287\n",
            "Epoch 4218/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9613\n",
            "Epoch 4218: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0766 - accuracy: 0.9613 - val_loss: 7.4305 - val_accuracy: 0.5337\n",
            "Epoch 4219/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9688\n",
            "Epoch 4219: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0695 - accuracy: 0.9688 - val_loss: 7.4661 - val_accuracy: 0.5354\n",
            "Epoch 4220/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9684\n",
            "Epoch 4220: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0688 - accuracy: 0.9684 - val_loss: 7.5624 - val_accuracy: 0.5343\n",
            "Epoch 4221/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9689\n",
            "Epoch 4221: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0689 - accuracy: 0.9689 - val_loss: 7.3984 - val_accuracy: 0.5381\n",
            "Epoch 4222/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9679\n",
            "Epoch 4222: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0688 - accuracy: 0.9679 - val_loss: 7.5062 - val_accuracy: 0.5354\n",
            "Epoch 4223/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9708\n",
            "Epoch 4223: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0673 - accuracy: 0.9708 - val_loss: 7.5151 - val_accuracy: 0.5340\n",
            "Epoch 4224/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9695\n",
            "Epoch 4224: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 241ms/step - loss: 0.0676 - accuracy: 0.9695 - val_loss: 7.4659 - val_accuracy: 0.5354\n",
            "Epoch 4225/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9706\n",
            "Epoch 4225: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 0.0671 - accuracy: 0.9706 - val_loss: 7.5154 - val_accuracy: 0.5343\n",
            "Epoch 4226/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9711\n",
            "Epoch 4226: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0672 - accuracy: 0.9711 - val_loss: 7.4362 - val_accuracy: 0.5363\n",
            "Epoch 4227/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9706\n",
            "Epoch 4227: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 257ms/step - loss: 0.0676 - accuracy: 0.9706 - val_loss: 7.4493 - val_accuracy: 0.5351\n",
            "Epoch 4228/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9690\n",
            "Epoch 4228: loss did not improve from 0.06650\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0683 - accuracy: 0.9690 - val_loss: 7.5164 - val_accuracy: 0.5346\n",
            "Epoch 4229/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9719\n",
            "Epoch 4229: loss improved from 0.06650 to 0.06643, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 797ms/step - loss: 0.0664 - accuracy: 0.9719 - val_loss: 7.4966 - val_accuracy: 0.5328\n",
            "Epoch 4230/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9707\n",
            "Epoch 4230: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.0672 - accuracy: 0.9707 - val_loss: 7.5276 - val_accuracy: 0.5334\n",
            "Epoch 4231/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9712\n",
            "Epoch 4231: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0667 - accuracy: 0.9712 - val_loss: 7.5099 - val_accuracy: 0.5319\n",
            "Epoch 4232/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9700\n",
            "Epoch 4232: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0675 - accuracy: 0.9700 - val_loss: 7.4706 - val_accuracy: 0.5351\n",
            "Epoch 4233/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9697\n",
            "Epoch 4233: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 238ms/step - loss: 0.0674 - accuracy: 0.9697 - val_loss: 7.5367 - val_accuracy: 0.5340\n",
            "Epoch 4234/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9714\n",
            "Epoch 4234: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0671 - accuracy: 0.9714 - val_loss: 7.5360 - val_accuracy: 0.5328\n",
            "Epoch 4235/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9697\n",
            "Epoch 4235: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 237ms/step - loss: 0.0677 - accuracy: 0.9697 - val_loss: 7.4627 - val_accuracy: 0.5378\n",
            "Epoch 4236/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9692\n",
            "Epoch 4236: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 0.0677 - accuracy: 0.9692 - val_loss: 7.5541 - val_accuracy: 0.5322\n",
            "Epoch 4237/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9695\n",
            "Epoch 4237: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0681 - accuracy: 0.9695 - val_loss: 7.4856 - val_accuracy: 0.5375\n",
            "Epoch 4238/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9699\n",
            "Epoch 4238: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0688 - accuracy: 0.9699 - val_loss: 7.4302 - val_accuracy: 0.5360\n",
            "Epoch 4239/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9673\n",
            "Epoch 4239: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.0695 - accuracy: 0.9673 - val_loss: 7.5879 - val_accuracy: 0.5305\n",
            "Epoch 4240/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9669\n",
            "Epoch 4240: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0703 - accuracy: 0.9669 - val_loss: 7.4751 - val_accuracy: 0.5372\n",
            "Epoch 4241/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9689\n",
            "Epoch 4241: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0687 - accuracy: 0.9689 - val_loss: 7.5120 - val_accuracy: 0.5331\n",
            "Epoch 4242/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9681\n",
            "Epoch 4242: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0696 - accuracy: 0.9681 - val_loss: 7.6382 - val_accuracy: 0.5275\n",
            "Epoch 4243/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9637\n",
            "Epoch 4243: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0734 - accuracy: 0.9637 - val_loss: 7.3703 - val_accuracy: 0.5395\n",
            "Epoch 4244/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9621\n",
            "Epoch 4244: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0745 - accuracy: 0.9621 - val_loss: 7.5240 - val_accuracy: 0.5354\n",
            "Epoch 4245/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9683\n",
            "Epoch 4245: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0696 - accuracy: 0.9683 - val_loss: 7.5132 - val_accuracy: 0.5366\n",
            "Epoch 4246/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9662\n",
            "Epoch 4246: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0714 - accuracy: 0.9662 - val_loss: 7.4234 - val_accuracy: 0.5387\n",
            "Epoch 4247/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9671\n",
            "Epoch 4247: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0707 - accuracy: 0.9671 - val_loss: 7.5741 - val_accuracy: 0.5267\n",
            "Epoch 4248/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9680\n",
            "Epoch 4248: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0696 - accuracy: 0.9680 - val_loss: 7.4192 - val_accuracy: 0.5340\n",
            "Epoch 4249/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9695\n",
            "Epoch 4249: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0697 - accuracy: 0.9695 - val_loss: 7.5060 - val_accuracy: 0.5346\n",
            "Epoch 4250/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9667\n",
            "Epoch 4250: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0705 - accuracy: 0.9667 - val_loss: 7.5811 - val_accuracy: 0.5337\n",
            "Epoch 4251/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9680\n",
            "Epoch 4251: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0693 - accuracy: 0.9680 - val_loss: 7.4084 - val_accuracy: 0.5369\n",
            "Epoch 4252/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9693\n",
            "Epoch 4252: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0695 - accuracy: 0.9693 - val_loss: 7.5808 - val_accuracy: 0.5325\n",
            "Epoch 4253/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9697\n",
            "Epoch 4253: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0694 - accuracy: 0.9697 - val_loss: 7.4236 - val_accuracy: 0.5363\n",
            "Epoch 4254/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9668\n",
            "Epoch 4254: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0701 - accuracy: 0.9668 - val_loss: 7.4946 - val_accuracy: 0.5384\n",
            "Epoch 4255/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9686\n",
            "Epoch 4255: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0690 - accuracy: 0.9686 - val_loss: 7.5206 - val_accuracy: 0.5331\n",
            "Epoch 4256/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9657\n",
            "Epoch 4256: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0710 - accuracy: 0.9657 - val_loss: 7.4550 - val_accuracy: 0.5369\n",
            "Epoch 4257/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9659\n",
            "Epoch 4257: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0703 - accuracy: 0.9659 - val_loss: 7.5309 - val_accuracy: 0.5351\n",
            "Epoch 4258/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9664\n",
            "Epoch 4258: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0720 - accuracy: 0.9664 - val_loss: 7.5512 - val_accuracy: 0.5325\n",
            "Epoch 4259/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9648\n",
            "Epoch 4259: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0721 - accuracy: 0.9648 - val_loss: 7.3654 - val_accuracy: 0.5372\n",
            "Epoch 4260/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9588\n",
            "Epoch 4260: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0786 - accuracy: 0.9588 - val_loss: 7.4902 - val_accuracy: 0.5331\n",
            "Epoch 4261/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9631\n",
            "Epoch 4261: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.0757 - accuracy: 0.9631 - val_loss: 7.6344 - val_accuracy: 0.5281\n",
            "Epoch 4262/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9648\n",
            "Epoch 4262: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.0741 - accuracy: 0.9648 - val_loss: 7.3571 - val_accuracy: 0.5360\n",
            "Epoch 4263/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9654\n",
            "Epoch 4263: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0720 - accuracy: 0.9654 - val_loss: 7.5543 - val_accuracy: 0.5319\n",
            "Epoch 4264/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9634\n",
            "Epoch 4264: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0723 - accuracy: 0.9634 - val_loss: 7.4512 - val_accuracy: 0.5296\n",
            "Epoch 4265/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9699\n",
            "Epoch 4265: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0700 - accuracy: 0.9699 - val_loss: 7.4917 - val_accuracy: 0.5349\n",
            "Epoch 4266/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9685\n",
            "Epoch 4266: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0696 - accuracy: 0.9685 - val_loss: 7.5065 - val_accuracy: 0.5316\n",
            "Epoch 4267/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9689\n",
            "Epoch 4267: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0704 - accuracy: 0.9689 - val_loss: 7.4267 - val_accuracy: 0.5328\n",
            "Epoch 4268/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9674\n",
            "Epoch 4268: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0699 - accuracy: 0.9674 - val_loss: 7.5287 - val_accuracy: 0.5313\n",
            "Epoch 4269/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9659\n",
            "Epoch 4269: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0713 - accuracy: 0.9659 - val_loss: 7.4979 - val_accuracy: 0.5346\n",
            "Epoch 4270/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9687\n",
            "Epoch 4270: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0689 - accuracy: 0.9687 - val_loss: 7.4747 - val_accuracy: 0.5381\n",
            "Epoch 4271/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9673\n",
            "Epoch 4271: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0699 - accuracy: 0.9673 - val_loss: 7.5194 - val_accuracy: 0.5322\n",
            "Epoch 4272/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9683\n",
            "Epoch 4272: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0694 - accuracy: 0.9683 - val_loss: 7.4897 - val_accuracy: 0.5334\n",
            "Epoch 4273/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9688\n",
            "Epoch 4273: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0695 - accuracy: 0.9688 - val_loss: 7.4647 - val_accuracy: 0.5357\n",
            "Epoch 4274/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9697\n",
            "Epoch 4274: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0687 - accuracy: 0.9697 - val_loss: 7.5174 - val_accuracy: 0.5322\n",
            "Epoch 4275/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9682\n",
            "Epoch 4275: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0690 - accuracy: 0.9682 - val_loss: 7.4894 - val_accuracy: 0.5369\n",
            "Epoch 4276/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9691\n",
            "Epoch 4276: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0697 - accuracy: 0.9691 - val_loss: 7.4491 - val_accuracy: 0.5310\n",
            "Epoch 4277/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9696\n",
            "Epoch 4277: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 244ms/step - loss: 0.0681 - accuracy: 0.9696 - val_loss: 7.5331 - val_accuracy: 0.5334\n",
            "Epoch 4278/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9697\n",
            "Epoch 4278: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0681 - accuracy: 0.9697 - val_loss: 7.4464 - val_accuracy: 0.5316\n",
            "Epoch 4279/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9689\n",
            "Epoch 4279: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.0683 - accuracy: 0.9689 - val_loss: 7.5283 - val_accuracy: 0.5331\n",
            "Epoch 4280/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9669\n",
            "Epoch 4280: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0695 - accuracy: 0.9669 - val_loss: 7.5294 - val_accuracy: 0.5349\n",
            "Epoch 4281/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9706\n",
            "Epoch 4281: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0673 - accuracy: 0.9706 - val_loss: 7.4803 - val_accuracy: 0.5360\n",
            "Epoch 4282/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9683\n",
            "Epoch 4282: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0688 - accuracy: 0.9683 - val_loss: 7.5388 - val_accuracy: 0.5322\n",
            "Epoch 4283/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9695\n",
            "Epoch 4283: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0687 - accuracy: 0.9695 - val_loss: 7.4513 - val_accuracy: 0.5343\n",
            "Epoch 4284/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9700\n",
            "Epoch 4284: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 238ms/step - loss: 0.0673 - accuracy: 0.9700 - val_loss: 7.5337 - val_accuracy: 0.5372\n",
            "Epoch 4285/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9691\n",
            "Epoch 4285: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.0683 - accuracy: 0.9691 - val_loss: 7.4989 - val_accuracy: 0.5343\n",
            "Epoch 4286/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9711\n",
            "Epoch 4286: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 0.0673 - accuracy: 0.9711 - val_loss: 7.4832 - val_accuracy: 0.5360\n",
            "Epoch 4287/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9708\n",
            "Epoch 4287: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0666 - accuracy: 0.9708 - val_loss: 7.5832 - val_accuracy: 0.5299\n",
            "Epoch 4288/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9696\n",
            "Epoch 4288: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0680 - accuracy: 0.9696 - val_loss: 7.4792 - val_accuracy: 0.5363\n",
            "Epoch 4289/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9708\n",
            "Epoch 4289: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 235ms/step - loss: 0.0666 - accuracy: 0.9708 - val_loss: 7.5237 - val_accuracy: 0.5340\n",
            "Epoch 4290/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9712\n",
            "Epoch 4290: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0672 - accuracy: 0.9712 - val_loss: 7.5479 - val_accuracy: 0.5331\n",
            "Epoch 4291/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9710\n",
            "Epoch 4291: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0667 - accuracy: 0.9710 - val_loss: 7.4390 - val_accuracy: 0.5343\n",
            "Epoch 4292/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9689\n",
            "Epoch 4292: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0679 - accuracy: 0.9689 - val_loss: 7.5784 - val_accuracy: 0.5302\n",
            "Epoch 4293/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9673\n",
            "Epoch 4293: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.0688 - accuracy: 0.9673 - val_loss: 7.4798 - val_accuracy: 0.5351\n",
            "Epoch 4294/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9692\n",
            "Epoch 4294: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0687 - accuracy: 0.9692 - val_loss: 7.4912 - val_accuracy: 0.5366\n",
            "Epoch 4295/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9684\n",
            "Epoch 4295: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0696 - accuracy: 0.9684 - val_loss: 7.5733 - val_accuracy: 0.5305\n",
            "Epoch 4296/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9689\n",
            "Epoch 4296: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0681 - accuracy: 0.9689 - val_loss: 7.4191 - val_accuracy: 0.5360\n",
            "Epoch 4297/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9683\n",
            "Epoch 4297: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0694 - accuracy: 0.9683 - val_loss: 7.6061 - val_accuracy: 0.5281\n",
            "Epoch 4298/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9651\n",
            "Epoch 4298: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0725 - accuracy: 0.9651 - val_loss: 7.4426 - val_accuracy: 0.5351\n",
            "Epoch 4299/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9659\n",
            "Epoch 4299: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0706 - accuracy: 0.9659 - val_loss: 7.5194 - val_accuracy: 0.5393\n",
            "Epoch 4300/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9697\n",
            "Epoch 4300: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0684 - accuracy: 0.9697 - val_loss: 7.4688 - val_accuracy: 0.5325\n",
            "Epoch 4301/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9679\n",
            "Epoch 4301: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0686 - accuracy: 0.9679 - val_loss: 7.5489 - val_accuracy: 0.5331\n",
            "Epoch 4302/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9684\n",
            "Epoch 4302: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0683 - accuracy: 0.9684 - val_loss: 7.4222 - val_accuracy: 0.5349\n",
            "Epoch 4303/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9672\n",
            "Epoch 4303: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0697 - accuracy: 0.9672 - val_loss: 7.5733 - val_accuracy: 0.5328\n",
            "Epoch 4304/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9665\n",
            "Epoch 4304: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0704 - accuracy: 0.9665 - val_loss: 7.4971 - val_accuracy: 0.5351\n",
            "Epoch 4305/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9708\n",
            "Epoch 4305: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0673 - accuracy: 0.9708 - val_loss: 7.4762 - val_accuracy: 0.5354\n",
            "Epoch 4306/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9716\n",
            "Epoch 4306: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 0.0667 - accuracy: 0.9716 - val_loss: 7.5798 - val_accuracy: 0.5349\n",
            "Epoch 4307/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9701\n",
            "Epoch 4307: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.0677 - accuracy: 0.9701 - val_loss: 7.4411 - val_accuracy: 0.5351\n",
            "Epoch 4308/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9693\n",
            "Epoch 4308: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0680 - accuracy: 0.9693 - val_loss: 7.5979 - val_accuracy: 0.5299\n",
            "Epoch 4309/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9690\n",
            "Epoch 4309: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0684 - accuracy: 0.9690 - val_loss: 7.4762 - val_accuracy: 0.5322\n",
            "Epoch 4310/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9695\n",
            "Epoch 4310: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0686 - accuracy: 0.9695 - val_loss: 7.4936 - val_accuracy: 0.5369\n",
            "Epoch 4311/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9688\n",
            "Epoch 4311: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0692 - accuracy: 0.9688 - val_loss: 7.5762 - val_accuracy: 0.5343\n",
            "Epoch 4312/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9701\n",
            "Epoch 4312: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0684 - accuracy: 0.9701 - val_loss: 7.4648 - val_accuracy: 0.5363\n",
            "Epoch 4313/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9651\n",
            "Epoch 4313: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0727 - accuracy: 0.9651 - val_loss: 7.4638 - val_accuracy: 0.5384\n",
            "Epoch 4314/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9692\n",
            "Epoch 4314: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0683 - accuracy: 0.9692 - val_loss: 7.5599 - val_accuracy: 0.5331\n",
            "Epoch 4315/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9666\n",
            "Epoch 4315: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0708 - accuracy: 0.9666 - val_loss: 7.5395 - val_accuracy: 0.5372\n",
            "Epoch 4316/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9680\n",
            "Epoch 4316: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0698 - accuracy: 0.9680 - val_loss: 7.4706 - val_accuracy: 0.5346\n",
            "Epoch 4317/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9700\n",
            "Epoch 4317: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0674 - accuracy: 0.9700 - val_loss: 7.5555 - val_accuracy: 0.5351\n",
            "Epoch 4318/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9702\n",
            "Epoch 4318: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0671 - accuracy: 0.9702 - val_loss: 7.4520 - val_accuracy: 0.5337\n",
            "Epoch 4319/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9689\n",
            "Epoch 4319: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0672 - accuracy: 0.9689 - val_loss: 7.5340 - val_accuracy: 0.5328\n",
            "Epoch 4320/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9708\n",
            "Epoch 4320: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0672 - accuracy: 0.9708 - val_loss: 7.5460 - val_accuracy: 0.5331\n",
            "Epoch 4321/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9708\n",
            "Epoch 4321: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0668 - accuracy: 0.9708 - val_loss: 7.5129 - val_accuracy: 0.5343\n",
            "Epoch 4322/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9707\n",
            "Epoch 4322: loss did not improve from 0.06643\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0672 - accuracy: 0.9707 - val_loss: 7.5439 - val_accuracy: 0.5360\n",
            "Epoch 4323/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9709\n",
            "Epoch 4323: loss improved from 0.06643 to 0.06626, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 728ms/step - loss: 0.0663 - accuracy: 0.9709 - val_loss: 7.5029 - val_accuracy: 0.5340\n",
            "Epoch 4324/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9707\n",
            "Epoch 4324: loss did not improve from 0.06626\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0663 - accuracy: 0.9707 - val_loss: 7.5786 - val_accuracy: 0.5334\n",
            "Epoch 4325/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9703\n",
            "Epoch 4325: loss did not improve from 0.06626\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0669 - accuracy: 0.9703 - val_loss: 7.5386 - val_accuracy: 0.5340\n",
            "Epoch 4326/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9713\n",
            "Epoch 4326: loss did not improve from 0.06626\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 0.0669 - accuracy: 0.9713 - val_loss: 7.5005 - val_accuracy: 0.5363\n",
            "Epoch 4327/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9714\n",
            "Epoch 4327: loss did not improve from 0.06626\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0665 - accuracy: 0.9714 - val_loss: 7.5523 - val_accuracy: 0.5351\n",
            "Epoch 4328/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9696\n",
            "Epoch 4328: loss did not improve from 0.06626\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0667 - accuracy: 0.9696 - val_loss: 7.5015 - val_accuracy: 0.5369\n",
            "Epoch 4329/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9710\n",
            "Epoch 4329: loss did not improve from 0.06626\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0670 - accuracy: 0.9710 - val_loss: 7.5471 - val_accuracy: 0.5357\n",
            "Epoch 4330/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9715\n",
            "Epoch 4330: loss improved from 0.06626 to 0.06586, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 297ms/step - loss: 0.0659 - accuracy: 0.9715 - val_loss: 7.5230 - val_accuracy: 0.5322\n",
            "Epoch 4331/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9720\n",
            "Epoch 4331: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0663 - accuracy: 0.9720 - val_loss: 7.5909 - val_accuracy: 0.5360\n",
            "Epoch 4332/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9705\n",
            "Epoch 4332: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0673 - accuracy: 0.9705 - val_loss: 7.4801 - val_accuracy: 0.5325\n",
            "Epoch 4333/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9698\n",
            "Epoch 4333: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0674 - accuracy: 0.9698 - val_loss: 7.5274 - val_accuracy: 0.5337\n",
            "Epoch 4334/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9692\n",
            "Epoch 4334: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0677 - accuracy: 0.9692 - val_loss: 7.5294 - val_accuracy: 0.5378\n",
            "Epoch 4335/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9686\n",
            "Epoch 4335: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0687 - accuracy: 0.9686 - val_loss: 7.5206 - val_accuracy: 0.5381\n",
            "Epoch 4336/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9702\n",
            "Epoch 4336: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0672 - accuracy: 0.9702 - val_loss: 7.5846 - val_accuracy: 0.5351\n",
            "Epoch 4337/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9698\n",
            "Epoch 4337: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0677 - accuracy: 0.9698 - val_loss: 7.5025 - val_accuracy: 0.5343\n",
            "Epoch 4338/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9694\n",
            "Epoch 4338: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0685 - accuracy: 0.9694 - val_loss: 7.5440 - val_accuracy: 0.5357\n",
            "Epoch 4339/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9653\n",
            "Epoch 4339: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0719 - accuracy: 0.9653 - val_loss: 7.5776 - val_accuracy: 0.5299\n",
            "Epoch 4340/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9686\n",
            "Epoch 4340: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0701 - accuracy: 0.9686 - val_loss: 7.5134 - val_accuracy: 0.5384\n",
            "Epoch 4341/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9664\n",
            "Epoch 4341: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0711 - accuracy: 0.9664 - val_loss: 7.4497 - val_accuracy: 0.5349\n",
            "Epoch 4342/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9655\n",
            "Epoch 4342: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0722 - accuracy: 0.9655 - val_loss: 7.6556 - val_accuracy: 0.5299\n",
            "Epoch 4343/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9637\n",
            "Epoch 4343: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0730 - accuracy: 0.9637 - val_loss: 7.4113 - val_accuracy: 0.5346\n",
            "Epoch 4344/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9660\n",
            "Epoch 4344: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0733 - accuracy: 0.9660 - val_loss: 7.5021 - val_accuracy: 0.5337\n",
            "Epoch 4345/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9643\n",
            "Epoch 4345: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 0.0722 - accuracy: 0.9643 - val_loss: 7.6975 - val_accuracy: 0.5299\n",
            "Epoch 4346/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9630\n",
            "Epoch 4346: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0753 - accuracy: 0.9630 - val_loss: 7.3133 - val_accuracy: 0.5340\n",
            "Epoch 4347/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9575\n",
            "Epoch 4347: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0834 - accuracy: 0.9575 - val_loss: 7.5350 - val_accuracy: 0.5354\n",
            "Epoch 4348/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9636\n",
            "Epoch 4348: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 170ms/step - loss: 0.0760 - accuracy: 0.9636 - val_loss: 7.6552 - val_accuracy: 0.5284\n",
            "Epoch 4349/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9529\n",
            "Epoch 4349: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 236ms/step - loss: 0.0898 - accuracy: 0.9529 - val_loss: 7.4117 - val_accuracy: 0.5390\n",
            "Epoch 4350/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9577\n",
            "Epoch 4350: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 172ms/step - loss: 0.0848 - accuracy: 0.9577 - val_loss: 7.3580 - val_accuracy: 0.5346\n",
            "Epoch 4351/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9629\n",
            "Epoch 4351: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0759 - accuracy: 0.9629 - val_loss: 7.6684 - val_accuracy: 0.5237\n",
            "Epoch 4352/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9592\n",
            "Epoch 4352: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0819 - accuracy: 0.9592 - val_loss: 7.3201 - val_accuracy: 0.5366\n",
            "Epoch 4353/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9576\n",
            "Epoch 4353: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0847 - accuracy: 0.9576 - val_loss: 7.4249 - val_accuracy: 0.5343\n",
            "Epoch 4354/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9646\n",
            "Epoch 4354: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0738 - accuracy: 0.9646 - val_loss: 7.4837 - val_accuracy: 0.5331\n",
            "Epoch 4355/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9645\n",
            "Epoch 4355: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0730 - accuracy: 0.9645 - val_loss: 7.2928 - val_accuracy: 0.5369\n",
            "Epoch 4356/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9628\n",
            "Epoch 4356: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0749 - accuracy: 0.9628 - val_loss: 7.6784 - val_accuracy: 0.5269\n",
            "Epoch 4357/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9552\n",
            "Epoch 4357: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0865 - accuracy: 0.9552 - val_loss: 7.2753 - val_accuracy: 0.5354\n",
            "Epoch 4358/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9563\n",
            "Epoch 4358: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0862 - accuracy: 0.9563 - val_loss: 7.4275 - val_accuracy: 0.5404\n",
            "Epoch 4359/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9623\n",
            "Epoch 4359: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0759 - accuracy: 0.9623 - val_loss: 7.5647 - val_accuracy: 0.5296\n",
            "Epoch 4360/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9626\n",
            "Epoch 4360: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0770 - accuracy: 0.9626 - val_loss: 7.2905 - val_accuracy: 0.5384\n",
            "Epoch 4361/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9582\n",
            "Epoch 4361: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0838 - accuracy: 0.9582 - val_loss: 7.5939 - val_accuracy: 0.5290\n",
            "Epoch 4362/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9596\n",
            "Epoch 4362: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0802 - accuracy: 0.9596 - val_loss: 7.4098 - val_accuracy: 0.5343\n",
            "Epoch 4363/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9625\n",
            "Epoch 4363: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0763 - accuracy: 0.9625 - val_loss: 7.3887 - val_accuracy: 0.5378\n",
            "Epoch 4364/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9603\n",
            "Epoch 4364: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0773 - accuracy: 0.9603 - val_loss: 7.5318 - val_accuracy: 0.5284\n",
            "Epoch 4365/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9586\n",
            "Epoch 4365: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0818 - accuracy: 0.9586 - val_loss: 7.4140 - val_accuracy: 0.5337\n",
            "Epoch 4366/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9662\n",
            "Epoch 4366: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0743 - accuracy: 0.9662 - val_loss: 7.3720 - val_accuracy: 0.5357\n",
            "Epoch 4367/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9653\n",
            "Epoch 4367: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0732 - accuracy: 0.9653 - val_loss: 7.4554 - val_accuracy: 0.5328\n",
            "Epoch 4368/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9670\n",
            "Epoch 4368: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0709 - accuracy: 0.9670 - val_loss: 7.4279 - val_accuracy: 0.5366\n",
            "Epoch 4369/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9651\n",
            "Epoch 4369: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0727 - accuracy: 0.9651 - val_loss: 7.5215 - val_accuracy: 0.5281\n",
            "Epoch 4370/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9648\n",
            "Epoch 4370: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0753 - accuracy: 0.9648 - val_loss: 7.4403 - val_accuracy: 0.5346\n",
            "Epoch 4371/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9670\n",
            "Epoch 4371: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0704 - accuracy: 0.9670 - val_loss: 7.3627 - val_accuracy: 0.5340\n",
            "Epoch 4372/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9681\n",
            "Epoch 4372: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0702 - accuracy: 0.9681 - val_loss: 7.5403 - val_accuracy: 0.5296\n",
            "Epoch 4373/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9650\n",
            "Epoch 4373: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0723 - accuracy: 0.9650 - val_loss: 7.4571 - val_accuracy: 0.5351\n",
            "Epoch 4374/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9692\n",
            "Epoch 4374: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0684 - accuracy: 0.9692 - val_loss: 7.4609 - val_accuracy: 0.5310\n",
            "Epoch 4375/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9700\n",
            "Epoch 4375: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0680 - accuracy: 0.9700 - val_loss: 7.5764 - val_accuracy: 0.5334\n",
            "Epoch 4376/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9679\n",
            "Epoch 4376: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0696 - accuracy: 0.9679 - val_loss: 7.4006 - val_accuracy: 0.5363\n",
            "Epoch 4377/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9703\n",
            "Epoch 4377: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0682 - accuracy: 0.9703 - val_loss: 7.5468 - val_accuracy: 0.5334\n",
            "Epoch 4378/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9675\n",
            "Epoch 4378: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.0700 - accuracy: 0.9675 - val_loss: 7.5116 - val_accuracy: 0.5316\n",
            "Epoch 4379/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9684\n",
            "Epoch 4379: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0685 - accuracy: 0.9684 - val_loss: 7.4421 - val_accuracy: 0.5375\n",
            "Epoch 4380/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9662\n",
            "Epoch 4380: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0703 - accuracy: 0.9662 - val_loss: 7.5789 - val_accuracy: 0.5308\n",
            "Epoch 4381/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9679\n",
            "Epoch 4381: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0699 - accuracy: 0.9679 - val_loss: 7.3653 - val_accuracy: 0.5343\n",
            "Epoch 4382/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9650\n",
            "Epoch 4382: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0715 - accuracy: 0.9650 - val_loss: 7.5021 - val_accuracy: 0.5366\n",
            "Epoch 4383/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9662\n",
            "Epoch 4383: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0705 - accuracy: 0.9662 - val_loss: 7.5262 - val_accuracy: 0.5290\n",
            "Epoch 4384/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9676\n",
            "Epoch 4384: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0702 - accuracy: 0.9676 - val_loss: 7.4714 - val_accuracy: 0.5378\n",
            "Epoch 4385/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9672\n",
            "Epoch 4385: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0701 - accuracy: 0.9672 - val_loss: 7.5450 - val_accuracy: 0.5302\n",
            "Epoch 4386/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9683\n",
            "Epoch 4386: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0696 - accuracy: 0.9683 - val_loss: 7.4431 - val_accuracy: 0.5369\n",
            "Epoch 4387/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9688\n",
            "Epoch 4387: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0696 - accuracy: 0.9688 - val_loss: 7.4562 - val_accuracy: 0.5378\n",
            "Epoch 4388/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9682\n",
            "Epoch 4388: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0687 - accuracy: 0.9682 - val_loss: 7.5635 - val_accuracy: 0.5290\n",
            "Epoch 4389/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9675\n",
            "Epoch 4389: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0704 - accuracy: 0.9675 - val_loss: 7.4729 - val_accuracy: 0.5393\n",
            "Epoch 4390/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9676\n",
            "Epoch 4390: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0711 - accuracy: 0.9676 - val_loss: 7.4783 - val_accuracy: 0.5305\n",
            "Epoch 4391/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9680\n",
            "Epoch 4391: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.0694 - accuracy: 0.9680 - val_loss: 7.5028 - val_accuracy: 0.5334\n",
            "Epoch 4392/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9689\n",
            "Epoch 4392: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0679 - accuracy: 0.9689 - val_loss: 7.4658 - val_accuracy: 0.5360\n",
            "Epoch 4393/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9699\n",
            "Epoch 4393: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0672 - accuracy: 0.9699 - val_loss: 7.5152 - val_accuracy: 0.5305\n",
            "Epoch 4394/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9699\n",
            "Epoch 4394: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0675 - accuracy: 0.9699 - val_loss: 7.4685 - val_accuracy: 0.5372\n",
            "Epoch 4395/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9699\n",
            "Epoch 4395: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0681 - accuracy: 0.9699 - val_loss: 7.4873 - val_accuracy: 0.5331\n",
            "Epoch 4396/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9714\n",
            "Epoch 4396: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0668 - accuracy: 0.9714 - val_loss: 7.5171 - val_accuracy: 0.5354\n",
            "Epoch 4397/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9723\n",
            "Epoch 4397: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0662 - accuracy: 0.9723 - val_loss: 7.4761 - val_accuracy: 0.5319\n",
            "Epoch 4398/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9712\n",
            "Epoch 4398: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0662 - accuracy: 0.9712 - val_loss: 7.4871 - val_accuracy: 0.5316\n",
            "Epoch 4399/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9709\n",
            "Epoch 4399: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0664 - accuracy: 0.9709 - val_loss: 7.5232 - val_accuracy: 0.5340\n",
            "Epoch 4400/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9700\n",
            "Epoch 4400: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0667 - accuracy: 0.9700 - val_loss: 7.5015 - val_accuracy: 0.5337\n",
            "Epoch 4401/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9708\n",
            "Epoch 4401: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 0.0667 - accuracy: 0.9708 - val_loss: 7.5823 - val_accuracy: 0.5340\n",
            "Epoch 4402/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9714\n",
            "Epoch 4402: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0663 - accuracy: 0.9714 - val_loss: 7.5114 - val_accuracy: 0.5375\n",
            "Epoch 4403/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9720\n",
            "Epoch 4403: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0667 - accuracy: 0.9720 - val_loss: 7.4450 - val_accuracy: 0.5354\n",
            "Epoch 4404/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9706\n",
            "Epoch 4404: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 0.0668 - accuracy: 0.9706 - val_loss: 7.6279 - val_accuracy: 0.5331\n",
            "Epoch 4405/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9703\n",
            "Epoch 4405: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0683 - accuracy: 0.9703 - val_loss: 7.4315 - val_accuracy: 0.5351\n",
            "Epoch 4406/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9673\n",
            "Epoch 4406: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0686 - accuracy: 0.9673 - val_loss: 7.5675 - val_accuracy: 0.5337\n",
            "Epoch 4407/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9699\n",
            "Epoch 4407: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0670 - accuracy: 0.9699 - val_loss: 7.5268 - val_accuracy: 0.5331\n",
            "Epoch 4408/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9700\n",
            "Epoch 4408: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0686 - accuracy: 0.9700 - val_loss: 7.4266 - val_accuracy: 0.5375\n",
            "Epoch 4409/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9653\n",
            "Epoch 4409: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0706 - accuracy: 0.9653 - val_loss: 7.6996 - val_accuracy: 0.5275\n",
            "Epoch 4410/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9547\n",
            "Epoch 4410: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0857 - accuracy: 0.9547 - val_loss: 7.5022 - val_accuracy: 0.5322\n",
            "Epoch 4411/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9671\n",
            "Epoch 4411: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0707 - accuracy: 0.9671 - val_loss: 7.4681 - val_accuracy: 0.5343\n",
            "Epoch 4412/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9647\n",
            "Epoch 4412: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 0.0722 - accuracy: 0.9647 - val_loss: 7.6651 - val_accuracy: 0.5275\n",
            "Epoch 4413/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9587\n",
            "Epoch 4413: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 0.0816 - accuracy: 0.9587 - val_loss: 7.3961 - val_accuracy: 0.5354\n",
            "Epoch 4414/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9629\n",
            "Epoch 4414: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0771 - accuracy: 0.9629 - val_loss: 7.4633 - val_accuracy: 0.5378\n",
            "Epoch 4415/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9654\n",
            "Epoch 4415: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0721 - accuracy: 0.9654 - val_loss: 7.6138 - val_accuracy: 0.5296\n",
            "Epoch 4416/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9645\n",
            "Epoch 4416: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0731 - accuracy: 0.9645 - val_loss: 7.3357 - val_accuracy: 0.5340\n",
            "Epoch 4417/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9594\n",
            "Epoch 4417: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0790 - accuracy: 0.9594 - val_loss: 7.5318 - val_accuracy: 0.5302\n",
            "Epoch 4418/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9636\n",
            "Epoch 4418: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0735 - accuracy: 0.9636 - val_loss: 7.4694 - val_accuracy: 0.5299\n",
            "Epoch 4419/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9682\n",
            "Epoch 4419: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0714 - accuracy: 0.9682 - val_loss: 7.4374 - val_accuracy: 0.5384\n",
            "Epoch 4420/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9646\n",
            "Epoch 4420: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0726 - accuracy: 0.9646 - val_loss: 7.6067 - val_accuracy: 0.5255\n",
            "Epoch 4421/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9604\n",
            "Epoch 4421: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0780 - accuracy: 0.9604 - val_loss: 7.4086 - val_accuracy: 0.5363\n",
            "Epoch 4422/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9644\n",
            "Epoch 4422: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0742 - accuracy: 0.9644 - val_loss: 7.4583 - val_accuracy: 0.5340\n",
            "Epoch 4423/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9678\n",
            "Epoch 4423: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0699 - accuracy: 0.9678 - val_loss: 7.5298 - val_accuracy: 0.5316\n",
            "Epoch 4424/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9670\n",
            "Epoch 4424: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0697 - accuracy: 0.9670 - val_loss: 7.4410 - val_accuracy: 0.5387\n",
            "Epoch 4425/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9667\n",
            "Epoch 4425: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0704 - accuracy: 0.9667 - val_loss: 7.5109 - val_accuracy: 0.5281\n",
            "Epoch 4426/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9644\n",
            "Epoch 4426: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0731 - accuracy: 0.9644 - val_loss: 7.4821 - val_accuracy: 0.5366\n",
            "Epoch 4427/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9695\n",
            "Epoch 4427: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0682 - accuracy: 0.9695 - val_loss: 7.4941 - val_accuracy: 0.5351\n",
            "Epoch 4428/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9689\n",
            "Epoch 4428: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0688 - accuracy: 0.9689 - val_loss: 7.5529 - val_accuracy: 0.5325\n",
            "Epoch 4429/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9692\n",
            "Epoch 4429: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0682 - accuracy: 0.9692 - val_loss: 7.4127 - val_accuracy: 0.5360\n",
            "Epoch 4430/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9675\n",
            "Epoch 4430: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0693 - accuracy: 0.9675 - val_loss: 7.5160 - val_accuracy: 0.5331\n",
            "Epoch 4431/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9704\n",
            "Epoch 4431: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0676 - accuracy: 0.9704 - val_loss: 7.4831 - val_accuracy: 0.5349\n",
            "Epoch 4432/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9699\n",
            "Epoch 4432: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0681 - accuracy: 0.9699 - val_loss: 7.4525 - val_accuracy: 0.5384\n",
            "Epoch 4433/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9710\n",
            "Epoch 4433: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0671 - accuracy: 0.9710 - val_loss: 7.5692 - val_accuracy: 0.5343\n",
            "Epoch 4434/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9689\n",
            "Epoch 4434: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0681 - accuracy: 0.9689 - val_loss: 7.4583 - val_accuracy: 0.5346\n",
            "Epoch 4435/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9688\n",
            "Epoch 4435: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0677 - accuracy: 0.9688 - val_loss: 7.5396 - val_accuracy: 0.5305\n",
            "Epoch 4436/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9684\n",
            "Epoch 4436: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0681 - accuracy: 0.9684 - val_loss: 7.4978 - val_accuracy: 0.5354\n",
            "Epoch 4437/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9695\n",
            "Epoch 4437: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0676 - accuracy: 0.9695 - val_loss: 7.4776 - val_accuracy: 0.5343\n",
            "Epoch 4438/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9695\n",
            "Epoch 4438: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0685 - accuracy: 0.9695 - val_loss: 7.6326 - val_accuracy: 0.5290\n",
            "Epoch 4439/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9679\n",
            "Epoch 4439: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0691 - accuracy: 0.9679 - val_loss: 7.4056 - val_accuracy: 0.5357\n",
            "Epoch 4440/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9607\n",
            "Epoch 4440: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0754 - accuracy: 0.9607 - val_loss: 7.5248 - val_accuracy: 0.5340\n",
            "Epoch 4441/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9671\n",
            "Epoch 4441: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0697 - accuracy: 0.9671 - val_loss: 7.5976 - val_accuracy: 0.5340\n",
            "Epoch 4442/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9665\n",
            "Epoch 4442: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0708 - accuracy: 0.9665 - val_loss: 7.3582 - val_accuracy: 0.5360\n",
            "Epoch 4443/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9628\n",
            "Epoch 4443: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0756 - accuracy: 0.9628 - val_loss: 7.6878 - val_accuracy: 0.5261\n",
            "Epoch 4444/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9563\n",
            "Epoch 4444: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0826 - accuracy: 0.9563 - val_loss: 7.4594 - val_accuracy: 0.5360\n",
            "Epoch 4445/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9648\n",
            "Epoch 4445: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0729 - accuracy: 0.9648 - val_loss: 7.4328 - val_accuracy: 0.5366\n",
            "Epoch 4446/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9645\n",
            "Epoch 4446: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 0.0713 - accuracy: 0.9645 - val_loss: 7.5575 - val_accuracy: 0.5299\n",
            "Epoch 4447/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9645\n",
            "Epoch 4447: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0724 - accuracy: 0.9645 - val_loss: 7.3494 - val_accuracy: 0.5378\n",
            "Epoch 4448/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9634\n",
            "Epoch 4448: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 244ms/step - loss: 0.0752 - accuracy: 0.9634 - val_loss: 7.5301 - val_accuracy: 0.5281\n",
            "Epoch 4449/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9637\n",
            "Epoch 4449: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0721 - accuracy: 0.9637 - val_loss: 7.4817 - val_accuracy: 0.5334\n",
            "Epoch 4450/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9665\n",
            "Epoch 4450: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 239ms/step - loss: 0.0729 - accuracy: 0.9665 - val_loss: 7.4012 - val_accuracy: 0.5334\n",
            "Epoch 4451/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9639\n",
            "Epoch 4451: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0729 - accuracy: 0.9639 - val_loss: 7.5789 - val_accuracy: 0.5293\n",
            "Epoch 4452/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9589\n",
            "Epoch 4452: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0771 - accuracy: 0.9589 - val_loss: 7.4294 - val_accuracy: 0.5360\n",
            "Epoch 4453/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9667\n",
            "Epoch 4453: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0717 - accuracy: 0.9667 - val_loss: 7.4616 - val_accuracy: 0.5322\n",
            "Epoch 4454/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9665\n",
            "Epoch 4454: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0712 - accuracy: 0.9665 - val_loss: 7.5681 - val_accuracy: 0.5284\n",
            "Epoch 4455/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9659\n",
            "Epoch 4455: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0706 - accuracy: 0.9659 - val_loss: 7.3462 - val_accuracy: 0.5346\n",
            "Epoch 4456/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9645\n",
            "Epoch 4456: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0726 - accuracy: 0.9645 - val_loss: 7.5385 - val_accuracy: 0.5267\n",
            "Epoch 4457/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9645\n",
            "Epoch 4457: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0717 - accuracy: 0.9645 - val_loss: 7.5119 - val_accuracy: 0.5331\n",
            "Epoch 4458/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9684\n",
            "Epoch 4458: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0693 - accuracy: 0.9684 - val_loss: 7.4448 - val_accuracy: 0.5331\n",
            "Epoch 4459/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9690\n",
            "Epoch 4459: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0688 - accuracy: 0.9690 - val_loss: 7.5111 - val_accuracy: 0.5325\n",
            "Epoch 4460/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9700\n",
            "Epoch 4460: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0684 - accuracy: 0.9700 - val_loss: 7.4663 - val_accuracy: 0.5351\n",
            "Epoch 4461/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9693\n",
            "Epoch 4461: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 0.0677 - accuracy: 0.9693 - val_loss: 7.4678 - val_accuracy: 0.5387\n",
            "Epoch 4462/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9710\n",
            "Epoch 4462: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0665 - accuracy: 0.9710 - val_loss: 7.5340 - val_accuracy: 0.5354\n",
            "Epoch 4463/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9717\n",
            "Epoch 4463: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0670 - accuracy: 0.9717 - val_loss: 7.4994 - val_accuracy: 0.5366\n",
            "Epoch 4464/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9725\n",
            "Epoch 4464: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0661 - accuracy: 0.9725 - val_loss: 7.4910 - val_accuracy: 0.5369\n",
            "Epoch 4465/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9714\n",
            "Epoch 4465: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0662 - accuracy: 0.9714 - val_loss: 7.4716 - val_accuracy: 0.5372\n",
            "Epoch 4466/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9716\n",
            "Epoch 4466: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0664 - accuracy: 0.9716 - val_loss: 7.4748 - val_accuracy: 0.5351\n",
            "Epoch 4467/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9708\n",
            "Epoch 4467: loss did not improve from 0.06586\n",
            "2/2 [==============================] - 0s 168ms/step - loss: 0.0662 - accuracy: 0.9708 - val_loss: 7.5308 - val_accuracy: 0.5337\n",
            "Epoch 4468/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9722\n",
            "Epoch 4468: loss improved from 0.06586 to 0.06581, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 710ms/step - loss: 0.0658 - accuracy: 0.9722 - val_loss: 7.5052 - val_accuracy: 0.5357\n",
            "Epoch 4469/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9725\n",
            "Epoch 4469: loss improved from 0.06581 to 0.06539, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0654 - accuracy: 0.9725 - val_loss: 7.5252 - val_accuracy: 0.5354\n",
            "Epoch 4470/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9725\n",
            "Epoch 4470: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0658 - accuracy: 0.9725 - val_loss: 7.5204 - val_accuracy: 0.5334\n",
            "Epoch 4471/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9718\n",
            "Epoch 4471: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0655 - accuracy: 0.9718 - val_loss: 7.5726 - val_accuracy: 0.5346\n",
            "Epoch 4472/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9696\n",
            "Epoch 4472: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0664 - accuracy: 0.9696 - val_loss: 7.5062 - val_accuracy: 0.5357\n",
            "Epoch 4473/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9741\n",
            "Epoch 4473: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0654 - accuracy: 0.9741 - val_loss: 7.5249 - val_accuracy: 0.5357\n",
            "Epoch 4474/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9722\n",
            "Epoch 4474: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0660 - accuracy: 0.9722 - val_loss: 7.5442 - val_accuracy: 0.5375\n",
            "Epoch 4475/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9714\n",
            "Epoch 4475: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0660 - accuracy: 0.9714 - val_loss: 7.5547 - val_accuracy: 0.5328\n",
            "Epoch 4476/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9690\n",
            "Epoch 4476: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0672 - accuracy: 0.9690 - val_loss: 7.5156 - val_accuracy: 0.5378\n",
            "Epoch 4477/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9698\n",
            "Epoch 4477: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0671 - accuracy: 0.9698 - val_loss: 7.5114 - val_accuracy: 0.5369\n",
            "Epoch 4478/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9709\n",
            "Epoch 4478: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0667 - accuracy: 0.9709 - val_loss: 7.6013 - val_accuracy: 0.5310\n",
            "Epoch 4479/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9675\n",
            "Epoch 4479: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0695 - accuracy: 0.9675 - val_loss: 7.5233 - val_accuracy: 0.5360\n",
            "Epoch 4480/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9712\n",
            "Epoch 4480: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0668 - accuracy: 0.9712 - val_loss: 7.5226 - val_accuracy: 0.5360\n",
            "Epoch 4481/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9703\n",
            "Epoch 4481: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0664 - accuracy: 0.9703 - val_loss: 7.5709 - val_accuracy: 0.5331\n",
            "Epoch 4482/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9711\n",
            "Epoch 4482: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0664 - accuracy: 0.9711 - val_loss: 7.5069 - val_accuracy: 0.5360\n",
            "Epoch 4483/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9722\n",
            "Epoch 4483: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 176ms/step - loss: 0.0661 - accuracy: 0.9722 - val_loss: 7.5834 - val_accuracy: 0.5363\n",
            "Epoch 4484/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9720\n",
            "Epoch 4484: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0662 - accuracy: 0.9720 - val_loss: 7.5505 - val_accuracy: 0.5337\n",
            "Epoch 4485/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9700\n",
            "Epoch 4485: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0669 - accuracy: 0.9700 - val_loss: 7.4959 - val_accuracy: 0.5375\n",
            "Epoch 4486/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9675\n",
            "Epoch 4486: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0685 - accuracy: 0.9675 - val_loss: 7.5541 - val_accuracy: 0.5369\n",
            "Epoch 4487/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9700\n",
            "Epoch 4487: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0668 - accuracy: 0.9700 - val_loss: 7.5287 - val_accuracy: 0.5346\n",
            "Epoch 4488/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9720\n",
            "Epoch 4488: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0654 - accuracy: 0.9720 - val_loss: 7.5308 - val_accuracy: 0.5331\n",
            "Epoch 4489/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9711\n",
            "Epoch 4489: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0661 - accuracy: 0.9711 - val_loss: 7.5459 - val_accuracy: 0.5360\n",
            "Epoch 4490/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9695\n",
            "Epoch 4490: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0674 - accuracy: 0.9695 - val_loss: 7.5076 - val_accuracy: 0.5390\n",
            "Epoch 4491/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9711\n",
            "Epoch 4491: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0667 - accuracy: 0.9711 - val_loss: 7.5810 - val_accuracy: 0.5340\n",
            "Epoch 4492/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9703\n",
            "Epoch 4492: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0671 - accuracy: 0.9703 - val_loss: 7.4654 - val_accuracy: 0.5354\n",
            "Epoch 4493/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9667\n",
            "Epoch 4493: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0698 - accuracy: 0.9667 - val_loss: 7.5664 - val_accuracy: 0.5354\n",
            "Epoch 4494/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9682\n",
            "Epoch 4494: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0685 - accuracy: 0.9682 - val_loss: 7.6192 - val_accuracy: 0.5296\n",
            "Epoch 4495/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9661\n",
            "Epoch 4495: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0710 - accuracy: 0.9661 - val_loss: 7.5499 - val_accuracy: 0.5349\n",
            "Epoch 4496/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9701\n",
            "Epoch 4496: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0673 - accuracy: 0.9701 - val_loss: 7.4804 - val_accuracy: 0.5351\n",
            "Epoch 4497/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9706\n",
            "Epoch 4497: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0669 - accuracy: 0.9706 - val_loss: 7.5624 - val_accuracy: 0.5305\n",
            "Epoch 4498/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9697\n",
            "Epoch 4498: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0669 - accuracy: 0.9697 - val_loss: 7.5092 - val_accuracy: 0.5369\n",
            "Epoch 4499/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9662\n",
            "Epoch 4499: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0691 - accuracy: 0.9662 - val_loss: 7.5310 - val_accuracy: 0.5360\n",
            "Epoch 4500/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9703\n",
            "Epoch 4500: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0674 - accuracy: 0.9703 - val_loss: 7.6012 - val_accuracy: 0.5322\n",
            "Epoch 4501/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9687\n",
            "Epoch 4501: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 0.0678 - accuracy: 0.9687 - val_loss: 7.5026 - val_accuracy: 0.5343\n",
            "Epoch 4502/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9689\n",
            "Epoch 4502: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0681 - accuracy: 0.9689 - val_loss: 7.5764 - val_accuracy: 0.5325\n",
            "Epoch 4503/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9690\n",
            "Epoch 4503: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.0676 - accuracy: 0.9690 - val_loss: 7.5361 - val_accuracy: 0.5343\n",
            "Epoch 4504/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9703\n",
            "Epoch 4504: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0668 - accuracy: 0.9703 - val_loss: 7.5054 - val_accuracy: 0.5349\n",
            "Epoch 4505/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9707\n",
            "Epoch 4505: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0661 - accuracy: 0.9707 - val_loss: 7.5902 - val_accuracy: 0.5331\n",
            "Epoch 4506/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9702\n",
            "Epoch 4506: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 200ms/step - loss: 0.0665 - accuracy: 0.9702 - val_loss: 7.5387 - val_accuracy: 0.5328\n",
            "Epoch 4507/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9714\n",
            "Epoch 4507: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0661 - accuracy: 0.9714 - val_loss: 7.4945 - val_accuracy: 0.5369\n",
            "Epoch 4508/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9686\n",
            "Epoch 4508: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0676 - accuracy: 0.9686 - val_loss: 7.5600 - val_accuracy: 0.5360\n",
            "Epoch 4509/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9712\n",
            "Epoch 4509: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0668 - accuracy: 0.9712 - val_loss: 7.5663 - val_accuracy: 0.5340\n",
            "Epoch 4510/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9709\n",
            "Epoch 4510: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 272ms/step - loss: 0.0662 - accuracy: 0.9709 - val_loss: 7.5545 - val_accuracy: 0.5366\n",
            "Epoch 4511/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9706\n",
            "Epoch 4511: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0666 - accuracy: 0.9706 - val_loss: 7.5374 - val_accuracy: 0.5357\n",
            "Epoch 4512/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9709\n",
            "Epoch 4512: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0660 - accuracy: 0.9709 - val_loss: 7.5762 - val_accuracy: 0.5366\n",
            "Epoch 4513/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9697\n",
            "Epoch 4513: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0676 - accuracy: 0.9697 - val_loss: 7.5841 - val_accuracy: 0.5334\n",
            "Epoch 4514/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9697\n",
            "Epoch 4514: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.0668 - accuracy: 0.9697 - val_loss: 7.5319 - val_accuracy: 0.5357\n",
            "Epoch 4515/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9708\n",
            "Epoch 4515: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0662 - accuracy: 0.9708 - val_loss: 7.5390 - val_accuracy: 0.5351\n",
            "Epoch 4516/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9720\n",
            "Epoch 4516: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0659 - accuracy: 0.9720 - val_loss: 7.5829 - val_accuracy: 0.5334\n",
            "Epoch 4517/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9717\n",
            "Epoch 4517: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0660 - accuracy: 0.9717 - val_loss: 7.5849 - val_accuracy: 0.5351\n",
            "Epoch 4518/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9726\n",
            "Epoch 4518: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.0662 - accuracy: 0.9726 - val_loss: 7.5745 - val_accuracy: 0.5354\n",
            "Epoch 4519/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9719\n",
            "Epoch 4519: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 242ms/step - loss: 0.0655 - accuracy: 0.9719 - val_loss: 7.5443 - val_accuracy: 0.5354\n",
            "Epoch 4520/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9717\n",
            "Epoch 4520: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0656 - accuracy: 0.9717 - val_loss: 7.5654 - val_accuracy: 0.5363\n",
            "Epoch 4521/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9725\n",
            "Epoch 4521: loss did not improve from 0.06539\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0654 - accuracy: 0.9725 - val_loss: 7.5549 - val_accuracy: 0.5369\n",
            "Epoch 4522/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9727\n",
            "Epoch 4522: loss improved from 0.06539 to 0.06534, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 707ms/step - loss: 0.0653 - accuracy: 0.9727 - val_loss: 7.5262 - val_accuracy: 0.5366\n",
            "Epoch 4523/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9724\n",
            "Epoch 4523: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0663 - accuracy: 0.9724 - val_loss: 7.5391 - val_accuracy: 0.5381\n",
            "Epoch 4524/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9703\n",
            "Epoch 4524: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0668 - accuracy: 0.9703 - val_loss: 7.5906 - val_accuracy: 0.5346\n",
            "Epoch 4525/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9710\n",
            "Epoch 4525: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0673 - accuracy: 0.9710 - val_loss: 7.6157 - val_accuracy: 0.5328\n",
            "Epoch 4526/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9702\n",
            "Epoch 4526: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0662 - accuracy: 0.9702 - val_loss: 7.5326 - val_accuracy: 0.5366\n",
            "Epoch 4527/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9691\n",
            "Epoch 4527: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0673 - accuracy: 0.9691 - val_loss: 7.6650 - val_accuracy: 0.5313\n",
            "Epoch 4528/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9661\n",
            "Epoch 4528: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0713 - accuracy: 0.9661 - val_loss: 7.6298 - val_accuracy: 0.5328\n",
            "Epoch 4529/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9695\n",
            "Epoch 4529: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0681 - accuracy: 0.9695 - val_loss: 7.4237 - val_accuracy: 0.5354\n",
            "Epoch 4530/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9624\n",
            "Epoch 4530: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0739 - accuracy: 0.9624 - val_loss: 7.6361 - val_accuracy: 0.5310\n",
            "Epoch 4531/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9653\n",
            "Epoch 4531: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0727 - accuracy: 0.9653 - val_loss: 7.6124 - val_accuracy: 0.5313\n",
            "Epoch 4532/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9686\n",
            "Epoch 4532: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0688 - accuracy: 0.9686 - val_loss: 7.4924 - val_accuracy: 0.5351\n",
            "Epoch 4533/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9673\n",
            "Epoch 4533: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0702 - accuracy: 0.9673 - val_loss: 7.5323 - val_accuracy: 0.5351\n",
            "Epoch 4534/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9708\n",
            "Epoch 4534: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0677 - accuracy: 0.9708 - val_loss: 7.5570 - val_accuracy: 0.5349\n",
            "Epoch 4535/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9686\n",
            "Epoch 4535: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0679 - accuracy: 0.9686 - val_loss: 7.4965 - val_accuracy: 0.5378\n",
            "Epoch 4536/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9677\n",
            "Epoch 4536: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0686 - accuracy: 0.9677 - val_loss: 7.5225 - val_accuracy: 0.5360\n",
            "Epoch 4537/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9712\n",
            "Epoch 4537: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0669 - accuracy: 0.9712 - val_loss: 7.5762 - val_accuracy: 0.5346\n",
            "Epoch 4538/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9689\n",
            "Epoch 4538: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0671 - accuracy: 0.9689 - val_loss: 7.4452 - val_accuracy: 0.5343\n",
            "Epoch 4539/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9679\n",
            "Epoch 4539: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0687 - accuracy: 0.9679 - val_loss: 7.6563 - val_accuracy: 0.5322\n",
            "Epoch 4540/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9650\n",
            "Epoch 4540: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0726 - accuracy: 0.9650 - val_loss: 7.5624 - val_accuracy: 0.5308\n",
            "Epoch 4541/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9673\n",
            "Epoch 4541: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0694 - accuracy: 0.9673 - val_loss: 7.4171 - val_accuracy: 0.5360\n",
            "Epoch 4542/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9613\n",
            "Epoch 4542: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0763 - accuracy: 0.9613 - val_loss: 7.5928 - val_accuracy: 0.5337\n",
            "Epoch 4543/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9663\n",
            "Epoch 4543: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0710 - accuracy: 0.9663 - val_loss: 7.6178 - val_accuracy: 0.5349\n",
            "Epoch 4544/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9695\n",
            "Epoch 4544: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0690 - accuracy: 0.9695 - val_loss: 7.4464 - val_accuracy: 0.5340\n",
            "Epoch 4545/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9612\n",
            "Epoch 4545: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0756 - accuracy: 0.9612 - val_loss: 7.5225 - val_accuracy: 0.5319\n",
            "Epoch 4546/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9647\n",
            "Epoch 4546: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0734 - accuracy: 0.9647 - val_loss: 7.7028 - val_accuracy: 0.5261\n",
            "Epoch 4547/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9649\n",
            "Epoch 4547: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0741 - accuracy: 0.9649 - val_loss: 7.4736 - val_accuracy: 0.5378\n",
            "Epoch 4548/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9676\n",
            "Epoch 4548: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0700 - accuracy: 0.9676 - val_loss: 7.5442 - val_accuracy: 0.5316\n",
            "Epoch 4549/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9689\n",
            "Epoch 4549: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0677 - accuracy: 0.9689 - val_loss: 7.5337 - val_accuracy: 0.5319\n",
            "Epoch 4550/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9690\n",
            "Epoch 4550: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0673 - accuracy: 0.9690 - val_loss: 7.5223 - val_accuracy: 0.5357\n",
            "Epoch 4551/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9712\n",
            "Epoch 4551: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0667 - accuracy: 0.9712 - val_loss: 7.5207 - val_accuracy: 0.5366\n",
            "Epoch 4552/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9703\n",
            "Epoch 4552: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 0.0670 - accuracy: 0.9703 - val_loss: 7.5154 - val_accuracy: 0.5357\n",
            "Epoch 4553/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9684\n",
            "Epoch 4553: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0673 - accuracy: 0.9684 - val_loss: 7.5571 - val_accuracy: 0.5340\n",
            "Epoch 4554/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9702\n",
            "Epoch 4554: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0662 - accuracy: 0.9702 - val_loss: 7.5736 - val_accuracy: 0.5360\n",
            "Epoch 4555/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9719\n",
            "Epoch 4555: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0662 - accuracy: 0.9719 - val_loss: 7.5094 - val_accuracy: 0.5357\n",
            "Epoch 4556/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9710\n",
            "Epoch 4556: loss did not improve from 0.06534\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0663 - accuracy: 0.9710 - val_loss: 7.5789 - val_accuracy: 0.5360\n",
            "Epoch 4557/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9714\n",
            "Epoch 4557: loss improved from 0.06534 to 0.06524, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 936ms/step - loss: 0.0652 - accuracy: 0.9714 - val_loss: 7.5938 - val_accuracy: 0.5343\n",
            "Epoch 4558/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9728\n",
            "Epoch 4558: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 239ms/step - loss: 0.0655 - accuracy: 0.9728 - val_loss: 7.5804 - val_accuracy: 0.5351\n",
            "Epoch 4559/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9707\n",
            "Epoch 4559: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0658 - accuracy: 0.9707 - val_loss: 7.5960 - val_accuracy: 0.5366\n",
            "Epoch 4560/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9728\n",
            "Epoch 4560: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0653 - accuracy: 0.9728 - val_loss: 7.6133 - val_accuracy: 0.5366\n",
            "Epoch 4561/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9708\n",
            "Epoch 4561: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 184ms/step - loss: 0.0664 - accuracy: 0.9708 - val_loss: 7.5469 - val_accuracy: 0.5378\n",
            "Epoch 4562/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9711\n",
            "Epoch 4562: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0658 - accuracy: 0.9711 - val_loss: 7.5717 - val_accuracy: 0.5337\n",
            "Epoch 4563/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9703\n",
            "Epoch 4563: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 262ms/step - loss: 0.0665 - accuracy: 0.9703 - val_loss: 7.4968 - val_accuracy: 0.5354\n",
            "Epoch 4564/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9654\n",
            "Epoch 4564: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0707 - accuracy: 0.9654 - val_loss: 7.5795 - val_accuracy: 0.5372\n",
            "Epoch 4565/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9708\n",
            "Epoch 4565: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0669 - accuracy: 0.9708 - val_loss: 7.6344 - val_accuracy: 0.5319\n",
            "Epoch 4566/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9692\n",
            "Epoch 4566: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0675 - accuracy: 0.9692 - val_loss: 7.4866 - val_accuracy: 0.5366\n",
            "Epoch 4567/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9683\n",
            "Epoch 4567: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0684 - accuracy: 0.9683 - val_loss: 7.6040 - val_accuracy: 0.5349\n",
            "Epoch 4568/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9709\n",
            "Epoch 4568: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0660 - accuracy: 0.9709 - val_loss: 7.5945 - val_accuracy: 0.5328\n",
            "Epoch 4569/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9706\n",
            "Epoch 4569: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0666 - accuracy: 0.9706 - val_loss: 7.4552 - val_accuracy: 0.5363\n",
            "Epoch 4570/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9652\n",
            "Epoch 4570: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.0714 - accuracy: 0.9652 - val_loss: 7.5784 - val_accuracy: 0.5360\n",
            "Epoch 4571/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9704\n",
            "Epoch 4571: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0684 - accuracy: 0.9704 - val_loss: 7.6697 - val_accuracy: 0.5293\n",
            "Epoch 4572/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9677\n",
            "Epoch 4572: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 172ms/step - loss: 0.0686 - accuracy: 0.9677 - val_loss: 7.4748 - val_accuracy: 0.5363\n",
            "Epoch 4573/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9637\n",
            "Epoch 4573: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0722 - accuracy: 0.9637 - val_loss: 7.5803 - val_accuracy: 0.5354\n",
            "Epoch 4574/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9701\n",
            "Epoch 4574: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0681 - accuracy: 0.9701 - val_loss: 7.6140 - val_accuracy: 0.5313\n",
            "Epoch 4575/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9678\n",
            "Epoch 4575: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0681 - accuracy: 0.9678 - val_loss: 7.4727 - val_accuracy: 0.5360\n",
            "Epoch 4576/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9661\n",
            "Epoch 4576: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0705 - accuracy: 0.9661 - val_loss: 7.6354 - val_accuracy: 0.5287\n",
            "Epoch 4577/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9665\n",
            "Epoch 4577: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0708 - accuracy: 0.9665 - val_loss: 7.5590 - val_accuracy: 0.5316\n",
            "Epoch 4578/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9700\n",
            "Epoch 4578: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0682 - accuracy: 0.9700 - val_loss: 7.5453 - val_accuracy: 0.5381\n",
            "Epoch 4579/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9681\n",
            "Epoch 4579: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0698 - accuracy: 0.9681 - val_loss: 7.5747 - val_accuracy: 0.5305\n",
            "Epoch 4580/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9651\n",
            "Epoch 4580: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0713 - accuracy: 0.9651 - val_loss: 7.6007 - val_accuracy: 0.5325\n",
            "Epoch 4581/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9684\n",
            "Epoch 4581: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0679 - accuracy: 0.9684 - val_loss: 7.4534 - val_accuracy: 0.5337\n",
            "Epoch 4582/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9670\n",
            "Epoch 4582: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0692 - accuracy: 0.9670 - val_loss: 7.5800 - val_accuracy: 0.5337\n",
            "Epoch 4583/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9689\n",
            "Epoch 4583: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0679 - accuracy: 0.9689 - val_loss: 7.6079 - val_accuracy: 0.5310\n",
            "Epoch 4584/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9681\n",
            "Epoch 4584: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0683 - accuracy: 0.9681 - val_loss: 7.5110 - val_accuracy: 0.5354\n",
            "Epoch 4585/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9681\n",
            "Epoch 4585: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0686 - accuracy: 0.9681 - val_loss: 7.5646 - val_accuracy: 0.5328\n",
            "Epoch 4586/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9695\n",
            "Epoch 4586: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0670 - accuracy: 0.9695 - val_loss: 7.6018 - val_accuracy: 0.5334\n",
            "Epoch 4587/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9700\n",
            "Epoch 4587: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0670 - accuracy: 0.9700 - val_loss: 7.5189 - val_accuracy: 0.5357\n",
            "Epoch 4588/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9667\n",
            "Epoch 4588: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0697 - accuracy: 0.9667 - val_loss: 7.5377 - val_accuracy: 0.5334\n",
            "Epoch 4589/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9696\n",
            "Epoch 4589: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0674 - accuracy: 0.9696 - val_loss: 7.6111 - val_accuracy: 0.5357\n",
            "Epoch 4590/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9708\n",
            "Epoch 4590: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0670 - accuracy: 0.9708 - val_loss: 7.5348 - val_accuracy: 0.5354\n",
            "Epoch 4591/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9708\n",
            "Epoch 4591: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0671 - accuracy: 0.9708 - val_loss: 7.5853 - val_accuracy: 0.5357\n",
            "Epoch 4592/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9679\n",
            "Epoch 4592: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.0678 - accuracy: 0.9679 - val_loss: 7.5715 - val_accuracy: 0.5346\n",
            "Epoch 4593/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9708\n",
            "Epoch 4593: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0663 - accuracy: 0.9708 - val_loss: 7.5552 - val_accuracy: 0.5351\n",
            "Epoch 4594/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9697\n",
            "Epoch 4594: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0673 - accuracy: 0.9697 - val_loss: 7.6016 - val_accuracy: 0.5360\n",
            "Epoch 4595/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9696\n",
            "Epoch 4595: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0670 - accuracy: 0.9696 - val_loss: 7.5625 - val_accuracy: 0.5322\n",
            "Epoch 4596/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9708\n",
            "Epoch 4596: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0661 - accuracy: 0.9708 - val_loss: 7.6271 - val_accuracy: 0.5328\n",
            "Epoch 4597/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9708\n",
            "Epoch 4597: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0662 - accuracy: 0.9708 - val_loss: 7.4902 - val_accuracy: 0.5360\n",
            "Epoch 4598/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9665\n",
            "Epoch 4598: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0704 - accuracy: 0.9665 - val_loss: 7.5162 - val_accuracy: 0.5357\n",
            "Epoch 4599/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9679\n",
            "Epoch 4599: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0675 - accuracy: 0.9679 - val_loss: 7.6463 - val_accuracy: 0.5284\n",
            "Epoch 4600/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9632\n",
            "Epoch 4600: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0728 - accuracy: 0.9632 - val_loss: 7.6453 - val_accuracy: 0.5310\n",
            "Epoch 4601/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9667\n",
            "Epoch 4601: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0697 - accuracy: 0.9667 - val_loss: 7.4296 - val_accuracy: 0.5351\n",
            "Epoch 4602/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9563\n",
            "Epoch 4602: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0820 - accuracy: 0.9563 - val_loss: 7.6197 - val_accuracy: 0.5310\n",
            "Epoch 4603/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9662\n",
            "Epoch 4603: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0728 - accuracy: 0.9662 - val_loss: 7.5598 - val_accuracy: 0.5313\n",
            "Epoch 4604/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9642\n",
            "Epoch 4604: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0720 - accuracy: 0.9642 - val_loss: 7.3984 - val_accuracy: 0.5351\n",
            "Epoch 4605/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9607\n",
            "Epoch 4605: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0784 - accuracy: 0.9607 - val_loss: 7.7350 - val_accuracy: 0.5278\n",
            "Epoch 4606/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9518\n",
            "Epoch 4606: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0873 - accuracy: 0.9518 - val_loss: 7.5709 - val_accuracy: 0.5337\n",
            "Epoch 4607/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9601\n",
            "Epoch 4607: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 0.0781 - accuracy: 0.9601 - val_loss: 7.3569 - val_accuracy: 0.5398\n",
            "Epoch 4608/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9515\n",
            "Epoch 4608: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0947 - accuracy: 0.9515 - val_loss: 7.6833 - val_accuracy: 0.5234\n",
            "Epoch 4609/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9535\n",
            "Epoch 4609: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 170ms/step - loss: 0.0918 - accuracy: 0.9535 - val_loss: 7.5593 - val_accuracy: 0.5290\n",
            "Epoch 4610/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9591\n",
            "Epoch 4610: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0811 - accuracy: 0.9591 - val_loss: 7.2632 - val_accuracy: 0.5366\n",
            "Epoch 4611/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9522\n",
            "Epoch 4611: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.0920 - accuracy: 0.9522 - val_loss: 7.6500 - val_accuracy: 0.5264\n",
            "Epoch 4612/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9515\n",
            "Epoch 4612: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0957 - accuracy: 0.9515 - val_loss: 7.4367 - val_accuracy: 0.5360\n",
            "Epoch 4613/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9612\n",
            "Epoch 4613: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0819 - accuracy: 0.9612 - val_loss: 7.2368 - val_accuracy: 0.5372\n",
            "Epoch 4614/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9569\n",
            "Epoch 4614: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0850 - accuracy: 0.9569 - val_loss: 7.6879 - val_accuracy: 0.5231\n",
            "Epoch 4615/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9517\n",
            "Epoch 4615: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.0953 - accuracy: 0.9517 - val_loss: 7.3257 - val_accuracy: 0.5363\n",
            "Epoch 4616/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9546\n",
            "Epoch 4616: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 233ms/step - loss: 0.0873 - accuracy: 0.9546 - val_loss: 7.4051 - val_accuracy: 0.5357\n",
            "Epoch 4617/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9619\n",
            "Epoch 4617: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0789 - accuracy: 0.9619 - val_loss: 7.5716 - val_accuracy: 0.5299\n",
            "Epoch 4618/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9598\n",
            "Epoch 4618: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0805 - accuracy: 0.9598 - val_loss: 7.2561 - val_accuracy: 0.5354\n",
            "Epoch 4619/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9587\n",
            "Epoch 4619: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0823 - accuracy: 0.9587 - val_loss: 7.5345 - val_accuracy: 0.5284\n",
            "Epoch 4620/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9590\n",
            "Epoch 4620: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 0.0788 - accuracy: 0.9590 - val_loss: 7.4140 - val_accuracy: 0.5337\n",
            "Epoch 4621/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9659\n",
            "Epoch 4621: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0729 - accuracy: 0.9659 - val_loss: 7.3688 - val_accuracy: 0.5319\n",
            "Epoch 4622/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9651\n",
            "Epoch 4622: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0722 - accuracy: 0.9651 - val_loss: 7.5032 - val_accuracy: 0.5340\n",
            "Epoch 4623/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9665\n",
            "Epoch 4623: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0714 - accuracy: 0.9665 - val_loss: 7.3553 - val_accuracy: 0.5354\n",
            "Epoch 4624/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9678\n",
            "Epoch 4624: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 235ms/step - loss: 0.0697 - accuracy: 0.9678 - val_loss: 7.5259 - val_accuracy: 0.5351\n",
            "Epoch 4625/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9651\n",
            "Epoch 4625: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0726 - accuracy: 0.9651 - val_loss: 7.4864 - val_accuracy: 0.5360\n",
            "Epoch 4626/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9682\n",
            "Epoch 4626: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0694 - accuracy: 0.9682 - val_loss: 7.4809 - val_accuracy: 0.5357\n",
            "Epoch 4627/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9684\n",
            "Epoch 4627: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0687 - accuracy: 0.9684 - val_loss: 7.4989 - val_accuracy: 0.5369\n",
            "Epoch 4628/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9683\n",
            "Epoch 4628: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 0.0697 - accuracy: 0.9683 - val_loss: 7.4389 - val_accuracy: 0.5378\n",
            "Epoch 4629/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9686\n",
            "Epoch 4629: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.0699 - accuracy: 0.9686 - val_loss: 7.4408 - val_accuracy: 0.5363\n",
            "Epoch 4630/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9691\n",
            "Epoch 4630: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0689 - accuracy: 0.9691 - val_loss: 7.4552 - val_accuracy: 0.5369\n",
            "Epoch 4631/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9695\n",
            "Epoch 4631: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0670 - accuracy: 0.9695 - val_loss: 7.5150 - val_accuracy: 0.5357\n",
            "Epoch 4632/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9704\n",
            "Epoch 4632: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0671 - accuracy: 0.9704 - val_loss: 7.5172 - val_accuracy: 0.5349\n",
            "Epoch 4633/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9705\n",
            "Epoch 4633: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0675 - accuracy: 0.9705 - val_loss: 7.4745 - val_accuracy: 0.5343\n",
            "Epoch 4634/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9702\n",
            "Epoch 4634: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0678 - accuracy: 0.9702 - val_loss: 7.4490 - val_accuracy: 0.5343\n",
            "Epoch 4635/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9705\n",
            "Epoch 4635: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0663 - accuracy: 0.9705 - val_loss: 7.5637 - val_accuracy: 0.5328\n",
            "Epoch 4636/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9687\n",
            "Epoch 4636: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0682 - accuracy: 0.9687 - val_loss: 7.4519 - val_accuracy: 0.5366\n",
            "Epoch 4637/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9687\n",
            "Epoch 4637: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0679 - accuracy: 0.9687 - val_loss: 7.5427 - val_accuracy: 0.5331\n",
            "Epoch 4638/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9686\n",
            "Epoch 4638: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0684 - accuracy: 0.9686 - val_loss: 7.5669 - val_accuracy: 0.5340\n",
            "Epoch 4639/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9698\n",
            "Epoch 4639: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0673 - accuracy: 0.9698 - val_loss: 7.4013 - val_accuracy: 0.5372\n",
            "Epoch 4640/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9638\n",
            "Epoch 4640: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0716 - accuracy: 0.9638 - val_loss: 7.5893 - val_accuracy: 0.5319\n",
            "Epoch 4641/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9685\n",
            "Epoch 4641: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0677 - accuracy: 0.9685 - val_loss: 7.4727 - val_accuracy: 0.5378\n",
            "Epoch 4642/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9683\n",
            "Epoch 4642: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0682 - accuracy: 0.9683 - val_loss: 7.4526 - val_accuracy: 0.5375\n",
            "Epoch 4643/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9696\n",
            "Epoch 4643: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0676 - accuracy: 0.9696 - val_loss: 7.5945 - val_accuracy: 0.5322\n",
            "Epoch 4644/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9685\n",
            "Epoch 4644: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0679 - accuracy: 0.9685 - val_loss: 7.5087 - val_accuracy: 0.5375\n",
            "Epoch 4645/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9713\n",
            "Epoch 4645: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0663 - accuracy: 0.9713 - val_loss: 7.4947 - val_accuracy: 0.5346\n",
            "Epoch 4646/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9692\n",
            "Epoch 4646: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0664 - accuracy: 0.9692 - val_loss: 7.5671 - val_accuracy: 0.5316\n",
            "Epoch 4647/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9707\n",
            "Epoch 4647: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0664 - accuracy: 0.9707 - val_loss: 7.5026 - val_accuracy: 0.5393\n",
            "Epoch 4648/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9702\n",
            "Epoch 4648: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0675 - accuracy: 0.9702 - val_loss: 7.5944 - val_accuracy: 0.5346\n",
            "Epoch 4649/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9681\n",
            "Epoch 4649: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0691 - accuracy: 0.9681 - val_loss: 7.5423 - val_accuracy: 0.5366\n",
            "Epoch 4650/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9649\n",
            "Epoch 4650: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0690 - accuracy: 0.9649 - val_loss: 7.4595 - val_accuracy: 0.5366\n",
            "Epoch 4651/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9676\n",
            "Epoch 4651: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0691 - accuracy: 0.9676 - val_loss: 7.6730 - val_accuracy: 0.5278\n",
            "Epoch 4652/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9610\n",
            "Epoch 4652: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0761 - accuracy: 0.9610 - val_loss: 7.4128 - val_accuracy: 0.5351\n",
            "Epoch 4653/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9675\n",
            "Epoch 4653: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0699 - accuracy: 0.9675 - val_loss: 7.5300 - val_accuracy: 0.5319\n",
            "Epoch 4654/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9671\n",
            "Epoch 4654: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0691 - accuracy: 0.9671 - val_loss: 7.5350 - val_accuracy: 0.5331\n",
            "Epoch 4655/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9694\n",
            "Epoch 4655: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0682 - accuracy: 0.9694 - val_loss: 7.4485 - val_accuracy: 0.5375\n",
            "Epoch 4656/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9695\n",
            "Epoch 4656: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0679 - accuracy: 0.9695 - val_loss: 7.6198 - val_accuracy: 0.5310\n",
            "Epoch 4657/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9678\n",
            "Epoch 4657: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0684 - accuracy: 0.9678 - val_loss: 7.4281 - val_accuracy: 0.5354\n",
            "Epoch 4658/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9659\n",
            "Epoch 4658: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0702 - accuracy: 0.9659 - val_loss: 7.5847 - val_accuracy: 0.5331\n",
            "Epoch 4659/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9695\n",
            "Epoch 4659: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0675 - accuracy: 0.9695 - val_loss: 7.5034 - val_accuracy: 0.5346\n",
            "Epoch 4660/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9696\n",
            "Epoch 4660: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0674 - accuracy: 0.9696 - val_loss: 7.4545 - val_accuracy: 0.5378\n",
            "Epoch 4661/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9690\n",
            "Epoch 4661: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0672 - accuracy: 0.9690 - val_loss: 7.5829 - val_accuracy: 0.5322\n",
            "Epoch 4662/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9665\n",
            "Epoch 4662: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0687 - accuracy: 0.9665 - val_loss: 7.5492 - val_accuracy: 0.5366\n",
            "Epoch 4663/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9706\n",
            "Epoch 4663: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0664 - accuracy: 0.9706 - val_loss: 7.5187 - val_accuracy: 0.5343\n",
            "Epoch 4664/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9714\n",
            "Epoch 4664: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 183ms/step - loss: 0.0665 - accuracy: 0.9714 - val_loss: 7.5617 - val_accuracy: 0.5322\n",
            "Epoch 4665/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9708\n",
            "Epoch 4665: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0670 - accuracy: 0.9708 - val_loss: 7.5484 - val_accuracy: 0.5363\n",
            "Epoch 4666/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9697\n",
            "Epoch 4666: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 262ms/step - loss: 0.0671 - accuracy: 0.9697 - val_loss: 7.5497 - val_accuracy: 0.5331\n",
            "Epoch 4667/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9700\n",
            "Epoch 4667: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0674 - accuracy: 0.9700 - val_loss: 7.5616 - val_accuracy: 0.5346\n",
            "Epoch 4668/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9717\n",
            "Epoch 4668: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.0659 - accuracy: 0.9717 - val_loss: 7.5244 - val_accuracy: 0.5325\n",
            "Epoch 4669/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9696\n",
            "Epoch 4669: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 0.0671 - accuracy: 0.9696 - val_loss: 7.5719 - val_accuracy: 0.5381\n",
            "Epoch 4670/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9708\n",
            "Epoch 4670: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 0.0668 - accuracy: 0.9708 - val_loss: 7.5378 - val_accuracy: 0.5331\n",
            "Epoch 4671/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9703\n",
            "Epoch 4671: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 0.0672 - accuracy: 0.9703 - val_loss: 7.5223 - val_accuracy: 0.5363\n",
            "Epoch 4672/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9708\n",
            "Epoch 4672: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0657 - accuracy: 0.9708 - val_loss: 7.5647 - val_accuracy: 0.5351\n",
            "Epoch 4673/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9710\n",
            "Epoch 4673: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0664 - accuracy: 0.9710 - val_loss: 7.5632 - val_accuracy: 0.5354\n",
            "Epoch 4674/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9719\n",
            "Epoch 4674: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 240ms/step - loss: 0.0659 - accuracy: 0.9719 - val_loss: 7.5117 - val_accuracy: 0.5372\n",
            "Epoch 4675/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9713\n",
            "Epoch 4675: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.0666 - accuracy: 0.9713 - val_loss: 7.4924 - val_accuracy: 0.5372\n",
            "Epoch 4676/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9697\n",
            "Epoch 4676: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.0664 - accuracy: 0.9697 - val_loss: 7.5562 - val_accuracy: 0.5384\n",
            "Epoch 4677/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9706\n",
            "Epoch 4677: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.0656 - accuracy: 0.9706 - val_loss: 7.5954 - val_accuracy: 0.5346\n",
            "Epoch 4678/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9714\n",
            "Epoch 4678: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0656 - accuracy: 0.9714 - val_loss: 7.5395 - val_accuracy: 0.5354\n",
            "Epoch 4679/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9705\n",
            "Epoch 4679: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0665 - accuracy: 0.9705 - val_loss: 7.5394 - val_accuracy: 0.5366\n",
            "Epoch 4680/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9711\n",
            "Epoch 4680: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 243ms/step - loss: 0.0657 - accuracy: 0.9711 - val_loss: 7.5953 - val_accuracy: 0.5349\n",
            "Epoch 4681/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9717\n",
            "Epoch 4681: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0660 - accuracy: 0.9717 - val_loss: 7.6007 - val_accuracy: 0.5349\n",
            "Epoch 4682/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9692\n",
            "Epoch 4682: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0665 - accuracy: 0.9692 - val_loss: 7.5314 - val_accuracy: 0.5357\n",
            "Epoch 4683/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9729\n",
            "Epoch 4683: loss did not improve from 0.06524\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0656 - accuracy: 0.9729 - val_loss: 7.5752 - val_accuracy: 0.5381\n",
            "Epoch 4684/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9729\n",
            "Epoch 4684: loss improved from 0.06524 to 0.06503, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 764ms/step - loss: 0.0650 - accuracy: 0.9729 - val_loss: 7.5632 - val_accuracy: 0.5375\n",
            "Epoch 4685/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9730\n",
            "Epoch 4685: loss improved from 0.06503 to 0.06496, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0650 - accuracy: 0.9730 - val_loss: 7.5966 - val_accuracy: 0.5360\n",
            "Epoch 4686/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9719\n",
            "Epoch 4686: loss did not improve from 0.06496\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0650 - accuracy: 0.9719 - val_loss: 7.5581 - val_accuracy: 0.5390\n",
            "Epoch 4687/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9712\n",
            "Epoch 4687: loss did not improve from 0.06496\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0652 - accuracy: 0.9712 - val_loss: 7.5431 - val_accuracy: 0.5378\n",
            "Epoch 4688/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9712\n",
            "Epoch 4688: loss did not improve from 0.06496\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0654 - accuracy: 0.9712 - val_loss: 7.6087 - val_accuracy: 0.5375\n",
            "Epoch 4689/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9726\n",
            "Epoch 4689: loss improved from 0.06496 to 0.06461, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0646 - accuracy: 0.9726 - val_loss: 7.5559 - val_accuracy: 0.5375\n",
            "Epoch 4690/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9728\n",
            "Epoch 4690: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0651 - accuracy: 0.9728 - val_loss: 7.5489 - val_accuracy: 0.5381\n",
            "Epoch 4691/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9730\n",
            "Epoch 4691: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0651 - accuracy: 0.9730 - val_loss: 7.6596 - val_accuracy: 0.5331\n",
            "Epoch 4692/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9680\n",
            "Epoch 4692: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0672 - accuracy: 0.9680 - val_loss: 7.5463 - val_accuracy: 0.5360\n",
            "Epoch 4693/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9688\n",
            "Epoch 4693: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0679 - accuracy: 0.9688 - val_loss: 7.5152 - val_accuracy: 0.5378\n",
            "Epoch 4694/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9678\n",
            "Epoch 4694: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0680 - accuracy: 0.9678 - val_loss: 7.6604 - val_accuracy: 0.5290\n",
            "Epoch 4695/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9668\n",
            "Epoch 4695: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0693 - accuracy: 0.9668 - val_loss: 7.5586 - val_accuracy: 0.5369\n",
            "Epoch 4696/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9682\n",
            "Epoch 4696: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0690 - accuracy: 0.9682 - val_loss: 7.4883 - val_accuracy: 0.5340\n",
            "Epoch 4697/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9689\n",
            "Epoch 4697: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0681 - accuracy: 0.9689 - val_loss: 7.7080 - val_accuracy: 0.5302\n",
            "Epoch 4698/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9610\n",
            "Epoch 4698: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0750 - accuracy: 0.9610 - val_loss: 7.5901 - val_accuracy: 0.5331\n",
            "Epoch 4699/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9696\n",
            "Epoch 4699: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0690 - accuracy: 0.9696 - val_loss: 7.4095 - val_accuracy: 0.5340\n",
            "Epoch 4700/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9629\n",
            "Epoch 4700: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0748 - accuracy: 0.9629 - val_loss: 7.6841 - val_accuracy: 0.5293\n",
            "Epoch 4701/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9613\n",
            "Epoch 4701: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0770 - accuracy: 0.9613 - val_loss: 7.4732 - val_accuracy: 0.5366\n",
            "Epoch 4702/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9629\n",
            "Epoch 4702: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0760 - accuracy: 0.9629 - val_loss: 7.4993 - val_accuracy: 0.5369\n",
            "Epoch 4703/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9629\n",
            "Epoch 4703: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.0753 - accuracy: 0.9629 - val_loss: 7.6274 - val_accuracy: 0.5275\n",
            "Epoch 4704/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9602\n",
            "Epoch 4704: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0786 - accuracy: 0.9602 - val_loss: 7.4667 - val_accuracy: 0.5357\n",
            "Epoch 4705/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9591\n",
            "Epoch 4705: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0784 - accuracy: 0.9591 - val_loss: 7.4761 - val_accuracy: 0.5351\n",
            "Epoch 4706/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9638\n",
            "Epoch 4706: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0738 - accuracy: 0.9638 - val_loss: 7.7173 - val_accuracy: 0.5287\n",
            "Epoch 4707/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9603\n",
            "Epoch 4707: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0793 - accuracy: 0.9603 - val_loss: 7.3405 - val_accuracy: 0.5363\n",
            "Epoch 4708/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9572\n",
            "Epoch 4708: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0824 - accuracy: 0.9572 - val_loss: 7.4449 - val_accuracy: 0.5366\n",
            "Epoch 4709/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9649\n",
            "Epoch 4709: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0747 - accuracy: 0.9649 - val_loss: 7.7029 - val_accuracy: 0.5293\n",
            "Epoch 4710/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9600\n",
            "Epoch 4710: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0802 - accuracy: 0.9600 - val_loss: 7.3292 - val_accuracy: 0.5375\n",
            "Epoch 4711/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9541\n",
            "Epoch 4711: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0893 - accuracy: 0.9541 - val_loss: 7.5218 - val_accuracy: 0.5302\n",
            "Epoch 4712/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9566\n",
            "Epoch 4712: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0838 - accuracy: 0.9566 - val_loss: 7.5801 - val_accuracy: 0.5293\n",
            "Epoch 4713/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9607\n",
            "Epoch 4713: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0784 - accuracy: 0.9607 - val_loss: 7.3646 - val_accuracy: 0.5401\n",
            "Epoch 4714/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9592\n",
            "Epoch 4714: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0822 - accuracy: 0.9592 - val_loss: 7.6359 - val_accuracy: 0.5278\n",
            "Epoch 4715/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9506\n",
            "Epoch 4715: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 191ms/step - loss: 0.0935 - accuracy: 0.9506 - val_loss: 7.4509 - val_accuracy: 0.5369\n",
            "Epoch 4716/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9574\n",
            "Epoch 4716: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0836 - accuracy: 0.9574 - val_loss: 7.3181 - val_accuracy: 0.5381\n",
            "Epoch 4717/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9593\n",
            "Epoch 4717: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0812 - accuracy: 0.9593 - val_loss: 7.5680 - val_accuracy: 0.5278\n",
            "Epoch 4718/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9547\n",
            "Epoch 4718: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0868 - accuracy: 0.9547 - val_loss: 7.2954 - val_accuracy: 0.5372\n",
            "Epoch 4719/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9525\n",
            "Epoch 4719: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0894 - accuracy: 0.9525 - val_loss: 7.4883 - val_accuracy: 0.5325\n",
            "Epoch 4720/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9624\n",
            "Epoch 4720: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0768 - accuracy: 0.9624 - val_loss: 7.4171 - val_accuracy: 0.5328\n",
            "Epoch 4721/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9633\n",
            "Epoch 4721: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0767 - accuracy: 0.9633 - val_loss: 7.2921 - val_accuracy: 0.5357\n",
            "Epoch 4722/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9590\n",
            "Epoch 4722: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0803 - accuracy: 0.9590 - val_loss: 7.6319 - val_accuracy: 0.5258\n",
            "Epoch 4723/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9536\n",
            "Epoch 4723: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0887 - accuracy: 0.9536 - val_loss: 7.3314 - val_accuracy: 0.5360\n",
            "Epoch 4724/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9604\n",
            "Epoch 4724: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 0.0804 - accuracy: 0.9604 - val_loss: 7.3878 - val_accuracy: 0.5357\n",
            "Epoch 4725/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9637\n",
            "Epoch 4725: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0750 - accuracy: 0.9637 - val_loss: 7.4814 - val_accuracy: 0.5310\n",
            "Epoch 4726/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9625\n",
            "Epoch 4726: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0744 - accuracy: 0.9625 - val_loss: 7.3916 - val_accuracy: 0.5372\n",
            "Epoch 4727/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9634\n",
            "Epoch 4727: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0743 - accuracy: 0.9634 - val_loss: 7.4506 - val_accuracy: 0.5302\n",
            "Epoch 4728/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9691\n",
            "Epoch 4728: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0695 - accuracy: 0.9691 - val_loss: 7.4356 - val_accuracy: 0.5319\n",
            "Epoch 4729/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9683\n",
            "Epoch 4729: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 172ms/step - loss: 0.0704 - accuracy: 0.9683 - val_loss: 7.4278 - val_accuracy: 0.5340\n",
            "Epoch 4730/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9684\n",
            "Epoch 4730: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 0.0685 - accuracy: 0.9684 - val_loss: 7.4940 - val_accuracy: 0.5351\n",
            "Epoch 4731/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9681\n",
            "Epoch 4731: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0682 - accuracy: 0.9681 - val_loss: 7.4536 - val_accuracy: 0.5349\n",
            "Epoch 4732/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9699\n",
            "Epoch 4732: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0671 - accuracy: 0.9699 - val_loss: 7.4685 - val_accuracy: 0.5351\n",
            "Epoch 4733/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9719\n",
            "Epoch 4733: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0662 - accuracy: 0.9719 - val_loss: 7.5152 - val_accuracy: 0.5354\n",
            "Epoch 4734/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9707\n",
            "Epoch 4734: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0662 - accuracy: 0.9707 - val_loss: 7.4928 - val_accuracy: 0.5363\n",
            "Epoch 4735/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9714\n",
            "Epoch 4735: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0659 - accuracy: 0.9714 - val_loss: 7.4829 - val_accuracy: 0.5354\n",
            "Epoch 4736/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9704\n",
            "Epoch 4736: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0663 - accuracy: 0.9704 - val_loss: 7.4916 - val_accuracy: 0.5331\n",
            "Epoch 4737/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9719\n",
            "Epoch 4737: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0656 - accuracy: 0.9719 - val_loss: 7.5731 - val_accuracy: 0.5319\n",
            "Epoch 4738/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9706\n",
            "Epoch 4738: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0664 - accuracy: 0.9706 - val_loss: 7.5327 - val_accuracy: 0.5360\n",
            "Epoch 4739/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9703\n",
            "Epoch 4739: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0662 - accuracy: 0.9703 - val_loss: 7.4971 - val_accuracy: 0.5369\n",
            "Epoch 4740/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9701\n",
            "Epoch 4740: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0662 - accuracy: 0.9701 - val_loss: 7.5161 - val_accuracy: 0.5366\n",
            "Epoch 4741/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9708\n",
            "Epoch 4741: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0657 - accuracy: 0.9708 - val_loss: 7.4714 - val_accuracy: 0.5366\n",
            "Epoch 4742/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9705\n",
            "Epoch 4742: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0663 - accuracy: 0.9705 - val_loss: 7.5992 - val_accuracy: 0.5319\n",
            "Epoch 4743/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9711\n",
            "Epoch 4743: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0666 - accuracy: 0.9711 - val_loss: 7.4965 - val_accuracy: 0.5369\n",
            "Epoch 4744/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9708\n",
            "Epoch 4744: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0664 - accuracy: 0.9708 - val_loss: 7.5550 - val_accuracy: 0.5360\n",
            "Epoch 4745/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9712\n",
            "Epoch 4745: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0654 - accuracy: 0.9712 - val_loss: 7.5523 - val_accuracy: 0.5395\n",
            "Epoch 4746/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9715\n",
            "Epoch 4746: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0652 - accuracy: 0.9715 - val_loss: 7.4718 - val_accuracy: 0.5363\n",
            "Epoch 4747/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9710\n",
            "Epoch 4747: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0660 - accuracy: 0.9710 - val_loss: 7.5736 - val_accuracy: 0.5346\n",
            "Epoch 4748/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9703\n",
            "Epoch 4748: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0664 - accuracy: 0.9703 - val_loss: 7.5230 - val_accuracy: 0.5375\n",
            "Epoch 4749/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9732\n",
            "Epoch 4749: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0651 - accuracy: 0.9732 - val_loss: 7.5656 - val_accuracy: 0.5360\n",
            "Epoch 4750/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9708\n",
            "Epoch 4750: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0670 - accuracy: 0.9708 - val_loss: 7.6378 - val_accuracy: 0.5313\n",
            "Epoch 4751/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9693\n",
            "Epoch 4751: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0673 - accuracy: 0.9693 - val_loss: 7.4421 - val_accuracy: 0.5384\n",
            "Epoch 4752/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9622\n",
            "Epoch 4752: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0741 - accuracy: 0.9622 - val_loss: 7.5356 - val_accuracy: 0.5384\n",
            "Epoch 4753/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9714\n",
            "Epoch 4753: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0672 - accuracy: 0.9714 - val_loss: 7.5971 - val_accuracy: 0.5334\n",
            "Epoch 4754/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9702\n",
            "Epoch 4754: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0670 - accuracy: 0.9702 - val_loss: 7.5125 - val_accuracy: 0.5381\n",
            "Epoch 4755/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9710\n",
            "Epoch 4755: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0658 - accuracy: 0.9710 - val_loss: 7.5932 - val_accuracy: 0.5305\n",
            "Epoch 4756/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9670\n",
            "Epoch 4756: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0686 - accuracy: 0.9670 - val_loss: 7.5827 - val_accuracy: 0.5351\n",
            "Epoch 4757/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9702\n",
            "Epoch 4757: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0666 - accuracy: 0.9702 - val_loss: 7.5223 - val_accuracy: 0.5349\n",
            "Epoch 4758/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9708\n",
            "Epoch 4758: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0666 - accuracy: 0.9708 - val_loss: 7.5593 - val_accuracy: 0.5328\n",
            "Epoch 4759/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9710\n",
            "Epoch 4759: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0659 - accuracy: 0.9710 - val_loss: 7.5531 - val_accuracy: 0.5346\n",
            "Epoch 4760/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9715\n",
            "Epoch 4760: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0659 - accuracy: 0.9715 - val_loss: 7.5446 - val_accuracy: 0.5337\n",
            "Epoch 4761/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9706\n",
            "Epoch 4761: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0669 - accuracy: 0.9706 - val_loss: 7.5923 - val_accuracy: 0.5337\n",
            "Epoch 4762/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9703\n",
            "Epoch 4762: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0658 - accuracy: 0.9703 - val_loss: 7.4812 - val_accuracy: 0.5349\n",
            "Epoch 4763/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9701\n",
            "Epoch 4763: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0661 - accuracy: 0.9701 - val_loss: 7.6113 - val_accuracy: 0.5343\n",
            "Epoch 4764/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9716\n",
            "Epoch 4764: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0654 - accuracy: 0.9716 - val_loss: 7.5356 - val_accuracy: 0.5360\n",
            "Epoch 4765/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9690\n",
            "Epoch 4765: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 172ms/step - loss: 0.0682 - accuracy: 0.9690 - val_loss: 7.4732 - val_accuracy: 0.5390\n",
            "Epoch 4766/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9685\n",
            "Epoch 4766: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0682 - accuracy: 0.9685 - val_loss: 7.6735 - val_accuracy: 0.5313\n",
            "Epoch 4767/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9636\n",
            "Epoch 4767: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0732 - accuracy: 0.9636 - val_loss: 7.5600 - val_accuracy: 0.5366\n",
            "Epoch 4768/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9693\n",
            "Epoch 4768: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0678 - accuracy: 0.9693 - val_loss: 7.4509 - val_accuracy: 0.5366\n",
            "Epoch 4769/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9666\n",
            "Epoch 4769: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0689 - accuracy: 0.9666 - val_loss: 7.6711 - val_accuracy: 0.5281\n",
            "Epoch 4770/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9637\n",
            "Epoch 4770: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0723 - accuracy: 0.9637 - val_loss: 7.4967 - val_accuracy: 0.5384\n",
            "Epoch 4771/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9640\n",
            "Epoch 4771: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0752 - accuracy: 0.9640 - val_loss: 7.4990 - val_accuracy: 0.5381\n",
            "Epoch 4772/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9654\n",
            "Epoch 4772: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0709 - accuracy: 0.9654 - val_loss: 7.6781 - val_accuracy: 0.5272\n",
            "Epoch 4773/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9610\n",
            "Epoch 4773: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0770 - accuracy: 0.9610 - val_loss: 7.3795 - val_accuracy: 0.5372\n",
            "Epoch 4774/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9628\n",
            "Epoch 4774: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0762 - accuracy: 0.9628 - val_loss: 7.5677 - val_accuracy: 0.5328\n",
            "Epoch 4775/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9671\n",
            "Epoch 4775: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0707 - accuracy: 0.9671 - val_loss: 7.5963 - val_accuracy: 0.5278\n",
            "Epoch 4776/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9668\n",
            "Epoch 4776: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0702 - accuracy: 0.9668 - val_loss: 7.3470 - val_accuracy: 0.5366\n",
            "Epoch 4777/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9605\n",
            "Epoch 4777: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 173ms/step - loss: 0.0769 - accuracy: 0.9605 - val_loss: 7.6491 - val_accuracy: 0.5275\n",
            "Epoch 4778/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9634\n",
            "Epoch 4778: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 177ms/step - loss: 0.0738 - accuracy: 0.9634 - val_loss: 7.4653 - val_accuracy: 0.5349\n",
            "Epoch 4779/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9639\n",
            "Epoch 4779: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0734 - accuracy: 0.9639 - val_loss: 7.4593 - val_accuracy: 0.5366\n",
            "Epoch 4780/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9678\n",
            "Epoch 4780: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 265ms/step - loss: 0.0704 - accuracy: 0.9678 - val_loss: 7.5943 - val_accuracy: 0.5331\n",
            "Epoch 4781/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9659\n",
            "Epoch 4781: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0702 - accuracy: 0.9659 - val_loss: 7.4130 - val_accuracy: 0.5351\n",
            "Epoch 4782/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9628\n",
            "Epoch 4782: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0732 - accuracy: 0.9628 - val_loss: 7.5065 - val_accuracy: 0.5351\n",
            "Epoch 4783/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9669\n",
            "Epoch 4783: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 248ms/step - loss: 0.0698 - accuracy: 0.9669 - val_loss: 7.6095 - val_accuracy: 0.5316\n",
            "Epoch 4784/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9650\n",
            "Epoch 4784: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0705 - accuracy: 0.9650 - val_loss: 7.4458 - val_accuracy: 0.5360\n",
            "Epoch 4785/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9643\n",
            "Epoch 4785: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0725 - accuracy: 0.9643 - val_loss: 7.5575 - val_accuracy: 0.5334\n",
            "Epoch 4786/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9647\n",
            "Epoch 4786: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0723 - accuracy: 0.9647 - val_loss: 7.4979 - val_accuracy: 0.5346\n",
            "Epoch 4787/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9700\n",
            "Epoch 4787: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0682 - accuracy: 0.9700 - val_loss: 7.5145 - val_accuracy: 0.5346\n",
            "Epoch 4788/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9694\n",
            "Epoch 4788: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0672 - accuracy: 0.9694 - val_loss: 7.5572 - val_accuracy: 0.5331\n",
            "Epoch 4789/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9693\n",
            "Epoch 4789: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 0.0669 - accuracy: 0.9693 - val_loss: 7.4843 - val_accuracy: 0.5343\n",
            "Epoch 4790/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9679\n",
            "Epoch 4790: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0686 - accuracy: 0.9679 - val_loss: 7.5697 - val_accuracy: 0.5334\n",
            "Epoch 4791/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9700\n",
            "Epoch 4791: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0667 - accuracy: 0.9700 - val_loss: 7.5395 - val_accuracy: 0.5346\n",
            "Epoch 4792/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9714\n",
            "Epoch 4792: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 0.0658 - accuracy: 0.9714 - val_loss: 7.5091 - val_accuracy: 0.5395\n",
            "Epoch 4793/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9701\n",
            "Epoch 4793: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.0672 - accuracy: 0.9701 - val_loss: 7.4865 - val_accuracy: 0.5381\n",
            "Epoch 4794/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9701\n",
            "Epoch 4794: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 0.0662 - accuracy: 0.9701 - val_loss: 7.5677 - val_accuracy: 0.5340\n",
            "Epoch 4795/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9714\n",
            "Epoch 4795: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0658 - accuracy: 0.9714 - val_loss: 7.5437 - val_accuracy: 0.5372\n",
            "Epoch 4796/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9699\n",
            "Epoch 4796: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0670 - accuracy: 0.9699 - val_loss: 7.4985 - val_accuracy: 0.5393\n",
            "Epoch 4797/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9672\n",
            "Epoch 4797: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 188ms/step - loss: 0.0682 - accuracy: 0.9672 - val_loss: 7.5932 - val_accuracy: 0.5337\n",
            "Epoch 4798/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9668\n",
            "Epoch 4798: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0686 - accuracy: 0.9668 - val_loss: 7.5396 - val_accuracy: 0.5363\n",
            "Epoch 4799/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9682\n",
            "Epoch 4799: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0682 - accuracy: 0.9682 - val_loss: 7.4465 - val_accuracy: 0.5346\n",
            "Epoch 4800/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9638\n",
            "Epoch 4800: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 187ms/step - loss: 0.0724 - accuracy: 0.9638 - val_loss: 7.5719 - val_accuracy: 0.5316\n",
            "Epoch 4801/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9703\n",
            "Epoch 4801: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0665 - accuracy: 0.9703 - val_loss: 7.5860 - val_accuracy: 0.5331\n",
            "Epoch 4802/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9708\n",
            "Epoch 4802: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0661 - accuracy: 0.9708 - val_loss: 7.4869 - val_accuracy: 0.5360\n",
            "Epoch 4803/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9719\n",
            "Epoch 4803: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0660 - accuracy: 0.9719 - val_loss: 7.6153 - val_accuracy: 0.5319\n",
            "Epoch 4804/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9695\n",
            "Epoch 4804: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0672 - accuracy: 0.9695 - val_loss: 7.5314 - val_accuracy: 0.5395\n",
            "Epoch 4805/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9725\n",
            "Epoch 4805: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0653 - accuracy: 0.9725 - val_loss: 7.5088 - val_accuracy: 0.5404\n",
            "Epoch 4806/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9711\n",
            "Epoch 4806: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 0.0657 - accuracy: 0.9711 - val_loss: 7.5897 - val_accuracy: 0.5369\n",
            "Epoch 4807/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9725\n",
            "Epoch 4807: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0652 - accuracy: 0.9725 - val_loss: 7.5570 - val_accuracy: 0.5351\n",
            "Epoch 4808/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9714\n",
            "Epoch 4808: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0652 - accuracy: 0.9714 - val_loss: 7.5130 - val_accuracy: 0.5372\n",
            "Epoch 4809/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9713\n",
            "Epoch 4809: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0652 - accuracy: 0.9713 - val_loss: 7.6095 - val_accuracy: 0.5340\n",
            "Epoch 4810/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9708\n",
            "Epoch 4810: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0668 - accuracy: 0.9708 - val_loss: 7.6199 - val_accuracy: 0.5340\n",
            "Epoch 4811/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9717\n",
            "Epoch 4811: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0657 - accuracy: 0.9717 - val_loss: 7.5428 - val_accuracy: 0.5384\n",
            "Epoch 4812/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9724\n",
            "Epoch 4812: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0653 - accuracy: 0.9724 - val_loss: 7.4965 - val_accuracy: 0.5354\n",
            "Epoch 4813/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9697\n",
            "Epoch 4813: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0664 - accuracy: 0.9697 - val_loss: 7.6496 - val_accuracy: 0.5328\n",
            "Epoch 4814/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9708\n",
            "Epoch 4814: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0660 - accuracy: 0.9708 - val_loss: 7.5328 - val_accuracy: 0.5366\n",
            "Epoch 4815/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9689\n",
            "Epoch 4815: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0678 - accuracy: 0.9689 - val_loss: 7.5869 - val_accuracy: 0.5384\n",
            "Epoch 4816/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9714\n",
            "Epoch 4816: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0660 - accuracy: 0.9714 - val_loss: 7.5662 - val_accuracy: 0.5325\n",
            "Epoch 4817/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9705\n",
            "Epoch 4817: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0661 - accuracy: 0.9705 - val_loss: 7.5119 - val_accuracy: 0.5360\n",
            "Epoch 4818/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9688\n",
            "Epoch 4818: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0678 - accuracy: 0.9688 - val_loss: 7.5539 - val_accuracy: 0.5349\n",
            "Epoch 4819/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9693\n",
            "Epoch 4819: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0667 - accuracy: 0.9693 - val_loss: 7.5711 - val_accuracy: 0.5390\n",
            "Epoch 4820/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9710\n",
            "Epoch 4820: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0667 - accuracy: 0.9710 - val_loss: 7.5238 - val_accuracy: 0.5390\n",
            "Epoch 4821/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9686\n",
            "Epoch 4821: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0672 - accuracy: 0.9686 - val_loss: 7.6431 - val_accuracy: 0.5328\n",
            "Epoch 4822/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9682\n",
            "Epoch 4822: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0696 - accuracy: 0.9682 - val_loss: 7.5554 - val_accuracy: 0.5366\n",
            "Epoch 4823/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9697\n",
            "Epoch 4823: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 178ms/step - loss: 0.0671 - accuracy: 0.9697 - val_loss: 7.4562 - val_accuracy: 0.5393\n",
            "Epoch 4824/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9680\n",
            "Epoch 4824: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0684 - accuracy: 0.9680 - val_loss: 7.6391 - val_accuracy: 0.5346\n",
            "Epoch 4825/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9706\n",
            "Epoch 4825: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0666 - accuracy: 0.9706 - val_loss: 7.5700 - val_accuracy: 0.5357\n",
            "Epoch 4826/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9700\n",
            "Epoch 4826: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0669 - accuracy: 0.9700 - val_loss: 7.4928 - val_accuracy: 0.5378\n",
            "Epoch 4827/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9660\n",
            "Epoch 4827: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0697 - accuracy: 0.9660 - val_loss: 7.5736 - val_accuracy: 0.5357\n",
            "Epoch 4828/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9699\n",
            "Epoch 4828: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0668 - accuracy: 0.9699 - val_loss: 7.5849 - val_accuracy: 0.5337\n",
            "Epoch 4829/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9708\n",
            "Epoch 4829: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0666 - accuracy: 0.9708 - val_loss: 7.5412 - val_accuracy: 0.5378\n",
            "Epoch 4830/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9700\n",
            "Epoch 4830: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0668 - accuracy: 0.9700 - val_loss: 7.5836 - val_accuracy: 0.5375\n",
            "Epoch 4831/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9699\n",
            "Epoch 4831: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0669 - accuracy: 0.9699 - val_loss: 7.6643 - val_accuracy: 0.5299\n",
            "Epoch 4832/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9665\n",
            "Epoch 4832: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0684 - accuracy: 0.9665 - val_loss: 7.5238 - val_accuracy: 0.5357\n",
            "Epoch 4833/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9701\n",
            "Epoch 4833: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0679 - accuracy: 0.9701 - val_loss: 7.6437 - val_accuracy: 0.5340\n",
            "Epoch 4834/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9677\n",
            "Epoch 4834: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0686 - accuracy: 0.9677 - val_loss: 7.5699 - val_accuracy: 0.5366\n",
            "Epoch 4835/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9671\n",
            "Epoch 4835: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 195ms/step - loss: 0.0710 - accuracy: 0.9671 - val_loss: 7.4711 - val_accuracy: 0.5369\n",
            "Epoch 4836/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9651\n",
            "Epoch 4836: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0715 - accuracy: 0.9651 - val_loss: 7.7649 - val_accuracy: 0.5272\n",
            "Epoch 4837/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9564\n",
            "Epoch 4837: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 0.0825 - accuracy: 0.9564 - val_loss: 7.4753 - val_accuracy: 0.5363\n",
            "Epoch 4838/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9614\n",
            "Epoch 4838: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0789 - accuracy: 0.9614 - val_loss: 7.4754 - val_accuracy: 0.5369\n",
            "Epoch 4839/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9639\n",
            "Epoch 4839: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0742 - accuracy: 0.9639 - val_loss: 7.7019 - val_accuracy: 0.5269\n",
            "Epoch 4840/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9613\n",
            "Epoch 4840: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0752 - accuracy: 0.9613 - val_loss: 7.4035 - val_accuracy: 0.5366\n",
            "Epoch 4841/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9605\n",
            "Epoch 4841: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 0.0778 - accuracy: 0.9605 - val_loss: 7.5627 - val_accuracy: 0.5375\n",
            "Epoch 4842/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9642\n",
            "Epoch 4842: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 0.0739 - accuracy: 0.9642 - val_loss: 7.6026 - val_accuracy: 0.5287\n",
            "Epoch 4843/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9636\n",
            "Epoch 4843: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.0736 - accuracy: 0.9636 - val_loss: 7.3925 - val_accuracy: 0.5360\n",
            "Epoch 4844/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9634\n",
            "Epoch 4844: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 0.0742 - accuracy: 0.9634 - val_loss: 7.6210 - val_accuracy: 0.5313\n",
            "Epoch 4845/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9655\n",
            "Epoch 4845: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 230ms/step - loss: 0.0721 - accuracy: 0.9655 - val_loss: 7.4438 - val_accuracy: 0.5316\n",
            "Epoch 4846/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9664\n",
            "Epoch 4846: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 259ms/step - loss: 0.0722 - accuracy: 0.9664 - val_loss: 7.5771 - val_accuracy: 0.5360\n",
            "Epoch 4847/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9674\n",
            "Epoch 4847: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0709 - accuracy: 0.9674 - val_loss: 7.5635 - val_accuracy: 0.5313\n",
            "Epoch 4848/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9666\n",
            "Epoch 4848: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0703 - accuracy: 0.9666 - val_loss: 7.4762 - val_accuracy: 0.5360\n",
            "Epoch 4849/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9654\n",
            "Epoch 4849: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0713 - accuracy: 0.9654 - val_loss: 7.5476 - val_accuracy: 0.5351\n",
            "Epoch 4850/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9694\n",
            "Epoch 4850: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 0.0696 - accuracy: 0.9694 - val_loss: 7.5334 - val_accuracy: 0.5322\n",
            "Epoch 4851/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9684\n",
            "Epoch 4851: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 0.0682 - accuracy: 0.9684 - val_loss: 7.5249 - val_accuracy: 0.5378\n",
            "Epoch 4852/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9689\n",
            "Epoch 4852: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 0.0678 - accuracy: 0.9689 - val_loss: 7.4953 - val_accuracy: 0.5337\n",
            "Epoch 4853/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9693\n",
            "Epoch 4853: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0676 - accuracy: 0.9693 - val_loss: 7.6185 - val_accuracy: 0.5334\n",
            "Epoch 4854/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9692\n",
            "Epoch 4854: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 172ms/step - loss: 0.0671 - accuracy: 0.9692 - val_loss: 7.4648 - val_accuracy: 0.5349\n",
            "Epoch 4855/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9692\n",
            "Epoch 4855: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0675 - accuracy: 0.9692 - val_loss: 7.6200 - val_accuracy: 0.5296\n",
            "Epoch 4856/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9662\n",
            "Epoch 4856: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 144ms/step - loss: 0.0701 - accuracy: 0.9662 - val_loss: 7.5918 - val_accuracy: 0.5372\n",
            "Epoch 4857/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9689\n",
            "Epoch 4857: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0676 - accuracy: 0.9689 - val_loss: 7.4783 - val_accuracy: 0.5363\n",
            "Epoch 4858/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9697\n",
            "Epoch 4858: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0673 - accuracy: 0.9697 - val_loss: 7.5706 - val_accuracy: 0.5363\n",
            "Epoch 4859/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9695\n",
            "Epoch 4859: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0662 - accuracy: 0.9695 - val_loss: 7.4717 - val_accuracy: 0.5381\n",
            "Epoch 4860/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9707\n",
            "Epoch 4860: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0669 - accuracy: 0.9707 - val_loss: 7.6161 - val_accuracy: 0.5354\n",
            "Epoch 4861/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9705\n",
            "Epoch 4861: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0668 - accuracy: 0.9705 - val_loss: 7.5706 - val_accuracy: 0.5334\n",
            "Epoch 4862/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9689\n",
            "Epoch 4862: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0666 - accuracy: 0.9689 - val_loss: 7.4958 - val_accuracy: 0.5372\n",
            "Epoch 4863/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9703\n",
            "Epoch 4863: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 0.0663 - accuracy: 0.9703 - val_loss: 7.6480 - val_accuracy: 0.5281\n",
            "Epoch 4864/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9669\n",
            "Epoch 4864: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0689 - accuracy: 0.9669 - val_loss: 7.4907 - val_accuracy: 0.5369\n",
            "Epoch 4865/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9658\n",
            "Epoch 4865: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0704 - accuracy: 0.9658 - val_loss: 7.5389 - val_accuracy: 0.5360\n",
            "Epoch 4866/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9699\n",
            "Epoch 4866: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0669 - accuracy: 0.9699 - val_loss: 7.6044 - val_accuracy: 0.5325\n",
            "Epoch 4867/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9673\n",
            "Epoch 4867: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0686 - accuracy: 0.9673 - val_loss: 7.4879 - val_accuracy: 0.5372\n",
            "Epoch 4868/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9663\n",
            "Epoch 4868: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0723 - accuracy: 0.9663 - val_loss: 7.5786 - val_accuracy: 0.5346\n",
            "Epoch 4869/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9662\n",
            "Epoch 4869: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0722 - accuracy: 0.9662 - val_loss: 7.5919 - val_accuracy: 0.5360\n",
            "Epoch 4870/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9676\n",
            "Epoch 4870: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0729 - accuracy: 0.9676 - val_loss: 7.3891 - val_accuracy: 0.5351\n",
            "Epoch 4871/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9614\n",
            "Epoch 4871: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0767 - accuracy: 0.9614 - val_loss: 7.6642 - val_accuracy: 0.5284\n",
            "Epoch 4872/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9555\n",
            "Epoch 4872: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0851 - accuracy: 0.9555 - val_loss: 7.5082 - val_accuracy: 0.5319\n",
            "Epoch 4873/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9630\n",
            "Epoch 4873: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0772 - accuracy: 0.9630 - val_loss: 7.3123 - val_accuracy: 0.5349\n",
            "Epoch 4874/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9604\n",
            "Epoch 4874: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0788 - accuracy: 0.9604 - val_loss: 7.7180 - val_accuracy: 0.5284\n",
            "Epoch 4875/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9577\n",
            "Epoch 4875: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0830 - accuracy: 0.9577 - val_loss: 7.3110 - val_accuracy: 0.5357\n",
            "Epoch 4876/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9518\n",
            "Epoch 4876: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0901 - accuracy: 0.9518 - val_loss: 7.5068 - val_accuracy: 0.5346\n",
            "Epoch 4877/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9586\n",
            "Epoch 4877: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0817 - accuracy: 0.9586 - val_loss: 7.5786 - val_accuracy: 0.5267\n",
            "Epoch 4878/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9619\n",
            "Epoch 4878: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0795 - accuracy: 0.9619 - val_loss: 7.3184 - val_accuracy: 0.5372\n",
            "Epoch 4879/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9541\n",
            "Epoch 4879: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 182ms/step - loss: 0.0905 - accuracy: 0.9541 - val_loss: 7.7262 - val_accuracy: 0.5255\n",
            "Epoch 4880/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9463\n",
            "Epoch 4880: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.1028 - accuracy: 0.9463 - val_loss: 7.3384 - val_accuracy: 0.5331\n",
            "Epoch 4881/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9574\n",
            "Epoch 4881: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0855 - accuracy: 0.9574 - val_loss: 7.3590 - val_accuracy: 0.5404\n",
            "Epoch 4882/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9558\n",
            "Epoch 4882: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 145ms/step - loss: 0.0866 - accuracy: 0.9558 - val_loss: 7.6866 - val_accuracy: 0.5243\n",
            "Epoch 4883/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9516\n",
            "Epoch 4883: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 146ms/step - loss: 0.0986 - accuracy: 0.9516 - val_loss: 7.2348 - val_accuracy: 0.5401\n",
            "Epoch 4884/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9500\n",
            "Epoch 4884: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0973 - accuracy: 0.9500 - val_loss: 7.4953 - val_accuracy: 0.5302\n",
            "Epoch 4885/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9556\n",
            "Epoch 4885: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0872 - accuracy: 0.9556 - val_loss: 7.4029 - val_accuracy: 0.5328\n",
            "Epoch 4886/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9614\n",
            "Epoch 4886: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0796 - accuracy: 0.9614 - val_loss: 7.3121 - val_accuracy: 0.5381\n",
            "Epoch 4887/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9600\n",
            "Epoch 4887: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0789 - accuracy: 0.9600 - val_loss: 7.5506 - val_accuracy: 0.5269\n",
            "Epoch 4888/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9588\n",
            "Epoch 4888: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0818 - accuracy: 0.9588 - val_loss: 7.2884 - val_accuracy: 0.5378\n",
            "Epoch 4889/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9590\n",
            "Epoch 4889: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0806 - accuracy: 0.9590 - val_loss: 7.5456 - val_accuracy: 0.5302\n",
            "Epoch 4890/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9637\n",
            "Epoch 4890: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 171ms/step - loss: 0.0747 - accuracy: 0.9637 - val_loss: 7.2880 - val_accuracy: 0.5357\n",
            "Epoch 4891/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9603\n",
            "Epoch 4891: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0802 - accuracy: 0.9603 - val_loss: 7.5530 - val_accuracy: 0.5290\n",
            "Epoch 4892/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9598\n",
            "Epoch 4892: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 0.0797 - accuracy: 0.9598 - val_loss: 7.4372 - val_accuracy: 0.5331\n",
            "Epoch 4893/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9639\n",
            "Epoch 4893: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0747 - accuracy: 0.9639 - val_loss: 7.3048 - val_accuracy: 0.5360\n",
            "Epoch 4894/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9603\n",
            "Epoch 4894: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0762 - accuracy: 0.9603 - val_loss: 7.6406 - val_accuracy: 0.5302\n",
            "Epoch 4895/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9640\n",
            "Epoch 4895: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0753 - accuracy: 0.9640 - val_loss: 7.2153 - val_accuracy: 0.5343\n",
            "Epoch 4896/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9588\n",
            "Epoch 4896: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 0.0820 - accuracy: 0.9588 - val_loss: 7.5882 - val_accuracy: 0.5325\n",
            "Epoch 4897/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9605\n",
            "Epoch 4897: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0781 - accuracy: 0.9605 - val_loss: 7.3925 - val_accuracy: 0.5375\n",
            "Epoch 4898/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9644\n",
            "Epoch 4898: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0725 - accuracy: 0.9644 - val_loss: 7.4512 - val_accuracy: 0.5331\n",
            "Epoch 4899/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9661\n",
            "Epoch 4899: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 198ms/step - loss: 0.0708 - accuracy: 0.9661 - val_loss: 7.5302 - val_accuracy: 0.5346\n",
            "Epoch 4900/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9651\n",
            "Epoch 4900: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 0.0717 - accuracy: 0.9651 - val_loss: 7.3197 - val_accuracy: 0.5340\n",
            "Epoch 4901/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9639\n",
            "Epoch 4901: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0726 - accuracy: 0.9639 - val_loss: 7.6092 - val_accuracy: 0.5302\n",
            "Epoch 4902/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9638\n",
            "Epoch 4902: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0728 - accuracy: 0.9638 - val_loss: 7.3122 - val_accuracy: 0.5354\n",
            "Epoch 4903/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9641\n",
            "Epoch 4903: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 197ms/step - loss: 0.0740 - accuracy: 0.9641 - val_loss: 7.5765 - val_accuracy: 0.5331\n",
            "Epoch 4904/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9620\n",
            "Epoch 4904: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0741 - accuracy: 0.9620 - val_loss: 7.4182 - val_accuracy: 0.5384\n",
            "Epoch 4905/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9670\n",
            "Epoch 4905: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 0.0721 - accuracy: 0.9670 - val_loss: 7.4029 - val_accuracy: 0.5360\n",
            "Epoch 4906/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9668\n",
            "Epoch 4906: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 0.0708 - accuracy: 0.9668 - val_loss: 7.6158 - val_accuracy: 0.5322\n",
            "Epoch 4907/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9665\n",
            "Epoch 4907: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 0.0697 - accuracy: 0.9665 - val_loss: 7.3220 - val_accuracy: 0.5346\n",
            "Epoch 4908/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9667\n",
            "Epoch 4908: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0718 - accuracy: 0.9667 - val_loss: 7.5923 - val_accuracy: 0.5302\n",
            "Epoch 4909/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9674\n",
            "Epoch 4909: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 162ms/step - loss: 0.0700 - accuracy: 0.9674 - val_loss: 7.3636 - val_accuracy: 0.5360\n",
            "Epoch 4910/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9629\n",
            "Epoch 4910: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 0.0735 - accuracy: 0.9629 - val_loss: 7.5165 - val_accuracy: 0.5322\n",
            "Epoch 4911/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9636\n",
            "Epoch 4911: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.0750 - accuracy: 0.9636 - val_loss: 7.4950 - val_accuracy: 0.5340\n",
            "Epoch 4912/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9665\n",
            "Epoch 4912: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 163ms/step - loss: 0.0717 - accuracy: 0.9665 - val_loss: 7.3382 - val_accuracy: 0.5346\n",
            "Epoch 4913/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9639\n",
            "Epoch 4913: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0726 - accuracy: 0.9639 - val_loss: 7.6246 - val_accuracy: 0.5293\n",
            "Epoch 4914/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9635\n",
            "Epoch 4914: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 0.0732 - accuracy: 0.9635 - val_loss: 7.3963 - val_accuracy: 0.5360\n",
            "Epoch 4915/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9607\n",
            "Epoch 4915: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.0754 - accuracy: 0.9607 - val_loss: 7.5251 - val_accuracy: 0.5313\n",
            "Epoch 4916/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9636\n",
            "Epoch 4916: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 189ms/step - loss: 0.0734 - accuracy: 0.9636 - val_loss: 7.5600 - val_accuracy: 0.5313\n",
            "Epoch 4917/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9674\n",
            "Epoch 4917: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0693 - accuracy: 0.9674 - val_loss: 7.3827 - val_accuracy: 0.5308\n",
            "Epoch 4918/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9667\n",
            "Epoch 4918: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0702 - accuracy: 0.9667 - val_loss: 7.5576 - val_accuracy: 0.5325\n",
            "Epoch 4919/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9669\n",
            "Epoch 4919: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.0696 - accuracy: 0.9669 - val_loss: 7.3539 - val_accuracy: 0.5395\n",
            "Epoch 4920/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9663\n",
            "Epoch 4920: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0702 - accuracy: 0.9663 - val_loss: 7.5226 - val_accuracy: 0.5299\n",
            "Epoch 4921/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9677\n",
            "Epoch 4921: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0694 - accuracy: 0.9677 - val_loss: 7.4425 - val_accuracy: 0.5346\n",
            "Epoch 4922/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9665\n",
            "Epoch 4922: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 159ms/step - loss: 0.0706 - accuracy: 0.9665 - val_loss: 7.4244 - val_accuracy: 0.5319\n",
            "Epoch 4923/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9662\n",
            "Epoch 4923: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0720 - accuracy: 0.9662 - val_loss: 7.5599 - val_accuracy: 0.5322\n",
            "Epoch 4924/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9673\n",
            "Epoch 4924: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0709 - accuracy: 0.9673 - val_loss: 7.3375 - val_accuracy: 0.5351\n",
            "Epoch 4925/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9651\n",
            "Epoch 4925: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 176ms/step - loss: 0.0717 - accuracy: 0.9651 - val_loss: 7.5359 - val_accuracy: 0.5281\n",
            "Epoch 4926/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9643\n",
            "Epoch 4926: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 0.0708 - accuracy: 0.9643 - val_loss: 7.4594 - val_accuracy: 0.5378\n",
            "Epoch 4927/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9669\n",
            "Epoch 4927: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 184ms/step - loss: 0.0704 - accuracy: 0.9669 - val_loss: 7.4882 - val_accuracy: 0.5322\n",
            "Epoch 4928/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9679\n",
            "Epoch 4928: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0701 - accuracy: 0.9679 - val_loss: 7.5061 - val_accuracy: 0.5363\n",
            "Epoch 4929/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9680\n",
            "Epoch 4929: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0698 - accuracy: 0.9680 - val_loss: 7.3648 - val_accuracy: 0.5354\n",
            "Epoch 4930/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9665\n",
            "Epoch 4930: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 0.0699 - accuracy: 0.9665 - val_loss: 7.6147 - val_accuracy: 0.5284\n",
            "Epoch 4931/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9651\n",
            "Epoch 4931: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0712 - accuracy: 0.9651 - val_loss: 7.4009 - val_accuracy: 0.5354\n",
            "Epoch 4932/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9611\n",
            "Epoch 4932: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0750 - accuracy: 0.9611 - val_loss: 7.4591 - val_accuracy: 0.5351\n",
            "Epoch 4933/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9697\n",
            "Epoch 4933: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0673 - accuracy: 0.9697 - val_loss: 7.5293 - val_accuracy: 0.5387\n",
            "Epoch 4934/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9702\n",
            "Epoch 4934: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0674 - accuracy: 0.9702 - val_loss: 7.3648 - val_accuracy: 0.5390\n",
            "Epoch 4935/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9682\n",
            "Epoch 4935: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 141ms/step - loss: 0.0685 - accuracy: 0.9682 - val_loss: 7.5853 - val_accuracy: 0.5316\n",
            "Epoch 4936/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9687\n",
            "Epoch 4936: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0680 - accuracy: 0.9687 - val_loss: 7.4322 - val_accuracy: 0.5351\n",
            "Epoch 4937/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9695\n",
            "Epoch 4937: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0674 - accuracy: 0.9695 - val_loss: 7.5133 - val_accuracy: 0.5334\n",
            "Epoch 4938/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9703\n",
            "Epoch 4938: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0666 - accuracy: 0.9703 - val_loss: 7.5362 - val_accuracy: 0.5360\n",
            "Epoch 4939/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9710\n",
            "Epoch 4939: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0661 - accuracy: 0.9710 - val_loss: 7.4665 - val_accuracy: 0.5351\n",
            "Epoch 4940/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9698\n",
            "Epoch 4940: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0667 - accuracy: 0.9698 - val_loss: 7.5723 - val_accuracy: 0.5319\n",
            "Epoch 4941/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9695\n",
            "Epoch 4941: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0666 - accuracy: 0.9695 - val_loss: 7.4268 - val_accuracy: 0.5354\n",
            "Epoch 4942/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9675\n",
            "Epoch 4942: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0678 - accuracy: 0.9675 - val_loss: 7.5994 - val_accuracy: 0.5325\n",
            "Epoch 4943/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9678\n",
            "Epoch 4943: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0680 - accuracy: 0.9678 - val_loss: 7.5040 - val_accuracy: 0.5363\n",
            "Epoch 4944/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9690\n",
            "Epoch 4944: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 0.0674 - accuracy: 0.9690 - val_loss: 7.4131 - val_accuracy: 0.5360\n",
            "Epoch 4945/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9691\n",
            "Epoch 4945: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0676 - accuracy: 0.9691 - val_loss: 7.6212 - val_accuracy: 0.5319\n",
            "Epoch 4946/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9681\n",
            "Epoch 4946: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0685 - accuracy: 0.9681 - val_loss: 7.4118 - val_accuracy: 0.5384\n",
            "Epoch 4947/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9658\n",
            "Epoch 4947: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0689 - accuracy: 0.9658 - val_loss: 7.5610 - val_accuracy: 0.5346\n",
            "Epoch 4948/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9673\n",
            "Epoch 4948: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0687 - accuracy: 0.9673 - val_loss: 7.5174 - val_accuracy: 0.5381\n",
            "Epoch 4949/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9695\n",
            "Epoch 4949: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 0.0670 - accuracy: 0.9695 - val_loss: 7.4756 - val_accuracy: 0.5366\n",
            "Epoch 4950/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9707\n",
            "Epoch 4950: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 199ms/step - loss: 0.0664 - accuracy: 0.9707 - val_loss: 7.5969 - val_accuracy: 0.5372\n",
            "Epoch 4951/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9697\n",
            "Epoch 4951: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 192ms/step - loss: 0.0680 - accuracy: 0.9697 - val_loss: 7.3925 - val_accuracy: 0.5354\n",
            "Epoch 4952/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9647\n",
            "Epoch 4952: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 180ms/step - loss: 0.0715 - accuracy: 0.9647 - val_loss: 7.5819 - val_accuracy: 0.5346\n",
            "Epoch 4953/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9693\n",
            "Epoch 4953: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.0676 - accuracy: 0.9693 - val_loss: 7.5139 - val_accuracy: 0.5372\n",
            "Epoch 4954/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9721\n",
            "Epoch 4954: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0658 - accuracy: 0.9721 - val_loss: 7.4964 - val_accuracy: 0.5366\n",
            "Epoch 4955/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9707\n",
            "Epoch 4955: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0669 - accuracy: 0.9707 - val_loss: 7.6681 - val_accuracy: 0.5293\n",
            "Epoch 4956/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9671\n",
            "Epoch 4956: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 185ms/step - loss: 0.0688 - accuracy: 0.9671 - val_loss: 7.4278 - val_accuracy: 0.5375\n",
            "Epoch 4957/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9686\n",
            "Epoch 4957: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 0.0676 - accuracy: 0.9686 - val_loss: 7.5821 - val_accuracy: 0.5337\n",
            "Epoch 4958/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9678\n",
            "Epoch 4958: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 232ms/step - loss: 0.0675 - accuracy: 0.9678 - val_loss: 7.4614 - val_accuracy: 0.5363\n",
            "Epoch 4959/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9689\n",
            "Epoch 4959: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 0.0680 - accuracy: 0.9689 - val_loss: 7.5308 - val_accuracy: 0.5357\n",
            "Epoch 4960/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9692\n",
            "Epoch 4960: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 190ms/step - loss: 0.0670 - accuracy: 0.9692 - val_loss: 7.6386 - val_accuracy: 0.5308\n",
            "Epoch 4961/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9688\n",
            "Epoch 4961: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 0.0677 - accuracy: 0.9688 - val_loss: 7.4381 - val_accuracy: 0.5363\n",
            "Epoch 4962/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9689\n",
            "Epoch 4962: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0670 - accuracy: 0.9689 - val_loss: 7.5430 - val_accuracy: 0.5393\n",
            "Epoch 4963/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9713\n",
            "Epoch 4963: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 0.0649 - accuracy: 0.9713 - val_loss: 7.5034 - val_accuracy: 0.5363\n",
            "Epoch 4964/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9711\n",
            "Epoch 4964: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 254ms/step - loss: 0.0655 - accuracy: 0.9711 - val_loss: 7.5465 - val_accuracy: 0.5369\n",
            "Epoch 4965/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9704\n",
            "Epoch 4965: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 0.0657 - accuracy: 0.9704 - val_loss: 7.5323 - val_accuracy: 0.5369\n",
            "Epoch 4966/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9722\n",
            "Epoch 4966: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 0.0647 - accuracy: 0.9722 - val_loss: 7.5115 - val_accuracy: 0.5351\n",
            "Epoch 4967/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9718\n",
            "Epoch 4967: loss did not improve from 0.06461\n",
            "2/2 [==============================] - 0s 202ms/step - loss: 0.0651 - accuracy: 0.9718 - val_loss: 7.5816 - val_accuracy: 0.5343\n",
            "Epoch 4968/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9726\n",
            "Epoch 4968: loss improved from 0.06461 to 0.06454, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 1s 769ms/step - loss: 0.0645 - accuracy: 0.9726 - val_loss: 7.4989 - val_accuracy: 0.5337\n",
            "Epoch 4969/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9728\n",
            "Epoch 4969: loss did not improve from 0.06454\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0647 - accuracy: 0.9728 - val_loss: 7.5721 - val_accuracy: 0.5340\n",
            "Epoch 4970/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9722\n",
            "Epoch 4970: loss did not improve from 0.06454\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0647 - accuracy: 0.9722 - val_loss: 7.5174 - val_accuracy: 0.5372\n",
            "Epoch 4971/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9733\n",
            "Epoch 4971: loss improved from 0.06454 to 0.06438, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 234ms/step - loss: 0.0644 - accuracy: 0.9733 - val_loss: 7.5810 - val_accuracy: 0.5331\n",
            "Epoch 4972/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9725\n",
            "Epoch 4972: loss did not improve from 0.06438\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0655 - accuracy: 0.9725 - val_loss: 7.5505 - val_accuracy: 0.5334\n",
            "Epoch 4973/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9749\n",
            "Epoch 4973: loss improved from 0.06438 to 0.06373, saving model to /content/drive/MyDrive/new_df/best_model_by_class0.hdf5\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 0.0637 - accuracy: 0.9749 - val_loss: 7.5556 - val_accuracy: 0.5354\n",
            "Epoch 4974/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9730\n",
            "Epoch 4974: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0645 - accuracy: 0.9730 - val_loss: 7.5341 - val_accuracy: 0.5357\n",
            "Epoch 4975/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9714\n",
            "Epoch 4975: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 196ms/step - loss: 0.0648 - accuracy: 0.9714 - val_loss: 7.5178 - val_accuracy: 0.5366\n",
            "Epoch 4976/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9707\n",
            "Epoch 4976: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.0658 - accuracy: 0.9707 - val_loss: 7.5818 - val_accuracy: 0.5387\n",
            "Epoch 4977/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9733\n",
            "Epoch 4977: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.0640 - accuracy: 0.9733 - val_loss: 7.5358 - val_accuracy: 0.5390\n",
            "Epoch 4978/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9733\n",
            "Epoch 4978: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0646 - accuracy: 0.9733 - val_loss: 7.5683 - val_accuracy: 0.5366\n",
            "Epoch 4979/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9725\n",
            "Epoch 4979: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0644 - accuracy: 0.9725 - val_loss: 7.5914 - val_accuracy: 0.5351\n",
            "Epoch 4980/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9710\n",
            "Epoch 4980: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 194ms/step - loss: 0.0651 - accuracy: 0.9710 - val_loss: 7.5252 - val_accuracy: 0.5349\n",
            "Epoch 4981/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9713\n",
            "Epoch 4981: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 157ms/step - loss: 0.0651 - accuracy: 0.9713 - val_loss: 7.6250 - val_accuracy: 0.5316\n",
            "Epoch 4982/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9719\n",
            "Epoch 4982: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 0.0658 - accuracy: 0.9719 - val_loss: 7.5378 - val_accuracy: 0.5357\n",
            "Epoch 4983/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9714\n",
            "Epoch 4983: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0650 - accuracy: 0.9714 - val_loss: 7.5275 - val_accuracy: 0.5346\n",
            "Epoch 4984/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9703\n",
            "Epoch 4984: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 0.0667 - accuracy: 0.9703 - val_loss: 7.5600 - val_accuracy: 0.5343\n",
            "Epoch 4985/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9711\n",
            "Epoch 4985: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0658 - accuracy: 0.9711 - val_loss: 7.5159 - val_accuracy: 0.5375\n",
            "Epoch 4986/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9695\n",
            "Epoch 4986: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0666 - accuracy: 0.9695 - val_loss: 7.6398 - val_accuracy: 0.5354\n",
            "Epoch 4987/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9706\n",
            "Epoch 4987: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 155ms/step - loss: 0.0674 - accuracy: 0.9706 - val_loss: 7.4302 - val_accuracy: 0.5351\n",
            "Epoch 4988/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9670\n",
            "Epoch 4988: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 148ms/step - loss: 0.0691 - accuracy: 0.9670 - val_loss: 7.6270 - val_accuracy: 0.5360\n",
            "Epoch 4989/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9695\n",
            "Epoch 4989: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 140ms/step - loss: 0.0665 - accuracy: 0.9695 - val_loss: 7.5172 - val_accuracy: 0.5375\n",
            "Epoch 4990/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9687\n",
            "Epoch 4990: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 0.0672 - accuracy: 0.9687 - val_loss: 7.5478 - val_accuracy: 0.5351\n",
            "Epoch 4991/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9718\n",
            "Epoch 4991: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.0650 - accuracy: 0.9718 - val_loss: 7.5683 - val_accuracy: 0.5369\n",
            "Epoch 4992/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9706\n",
            "Epoch 4992: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 160ms/step - loss: 0.0652 - accuracy: 0.9706 - val_loss: 7.5003 - val_accuracy: 0.5360\n",
            "Epoch 4993/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9704\n",
            "Epoch 4993: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 0.0667 - accuracy: 0.9704 - val_loss: 7.5975 - val_accuracy: 0.5325\n",
            "Epoch 4994/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9699\n",
            "Epoch 4994: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 138ms/step - loss: 0.0678 - accuracy: 0.9699 - val_loss: 7.5304 - val_accuracy: 0.5351\n",
            "Epoch 4995/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9697\n",
            "Epoch 4995: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0667 - accuracy: 0.9697 - val_loss: 7.4920 - val_accuracy: 0.5384\n",
            "Epoch 4996/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9668\n",
            "Epoch 4996: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 211ms/step - loss: 0.0697 - accuracy: 0.9668 - val_loss: 7.6302 - val_accuracy: 0.5305\n",
            "Epoch 4997/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9703\n",
            "Epoch 4997: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 181ms/step - loss: 0.0668 - accuracy: 0.9703 - val_loss: 7.5074 - val_accuracy: 0.5366\n",
            "Epoch 4998/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9654\n",
            "Epoch 4998: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.0698 - accuracy: 0.9654 - val_loss: 7.5294 - val_accuracy: 0.5366\n",
            "Epoch 4999/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9693\n",
            "Epoch 4999: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 139ms/step - loss: 0.0666 - accuracy: 0.9693 - val_loss: 7.6233 - val_accuracy: 0.5313\n",
            "Epoch 5000/5000\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9693\n",
            "Epoch 5000: loss did not improve from 0.06373\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 0.0680 - accuracy: 0.9693 - val_loss: 7.4495 - val_accuracy: 0.5384\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 7.6339 - accuracy: 0.5097\n",
            "[7.63391637802124, 0.50970059633255]\n",
            "131/131 [==============================] - 0s 3ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.52      0.51      0.52      2133\n",
            "     class 0       0.50      0.51      0.50      2042\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.51      0.51      0.51      4175\n",
            "weighted avg       0.51      0.51      0.51      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "534/534 [==============================] - 2s 4ms/step - loss: 1.5458 - accuracy: 0.8815\n",
            "[1.5458064079284668, 0.8814601898193359]\n",
            "534/534 [==============================] - 2s 3ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.90      0.86      0.88      8533\n",
            "     class 0       0.87      0.90      0.88      8533\n",
            "\n",
            "    accuracy                           0.88     17066\n",
            "   macro avg       0.88      0.88      0.88     17066\n",
            "weighted avg       0.88      0.88      0.88     17066\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.best"
      ],
      "metadata": {
        "id": "4kqxPICBfB3D",
        "outputId": "ac80d4ac-3a84-4fd6-dc19-5fbc680fb255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0637253150343895"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Best Model Test Only***#\n",
        "#--------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 1_2', 'class 0']\n",
        "\n",
        "print(\"Test-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jjfj3XzjKvQU",
        "outputId": "911b2b9d-3daf-4de1-cde2-3e1c48b54777"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 7.7382 - accuracy: 0.5099\n",
            "[7.738237380981445, 0.5099401473999023]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.52      0.53      0.52      2133\n",
            "     class 0       0.50      0.49      0.49      2042\n",
            "\n",
            "    accuracy                           0.51      4175\n",
            "   macro avg       0.51      0.51      0.51      4175\n",
            "weighted avg       0.51      0.51      0.51      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGxCAYAAABr1xxGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKSUlEQVR4nO3deVxU9f7H8deA7DAoLiwJhooLZer1lmFWWlzRn5mm5bVFsYvaYlZ6c7sprmVXLdOuZVq5dFvMFm9ZmaallaRpYuZCahgqgiYKgrLO+f1BTpE4Mc4gzPR+Ph7n8WjO95zv+Rxj+fD5fs/3mAzDMBARERFxQx41HYCIiIhIdVGiIyIiIm5LiY6IiIi4LSU6IiIi4raU6IiIiIjbUqIjIiIibkuJjoiIiLgtJToiIiLiturUdABSOYvFQmZmJkFBQZhMppoOR0RE7GAYBqdPnyYiIgIPj+qrKRQWFlJcXOyUvry9vfH19XVKX7WJEp1aKjMzk8jIyJoOQ0REHHDo0CEaN25cLX0XFhYS3SSQrGNlTukvLCyM9PR0t0t2lOjUUkFBQQD89O3lmAM1wijuqV/7jjUdgki1KDVK2HjmbevP8upQXFxM1rEy0rc1wRzk2O+JvNMWojv8RHFxsRIduTTODVeZAz0c/gIWqa3qmLxrOgSRanUpph6Yg/R7whYlOiIiIi6szLBQ5uDrucsMi3OCqYWU6IiIiLgwCwYWHMt0HD2/NlOtS0RERNyWKjoiIiIuzIIFRweeHO+h9lKiIyIi4sLKDIMyw7GhJ0fPr800dCUiIiJuSxUdERERF6bJyLYp0REREXFhFgzKlOhckIauRERExG2poiMiIuLCNHRlmxIdERERF6anrmxToiMiIuLCLL9sjvbhrjRHR0RERNyWEh0REREXVvbLU1eObvbYuHEjvXr1IiIiApPJxMqVKyu0v/vuu3Tr1o369etjMplITU2t0J6Tk8OIESNo2bIlfn5+REVF8fDDD5Obm1vhuIyMDHr27Im/vz+NGjVi9OjRlJaW2hWrEh0REREXVmY4Z7NHQUEBbdu2Zf78+Rds79y5M//+978rbc/MzCQzM5PZs2fz/fffs2TJElavXk1SUtKv91VWRs+ePSkuLmbTpk0sXbqUJUuWkJycbFesmqMjIiIidunRowc9evS4YPvAgQMBOHjwYKXtV155Je+88471c7NmzXjiiSe45557KC0tpU6dOqxZs4bdu3fz6aefEhoaSrt27Zg2bRpjx45l8uTJeHt7VylWVXRERERcmMVJW03Lzc3FbDZTp055DSYlJYU2bdoQGhpqPSYhIYG8vDx27dpV5X5V0REREXFhFkyUYXK4D4C8vLwK+318fPDx8XGo76r4+eefmTZtGsOGDbPuy8rKqpDkANbPWVlZVe5bFR0REREBIDIykuDgYOs2Y8aMar9mXl4ePXv2JDY2lsmTJzu9f1V0REREXJjFKN8c7QPg0KFDmM1m6/7qruacPn2a7t27ExQUxHvvvYeXl5e1LSwsjC1btlQ4Pjs729pWVaroiIiIuLCyX4auHN0AzGZzha06E528vDy6deuGt7c377//Pr6+vhXa4+Li2LlzJ8eOHbPuW7t2LWazmdjY2CpfRxUdERERsUt+fj779++3fk5PTyc1NZWQkBCioqLIyckhIyODzMxMANLS0oDySkxYWJg1yTlz5gz//e9/ycvLs84PatiwIZ6ennTr1o3Y2FgGDhzIzJkzycrKYsKECQwfPtyuBEyJjoiIiAsrc8JkZHvP37p1K127drV+HjVqFACJiYksWbKE999/n3vvvdfaPmDAAAAmTZrE5MmT+fbbb9m8eTMAzZs3r9B3eno6l19+OZ6enqxatYoHHniAuLg4AgICSExMZOrUqXbFajIMN36TlwvLy8sjODiYkz80xRykEUZxTz1irqvpEESqRalRzPqCN6yPTFeHc78nvvw+gkAHf0/kn7bQ+crMao23pqiiIyIi4sJqoqLjSlQqEBEREbelio6IiIgLK8ODMgfrFmVOiqU2UqIjIiLiwgzDhMVwbOjJcPD82kxDVyIiIuK2VNERERFxYZqMbJsSHRERERdWZnhQZjg4R8eNF5rR0JWIiIi4LVV0REREXJgFExYH6xYW3Leko0RHRETEhWmOjm0auhIRERG3pYqOiIiIC3POZGQNXYmIiEgtVD5Hx7GhJ0fPr82U6IiIiLgwixNeAeHOk5E1R0dERETclio6IiIiLkxzdGxToiMiIuLCLHhoHR0bNHQlIiIibksVHRERERdWZpgoMxxcMNDB82szJToiIiIurMwJT12VaehKRERExPWooiMiIuLCLIYHFgefurLoqSsRERGpjTR0ZZuGrkRERMRtqaIjIiLiwiw4/tSUxTmh1EpKdERERFyYcxYMdN8BHiU6IiIiLsw5r4Bw30THfe9MRERE/vRU0REREXFhFkxYcHSOjlZGFhERkVpIQ1e2ue+diYiIyJ+eKjoiIiIuzDkLBrpv3UOJjoiIiAuzGCYsjq6j48ZvL3ffFE5ERET+9FTRERERcWEWJwxdacFAERERqZWc8/Zy90103PfORERE5E9PFR0REREXVoaJMgcX/HP0/NpMiY6IiIgL09CVbUp0REREXFgZjldkypwTSq3kvimciIiI/OmpoiMiIuLCNHRlmxIdERERF6aXetrmvncmIiIif3qq6IiIiLgwAxMWBycjG3q8XERERGojDV3Z5r53JiIiIn96quiIiIi4MIthwmI4NvTk6Pm1mSo6IiIiLqzsl7eXO7rZY+PGjfTq1YuIiAhMJhMrV66s0P7uu+/SrVs36tevj8lkIjU19bw+CgsLGT58OPXr1ycwMJB+/fqRnZ1d4ZiMjAx69uyJv78/jRo1YvTo0ZSWltoVqxIdERERsUtBQQFt27Zl/vz5F2zv3Lkz//73vy/Yx8iRI/nggw9YsWIFGzZsIDMzk759+1rby8rK6NmzJ8XFxWzatImlS5eyZMkSkpOT7YpVQ1ciIiIurCaGrnr06EGPHj0u2D5w4EAADh48WGl7bm4uL7/8Mq+//jo33XQTAIsXL6Z169Z8/fXXXHvttaxZs4bdu3fz6aefEhoaSrt27Zg2bRpjx45l8uTJeHt7VylWVXRERERcmAUPp2yX0rZt2ygpKSE+Pt66r1WrVkRFRZGSkgJASkoKbdq0ITQ01HpMQkICeXl57Nq1q8rXUkVHRETEhZUZJsocrOicOz8vL6/Cfh8fH3x8fBzquzJZWVl4e3tTt27dCvtDQ0PJysqyHvPbJOdc+7m2qlJFR0RERACIjIwkODjYus2YMaOmQ3KYKjoiIiIuzJlzdA4dOoTZbLbur45qDkBYWBjFxcWcOnWqQlUnOzubsLAw6zFbtmypcN65p7LOHVMVquiIiIi4MOOXt5c7shm/rIxsNpsrbNWV6HTo0AEvLy/WrVtn3ZeWlkZGRgZxcXEAxMXFsXPnTo4dO2Y9Zu3atZjNZmJjY6t8LVV0RERExC75+fns37/f+jk9PZ3U1FRCQkKIiooiJyeHjIwMMjMzgfIkBsorMWFhYQQHB5OUlMSoUaMICQnBbDYzYsQI4uLiuPbaawHo1q0bsbGxDBw4kJkzZ5KVlcWECRMYPny4XQmYKjoiIiIurAyTUzZ7bN26lfbt29O+fXsARo0aRfv27a1r3Lz//vu0b9+enj17AjBgwADat2/PggULrH3MmTOHW265hX79+nHDDTcQFhbGu+++a2339PRk1apVeHp6EhcXxz333MOgQYOYOnWqXbGaDMMw7DpDLom8vDyCg4M5+UNTzEHKR8U99Yi5rqZDEKkWpUYx6wveIDc3t8KcF2c693vi3s/74x1YtTVlLqQ4v5jFXd6q1nhrioauxG3s/DqAFc83Yt9Of3KyvZj0cjqdeuRa27/8KJgPl9Vn305/Tp+sw/Nr0mh25Vlre95JT16dHca3G4I4lulNcEgpnbrnkjjmKAFmCwBrlofw9MioSq+//LvvqdvAvqXJRexx5dW53D4kk+ZX5FM/tISpD7Qk5dP6vznCYOAjh+jeP5sAcxm7twXxn0lNyfzJz3pEYHAJDyan0/Gmk1gs8NUn9VkwPZrCM54V+umXlEn3v2cTelkRuTlefPh6GG++0PiS3auIs9TaROfgwYNER0ezfft22rVrV9PhiAsoPONB0yvOknBnDlOToittv+KaAm7odYpnR5+frORke3Ei24uhyZlEtSjk2GFv5o1rzIlsLyYuOgjAjbee5K9dK64zMfvRKEqKPJTkSLXz9bPw494A1rzdiInPp53XfsewI9w66ChPj4kh67APgx7NYPri3dzXvT0lxeWV4TFP7yOkUTH/GhxLnToGI5/az8PTDzBzVAtrP/dPTOcv1+Xy0lOXc/AHf4KCSwmqq6/v2urchGJH+3BXtTbRqWkLFy7k9ddf59tvv+X06dOcPHnyvIWNLuTgwYNMmzaN9evXk5WVRUREBPfccw+PP/54lZesFvtdfdNprr7p9AXb428/CUDWocr/H1zeqpDklw5aP0dcXszgsUeZOaIJZaXgWQd8/Ax8/H79gX/qhCc7vgpk5NOHnHMTIjZs3ViPrRvrXaDVoE/iUd58vjFfrwsBYPboGN74+hs6/S2HDR82ILLZGa6+8RQP33YV+74PBOCFqdFMfWkPLz11OTnHvIlsdoaed2Zzf892HEkvrwRlH74UdycXy4IJi51zbCrrw125bwrnoDNnztC9e3f+9a9/2X3u3r17sVgsvPjii+zatYs5c+awYMGCi+pLalZBnif+gRY8L/AnwacrQvDxM7i+56lLGpfI74VFFhHSqITtm+pa953Jr0PajiBatS//A6B1+9OczvW0JjkA2zfVxbBAq7blx3S86SRZh3zo2DWHxeu3seSzbTzyxH4Cg0su6f1I1Z1bGdnRzV3VaKJjsViYOXMmzZs3x8fHh6ioKJ544olKjy0rKyMpKYno6Gj8/Pxo2bIlc+fOrXDM559/zjXXXENAQAB169bluuuu46effgJgx44ddO3alaCgIMxmMx06dGDr1q0XjO3RRx9l3Lhx1sfc7NG9e3cWL15Mt27daNq0KbfeeiuPPfZYhdnkUvvlnvDk9WfD6HHPzxc85pM36tP1tpP4+GlOv9Sseg2KATj5s1eF/Sd/9rK21WtQQu6Jiu2WMhOnc+tQr2F5IhMWWUijy4q4vscJZo+J4emxzYm5soDHnzt/qEzEFdTo0NX48eNZtGgRc+bMoXPnzhw9epS9e/dWeqzFYqFx48asWLGC+vXrs2nTJoYNG0Z4eDj9+/entLSUPn36MHToUN544w2Ki4vZsmULJlN5lnr33XfTvn17XnjhBTw9PUlNTcXLy6vSa1WH3NxcQkJCLtheVFREUVGR9fPv3zcil1bBaQ8mDmpKVItCBv6z8neq7N7qT8Y+X8Y899Mljk6k+niYwNvHYPboGI4cLB+6mjO+Gf/533dcFn3WOpwltYfm6NhWY4nO6dOnmTt3Lv/5z39ITEwEoFmzZnTu3LnS4728vJgyZYr1c3R0NCkpKbz11lv079+fvLw8cnNzueWWW2jWrBkArVu3th6fkZHB6NGjadWqFQAxMTHVdWvn2b9/P8899xyzZ8++4DEzZsyocH9Sc87ke/D4Xc3wC7Aw6eV06lwgH179en2aXXGGmKvOVn6AyCV08ufyuWf1GpRw8viv89DqNSjhwJ6AX47xIrh+xSEoD0+DoOBSTh4v/0LPOe5FaYnJmuQAHDpQ/t+NIoqU6NRCFpzwCgjN0XG+PXv2UFRUxM0331zlc+bPn0+HDh1o2LAhgYGBLFy4kIyMDABCQkIYPHgwCQkJ9OrVi7lz53L06FHruaNGjWLIkCHEx8fz1FNPceDAAaffU2WOHDlC9+7dueOOOxg6dOgFjxs/fjy5ubnW7dAhTW6tCQWnPfjXnc3w8jaYsuRHvH0rH5I6W+DBxg/qknBnziWOUKRyWYd8yDnmRbu4U9Z9/oGltGx7mr3bgwDYsz2IoOAyml+Rbz2mXVwuJg/Yu6P8mN3fmqnjZRAeVWg95rLo8v8+dqR6XgcgUp1qLNHx87Pvr4I333yTxx57jKSkJNasWUNqair33nsvxcXF1mMWL15MSkoKnTp1Yvny5bRo0YKvv/4agMmTJ7Nr1y569uzJ+vXriY2N5b333nPqPf1eZmYmXbt2pVOnTixcuNDmsT4+Pue9Y0Tsc7bAgwPf+3Hg+/KvraxD3hz43o9jh8v/Us076cmB7/3I+KH8h/WhAz4c+N6PnGPlhc1zSU7hGQ9GPp3BmXxPco7VIedYHcrKKl5rw//qUlZm4uZ+Jy/dDcqfnq9/GU1bF9C0dQEAoY2LaNq6gIbhRYCJlUvDGfDgYTrelMPlLQr458z9nDjmzaa15cPmhw74882GujzyxAFaXHWa2L/k8UDyj2z4sAE5x8qrQNu/Cmbf9wGMnLGfZrH5NL8in4enHeDbL4MrVHmk9jB+eerKkc1w44pOja2MXFhYSEhICPPmzWPIkCHntf9+HZ0RI0awe/fuCi8Ai4+P5+effyY1NbXSa8TFxXH11Vczb96889ruvPNOCgoKeP/9923G+fnnn9O1a1e7Hi+H8kpO165d6dChA//973/x9PT845N+Qysj22/HpkDG3N78vP1/65/DY89mXHCxv3tGZTHwsawLng+wdPNuwiJ/Taof7RVDWFQR4+ZnOO8G/oS0MrJ92lyTy8zXdp23f+27DXlmbAzWBQP/nk2guZRdW83Mn9y0QoISGFzCg5PS6dg1B8Mw8dUn9XlhWsUFA0MaFfNA8o/85bpTFJ71ZOvGuiyacTn5uZduXqOru5QrI/f7NBGvAMeWLikpKOad+KVaGdmZfH19GTt2LGPGjMHb25vrrruO48ePs2vXLpKSks47PiYmhmXLlvHJJ58QHR3Nq6++yjfffEN0dPnCcOnp6SxcuJBbb72ViIgI0tLS2LdvH4MGDeLs2bOMHj2a22+/nejoaA4fPsw333xDv379LhhfVlYWWVlZ1peW7dy5k6CgIKKiomxOKobyJKdLly40adKE2bNnc/z4cWubPa+WF/u07ZTPJ5mpF2zv9vccuv39wkNNf3T+bz37wT47oxNx3M4twfSI6WTjCBOvzo3i1bmVr94NkJ/rVWFxwMrkHPPmiYdaXWSUIrVLjT51NXHiROrUqUNycjKZmZmEh4dz//33V3rsfffdx/bt2/n73/+OyWTizjvv5MEHH+Tjjz8GwN/fn71797J06VJOnDhBeHg4w4cP57777qO0tJQTJ04waNAgsrOzadCgAX379rU5+XfBggUV2m+44QagfHhs8ODBNu9r7dq17N+/n/3799O4ccUl0/VqMRERcSY9dWWbXupZS2noSv4MNHQl7upSDl31XvMPpwxd/a/bK245dKXfoCIiIuK2lOhchCeffJLAwMBKtx49etR0eCIi8ifi6BNXznhXVm2ml3pehPvvv5/+/ftX2mbvY/MiIiKOsBhOWDDQjd91pUTnIoSEhPzhk1ciIiKXghId2zR0JSIiIm5LFR0REREXpoqObUp0REREXJgSHds0dCUiIiJuSxUdERERF2aAw4+Hu/PKwUp0REREXJiGrmzT0JWIiIi4LVV0REREXJgqOrYp0REREXFhSnRs09CViIiIuC1VdERERFyYKjq2KdERERFxYYZhwnAwUXH0/NpMiY6IiIgLs2ByeB0dR8+vzTRHR0RERNyWKjoiIiIuTHN0bFOiIyIi4sI0R8c2DV2JiIiI21JFR0RExIVp6Mo2JToiIiIuTENXtmnoSkRERNyWKjoiIiIuzHDC0JU7V3SU6IiIiLgwAzAMx/twVxq6EhEREbelio6IiIgLs2DCpFdAXJASHRERERemp65sU6IjIiLiwiyGCZPW0bkgzdERERERt6WKjoiIiAszDCc8deXGj10p0REREXFhmqNjm4auRERExG2poiMiIuLCVNGxTRUdERERF3bu7eWObvbYuHEjvXr1IiIiApPJxMqVKyu0G4ZBcnIy4eHh+Pn5ER8fz759+yoc88MPP9C7d28aNGiA2Wymc+fOfPbZZxWOycjIoGfPnvj7+9OoUSNGjx5NaWmpXbEq0RERERG7FBQU0LZtW+bPn19p+8yZM5k3bx4LFixg8+bNBAQEkJCQQGFhofWYW265hdLSUtavX8+2bdto27Ytt9xyC1lZWQCUlZXRs2dPiouL2bRpE0uXLmXJkiUkJyfbFasSHRERERd27qkrRzd79OjRg+nTp3PbbbdVEo/Bs88+y4QJE+jduzdXXXUVy5YtIzMz01r5+fnnn9m3bx/jxo3jqquuIiYmhqeeeoozZ87w/fffA7BmzRp2797Nf//7X9q1a0ePHj2YNm0a8+fPp7i4uMqxKtERERFxYeWJisnBzXnxpKenk5WVRXx8vHVfcHAwHTt2JCUlBYD69evTsmVLli1bRkFBAaWlpbz44os0atSIDh06AJCSkkKbNm0IDQ219pOQkEBeXh67du2qcjyajCwiIiIA5OXlVfjs4+ODj4+PXX2cG3r6bYJy7vO5NpPJxKeffkqfPn0ICgrCw8ODRo0asXr1aurVq2ftp7I+fnuNqlBFR0RExIU5Xs359amtyMhIgoODrduMGTOqKWaD4cOH06hRI7744gu2bNlCnz596NWrF0ePHnXqtVTRERERcWHGL5ujfQAcOnQIs9ls3W9vNQcgLCwMgOzsbMLDw637s7OzadeuHQDr169n1apVnDx50nq9559/nrVr17J06VLGjRtHWFgYW7ZsqdB3dnZ2hWtUhSo6IiIiLsyZFR2z2Vxhu5hEJzo6mrCwMNatW2fdl5eXx+bNm4mLiwPgzJkzAHh4VExDPDw8sFgsAMTFxbFz506OHTtmbV+7di1ms5nY2Ngqx6OKjoiIiNglPz+f/fv3Wz+np6eTmppKSEgIUVFRPProo0yfPp2YmBiio6OZOHEiERER9OnTByhPYurVq0diYiLJycn4+fmxaNEi0tPT6dmzJwDdunUjNjaWgQMHMnPmTLKyspgwYQLDhw+3KwFToiMiIuLKnDl2VUVbt26la9eu1s+jRo0CIDExkSVLljBmzBgKCgoYNmwYp06donPnzqxevRpfX18AGjRowOrVq3n88ce56aabKCkp4YorruB///sfbdu2BcDT05NVq1bxwAMPEBcXR0BAAImJiUydOtWuWE2G4c7vLHVdeXl5BAcHc/KHppiDNMIo7qlHzHU1HYJItSg1illf8Aa5ubkV5rw407nfE02XPI6Hv69DfVnOFPLj4CeqNd6aot+gIiIi4rY0dCUiIuLCLmZl48r6cFdKdERERFyY3l5um4auRERExG2poiMiIuLKDFP55mgfbkqJjoiIiAvTHB3bNHQlIiIibksVHREREVdWAwsGupIqJTrvv/9+lTu89dZbLzoYERERsY+eurKtSonOuXdT/BGTyURZWZkj8YiIiIi93Lgi46gqJTrn3iQqIiIi4kocmqNTWFhofUGXiIiIXHoaurLN7qeuysrKmDZtGpdddhmBgYH8+OOPAEycOJGXX37Z6QGKiIiIDYaTNjdld6LzxBNPsGTJEmbOnIm3t7d1/5VXXslLL73k1OBEREREHGF3orNs2TIWLlzI3Xffjaenp3V/27Zt2bt3r1ODExERkT9ictLmnuyeo3PkyBGaN29+3n6LxUJJSYlTghIREZEq0jo6Ntld0YmNjeWLL744b//bb79N+/btnRKUiIiIiDPYXdFJTk4mMTGRI0eOYLFYePfdd0lLS2PZsmWsWrWqOmIUERGRC1FFxya7Kzq9e/fmgw8+4NNPPyUgIIDk5GT27NnDBx98wN/+9rfqiFFEREQu5Nzbyx3d3NRFraNz/fXXs3btWmfHIiIiIuJUF71g4NatW9mzZw9QPm+nQ4cOTgtKREREqsYwyjdH+3BXdic6hw8f5s477+Srr76ibt26AJw6dYpOnTrx5ptv0rhxY2fHKCIiIheiOTo22T1HZ8iQIZSUlLBnzx5ycnLIyclhz549WCwWhgwZUh0xioiIyIVojo5Ndld0NmzYwKZNm2jZsqV1X8uWLXnuuee4/vrrnRqciIiIiCPsTnQiIyMrXRiwrKyMiIgIpwQlIiIiVWMyyjdH+3BXdg9dzZo1ixEjRrB161brvq1bt/LII48we/ZspwYnIiIif0Av9bSpShWdevXqYTL9On5XUFBAx44dqVOn/PTS0lLq1KnDP/7xD/r06VMtgYqIiIjYq0qJzrPPPlvNYYiIiMhFccZk4j/7ZOTExMTqjkNEREQuhh4vt+miFwwEKCwspLi4uMI+s9nsUEAiIiIizmL3ZOSCggIeeughGjVqREBAAPXq1auwiYiIyCWkycg22Z3ojBkzhvXr1/PCCy/g4+PDSy+9xJQpU4iIiGDZsmXVEaOIiIhciBIdm+weuvrggw9YtmwZXbp04d577+X666+nefPmNGnShNdee4277767OuIUERERsZvdFZ2cnByaNm0KlM/HycnJAaBz585s3LjRudGJiIiIbXoFhE12JzpNmzYlPT0dgFatWvHWW28B5ZWecy/5FBERkUvj3MrIjm7uyu5E595772XHjh0AjBs3jvnz5+Pr68vIkSMZPXq00wMUERERGzRHxya75+iMHDnS+t/x8fHs3buXbdu20bx5c6666iqnBiciIiLiCIfW0QFo0qQJTZo0cUYsIiIiIk5VpURn3rx5Ve7w4YcfvuhgRERExD4mnPD2cqdEUjtVKdGZM2dOlTozmUxKdERERKTWqFKic+4pK7n0bpqUhKe3b02HIVIt6hak1HQIItXCYpRcuovppZ42OTxHR0RERGqQXuppk92Pl4uIiIi4ClV0REREXJkqOjYp0REREXFhzljZWCsji4iIiLigi0p0vvjiC+655x7i4uI4cuQIAK+++ipffvmlU4MTERGRP6BXQNhkd6LzzjvvkJCQgJ+fH9u3b6eoqAiA3NxcnnzySacHKCIiIjYo0bHJ7kRn+vTpLFiwgEWLFuHl5WXdf9111/Htt986NTgRERGxTW8vt83uRCctLY0bbrjhvP3BwcGcOnXKGTGJiIhILbZx40Z69epFREQEJpOJlStXVmg3DIPk5GTCw8Px8/MjPj6effv2ndfPhx9+SMeOHfHz86NevXr06dOnQntGRgY9e/bE39+fRo0aMXr0aEpLS+2K1e5EJywsjP3795+3/8svv6Rp06b2diciIiKOOLcysqObHQoKCmjbti3z58+vtH3mzJnMmzePBQsWsHnzZgICAkhISKCwsNB6zDvvvMPAgQO599572bFjB1999RV33XWXtb2srIyePXtSXFzMpk2bWLp0KUuWLCE5OdmuWO1+vHzo0KE88sgjvPLKK5hMJjIzM0lJSeGxxx5j4sSJ9nYnIiIijqiBdXR69OhBjx49Ku/KMHj22WeZMGECvXv3BmDZsmWEhoaycuVKBgwYQGlpKY888gizZs0iKSnJem5sbKz1v9esWcPu3bv59NNPCQ0NpV27dkybNo2xY8cyefJkvL29qxSr3RWdcePGcdddd3HzzTeTn5/PDTfcwJAhQ7jvvvsYMWKEvd2JiIhILZGXl1dhO/fAkT3S09PJysoiPj7eui84OJiOHTuSklL+frtvv/2WI0eO4OHhQfv27QkPD6dHjx58//331nNSUlJo06YNoaGh1n0JCQnk5eWxa9euKsdjd6JjMpl4/PHHycnJ4fvvv+frr7/m+PHjTJs2zd6uRERExEHOnIwcGRlJcHCwdZsxY4bd8WRlZQFUSFDOfT7X9uOPPwIwefJkJkyYwKpVq6hXrx5dunQhJyfH2k9lffz2GlVx0Ssje3t7VygxiYiISA1w4tDVoUOHMJvN1t0+Pj4Odlw5i8UCwOOPP06/fv0AWLx4MY0bN2bFihXcd999TruW3YlO165dMZkuPGlp/fr1DgUkIiIiNcNsNldIdC5GWFgYANnZ2YSHh1v3Z2dn065dOwDr/t8WTHx8fGjatCkZGRnWfrZs2VKh7+zs7ArXqAq7h67atWtH27ZtrVtsbCzFxcV8++23tGnTxt7uRERExBHOGLZy4jo60dHRhIWFsW7dOuu+vLw8Nm/eTFxcHAAdOnTAx8eHtLQ06zElJSUcPHiQJk2aABAXF8fOnTs5duyY9Zi1a9diNpvtGlGyu6IzZ86cSvdPnjyZ/Px8e7sTERERR9TAU1f5+fkVlppJT08nNTWVkJAQoqKiePTRR5k+fToxMTFER0czceJEIiIirOvkmM1m7r//fiZNmkRkZCRNmjRh1qxZANxxxx0AdOvWjdjYWAYOHMjMmTPJyspiwoQJDB8+3K4hNae9vfyee+7hmmuuYfbs2c7qUkRERGqhrVu30rVrV+vnUaNGAZCYmMiSJUsYM2YMBQUFDBs2jFOnTtG5c2dWr16Nr6+v9ZxZs2ZRp04dBg4cyNmzZ+nYsSPr16+nXr16AHh6erJq1SoeeOAB4uLiCAgIIDExkalTp9oVq9MSnZSUlAo3ICIiIpdADVR0unTpgmFc+CSTycTUqVNtJiVeXl7Mnj3bZoGkSZMmfPTRR/YF9zt2Jzp9+/at8NkwDI4ePcrWrVu1YKCIiMgl5ox3Vbnzu67sTnSCg4MrfPbw8KBly5ZMnTqVbt26OS0wEREREUfZleiUlZVx77330qZNG+sYmoiIiEhtZdfj5Z6ennTr1k1vKRcREaktDCdtbsrudXSuvPJK69LNIiIiUrOc+QoId2R3ojN9+nQee+wxVq1axdGjR897AZiIiIhIbVHlOTpTp07ln//8J//3f/8HwK233lrhVRCGYWAymSgrK3N+lCIiInJhblyRcVSVE50pU6Zw//3389lnn1VnPCIiImKPGlhHx5VUOdE5tzDQjTfeWG3BiIiIiDiTXY+X23pruYiIiFx6WjDQNrsSnRYtWvxhspOTk+NQQCIiImIHDV3ZZFeiM2XKlPNWRhYRERGprexKdAYMGECjRo2qKxYRERGxk4aubKtyoqP5OSIiIrWQhq5ssvupKxEREalFlOjYVOVEx2KxVGccIiIiIk5n1xwdERERqV00R8c2JToiIiKuTENXNtn9Uk8RERERV6GKjoiIiCtTRccmJToiIiIuTHN0bNPQlYiIiLgtVXRERERcmYaubFKiIyIi4sI0dGWbhq5ERETEbamiIyIi4so0dGWTEh0RERFXpkTHJiU6IiIiLsz0y+ZoH+5Kc3RERETEbamiIyIi4so0dGWTEh0REREXpsfLbdPQlYiIiLgtVXRERERcmYaubFKiIyIi4urcOFFxlIauRERExG2poiMiIuLCNBnZNiU6IiIirkxzdGzS0JWIiIi4LVV0REREXJiGrmxToiMiIuLKNHRlkxIdERERF6aKjm2aoyMiIiJuSxUdERERV6ahK5uU6IiIiLgyJTo2aehKRERE3JYqOiIiIi5Mk5FtU6IjIiLiyjR0ZZOGrkRERMRtKdERERFxYSbDcMpmj40bN9KrVy8iIiIwmUysXLmyQrthGCQnJxMeHo6fnx/x8fHs27ev0r6Kiopo164dJpOJ1NTUCm3fffcd119/Pb6+vkRGRjJz5ky74gQlOiIiIq7NcNJmh4KCAtq2bcv8+fMrbZ85cybz5s1jwYIFbN68mYCAABISEigsLDzv2DFjxhAREXHe/ry8PLp160aTJk3Ytm0bs2bNYvLkySxcuNCuWDVHR0REROzSo0cPevToUWmbYRg8++yzTJgwgd69ewOwbNkyQkNDWblyJQMGDLAe+/HHH7NmzRreeecdPv744wr9vPbaaxQXF/PKK6/g7e3NFVdcQWpqKs888wzDhg2rcqyq6IiIiLiwc09dObpBeRXlt1tRUZHd8aSnp5OVlUV8fLx1X3BwMB07diQlJcW6Lzs7m6FDh/Lqq6/i7+9/Xj8pKSnccMMNeHt7W/clJCSQlpbGyZMnqxyPEh0RERFX5sShq8jISIKDg63bjBkz7A4nKysLgNDQ0Ar7Q0NDrW2GYTB48GDuv/9+/vrXv16wn8r6+O01qkJDVyIiIi7MmevoHDp0CLPZbN3v4+PjWMcX8Nxzz3H69GnGjx9fLf3/lio6IiIiAoDZbK6wXUyiExYWBpQPTf1Wdna2tW39+vWkpKTg4+NDnTp1aN68OQB//etfSUxMtPZTWR+/vUZVKNERERFxZTXw1JUt0dHRhIWFsW7dOuu+vLw8Nm/eTFxcHADz5s1jx44dpKamkpqaykcffQTA8uXLeeKJJwCIi4tj48aNlJSUWPtZu3YtLVu2pF69elWOR0NXIiIiLqwmXgGRn5/P/v37rZ/T09NJTU0lJCSEqKgoHn30UaZPn05MTAzR0dFMnDiRiIgI+vTpA0BUVFSF/gIDAwFo1qwZjRs3BuCuu+5iypQpJCUlMXbsWL7//nvmzp3LnDlz7IpViY6IiIjYZevWrXTt2tX6edSoUQAkJiayZMkSxowZQ0FBAcOGDePUqVN07tyZ1atX4+vrW+VrBAcHs2bNGoYPH06HDh1o0KABycnJdj1aDkp0REREXFsNvOuqS5cuGDZWUzaZTEydOpWpU6dWqb/LL7+80v6uuuoqvvjiC/uC+x0lOiIiIi7Ond8+7ihNRhYRERG3pYqOiIiIKzOM8s3RPtyUEh0REREXVhNPXbkSDV2JiIiI21JFR0RExJXVwFNXrkSJjoiIiAszWco3R/twV0p0xG20i87knht20Oqy4zQ0n2H0sgQ27o7+zREGw/62ld5X7yHQr4jvDoYxc+X1HDpR97y+vDzLeGX4u7SIOME9c29n39EG1rbmYScY3fsLWjc+zqkCX97adCX/3di++m9Q/vSu7JjPHQ8eJ6bNGeqHlTL5H5eTsjr4N0cYDBqdTfe7ThBoLmP31gDmjWtMZvqv7ysKqlvKg9OP0PFveRgW+PKjurwwMYLCM54AXBWXT99hx2nR7gwBQRaOpHuz4vlGfPZe1Zfcl0tMFR2b3HKOzsGDBzGZTKSmptZ0KHIJ+XmVsu9ofWb97/pK2wfemEr/Tjv598rrSZrfl8ISL+b+40O865Sed+yI/0vh5zz/8/YH+BQzL2kVR08FkfhcP577KI6h8dvoc81up9+PyO/5+lv4cZcv//lX40rb+w8/Tu9/HOe5cY155JYYCs948OTrP+Ll8+uf62P/k0GTloWMH9CU5MRo2nTM59FZh63tsX8t4Mfdvkwfejn339yCNW+GMHpeBh3j86r9/kSqg1smOjWtsLCQ4cOHU79+fQIDA+nXr995b2AV50v5IYoX11zDhl3RlbQaDLhuJ4vX/4WNu6PZn1Wfycu70sB8hhtjD1Y4Mq5FBtfEHGbeR3Hn9ZLQbh91PC1Mf7sL6cdCWPtdc5ZvupI7O39XPTcl8htbPzOzdGY4mypUcc4x6DPkOG/MDSXlk2DS9/gx8+Eo6oeW0Kl7LgCRzQu5+qbTzPlnJGnbA9i1JZDnJ1zGjb1PERJa/uLEN58LZdmscHZvDeDoTz6sfLkhWz8L4rr/O3XpblTscu6pK0c3d6VEpxqMHDmSDz74gBUrVrBhwwYyMzPp27dvTYf1pxYRcpoG5jNs2f/rX8IFRT7sOtSINk2yrPtCAs/wr34bmLz8JgpLzh/ZbdMkm9T0cErLPK37Nv8QyeWNThHkV1S9NyFiQ1hUMfVDS/n2iyDrvjOnPdm73Z/WHc4A0PqvBZw+5cm+736tVn77RRCGBVq1P3PBvgPMZZw+pZkOtda5dXQc3dyUyyY6FouFmTNn0rx5c3x8fIiKirK+2v33ysrKSEpKIjo6Gj8/P1q2bMncuXMrHPP5559zzTXXEBAQQN26dbnuuuv46aefANixYwddu3YlKCgIs9lMhw4d2Lp1a6XXys3N5eWXX+aZZ57hpptuokOHDixevJhNmzbx9ddfO/cfQaqsfmD5D/GcfL8K+3Py/QgJPPvLJ4OJd3zGu5tj2Xuk0QX7qayP315DpCaENCofgj11vGJCcup4HUIalVdrQhqWcupExXZLmYnTp3495vdu6HWKFm3PsubNkGqIWqT6uWyKPn78eBYtWsScOXPo3LkzR48eZe/evZUea7FYaNy4MStWrKB+/fps2rSJYcOGER4eTv/+/SktLaVPnz4MHTqUN954g+LiYrZs2YLJZALg7rvvpn379rzwwgt4enqSmpqKl5dXpdfatm0bJSUlxMfHW/e1atWKqKgoUlJSuPbaays9r6ioiKKiXysCeXkaD7/U+nf6ngCfEpZ+ponFIgBtO+XzzzmHmDu6MT/9UPW3TsulpQUDbXPJROf06dPMnTuX//znPyQmJgLQrFkzOnfuXOnxXl5eTJkyxfo5OjqalJQU3nrrLfr3709eXh65ubnccsstNGvWDIDWrVtbj8/IyGD06NG0atUKgJiYmAvGlpWVhbe3N3Xr1q2wPzQ0lKysrMpPAmbMmFEhRnGuE/nlpfqQwLOcOB1g3R8SeJZ9R+sD8NdmR7gyKpsvpi+qcO6Sh97hk9QYpq64iRP5/r+pAP3ax2+vIVITco6V/ziv27CUnGO//iFWt2EpB3aVVx1zjtehbv2Kk+89PA2C6lY8B6DNtflMWZrOgkkRfPq2qjm1mp66ssklh6727NlDUVERN998c5XPmT9/Ph06dKBhw4YEBgaycOFCMjIyAAgJCWHw4MEkJCTQq1cv5s6dy9GjR63njho1iiFDhhAfH89TTz3FgQMHnH5P48ePJzc317odOnTI6df4M8vMCeLnPH+ubn7Eui/Ap5grIo+x86cwAJ5+/zrumXsHA+eVb6OW/B8AE974Gws+uQaAnT+F0i76KJ4eZdZ+rok5zMFjdTl91geRmpKV4c2J7Dq073zaus8/sIxW7c+wZ1t5Er5nawBBdcto3ubXYdZ2nfMxecDe7b8m6lfF5TPt1XRefiKcj1+rf+luQqQauGSi4+fn98cH/cabb77JY489RlJSEmvWrCE1NZV7772X4uJi6zGLFy8mJSWFTp06sXz5clq0aGGdUzN58mR27dpFz549Wb9+PbGxsbz33nuVXissLIzi4mJOnTpVYX92djZhYWEXjNHHxwez2VxhE/v4eZcQE/4zMeE/AxARkkdM+M+EBp8GTLz5VRvuvWkb17c+SLPQE0zqv56f8/zZsPtyALJzg/gxO8S6Zfxc/mTL4RNmjuUFAvBJanNKyzyYcPsGohvlEH/Vfv5+3U7e+PKqmrhl+ZPx9S+j6RVnaXpFeRUxLLKYplecpeFlxYCJlS815M5HjnFtt1wub3WW0fMyOJHtZX1K69B+X75ZH8Sjsw/Tst0ZYq8uYPj0w2z4X11ysssrOm07lSc5/3u5AV9+GEy9hiXUa1hCUN3zl2GQ2kFPXdnmkkNXMTEx+Pn5sW7dOoYMGfKHx3/11Vd06tSJBx980LqvsqpM+/btad++PePHjycuLo7XX3/dOqemRYsWtGjRgpEjR3LnnXeyePFibrvttvP66NChA15eXqxbt45+/foBkJaWRkZGBnFx5z+uLM7TuvExXhj2gfXzyFtSAFi1rQXTVtzEqxva4eddyvi+Gwj0LWbHwTAeWdyT4tKqfxsUFPnw8Mu3MLr3Fywd8Q65Z3x5eV0HVm6Jdfr9iPxei7ZnmfXOrz+77p+SCcCa5fV4emQUb81viK+/hUdmHibQXMaubwJ4/O6mlBT9+jftvx+KYvgTR3jqrQO/LBgYzPMTLrO2x9+Rg6+/hQEPH2PAw8es+3dsCmDM7c0vwV2K3fT2cptcMtHx9fVl7NixjBkzBm9vb6677jqOHz/Orl27SEpKOu/4mJgYli1bxieffEJ0dDSvvvoq33zzDdHR5eutpKens3DhQm699VYiIiJIS0tj3759DBo0iLNnzzJ69Ghuv/12oqOjOXz4MN988401ifm94OBgkpKSGDVqFCEhIZjNZkaMGEFcXNwFJyKLc3z742V0HHe/jSNMLFx7NQvXXl2l/o6eNFfa3/6s+tz3Yp+LC1LEAd+lBJIQ0dbGESaWzQpj2awLV49Pn6rDU8ObXLD96ZFRPD0yyoEoRWoXl0x0ACZOnEidOnVITk4mMzOT8PBw7r+/8l9y9913H9u3b+fvf/87JpOJO++8kwcffJCPP/4YAH9/f/bu3cvSpUs5ceIE4eHhDB8+nPvuu4/S0lJOnDjBoEGDyM7OpkGDBvTt29fmxOE5c+bg4eFBv379KCoqIiEhgeeff75a/h1EROTPTU9d2WYyDDeuV7mwvLw8goODaX/nE3h667FOcU91l6XUdAgi1aLUKOFz/kdubm61zbk893sirvtU6ng59nuitKSQlNXJ1RpvTXHZio6IiIioovNHXPKpKxEREZGqUEVHRETElVmM8s3RPtyUEh0RERFXppWRbdLQlYiIiLgtVXRERERcmAknTEZ2SiS1kxIdERERV6aVkW3S0JWIiIi4LVV0REREXJjW0bFNiY6IiIgr01NXNmnoSkRERNyWKjoiIiIuzGQYmBycTOzo+bWZEh0RERFXZvllc7QPN6VER0RExIWpomOb5uiIiIiI21JFR0RExJXpqSublOiIiIi4Mq2MbJOGrkRERMRtqaIjIiLiwrQysm1KdERERFyZhq5s0tCViIiIuC1VdERERFyYyVK+OdqHu1KiIyIi4so0dGWThq5ERETEbamiIyIi4sq0YKBNSnRERERcmN51ZZsSHREREVemOTo2aY6OiIiIuC0lOiIiIq7MACwObnYWdDZu3EivXr2IiIjAZDKxcuXKiiEZBsnJyYSHh+Pn50d8fDz79u2zth88eJCkpCSio6Px8/OjWbNmTJo0ieLi4gr9fPfdd1x//fX4+voSGRnJzJkz7QsUJToiIiIu7dwcHUc3exQUFNC2bVvmz59fafvMmTOZN28eCxYsYPPmzQQEBJCQkEBhYSEAe/fuxWKx8OKLL7Jr1y7mzJnDggUL+Ne//mXtIy8vj27dutGkSRO2bdvGrFmzmDx5MgsXLrQrVs3REREREbv06NGDHj16VNpmGAbPPvssEyZMoHfv3gAsW7aM0NBQVq5cyYABA+jevTvdu3e3ntO0aVPS0tJ44YUXmD17NgCvvfYaxcXFvPLKK3h7e3PFFVeQmprKM888w7Bhw6ocqyo6IiIirszg1wnJF72Vd5WXl1dhKyoqsjuc9PR0srKyiI+Pt+4LDg6mY8eOpKSkXPC83NxcQkJCrJ9TUlK44YYb8Pb2tu5LSEggLS2NkydPVjkeJToiIiKuzOEk59entiIjIwkODrZuM2bMsDucrKwsAEJDQyvsDw0Ntbb93v79+3nuuee47777KvRTWR+/vUZVaOhKREREADh06BBms9n62cfHp9qveeTIEbp3784dd9zB0KFDnd6/KjoiIiKuzNEnrs5tgNlsrrBdTKITFhYGQHZ2doX92dnZ1rZzMjMz6dq1K506dTpvknFYWFilffz2GlWhREdERMSF1cRTV7ZER0cTFhbGunXrrPvy8vLYvHkzcXFx1n1HjhyhS5cudOjQgcWLF+PhUTEliYuLY+PGjZSUlFj3rV27lpYtW1KvXr0qx6NER0RExJU5cY5OVeXn55OamkpqaipQPgE5NTWVjIwMTCYTjz76KNOnT+f9999n586dDBo0iIiICPr06QP8muRERUUxe/Zsjh8/TlZWVoW5N3fddRfe3t4kJSWxa9culi9fzty5cxk1apRdsWqOjoiIiNhl69atdO3a1fr5XPKRmJjIkiVLGDNmDAUFBQwbNoxTp07RuXNnVq9eja+vL1Bemdm/fz/79++ncePGFfo2fkm6goODWbNmDcOHD6dDhw40aNCA5ORkux4tBzAZhhPrVeI0eXl5BAcH0/7OJ/D09q3pcESqRd1lF37UVMSVlRolfM7/yM3NrTC515nO/Z64OfYx6ng6Nmm4tKyIdbtnV2u8NUUVHREREVeml3rapDk6IiIi4rZU0REREXFlFsDkhD7clBIdERERF+aMx8Od+Xh5baOhKxEREXFbquiIiIi4Mk1GtkmJjoiIiCuzGGByMFGxuG+io6ErERERcVuq6IiIiLgyDV3ZpERHRETEpTkh0UGJjoiIiNRGqujYpDk6IiIi4rZU0REREXFlFgOHh57c+KkrJToiIiKuzLCUb4724aY0dCUiIiJuSxUdERERV6bJyDYp0REREXFlmqNjk4auRERExG2poiMiIuLKNHRlkxIdERERV2bghETHKZHUShq6EhEREbelio6IiIgr09CVTUp0REREXJnFAji44J/FfRcMVKIjIiLiylTRsUlzdERERMRtqaIjIiLiylTRsUmJjoiIiCvTysg2aehKRERE3JYqOiIiIi7MMCwYhmNPTTl6fm2mREdERMSVGYbjQ09uPEdHQ1ciIiLitlTRERERcWWGEyYju3FFR4mOiIiIK7NYwOTgHBs3nqOjoSsRERFxW6roiIiIuDINXdmkREdERMSFGRYLhoNDV3q8XERERGonVXRs0hwdERERcVuq6IiIiLgyiwEmVXQuRImOiIiIKzMMwNHHy9030dHQlYiIiLgtVXRERERcmGExMBwcujLcuKKjREdERMSVGRYcH7py38fLNXQlIiIibksVHRERERemoSvblOiIiIi4Mg1d2aREp5Y6l12XlRTWcCQi1afUKKnpEESqRSnlX9uXolJSSonDCyOfi9cdKdGppU6fPg3Ad29Pq+FIRETkYp0+fZrg4OBq6dvb25uwsDC+zPrIKf2FhYXh7e3tlL5qE5PhzgNzLsxisZCZmUlQUBAmk6mmw3F7eXl5REZGcujQIcxmc02HI+J0+hq/tAzD4PTp00RERODhUX3P/RQWFlJcXOyUvry9vfH19XVKX7WJKjq1lIeHB40bN67pMP50zGazfgmIW9PX+KVTXZWc3/L19XXL5MSZ9Hi5iIiIuC0lOiIiIuK2lOiIAD4+PkyaNAkfH5+aDkWkWuhrXP6sNBlZRERE3JYqOiIiIuK2lOiIiIiI21KiIy7j4MGDmEwmUlNTazoUkRqh7wER+ynREamihQsX0qVLF8xmMyaTiVOnTlX53IMHD5KUlER0dDR+fn40a9aMSZMmOW2hL5FLobCwkOHDh1O/fn0CAwPp168f2dnZNR2WiE1KdESq6MyZM3Tv3p1//etfdp+7d+9eLBYLL774Irt27WLOnDksWLDgovoSqSkjR47kgw8+YMWKFWzYsIHMzEz69u1b02GJ2GaI1CJlZWXGv//9b6NZs2aGt7e3ERkZaUyfPt0wDMNIT083AGP79u2GYRhGaWmp8Y9//MO4/PLLDV9fX6NFixbGs88+W6G/zz77zLj66qsNf39/Izg42OjUqZNx8OBBwzAMIzU11ejSpYsRGBhoBAUFGX/5y1+Mb7755g9j/OyzzwzAOHnypEP3OnPmTCM6OtqhPsT91NbvgVOnThleXl7GihUrrPv27NljAEZKSko1/EuIOIdeASG1yvjx41m0aBFz5syhc+fOHD16lL1791Z6rMVioXHjxqxYsYL69euzadMmhg0bRnh4OP3796e0tJQ+ffowdOhQ3njjDYqLi9myZYv13WF333037du354UXXsDT05PU1FS8vLwu2b3m5uYSEhJyya4nrqG2fg9s27aNkpIS4uPjrftatWpFVFQUKSkpXHvttc7/xxBxhprOtETOycvLM3x8fIxFixZV2v77v2YrM3z4cKNfv36GYRjGiRMnDMD4/PPPKz02KCjIWLJkid1xOqOis2/fPsNsNhsLFy686D7E/dTm74HXXnvN8Pb2Pm//1VdfbYwZM6ZKfYjUBM3RkVpjz549FBUVcfPNN1f5nPnz59OhQwcaNmxIYGAgCxcuJCMjA4CQkBAGDx5MQkICvXr1Yu7cuRw9etR67qhRoxgyZAjx8fE89dRTHDhwwOn3VJkjR47QvXt37rjjDoYOHXpJrimu4c/yPSByKSnRkVrDz8/PruPffPNNHnvsMZKSklizZg2pqance++9FZ5kWrx4MSkpKXTq1Inly5fTokULvv76awAmT57Mrl276NmzJ+vXryc2Npb33nvPqff0e5mZmXTt2pVOnTqxcOHCar2WuJ7a/D0QFhZGcXHxeU8bZmdnExYWZt+NilxKNV1SEjnn7Nmzhp+fX5XL9g899JBx0003VTjm5ptvNtq2bXvBa1x77bXGiBEjKm0bMGCA0atXrz+M82KHrg4fPmzExMQYAwYMMEpLS+06V/4cavP3wLnJyG+//bZ13969ezUZWWo9TUaWWsPX15exY8cyZswYvL29ue666zh+/Di7du0iKSnpvONjYmJYtmwZn3zyCdHR0bz66qt88803REdHA5Cens7ChQu59dZbiYiIIC0tjX379jFo0CDOnj3L6NGjuf3224mOjubw4cN888039OvX74LxZWVlkZWVxf79+wHYuXMnQUFBREVF/eGk4iNHjtClSxeaNGnC7NmzOX78uLVNfw3LObX5eyA4OJikpCRGjRpFSEgIZrOZESNGEBcXp4nIUrvVdKYl8ltlZWXG9OnTjSZNmhheXl5GVFSU8eSTTxqGcf5fs4WFhcbgwYON4OBgo27dusYDDzxgjBs3zvrXbFZWltGnTx8jPDzc8Pb2Npo0aWIkJycbZWVlRlFRkTFgwAAjMjLS8Pb2NiIiIoyHHnrIOHv27AVjmzRpkgGcty1evPgP72vx4sWVnqtvQfm92vw9cPbsWePBBx806tWrZ/j7+xu33XabcfTo0er+JxFxiN5eLiIiIm5Lk5FFRETEbSnREXGCJ598ksDAwEq3Hj161HR4IiJ/Whq6EnGCnJwccnJyKm3z8/Pjsssuu8QRiYgIKNERERERN6ahKxEREXFbSnRERETEbSnREREREbelREdERETclhIdEbmgwYMH06dPH+vnLl268Oijj17yOD7//HNMJtN5L5T8LZPJxMqVK6vc5+TJk2nXrp1DcR08eBCTyURqaqpD/YhI9VGiI+JiBg8ejMlkwmQy4e3tTfPmzZk6dSqlpaXVfu13332XadOmVenYqiQnIiLVTS/1FHFB3bt3Z/HixRQVFfHRRx8xfPhwvLy8GD9+/HnHFhcX4+3t7ZTr/tHLS0VEahtVdERckI+PD2FhYTRp0oQHHniA+Ph43n//feDX4aYnnniCiIgIWrZsCcChQ4fo378/devWJSQkhN69e3Pw4EFrn2VlZYwaNYq6detSv359xowZw++X2fr90FVRURFjx44lMjISHx8fmjdvzssvv8zBgwfp2rUrAPXq1cNkMjF48GAALBYLM2bMIDo6Gj8/P9q2bcvbb79d4TofffQRLVq0wM/Pj65du1aIs6rGjh1LixYt8Pf3p2nTpkycOJGSkpLzjnvxxReJjIzE39+f/v37k5ubW6H9pZdeonXr1vj6+tKqVSuef/55u2MRkZqjREfEDfj5+VFcXGz9vG7dOtLS0li7di2rVq2ipKSEhIQEgoKC+OKLL/jqq68IDAyke/fu1vOefvpplixZwiuvvMKXX35JTk4O7733ns3rDho0iDfeeIN58+axZ88eXnzxRQIDA4mMjOSdd94BIC0tjaNHjzJ37lwAZsyYwbJly1iwYAG7du1i5MiR3HPPPWzYsAEoT8j69u1Lr169SE1NZciQIYwbN87uf5OgoCCWLFnC7t27mTt3LosWLWLOnDkVjtm/fz9vvfUWH3zwAatXr2b79u08+OCD1vbXXnuN5ORknnjiCfbs2cOTTz7JxIkTWbp0qd3xiEgNqcE3p4vIRUhMTDR69+5tGIZhWCwWY+3atYaPj4/x2GOPWdtDQ0ONoqIi6zmvvvqq0bJlS8NisVj3FRUVGX5+fsYnn3xiGIZhhIeHGzNnzrS2l5SUGI0bN7ZeyzAM48YbbzQeeeQRwzAMIy0tzQCMtWvXVhrnZ599ZgDGyZMnrfsKCwsNf39/Y9OmTRWOTUpKMu68807DMAxj/PjxRmxsbIX2sWPHntfX7wHGe++9d8H2WbNmGR06dLB+njRpkuHp6WkcPnzYuu/jjz82PDw8jKNHjxqGYRjNmjUzXn/99Qr9TJs2zYiLizMMwzDS09MNwNi+ffsFrysiNUtzdERc0KpVqwgMDKSkpASLxcJdd93F5MmTre1t2rSpMC9nx44d7N+/n6CgoAr9FBYWcuDAAXJzczl69CgdO3a0ttWpU4e//vWv5w1fnZOamoqnpyc33nhjlePev38/Z86c4W9/+1uF/cXFxbRv3x6APXv2VIgDIC4ursrXOGf58uXMmzePAwcOkJ+fT2lpKWazucIxUVFRFd5DFhcXh8ViIS0tjaCgIA4cOEBSUhJDhw61HlNaWkpwcLDd8YhIzVCiI+KCunbtygsvvIC3tzcRERHUqVPxWzkgIKDC5/z8fDp06MBrr712Xl8NGza8qBj8/PzsPic/Px+ADz/88LwXnfr4+FxUHJVJSUnh7rvvZsqUKSQkJBAcHMybb77J008/bXesixYtOi/x8vT0dFqsIlK9lOiIuKCAgACaN29e5eP/8pe/sHz5cho1anReVeOc8PBwNm/ezA033ACUVy62bdvGX/7yl0qPb9OmDRaLhQ0bNhAfH39e+7mKUllZmXVfbGwsPj4+ZGRkXLAS1Lp1a+vE6nO+/vrrP77J39i0aRNNmjTh8ccft+776aefzjsuIyODzMxMIiIirNfx8PCgZcuWhIaGEhERwY8//sjdd99t1/VFpPbQZGSRP4G7776bBg0a0Lt3b7744gvS09P5/PPPefjhhzl8+DAAjzzyCE899RQrV65k7969PPjggzbXwLn88stJTEzkH//4BytXrrT2+dZbbwHQpEkTTCYTq1at4vjx4+Tn5xMUFMRjjz3GyJEjWbp0KQcOHODbb7/lueees07wvf/++9m3bx+jR48mLS2N119/nSVLlth1vzExMWRkZPDmm29y4MAB5s2bV+nEal9fXxITE9mxYwdffPEFDz/8MP379ycsLAyAKVOmMGPGDObNm8cPP/zAzp07Wbx4Mc8884xd8YhIzVGiI/In4O/vz8aNG4mKiqJv3760bt2apKQkCgsLrRWef/7znwwcOJDExETi4uIICgritttus9nvCy+8wO23386DDz5Iq1atGDp0KAUFBQBcdtllTJkyhXHjxhEaGspDDz0EwLRp05g4cSIzZsygdevWdO/enQ8//JDo6GigfN7MO++8w8qVK2nbti0LFizgySeftOt+b731VkaOHMlDDz1Eu3bt2LRpExMnTjzvuObNm9O3b1/+7//+j27dunHVVVdVeHx8yJAhvPTSSyxevJg2bdpw4403smTJEmusIlL7mYwLzTQUERERcXGq6IiIiIjbUqIjIiIibkuJjoiIiLgtJToiIiLitpToiIiIiNtSoiMiIiJuS4mOiIiIuC0lOiIiIuK2lOiIiIiI21KiIyIiIm5LiY6IiIi4LSU6IiIi4rb+H5MYHnEEmto+AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "534/534 [==============================] - 1s 2ms/step - loss: 1.5619 - accuracy: 0.8876\n",
            "[1.561902403831482, 0.8875542283058167]\n",
            "534/534 [==============================] - 1s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 1_2       0.88      0.89      0.89      8533\n",
            "     class 0       0.89      0.88      0.89      8533\n",
            "\n",
            "    accuracy                           0.89     17066\n",
            "   macro avg       0.89      0.89      0.89     17066\n",
            "weighted avg       0.89      0.89      0.89     17066\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/new_df/best_model_by_class0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LthcmTlDK6hc",
        "outputId": "54ebade5-11de-4b6e-f60a-2ed734290567"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}
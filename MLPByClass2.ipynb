{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8pDWuXqMDmFgKlQ7eKWC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vajihe-ameri/predict-software-bugs-in-java-classes/blob/main/MLPByClass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ln9EzsXCRVdb",
        "outputId": "8c9eef17-ffb4-4266-a640-1b8de43a75eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2952 sha256=238d140c85115c6357d24e99e77f78d6097edcfc3ebd7751b76ce3771a39817f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn pandas\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_2.csv\")\n",
        "train_features = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_2.csv\")\n",
        "test_target = pd.read_csv(\"/content/drive/MyDrive/new_df/test_set_NB_2.csv\")\n",
        "train_target = pd.read_csv(\"/content/drive/MyDrive/new_df/train_set_NB_2.csv\")"
      ],
      "metadata": {
        "id": "2YhTDhGuRXr_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons\n",
        "f_measure = tensorflow_addons.metrics.F1Score(num_classes=2, average='macro', threshold=0.5)"
      ],
      "metadata": {
        "id": "wAUL5I-KRXuu",
        "outputId": "d6dae687-f519-4b1e-9d61-d343f6352a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/612.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/612.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "file_path = \"/content/drive/MyDrive/new_df/best_model_by_class2.hdf5\""
      ],
      "metadata": {
        "id": "QGYVJakTRXx6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(100, kernel_initializer = 'uniform', activation = 'relu', input_dim = train_features.shape[1]))\n",
        "model.add(Dense(80, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(60, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(40, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(20, kernel_initializer = 'uniform', kernel_constraint=max_norm(2.), activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1,save_best_only=True, mode='auto', period=1)"
      ],
      "metadata": {
        "id": "9mnmLA23RX1Z",
        "outputId": "49658c49-1b26-4f74-f85a-aa5d94479b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 600, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_1', 'class 2']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L1aZjzNKSJsx",
        "outputId": "896b02c2-e295-4720-d3c2-5a66d64e530f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1424 - accuracy: 0.9351\n",
            "Epoch 3752: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1430 - accuracy: 0.9341 - val_loss: 0.1461 - val_accuracy: 0.9625\n",
            "Epoch 3753/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9356\n",
            "Epoch 3753: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1363 - accuracy: 0.9357 - val_loss: 0.1721 - val_accuracy: 0.9473\n",
            "Epoch 3754/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1338 - accuracy: 0.9368\n",
            "Epoch 3754: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1334 - accuracy: 0.9376 - val_loss: 0.1384 - val_accuracy: 0.9697\n",
            "Epoch 3755/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9400\n",
            "Epoch 3755: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1302 - accuracy: 0.9400 - val_loss: 0.1606 - val_accuracy: 0.9586\n",
            "Epoch 3756/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9390\n",
            "Epoch 3756: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1298 - accuracy: 0.9388 - val_loss: 0.1787 - val_accuracy: 0.9354\n",
            "Epoch 3757/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1303 - accuracy: 0.9395\n",
            "Epoch 3757: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1308 - accuracy: 0.9394 - val_loss: 0.1200 - val_accuracy: 0.9781\n",
            "Epoch 3758/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1303 - accuracy: 0.9393\n",
            "Epoch 3758: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1324 - accuracy: 0.9381 - val_loss: 0.1862 - val_accuracy: 0.9482\n",
            "Epoch 3759/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9382\n",
            "Epoch 3759: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1356 - accuracy: 0.9385 - val_loss: 0.1525 - val_accuracy: 0.9640\n",
            "Epoch 3760/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.9379\n",
            "Epoch 3760: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1347 - accuracy: 0.9375 - val_loss: 0.1330 - val_accuracy: 0.9746\n",
            "Epoch 3761/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1443 - accuracy: 0.9326\n",
            "Epoch 3761: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1439 - accuracy: 0.9330 - val_loss: 0.1625 - val_accuracy: 0.9616\n",
            "Epoch 3762/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9369\n",
            "Epoch 3762: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1350 - accuracy: 0.9368 - val_loss: 0.1759 - val_accuracy: 0.9469\n",
            "Epoch 3763/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1285 - accuracy: 0.9394\n",
            "Epoch 3763: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1284 - accuracy: 0.9398 - val_loss: 0.1410 - val_accuracy: 0.9699\n",
            "Epoch 3764/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1292 - accuracy: 0.9392\n",
            "Epoch 3764: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1298 - accuracy: 0.9389 - val_loss: 0.1478 - val_accuracy: 0.9631\n",
            "Epoch 3765/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1335 - accuracy: 0.9379\n",
            "Epoch 3765: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9367 - val_loss: 0.1589 - val_accuracy: 0.9595\n",
            "Epoch 3766/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1298 - accuracy: 0.9413\n",
            "Epoch 3766: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1305 - accuracy: 0.9405 - val_loss: 0.2059 - val_accuracy: 0.9302\n",
            "Epoch 3767/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9370\n",
            "Epoch 3767: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1346 - accuracy: 0.9371 - val_loss: 0.2702 - val_accuracy: 0.8896\n",
            "Epoch 3768/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.9388\n",
            "Epoch 3768: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1334 - accuracy: 0.9384 - val_loss: 0.1767 - val_accuracy: 0.9512\n",
            "Epoch 3769/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9377\n",
            "Epoch 3769: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1325 - accuracy: 0.9377 - val_loss: 0.1362 - val_accuracy: 0.9686\n",
            "Epoch 3770/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9363\n",
            "Epoch 3770: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1422 - accuracy: 0.9360 - val_loss: 0.1676 - val_accuracy: 0.9595\n",
            "Epoch 3771/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1575 - accuracy: 0.9295\n",
            "Epoch 3771: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1577 - accuracy: 0.9296 - val_loss: 0.3744 - val_accuracy: 0.8530\n",
            "Epoch 3772/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9289\n",
            "Epoch 3772: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1586 - accuracy: 0.9289 - val_loss: 0.1626 - val_accuracy: 0.9660\n",
            "Epoch 3773/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9334\n",
            "Epoch 3773: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1409 - accuracy: 0.9345 - val_loss: 0.1909 - val_accuracy: 0.9430\n",
            "Epoch 3774/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9382\n",
            "Epoch 3774: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1332 - accuracy: 0.9382 - val_loss: 0.1366 - val_accuracy: 0.9697\n",
            "Epoch 3775/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1286 - accuracy: 0.9409\n",
            "Epoch 3775: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1285 - accuracy: 0.9413 - val_loss: 0.1551 - val_accuracy: 0.9631\n",
            "Epoch 3776/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9409\n",
            "Epoch 3776: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1250 - accuracy: 0.9409 - val_loss: 0.1619 - val_accuracy: 0.9538\n",
            "Epoch 3777/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1242 - accuracy: 0.9412\n",
            "Epoch 3777: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1250 - accuracy: 0.9410 - val_loss: 0.1641 - val_accuracy: 0.9542\n",
            "Epoch 3778/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9380\n",
            "Epoch 3778: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1303 - accuracy: 0.9388 - val_loss: 0.1805 - val_accuracy: 0.9452\n",
            "Epoch 3779/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9393\n",
            "Epoch 3779: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9390 - val_loss: 0.1945 - val_accuracy: 0.9356\n",
            "Epoch 3780/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1371 - accuracy: 0.9375\n",
            "Epoch 3780: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1378 - accuracy: 0.9364 - val_loss: 0.1404 - val_accuracy: 0.9681\n",
            "Epoch 3781/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1424 - accuracy: 0.9338\n",
            "Epoch 3781: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1423 - accuracy: 0.9338 - val_loss: 0.1696 - val_accuracy: 0.9512\n",
            "Epoch 3782/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9403\n",
            "Epoch 3782: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1306 - accuracy: 0.9405 - val_loss: 0.1547 - val_accuracy: 0.9586\n",
            "Epoch 3783/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1292 - accuracy: 0.9389\n",
            "Epoch 3783: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1290 - accuracy: 0.9394 - val_loss: 0.1293 - val_accuracy: 0.9738\n",
            "Epoch 3784/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1256 - accuracy: 0.9410\n",
            "Epoch 3784: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1263 - accuracy: 0.9409 - val_loss: 0.1595 - val_accuracy: 0.9564\n",
            "Epoch 3785/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9405\n",
            "Epoch 3785: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1299 - accuracy: 0.9401 - val_loss: 0.1630 - val_accuracy: 0.9516\n",
            "Epoch 3786/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1300 - accuracy: 0.9414\n",
            "Epoch 3786: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1328 - accuracy: 0.9399 - val_loss: 0.1587 - val_accuracy: 0.9566\n",
            "Epoch 3787/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9368\n",
            "Epoch 3787: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1370 - accuracy: 0.9367 - val_loss: 0.2095 - val_accuracy: 0.9319\n",
            "Epoch 3788/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1342 - accuracy: 0.9368\n",
            "Epoch 3788: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1340 - accuracy: 0.9371 - val_loss: 0.1494 - val_accuracy: 0.9683\n",
            "Epoch 3789/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1378 - accuracy: 0.9358\n",
            "Epoch 3789: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1396 - accuracy: 0.9351 - val_loss: 0.1070 - val_accuracy: 0.9809\n",
            "Epoch 3790/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9330\n",
            "Epoch 3790: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1429 - accuracy: 0.9331 - val_loss: 0.1391 - val_accuracy: 0.9705\n",
            "Epoch 3791/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9343\n",
            "Epoch 3791: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1416 - accuracy: 0.9343 - val_loss: 0.2470 - val_accuracy: 0.9098\n",
            "Epoch 3792/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1340 - accuracy: 0.9384\n",
            "Epoch 3792: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1367 - accuracy: 0.9370 - val_loss: 0.1641 - val_accuracy: 0.9607\n",
            "Epoch 3793/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1304 - accuracy: 0.9385\n",
            "Epoch 3793: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1304 - accuracy: 0.9386 - val_loss: 0.1353 - val_accuracy: 0.9736\n",
            "Epoch 3794/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1351 - accuracy: 0.9346\n",
            "Epoch 3794: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1345 - accuracy: 0.9351 - val_loss: 0.1423 - val_accuracy: 0.9710\n",
            "Epoch 3795/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1290 - accuracy: 0.9417\n",
            "Epoch 3795: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1292 - accuracy: 0.9414 - val_loss: 0.2051 - val_accuracy: 0.9293\n",
            "Epoch 3796/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1298 - accuracy: 0.9393\n",
            "Epoch 3796: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1308 - accuracy: 0.9390 - val_loss: 0.1615 - val_accuracy: 0.9595\n",
            "Epoch 3797/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1279 - accuracy: 0.9393\n",
            "Epoch 3797: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1279 - accuracy: 0.9395 - val_loss: 0.1298 - val_accuracy: 0.9735\n",
            "Epoch 3798/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1293 - accuracy: 0.9403\n",
            "Epoch 3798: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9397 - val_loss: 0.2253 - val_accuracy: 0.9237\n",
            "Epoch 3799/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9391\n",
            "Epoch 3799: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1304 - accuracy: 0.9389 - val_loss: 0.1706 - val_accuracy: 0.9523\n",
            "Epoch 3800/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1310 - accuracy: 0.9380\n",
            "Epoch 3800: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1314 - accuracy: 0.9378 - val_loss: 0.1453 - val_accuracy: 0.9662\n",
            "Epoch 3801/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1297 - accuracy: 0.9399\n",
            "Epoch 3801: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9404 - val_loss: 0.1570 - val_accuracy: 0.9582\n",
            "Epoch 3802/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9382\n",
            "Epoch 3802: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1314 - accuracy: 0.9380 - val_loss: 0.1305 - val_accuracy: 0.9699\n",
            "Epoch 3803/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9394\n",
            "Epoch 3803: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1328 - accuracy: 0.9394 - val_loss: 0.1686 - val_accuracy: 0.9542\n",
            "Epoch 3804/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1359 - accuracy: 0.9358\n",
            "Epoch 3804: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1380 - accuracy: 0.9351 - val_loss: 0.1849 - val_accuracy: 0.9497\n",
            "Epoch 3805/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9343\n",
            "Epoch 3805: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1435 - accuracy: 0.9343 - val_loss: 0.2250 - val_accuracy: 0.9304\n",
            "Epoch 3806/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9377\n",
            "Epoch 3806: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1367 - accuracy: 0.9377 - val_loss: 0.1554 - val_accuracy: 0.9623\n",
            "Epoch 3807/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9404\n",
            "Epoch 3807: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9399 - val_loss: 0.1801 - val_accuracy: 0.9460\n",
            "Epoch 3808/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1336 - accuracy: 0.9390\n",
            "Epoch 3808: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1362 - accuracy: 0.9381 - val_loss: 0.1881 - val_accuracy: 0.9462\n",
            "Epoch 3809/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9357\n",
            "Epoch 3809: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1396 - accuracy: 0.9357 - val_loss: 0.2123 - val_accuracy: 0.9220\n",
            "Epoch 3810/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1330 - accuracy: 0.9377\n",
            "Epoch 3810: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1341 - accuracy: 0.9373 - val_loss: 0.2138 - val_accuracy: 0.9269\n",
            "Epoch 3811/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9350\n",
            "Epoch 3811: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1419 - accuracy: 0.9352 - val_loss: 0.1454 - val_accuracy: 0.9664\n",
            "Epoch 3812/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9364\n",
            "Epoch 3812: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1370 - accuracy: 0.9364 - val_loss: 0.2159 - val_accuracy: 0.9345\n",
            "Epoch 3813/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9360\n",
            "Epoch 3813: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1391 - accuracy: 0.9362 - val_loss: 0.1153 - val_accuracy: 0.9814\n",
            "Epoch 3814/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9349\n",
            "Epoch 3814: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1406 - accuracy: 0.9349 - val_loss: 0.1716 - val_accuracy: 0.9558\n",
            "Epoch 3815/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1415 - accuracy: 0.9340\n",
            "Epoch 3815: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1394 - accuracy: 0.9350 - val_loss: 0.1523 - val_accuracy: 0.9581\n",
            "Epoch 3816/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9404\n",
            "Epoch 3816: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1282 - accuracy: 0.9405 - val_loss: 0.1403 - val_accuracy: 0.9692\n",
            "Epoch 3817/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1275 - accuracy: 0.9416\n",
            "Epoch 3817: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1282 - accuracy: 0.9410 - val_loss: 0.2630 - val_accuracy: 0.9128\n",
            "Epoch 3818/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1336 - accuracy: 0.9358\n",
            "Epoch 3818: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1335 - accuracy: 0.9360 - val_loss: 0.1725 - val_accuracy: 0.9506\n",
            "Epoch 3819/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1284 - accuracy: 0.9413\n",
            "Epoch 3819: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1285 - accuracy: 0.9406 - val_loss: 0.1644 - val_accuracy: 0.9530\n",
            "Epoch 3820/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9402\n",
            "Epoch 3820: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1295 - accuracy: 0.9402 - val_loss: 0.1577 - val_accuracy: 0.9547\n",
            "Epoch 3821/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1262 - accuracy: 0.9403\n",
            "Epoch 3821: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1268 - accuracy: 0.9402 - val_loss: 0.1670 - val_accuracy: 0.9473\n",
            "Epoch 3822/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1270 - accuracy: 0.9407\n",
            "Epoch 3822: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1283 - accuracy: 0.9403 - val_loss: 0.1614 - val_accuracy: 0.9560\n",
            "Epoch 3823/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1331 - accuracy: 0.9387\n",
            "Epoch 3823: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1336 - accuracy: 0.9380 - val_loss: 0.1937 - val_accuracy: 0.9350\n",
            "Epoch 3824/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9402\n",
            "Epoch 3824: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1337 - accuracy: 0.9395 - val_loss: 0.1440 - val_accuracy: 0.9659\n",
            "Epoch 3825/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1466 - accuracy: 0.9319\n",
            "Epoch 3825: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1469 - accuracy: 0.9318 - val_loss: 0.1673 - val_accuracy: 0.9473\n",
            "Epoch 3826/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1442 - accuracy: 0.9341\n",
            "Epoch 3826: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1423 - accuracy: 0.9346 - val_loss: 0.1781 - val_accuracy: 0.9475\n",
            "Epoch 3827/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9380\n",
            "Epoch 3827: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1328 - accuracy: 0.9379 - val_loss: 0.1758 - val_accuracy: 0.9464\n",
            "Epoch 3828/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1356 - accuracy: 0.9374\n",
            "Epoch 3828: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1364 - accuracy: 0.9365 - val_loss: 0.1735 - val_accuracy: 0.9462\n",
            "Epoch 3829/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1407 - accuracy: 0.9337\n",
            "Epoch 3829: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1408 - accuracy: 0.9345 - val_loss: 0.1680 - val_accuracy: 0.9577\n",
            "Epoch 3830/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1393 - accuracy: 0.9360\n",
            "Epoch 3830: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1393 - accuracy: 0.9358 - val_loss: 0.2508 - val_accuracy: 0.9003\n",
            "Epoch 3831/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1341 - accuracy: 0.9373\n",
            "Epoch 3831: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1341 - accuracy: 0.9375 - val_loss: 0.1560 - val_accuracy: 0.9581\n",
            "Epoch 3832/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1317 - accuracy: 0.9386\n",
            "Epoch 3832: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1314 - accuracy: 0.9389 - val_loss: 0.1711 - val_accuracy: 0.9447\n",
            "Epoch 3833/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9406\n",
            "Epoch 3833: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9407 - val_loss: 0.1631 - val_accuracy: 0.9553\n",
            "Epoch 3834/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1248 - accuracy: 0.9410\n",
            "Epoch 3834: loss did not improve from 0.12482\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1258 - accuracy: 0.9406 - val_loss: 0.1408 - val_accuracy: 0.9675\n",
            "Epoch 3835/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9414\n",
            "Epoch 3835: loss improved from 0.12482 to 0.12312, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 0.1231 - accuracy: 0.9421 - val_loss: 0.2165 - val_accuracy: 0.9105\n",
            "Epoch 3836/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9407\n",
            "Epoch 3836: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1267 - accuracy: 0.9404 - val_loss: 0.1204 - val_accuracy: 0.9735\n",
            "Epoch 3837/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9387\n",
            "Epoch 3837: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1315 - accuracy: 0.9387 - val_loss: 0.1586 - val_accuracy: 0.9579\n",
            "Epoch 3838/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9398\n",
            "Epoch 3838: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1295 - accuracy: 0.9401 - val_loss: 0.1688 - val_accuracy: 0.9477\n",
            "Epoch 3839/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1288 - accuracy: 0.9410\n",
            "Epoch 3839: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1294 - accuracy: 0.9407 - val_loss: 0.1738 - val_accuracy: 0.9484\n",
            "Epoch 3840/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1281 - accuracy: 0.9401\n",
            "Epoch 3840: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1297 - accuracy: 0.9394 - val_loss: 0.1453 - val_accuracy: 0.9627\n",
            "Epoch 3841/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9359\n",
            "Epoch 3841: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1397 - accuracy: 0.9357 - val_loss: 0.1670 - val_accuracy: 0.9517\n",
            "Epoch 3842/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9423\n",
            "Epoch 3842: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1283 - accuracy: 0.9422 - val_loss: 0.1487 - val_accuracy: 0.9616\n",
            "Epoch 3843/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1288 - accuracy: 0.9396\n",
            "Epoch 3843: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1295 - accuracy: 0.9398 - val_loss: 0.1670 - val_accuracy: 0.9534\n",
            "Epoch 3844/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9378\n",
            "Epoch 3844: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1334 - accuracy: 0.9381 - val_loss: 0.2001 - val_accuracy: 0.9345\n",
            "Epoch 3845/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9406\n",
            "Epoch 3845: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1307 - accuracy: 0.9406 - val_loss: 0.1758 - val_accuracy: 0.9475\n",
            "Epoch 3846/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1302 - accuracy: 0.9406\n",
            "Epoch 3846: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9411 - val_loss: 0.1816 - val_accuracy: 0.9441\n",
            "Epoch 3847/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9401\n",
            "Epoch 3847: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1310 - accuracy: 0.9391 - val_loss: 0.1923 - val_accuracy: 0.9454\n",
            "Epoch 3848/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1336 - accuracy: 0.9385\n",
            "Epoch 3848: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9370 - val_loss: 0.2145 - val_accuracy: 0.9289\n",
            "Epoch 3849/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1281 - accuracy: 0.9408\n",
            "Epoch 3849: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1294 - accuracy: 0.9401 - val_loss: 0.1467 - val_accuracy: 0.9573\n",
            "Epoch 3850/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9370\n",
            "Epoch 3850: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1355 - accuracy: 0.9370 - val_loss: 0.2167 - val_accuracy: 0.9265\n",
            "Epoch 3851/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9345\n",
            "Epoch 3851: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1405 - accuracy: 0.9345 - val_loss: 0.1469 - val_accuracy: 0.9692\n",
            "Epoch 3852/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1406 - accuracy: 0.9354\n",
            "Epoch 3852: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1411 - accuracy: 0.9353 - val_loss: 0.1381 - val_accuracy: 0.9679\n",
            "Epoch 3853/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9420\n",
            "Epoch 3853: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9416 - val_loss: 0.1698 - val_accuracy: 0.9482\n",
            "Epoch 3854/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9394\n",
            "Epoch 3854: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1284 - accuracy: 0.9394 - val_loss: 0.1430 - val_accuracy: 0.9681\n",
            "Epoch 3855/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9397\n",
            "Epoch 3855: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1318 - accuracy: 0.9392 - val_loss: 0.1345 - val_accuracy: 0.9709\n",
            "Epoch 3856/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.9386\n",
            "Epoch 3856: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1348 - accuracy: 0.9378 - val_loss: 0.1341 - val_accuracy: 0.9714\n",
            "Epoch 3857/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1349 - accuracy: 0.9378\n",
            "Epoch 3857: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1344 - accuracy: 0.9381 - val_loss: 0.1768 - val_accuracy: 0.9415\n",
            "Epoch 3858/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9386\n",
            "Epoch 3858: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1306 - accuracy: 0.9384 - val_loss: 0.1667 - val_accuracy: 0.9482\n",
            "Epoch 3859/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9425\n",
            "Epoch 3859: loss did not improve from 0.12312\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1242 - accuracy: 0.9421 - val_loss: 0.1961 - val_accuracy: 0.9291\n",
            "Epoch 3860/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9420\n",
            "Epoch 3860: loss improved from 0.12312 to 0.12216, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 24ms/step - loss: 0.1222 - accuracy: 0.9421 - val_loss: 0.1775 - val_accuracy: 0.9462\n",
            "Epoch 3861/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1243 - accuracy: 0.9417\n",
            "Epoch 3861: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1257 - accuracy: 0.9407 - val_loss: 0.1191 - val_accuracy: 0.9748\n",
            "Epoch 3862/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1403 - accuracy: 0.9337\n",
            "Epoch 3862: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1403 - accuracy: 0.9340 - val_loss: 0.1285 - val_accuracy: 0.9762\n",
            "Epoch 3863/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1407 - accuracy: 0.9353\n",
            "Epoch 3863: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1411 - accuracy: 0.9353 - val_loss: 0.2251 - val_accuracy: 0.9263\n",
            "Epoch 3864/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9351\n",
            "Epoch 3864: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1407 - accuracy: 0.9349 - val_loss: 0.1524 - val_accuracy: 0.9621\n",
            "Epoch 3865/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9385\n",
            "Epoch 3865: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1324 - accuracy: 0.9385 - val_loss: 0.1626 - val_accuracy: 0.9523\n",
            "Epoch 3866/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9386\n",
            "Epoch 3866: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9386 - val_loss: 0.1463 - val_accuracy: 0.9666\n",
            "Epoch 3867/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1365 - accuracy: 0.9368\n",
            "Epoch 3867: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1370 - accuracy: 0.9367 - val_loss: 0.1860 - val_accuracy: 0.9319\n",
            "Epoch 3868/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9392\n",
            "Epoch 3868: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1319 - accuracy: 0.9391 - val_loss: 0.1762 - val_accuracy: 0.9521\n",
            "Epoch 3869/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1322 - accuracy: 0.9378\n",
            "Epoch 3869: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1331 - accuracy: 0.9379 - val_loss: 0.1589 - val_accuracy: 0.9575\n",
            "Epoch 3870/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9393\n",
            "Epoch 3870: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1310 - accuracy: 0.9393 - val_loss: 0.1405 - val_accuracy: 0.9720\n",
            "Epoch 3871/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9380\n",
            "Epoch 3871: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1332 - accuracy: 0.9380 - val_loss: 0.1785 - val_accuracy: 0.9428\n",
            "Epoch 3872/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1330 - accuracy: 0.9374\n",
            "Epoch 3872: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1333 - accuracy: 0.9372 - val_loss: 0.1554 - val_accuracy: 0.9566\n",
            "Epoch 3873/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1339 - accuracy: 0.9386\n",
            "Epoch 3873: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1340 - accuracy: 0.9383 - val_loss: 0.1614 - val_accuracy: 0.9592\n",
            "Epoch 3874/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1317 - accuracy: 0.9389\n",
            "Epoch 3874: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1332 - accuracy: 0.9382 - val_loss: 0.1633 - val_accuracy: 0.9612\n",
            "Epoch 3875/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9402\n",
            "Epoch 3875: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1294 - accuracy: 0.9402 - val_loss: 0.1452 - val_accuracy: 0.9679\n",
            "Epoch 3876/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9379\n",
            "Epoch 3876: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1338 - accuracy: 0.9381 - val_loss: 0.1436 - val_accuracy: 0.9683\n",
            "Epoch 3877/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9363\n",
            "Epoch 3877: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1376 - accuracy: 0.9363 - val_loss: 0.1781 - val_accuracy: 0.9469\n",
            "Epoch 3878/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1387 - accuracy: 0.9366\n",
            "Epoch 3878: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1385 - accuracy: 0.9368 - val_loss: 0.2253 - val_accuracy: 0.9222\n",
            "Epoch 3879/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1403 - accuracy: 0.9357\n",
            "Epoch 3879: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1407 - accuracy: 0.9353 - val_loss: 0.1982 - val_accuracy: 0.9337\n",
            "Epoch 3880/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1368 - accuracy: 0.9348\n",
            "Epoch 3880: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1372 - accuracy: 0.9351 - val_loss: 0.1731 - val_accuracy: 0.9486\n",
            "Epoch 3881/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9397\n",
            "Epoch 3881: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1293 - accuracy: 0.9397 - val_loss: 0.1173 - val_accuracy: 0.9787\n",
            "Epoch 3882/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1345 - accuracy: 0.9369\n",
            "Epoch 3882: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1345 - accuracy: 0.9366 - val_loss: 0.1459 - val_accuracy: 0.9620\n",
            "Epoch 3883/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1294 - accuracy: 0.9394\n",
            "Epoch 3883: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1293 - accuracy: 0.9395 - val_loss: 0.2255 - val_accuracy: 0.9228\n",
            "Epoch 3884/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1274 - accuracy: 0.9400\n",
            "Epoch 3884: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1279 - accuracy: 0.9401 - val_loss: 0.1278 - val_accuracy: 0.9733\n",
            "Epoch 3885/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1300 - accuracy: 0.9390\n",
            "Epoch 3885: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9385 - val_loss: 0.1547 - val_accuracy: 0.9603\n",
            "Epoch 3886/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1310 - accuracy: 0.9380\n",
            "Epoch 3886: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1294 - accuracy: 0.9388 - val_loss: 0.1791 - val_accuracy: 0.9445\n",
            "Epoch 3887/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1270 - accuracy: 0.9407\n",
            "Epoch 3887: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1278 - accuracy: 0.9407 - val_loss: 0.1507 - val_accuracy: 0.9627\n",
            "Epoch 3888/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1330 - accuracy: 0.9391\n",
            "Epoch 3888: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9400 - val_loss: 0.2216 - val_accuracy: 0.9271\n",
            "Epoch 3889/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9375\n",
            "Epoch 3889: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1331 - accuracy: 0.9374 - val_loss: 0.1593 - val_accuracy: 0.9486\n",
            "Epoch 3890/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1347 - accuracy: 0.9376\n",
            "Epoch 3890: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1332 - accuracy: 0.9389 - val_loss: 0.1820 - val_accuracy: 0.9427\n",
            "Epoch 3891/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9415\n",
            "Epoch 3891: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1245 - accuracy: 0.9414 - val_loss: 0.1628 - val_accuracy: 0.9508\n",
            "Epoch 3892/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1592 - accuracy: 0.9270\n",
            "Epoch 3892: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1620 - accuracy: 0.9256 - val_loss: 0.2918 - val_accuracy: 0.8894\n",
            "Epoch 3893/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1603 - accuracy: 0.9277\n",
            "Epoch 3893: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1599 - accuracy: 0.9281 - val_loss: 0.1458 - val_accuracy: 0.9675\n",
            "Epoch 3894/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9339\n",
            "Epoch 3894: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1410 - accuracy: 0.9348 - val_loss: 0.1926 - val_accuracy: 0.9410\n",
            "Epoch 3895/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9369\n",
            "Epoch 3895: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1334 - accuracy: 0.9369 - val_loss: 0.2156 - val_accuracy: 0.9107\n",
            "Epoch 3896/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1299 - accuracy: 0.9397\n",
            "Epoch 3896: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9389 - val_loss: 0.1396 - val_accuracy: 0.9746\n",
            "Epoch 3897/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9375\n",
            "Epoch 3897: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1360 - accuracy: 0.9372 - val_loss: 0.1955 - val_accuracy: 0.9354\n",
            "Epoch 3898/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9372\n",
            "Epoch 3898: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1343 - accuracy: 0.9375 - val_loss: 0.1267 - val_accuracy: 0.9748\n",
            "Epoch 3899/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1272 - accuracy: 0.9406\n",
            "Epoch 3899: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1270 - accuracy: 0.9403 - val_loss: 0.1812 - val_accuracy: 0.9414\n",
            "Epoch 3900/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9417\n",
            "Epoch 3900: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1258 - accuracy: 0.9417 - val_loss: 0.1471 - val_accuracy: 0.9686\n",
            "Epoch 3901/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9400\n",
            "Epoch 3901: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1278 - accuracy: 0.9400 - val_loss: 0.1668 - val_accuracy: 0.9471\n",
            "Epoch 3902/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1257 - accuracy: 0.9423\n",
            "Epoch 3902: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1256 - accuracy: 0.9421 - val_loss: 0.1458 - val_accuracy: 0.9657\n",
            "Epoch 3903/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9410\n",
            "Epoch 3903: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1268 - accuracy: 0.9405 - val_loss: 0.1618 - val_accuracy: 0.9555\n",
            "Epoch 3904/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1272 - accuracy: 0.9418\n",
            "Epoch 3904: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1282 - accuracy: 0.9415 - val_loss: 0.1485 - val_accuracy: 0.9692\n",
            "Epoch 3905/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9399\n",
            "Epoch 3905: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9399 - val_loss: 0.1563 - val_accuracy: 0.9597\n",
            "Epoch 3906/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1319 - accuracy: 0.9394\n",
            "Epoch 3906: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1313 - accuracy: 0.9395 - val_loss: 0.1748 - val_accuracy: 0.9428\n",
            "Epoch 3907/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9375\n",
            "Epoch 3907: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1345 - accuracy: 0.9373 - val_loss: 0.1886 - val_accuracy: 0.9376\n",
            "Epoch 3908/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9380\n",
            "Epoch 3908: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1359 - accuracy: 0.9381 - val_loss: 0.1517 - val_accuracy: 0.9599\n",
            "Epoch 3909/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9404\n",
            "Epoch 3909: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1296 - accuracy: 0.9400 - val_loss: 0.1582 - val_accuracy: 0.9568\n",
            "Epoch 3910/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1283 - accuracy: 0.9396\n",
            "Epoch 3910: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1286 - accuracy: 0.9395 - val_loss: 0.1580 - val_accuracy: 0.9534\n",
            "Epoch 3911/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9382\n",
            "Epoch 3911: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1311 - accuracy: 0.9384 - val_loss: 0.2165 - val_accuracy: 0.9219\n",
            "Epoch 3912/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1350 - accuracy: 0.9368\n",
            "Epoch 3912: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1349 - accuracy: 0.9369 - val_loss: 0.1661 - val_accuracy: 0.9577\n",
            "Epoch 3913/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1325 - accuracy: 0.9375\n",
            "Epoch 3913: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1333 - accuracy: 0.9375 - val_loss: 0.2442 - val_accuracy: 0.9065\n",
            "Epoch 3914/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9378\n",
            "Epoch 3914: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1303 - accuracy: 0.9380 - val_loss: 0.1285 - val_accuracy: 0.9736\n",
            "Epoch 3915/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9423\n",
            "Epoch 3915: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1249 - accuracy: 0.9421 - val_loss: 0.1388 - val_accuracy: 0.9683\n",
            "Epoch 3916/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1313 - accuracy: 0.9389\n",
            "Epoch 3916: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1313 - accuracy: 0.9388 - val_loss: 0.1322 - val_accuracy: 0.9736\n",
            "Epoch 3917/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9320\n",
            "Epoch 3917: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1471 - accuracy: 0.9327 - val_loss: 0.1418 - val_accuracy: 0.9697\n",
            "Epoch 3918/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1414 - accuracy: 0.9342\n",
            "Epoch 3918: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1414 - accuracy: 0.9342 - val_loss: 0.1452 - val_accuracy: 0.9595\n",
            "Epoch 3919/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1397 - accuracy: 0.9347\n",
            "Epoch 3919: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1407 - accuracy: 0.9344 - val_loss: 0.2392 - val_accuracy: 0.9182\n",
            "Epoch 3920/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.9381\n",
            "Epoch 3920: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1377 - accuracy: 0.9381 - val_loss: 0.1692 - val_accuracy: 0.9521\n",
            "Epoch 3921/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9402\n",
            "Epoch 3921: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1282 - accuracy: 0.9404 - val_loss: 0.1719 - val_accuracy: 0.9486\n",
            "Epoch 3922/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9412\n",
            "Epoch 3922: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1259 - accuracy: 0.9412 - val_loss: 0.1699 - val_accuracy: 0.9566\n",
            "Epoch 3923/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9410\n",
            "Epoch 3923: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1268 - accuracy: 0.9410 - val_loss: 0.1816 - val_accuracy: 0.9423\n",
            "Epoch 3924/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1290 - accuracy: 0.9383\n",
            "Epoch 3924: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1297 - accuracy: 0.9387 - val_loss: 0.2030 - val_accuracy: 0.9410\n",
            "Epoch 3925/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9405\n",
            "Epoch 3925: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9403 - val_loss: 0.1349 - val_accuracy: 0.9696\n",
            "Epoch 3926/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9378\n",
            "Epoch 3926: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9378 - val_loss: 0.1570 - val_accuracy: 0.9549\n",
            "Epoch 3927/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9395\n",
            "Epoch 3927: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1340 - accuracy: 0.9395 - val_loss: 0.1766 - val_accuracy: 0.9482\n",
            "Epoch 3928/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1383 - accuracy: 0.9367\n",
            "Epoch 3928: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1389 - accuracy: 0.9364 - val_loss: 0.1615 - val_accuracy: 0.9605\n",
            "Epoch 3929/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1328 - accuracy: 0.9393\n",
            "Epoch 3929: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1328 - accuracy: 0.9392 - val_loss: 0.1914 - val_accuracy: 0.9389\n",
            "Epoch 3930/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9372\n",
            "Epoch 3930: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1316 - accuracy: 0.9372 - val_loss: 0.1906 - val_accuracy: 0.9421\n",
            "Epoch 3931/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1285 - accuracy: 0.9395\n",
            "Epoch 3931: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1314 - accuracy: 0.9381 - val_loss: 0.2169 - val_accuracy: 0.9222\n",
            "Epoch 3932/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1280 - accuracy: 0.9410\n",
            "Epoch 3932: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9407 - val_loss: 0.1632 - val_accuracy: 0.9556\n",
            "Epoch 3933/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1286 - accuracy: 0.9396\n",
            "Epoch 3933: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1286 - accuracy: 0.9401 - val_loss: 0.2409 - val_accuracy: 0.8992\n",
            "Epoch 3934/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1311 - accuracy: 0.9396\n",
            "Epoch 3934: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1318 - accuracy: 0.9394 - val_loss: 0.2332 - val_accuracy: 0.9072\n",
            "Epoch 3935/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1272 - accuracy: 0.9413\n",
            "Epoch 3935: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1280 - accuracy: 0.9406 - val_loss: 0.1619 - val_accuracy: 0.9529\n",
            "Epoch 3936/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9427\n",
            "Epoch 3936: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9422 - val_loss: 0.1862 - val_accuracy: 0.9439\n",
            "Epoch 3937/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1289 - accuracy: 0.9392\n",
            "Epoch 3937: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1283 - accuracy: 0.9393 - val_loss: 0.1490 - val_accuracy: 0.9595\n",
            "Epoch 3938/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1379 - accuracy: 0.9367\n",
            "Epoch 3938: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1382 - accuracy: 0.9362 - val_loss: 0.1526 - val_accuracy: 0.9627\n",
            "Epoch 3939/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9394\n",
            "Epoch 3939: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1296 - accuracy: 0.9394 - val_loss: 0.1720 - val_accuracy: 0.9467\n",
            "Epoch 3940/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9411\n",
            "Epoch 3940: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1286 - accuracy: 0.9411 - val_loss: 0.1392 - val_accuracy: 0.9696\n",
            "Epoch 3941/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.9367\n",
            "Epoch 3941: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1366 - accuracy: 0.9368 - val_loss: 0.1919 - val_accuracy: 0.9428\n",
            "Epoch 3942/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1339 - accuracy: 0.9373\n",
            "Epoch 3942: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1342 - accuracy: 0.9369 - val_loss: 0.1819 - val_accuracy: 0.9488\n",
            "Epoch 3943/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9354\n",
            "Epoch 3943: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1368 - accuracy: 0.9354 - val_loss: 0.2182 - val_accuracy: 0.9222\n",
            "Epoch 3944/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9370\n",
            "Epoch 3944: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1359 - accuracy: 0.9368 - val_loss: 0.1665 - val_accuracy: 0.9545\n",
            "Epoch 3945/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1300 - accuracy: 0.9402\n",
            "Epoch 3945: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1342 - accuracy: 0.9375 - val_loss: 0.1463 - val_accuracy: 0.9688\n",
            "Epoch 3946/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9348\n",
            "Epoch 3946: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1437 - accuracy: 0.9348 - val_loss: 0.2343 - val_accuracy: 0.9265\n",
            "Epoch 3947/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1410 - accuracy: 0.9350\n",
            "Epoch 3947: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1406 - accuracy: 0.9353 - val_loss: 0.1820 - val_accuracy: 0.9464\n",
            "Epoch 3948/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9390\n",
            "Epoch 3948: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1321 - accuracy: 0.9387 - val_loss: 0.1429 - val_accuracy: 0.9683\n",
            "Epoch 3949/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1337 - accuracy: 0.9375\n",
            "Epoch 3949: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1317 - accuracy: 0.9385 - val_loss: 0.1514 - val_accuracy: 0.9581\n",
            "Epoch 3950/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1283 - accuracy: 0.9414\n",
            "Epoch 3950: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1291 - accuracy: 0.9407 - val_loss: 0.2126 - val_accuracy: 0.9291\n",
            "Epoch 3951/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1331 - accuracy: 0.9397\n",
            "Epoch 3951: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1352 - accuracy: 0.9382 - val_loss: 0.1624 - val_accuracy: 0.9579\n",
            "Epoch 3952/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9410\n",
            "Epoch 3952: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9410 - val_loss: 0.1584 - val_accuracy: 0.9540\n",
            "Epoch 3953/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9398\n",
            "Epoch 3953: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1284 - accuracy: 0.9395 - val_loss: 0.1201 - val_accuracy: 0.9766\n",
            "Epoch 3954/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9380\n",
            "Epoch 3954: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1325 - accuracy: 0.9384 - val_loss: 0.1718 - val_accuracy: 0.9452\n",
            "Epoch 3955/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9374\n",
            "Epoch 3955: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9378 - val_loss: 0.1437 - val_accuracy: 0.9668\n",
            "Epoch 3956/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1382 - accuracy: 0.9370\n",
            "Epoch 3956: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1383 - accuracy: 0.9370 - val_loss: 0.1931 - val_accuracy: 0.9378\n",
            "Epoch 3957/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9375\n",
            "Epoch 3957: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1355 - accuracy: 0.9375 - val_loss: 0.1551 - val_accuracy: 0.9653\n",
            "Epoch 3958/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1298 - accuracy: 0.9380\n",
            "Epoch 3958: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1297 - accuracy: 0.9387 - val_loss: 0.1248 - val_accuracy: 0.9779\n",
            "Epoch 3959/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1377 - accuracy: 0.9354\n",
            "Epoch 3959: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1378 - accuracy: 0.9352 - val_loss: 0.1710 - val_accuracy: 0.9491\n",
            "Epoch 3960/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1374 - accuracy: 0.9359\n",
            "Epoch 3960: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1386 - accuracy: 0.9353 - val_loss: 0.1274 - val_accuracy: 0.9766\n",
            "Epoch 3961/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9379\n",
            "Epoch 3961: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1362 - accuracy: 0.9380 - val_loss: 0.1319 - val_accuracy: 0.9727\n",
            "Epoch 3962/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9374\n",
            "Epoch 3962: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1350 - accuracy: 0.9374 - val_loss: 0.1887 - val_accuracy: 0.9469\n",
            "Epoch 3963/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9410\n",
            "Epoch 3963: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9406 - val_loss: 0.2155 - val_accuracy: 0.9263\n",
            "Epoch 3964/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9400\n",
            "Epoch 3964: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1327 - accuracy: 0.9395 - val_loss: 0.1883 - val_accuracy: 0.9465\n",
            "Epoch 3965/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9402\n",
            "Epoch 3965: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1277 - accuracy: 0.9408 - val_loss: 0.1814 - val_accuracy: 0.9439\n",
            "Epoch 3966/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1304 - accuracy: 0.9406\n",
            "Epoch 3966: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1301 - accuracy: 0.9408 - val_loss: 0.1293 - val_accuracy: 0.9740\n",
            "Epoch 3967/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9381\n",
            "Epoch 3967: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1316 - accuracy: 0.9381 - val_loss: 0.1659 - val_accuracy: 0.9473\n",
            "Epoch 3968/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9399\n",
            "Epoch 3968: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1302 - accuracy: 0.9392 - val_loss: 0.1575 - val_accuracy: 0.9510\n",
            "Epoch 3969/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9394\n",
            "Epoch 3969: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1285 - accuracy: 0.9390 - val_loss: 0.1369 - val_accuracy: 0.9720\n",
            "Epoch 3970/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1238 - accuracy: 0.9422\n",
            "Epoch 3970: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1246 - accuracy: 0.9418 - val_loss: 0.1252 - val_accuracy: 0.9790\n",
            "Epoch 3971/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1332 - accuracy: 0.9382\n",
            "Epoch 3971: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1332 - accuracy: 0.9379 - val_loss: 0.1616 - val_accuracy: 0.9614\n",
            "Epoch 3972/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.9345\n",
            "Epoch 3972: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1432 - accuracy: 0.9343 - val_loss: 0.1774 - val_accuracy: 0.9445\n",
            "Epoch 3973/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1356 - accuracy: 0.9379\n",
            "Epoch 3973: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1350 - accuracy: 0.9378 - val_loss: 0.1868 - val_accuracy: 0.9428\n",
            "Epoch 3974/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1286 - accuracy: 0.9403\n",
            "Epoch 3974: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1288 - accuracy: 0.9400 - val_loss: 0.1818 - val_accuracy: 0.9454\n",
            "Epoch 3975/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1277 - accuracy: 0.9398\n",
            "Epoch 3975: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1284 - accuracy: 0.9391 - val_loss: 0.2151 - val_accuracy: 0.9343\n",
            "Epoch 3976/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1304 - accuracy: 0.9406\n",
            "Epoch 3976: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1321 - accuracy: 0.9395 - val_loss: 0.1305 - val_accuracy: 0.9749\n",
            "Epoch 3977/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1319 - accuracy: 0.9389\n",
            "Epoch 3977: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1331 - accuracy: 0.9383 - val_loss: 0.1406 - val_accuracy: 0.9684\n",
            "Epoch 3978/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1299 - accuracy: 0.9396\n",
            "Epoch 3978: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9392 - val_loss: 0.1355 - val_accuracy: 0.9742\n",
            "Epoch 3979/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9378\n",
            "Epoch 3979: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1376 - accuracy: 0.9378 - val_loss: 0.2086 - val_accuracy: 0.9254\n",
            "Epoch 3980/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.9391\n",
            "Epoch 3980: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1303 - accuracy: 0.9389 - val_loss: 0.1443 - val_accuracy: 0.9671\n",
            "Epoch 3981/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9397\n",
            "Epoch 3981: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1321 - accuracy: 0.9396 - val_loss: 0.2096 - val_accuracy: 0.9313\n",
            "Epoch 3982/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9387\n",
            "Epoch 3982: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1322 - accuracy: 0.9387 - val_loss: 0.1487 - val_accuracy: 0.9651\n",
            "Epoch 3983/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9366\n",
            "Epoch 3983: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1353 - accuracy: 0.9361 - val_loss: 0.1910 - val_accuracy: 0.9386\n",
            "Epoch 3984/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9340\n",
            "Epoch 3984: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1440 - accuracy: 0.9340 - val_loss: 0.1718 - val_accuracy: 0.9449\n",
            "Epoch 3985/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9381\n",
            "Epoch 3985: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9385 - val_loss: 0.1172 - val_accuracy: 0.9788\n",
            "Epoch 3986/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9390\n",
            "Epoch 3986: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1325 - accuracy: 0.9390 - val_loss: 0.1203 - val_accuracy: 0.9770\n",
            "Epoch 3987/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9375\n",
            "Epoch 3987: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1368 - accuracy: 0.9371 - val_loss: 0.1889 - val_accuracy: 0.9430\n",
            "Epoch 3988/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1324 - accuracy: 0.9391\n",
            "Epoch 3988: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1340 - accuracy: 0.9382 - val_loss: 0.2201 - val_accuracy: 0.9369\n",
            "Epoch 3989/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9369\n",
            "Epoch 3989: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1362 - accuracy: 0.9371 - val_loss: 0.1540 - val_accuracy: 0.9625\n",
            "Epoch 3990/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1327 - accuracy: 0.9397\n",
            "Epoch 3990: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1329 - accuracy: 0.9391 - val_loss: 0.1311 - val_accuracy: 0.9785\n",
            "Epoch 3991/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9362\n",
            "Epoch 3991: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1387 - accuracy: 0.9362 - val_loss: 0.1900 - val_accuracy: 0.9356\n",
            "Epoch 3992/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1383 - accuracy: 0.9355\n",
            "Epoch 3992: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1386 - accuracy: 0.9354 - val_loss: 0.1634 - val_accuracy: 0.9467\n",
            "Epoch 3993/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1321 - accuracy: 0.9394\n",
            "Epoch 3993: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1347 - accuracy: 0.9377 - val_loss: 0.2097 - val_accuracy: 0.9285\n",
            "Epoch 3994/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9323\n",
            "Epoch 3994: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1488 - accuracy: 0.9325 - val_loss: 0.1618 - val_accuracy: 0.9697\n",
            "Epoch 3995/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1357 - accuracy: 0.9378\n",
            "Epoch 3995: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1361 - accuracy: 0.9377 - val_loss: 0.2011 - val_accuracy: 0.9341\n",
            "Epoch 3996/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1275 - accuracy: 0.9410\n",
            "Epoch 3996: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1280 - accuracy: 0.9405 - val_loss: 0.1432 - val_accuracy: 0.9571\n",
            "Epoch 3997/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1254 - accuracy: 0.9417\n",
            "Epoch 3997: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1264 - accuracy: 0.9412 - val_loss: 0.1491 - val_accuracy: 0.9597\n",
            "Epoch 3998/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1239 - accuracy: 0.9414\n",
            "Epoch 3998: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1243 - accuracy: 0.9415 - val_loss: 0.2002 - val_accuracy: 0.9300\n",
            "Epoch 3999/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9390\n",
            "Epoch 3999: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1331 - accuracy: 0.9388 - val_loss: 0.1920 - val_accuracy: 0.9319\n",
            "Epoch 4000/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9390\n",
            "Epoch 4000: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1349 - accuracy: 0.9390 - val_loss: 0.1239 - val_accuracy: 0.9779\n",
            "Epoch 4001/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1303 - accuracy: 0.9382\n",
            "Epoch 4001: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1306 - accuracy: 0.9380 - val_loss: 0.1304 - val_accuracy: 0.9759\n",
            "Epoch 4002/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1284 - accuracy: 0.9412\n",
            "Epoch 4002: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1299 - accuracy: 0.9406 - val_loss: 0.1844 - val_accuracy: 0.9397\n",
            "Epoch 4003/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9381\n",
            "Epoch 4003: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1319 - accuracy: 0.9381 - val_loss: 0.1850 - val_accuracy: 0.9408\n",
            "Epoch 4004/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9396\n",
            "Epoch 4004: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1298 - accuracy: 0.9393 - val_loss: 0.1566 - val_accuracy: 0.9553\n",
            "Epoch 4005/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9412\n",
            "Epoch 4005: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1238 - accuracy: 0.9413 - val_loss: 0.1622 - val_accuracy: 0.9560\n",
            "Epoch 4006/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1297 - accuracy: 0.9394\n",
            "Epoch 4006: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1301 - accuracy: 0.9396 - val_loss: 0.1385 - val_accuracy: 0.9707\n",
            "Epoch 4007/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.9348\n",
            "Epoch 4007: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1420 - accuracy: 0.9344 - val_loss: 0.2250 - val_accuracy: 0.9037\n",
            "Epoch 4008/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1310 - accuracy: 0.9405\n",
            "Epoch 4008: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1302 - accuracy: 0.9409 - val_loss: 0.1579 - val_accuracy: 0.9536\n",
            "Epoch 4009/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9391\n",
            "Epoch 4009: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1330 - accuracy: 0.9391 - val_loss: 0.1815 - val_accuracy: 0.9402\n",
            "Epoch 4010/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1347 - accuracy: 0.9375\n",
            "Epoch 4010: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1352 - accuracy: 0.9370 - val_loss: 0.1949 - val_accuracy: 0.9380\n",
            "Epoch 4011/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1305 - accuracy: 0.9391\n",
            "Epoch 4011: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9387 - val_loss: 0.1187 - val_accuracy: 0.9777\n",
            "Epoch 4012/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9362\n",
            "Epoch 4012: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1399 - accuracy: 0.9362 - val_loss: 0.1869 - val_accuracy: 0.9415\n",
            "Epoch 4013/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9406\n",
            "Epoch 4013: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1330 - accuracy: 0.9406 - val_loss: 0.1795 - val_accuracy: 0.9467\n",
            "Epoch 4014/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9383\n",
            "Epoch 4014: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9387 - val_loss: 0.1468 - val_accuracy: 0.9671\n",
            "Epoch 4015/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9375\n",
            "Epoch 4015: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1325 - accuracy: 0.9375 - val_loss: 0.1337 - val_accuracy: 0.9683\n",
            "Epoch 4016/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1407 - accuracy: 0.9348\n",
            "Epoch 4016: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1414 - accuracy: 0.9344 - val_loss: 0.1913 - val_accuracy: 0.9569\n",
            "Epoch 4017/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1399 - accuracy: 0.9351\n",
            "Epoch 4017: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1387 - accuracy: 0.9358 - val_loss: 0.1614 - val_accuracy: 0.9614\n",
            "Epoch 4018/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9396\n",
            "Epoch 4018: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1265 - accuracy: 0.9401 - val_loss: 0.1473 - val_accuracy: 0.9736\n",
            "Epoch 4019/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9420\n",
            "Epoch 4019: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1231 - accuracy: 0.9420 - val_loss: 0.1450 - val_accuracy: 0.9634\n",
            "Epoch 4020/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9417\n",
            "Epoch 4020: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1244 - accuracy: 0.9411 - val_loss: 0.1917 - val_accuracy: 0.9441\n",
            "Epoch 4021/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9428\n",
            "Epoch 4021: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1226 - accuracy: 0.9428 - val_loss: 0.1340 - val_accuracy: 0.9683\n",
            "Epoch 4022/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9402\n",
            "Epoch 4022: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1313 - accuracy: 0.9396 - val_loss: 0.1716 - val_accuracy: 0.9493\n",
            "Epoch 4023/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9377\n",
            "Epoch 4023: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1337 - accuracy: 0.9377 - val_loss: 0.1827 - val_accuracy: 0.9447\n",
            "Epoch 4024/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9388\n",
            "Epoch 4024: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1332 - accuracy: 0.9388 - val_loss: 0.2539 - val_accuracy: 0.9057\n",
            "Epoch 4025/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1513 - accuracy: 0.9311\n",
            "Epoch 4025: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1520 - accuracy: 0.9305 - val_loss: 0.2048 - val_accuracy: 0.9267\n",
            "Epoch 4026/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1357 - accuracy: 0.9352\n",
            "Epoch 4026: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1365 - accuracy: 0.9349 - val_loss: 0.1491 - val_accuracy: 0.9631\n",
            "Epoch 4027/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9370\n",
            "Epoch 4027: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1326 - accuracy: 0.9373 - val_loss: 0.1323 - val_accuracy: 0.9699\n",
            "Epoch 4028/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1334 - accuracy: 0.9387\n",
            "Epoch 4028: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1327 - accuracy: 0.9387 - val_loss: 0.1661 - val_accuracy: 0.9523\n",
            "Epoch 4029/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1286 - accuracy: 0.9384\n",
            "Epoch 4029: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1295 - accuracy: 0.9379 - val_loss: 0.2002 - val_accuracy: 0.9313\n",
            "Epoch 4030/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1309 - accuracy: 0.9396\n",
            "Epoch 4030: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9391 - val_loss: 0.1454 - val_accuracy: 0.9640\n",
            "Epoch 4031/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9391\n",
            "Epoch 4031: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9391 - val_loss: 0.1825 - val_accuracy: 0.9430\n",
            "Epoch 4032/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1364 - accuracy: 0.9369\n",
            "Epoch 4032: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1386 - accuracy: 0.9358 - val_loss: 0.1700 - val_accuracy: 0.9499\n",
            "Epoch 4033/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9390\n",
            "Epoch 4033: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1297 - accuracy: 0.9387 - val_loss: 0.1550 - val_accuracy: 0.9605\n",
            "Epoch 4034/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1281 - accuracy: 0.9393\n",
            "Epoch 4034: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1275 - accuracy: 0.9398 - val_loss: 0.1249 - val_accuracy: 0.9744\n",
            "Epoch 4035/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1393 - accuracy: 0.9361\n",
            "Epoch 4035: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1387 - accuracy: 0.9365 - val_loss: 0.1683 - val_accuracy: 0.9519\n",
            "Epoch 4036/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9399\n",
            "Epoch 4036: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9396 - val_loss: 0.1956 - val_accuracy: 0.9295\n",
            "Epoch 4037/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1267 - accuracy: 0.9419\n",
            "Epoch 4037: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1279 - accuracy: 0.9411 - val_loss: 0.1920 - val_accuracy: 0.9412\n",
            "Epoch 4038/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1310 - accuracy: 0.9398\n",
            "Epoch 4038: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1324 - accuracy: 0.9389 - val_loss: 0.2805 - val_accuracy: 0.8781\n",
            "Epoch 4039/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1332 - accuracy: 0.9376\n",
            "Epoch 4039: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1329 - accuracy: 0.9383 - val_loss: 0.1669 - val_accuracy: 0.9510\n",
            "Epoch 4040/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1220 - accuracy: 0.9424\n",
            "Epoch 4040: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1237 - accuracy: 0.9418 - val_loss: 0.1905 - val_accuracy: 0.9339\n",
            "Epoch 4041/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1266 - accuracy: 0.9405\n",
            "Epoch 4041: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1280 - accuracy: 0.9398 - val_loss: 0.1384 - val_accuracy: 0.9712\n",
            "Epoch 4042/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9406\n",
            "Epoch 4042: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9409 - val_loss: 0.1366 - val_accuracy: 0.9647\n",
            "Epoch 4043/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1266 - accuracy: 0.9414\n",
            "Epoch 4043: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1278 - accuracy: 0.9409 - val_loss: 0.2257 - val_accuracy: 0.9152\n",
            "Epoch 4044/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1279 - accuracy: 0.9407\n",
            "Epoch 4044: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1286 - accuracy: 0.9405 - val_loss: 0.1899 - val_accuracy: 0.9369\n",
            "Epoch 4045/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1331 - accuracy: 0.9380\n",
            "Epoch 4045: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1317 - accuracy: 0.9385 - val_loss: 0.1327 - val_accuracy: 0.9723\n",
            "Epoch 4046/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1288 - accuracy: 0.9398\n",
            "Epoch 4046: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1285 - accuracy: 0.9398 - val_loss: 0.1419 - val_accuracy: 0.9714\n",
            "Epoch 4047/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9395\n",
            "Epoch 4047: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1283 - accuracy: 0.9393 - val_loss: 0.1088 - val_accuracy: 0.9840\n",
            "Epoch 4048/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9391\n",
            "Epoch 4048: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1309 - accuracy: 0.9391 - val_loss: 0.1439 - val_accuracy: 0.9631\n",
            "Epoch 4049/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9378\n",
            "Epoch 4049: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1341 - accuracy: 0.9377 - val_loss: 0.1555 - val_accuracy: 0.9627\n",
            "Epoch 4050/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1270 - accuracy: 0.9396\n",
            "Epoch 4050: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1281 - accuracy: 0.9390 - val_loss: 0.1481 - val_accuracy: 0.9627\n",
            "Epoch 4051/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9405\n",
            "Epoch 4051: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1280 - accuracy: 0.9399 - val_loss: 0.2198 - val_accuracy: 0.9117\n",
            "Epoch 4052/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.9367\n",
            "Epoch 4052: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1378 - accuracy: 0.9362 - val_loss: 0.1495 - val_accuracy: 0.9627\n",
            "Epoch 4053/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1512 - accuracy: 0.9324\n",
            "Epoch 4053: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1497 - accuracy: 0.9329 - val_loss: 0.2022 - val_accuracy: 0.9323\n",
            "Epoch 4054/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9370\n",
            "Epoch 4054: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1383 - accuracy: 0.9370 - val_loss: 0.1627 - val_accuracy: 0.9582\n",
            "Epoch 4055/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9412\n",
            "Epoch 4055: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9411 - val_loss: 0.1987 - val_accuracy: 0.9386\n",
            "Epoch 4056/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9401\n",
            "Epoch 4056: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1288 - accuracy: 0.9401 - val_loss: 0.1667 - val_accuracy: 0.9568\n",
            "Epoch 4057/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9383\n",
            "Epoch 4057: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1296 - accuracy: 0.9383 - val_loss: 0.1168 - val_accuracy: 0.9848\n",
            "Epoch 4058/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1256 - accuracy: 0.9419\n",
            "Epoch 4058: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1275 - accuracy: 0.9405 - val_loss: 0.1569 - val_accuracy: 0.9499\n",
            "Epoch 4059/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9393\n",
            "Epoch 4059: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1305 - accuracy: 0.9393 - val_loss: 0.2089 - val_accuracy: 0.9230\n",
            "Epoch 4060/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9363\n",
            "Epoch 4060: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1364 - accuracy: 0.9363 - val_loss: 0.2106 - val_accuracy: 0.9319\n",
            "Epoch 4061/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.9366\n",
            "Epoch 4061: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1369 - accuracy: 0.9364 - val_loss: 0.1639 - val_accuracy: 0.9627\n",
            "Epoch 4062/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1461 - accuracy: 0.9330\n",
            "Epoch 4062: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1464 - accuracy: 0.9329 - val_loss: 0.1259 - val_accuracy: 0.9753\n",
            "Epoch 4063/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1383 - accuracy: 0.9359\n",
            "Epoch 4063: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1396 - accuracy: 0.9355 - val_loss: 0.1966 - val_accuracy: 0.9352\n",
            "Epoch 4064/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9352\n",
            "Epoch 4064: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1388 - accuracy: 0.9352 - val_loss: 0.1622 - val_accuracy: 0.9577\n",
            "Epoch 4065/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1325 - accuracy: 0.9390\n",
            "Epoch 4065: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1339 - accuracy: 0.9382 - val_loss: 0.2048 - val_accuracy: 0.9319\n",
            "Epoch 4066/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1366 - accuracy: 0.9368\n",
            "Epoch 4066: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1359 - accuracy: 0.9372 - val_loss: 0.1972 - val_accuracy: 0.9323\n",
            "Epoch 4067/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9411\n",
            "Epoch 4067: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1273 - accuracy: 0.9405 - val_loss: 0.1656 - val_accuracy: 0.9477\n",
            "Epoch 4068/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1244 - accuracy: 0.9410\n",
            "Epoch 4068: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9408 - val_loss: 0.1493 - val_accuracy: 0.9633\n",
            "Epoch 4069/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1241 - accuracy: 0.9420\n",
            "Epoch 4069: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1253 - accuracy: 0.9414 - val_loss: 0.1907 - val_accuracy: 0.9362\n",
            "Epoch 4070/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9360\n",
            "Epoch 4070: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1354 - accuracy: 0.9366 - val_loss: 0.1492 - val_accuracy: 0.9629\n",
            "Epoch 4071/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9366\n",
            "Epoch 4071: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1374 - accuracy: 0.9359 - val_loss: 0.2223 - val_accuracy: 0.9187\n",
            "Epoch 4072/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9398\n",
            "Epoch 4072: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1292 - accuracy: 0.9396 - val_loss: 0.1329 - val_accuracy: 0.9766\n",
            "Epoch 4073/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9412\n",
            "Epoch 4073: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9407 - val_loss: 0.1662 - val_accuracy: 0.9471\n",
            "Epoch 4074/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1268 - accuracy: 0.9392\n",
            "Epoch 4074: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1275 - accuracy: 0.9392 - val_loss: 0.1454 - val_accuracy: 0.9647\n",
            "Epoch 4075/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9412\n",
            "Epoch 4075: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1273 - accuracy: 0.9407 - val_loss: 0.1500 - val_accuracy: 0.9631\n",
            "Epoch 4076/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1268 - accuracy: 0.9419\n",
            "Epoch 4076: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1293 - accuracy: 0.9405 - val_loss: 0.1457 - val_accuracy: 0.9644\n",
            "Epoch 4077/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1257 - accuracy: 0.9416\n",
            "Epoch 4077: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1258 - accuracy: 0.9409 - val_loss: 0.1626 - val_accuracy: 0.9556\n",
            "Epoch 4078/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1266 - accuracy: 0.9400\n",
            "Epoch 4078: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1266 - accuracy: 0.9400 - val_loss: 0.2065 - val_accuracy: 0.9235\n",
            "Epoch 4079/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1253 - accuracy: 0.9421\n",
            "Epoch 4079: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1266 - accuracy: 0.9411 - val_loss: 0.1849 - val_accuracy: 0.9467\n",
            "Epoch 4080/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1235 - accuracy: 0.9431\n",
            "Epoch 4080: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9429 - val_loss: 0.1280 - val_accuracy: 0.9735\n",
            "Epoch 4081/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.9362\n",
            "Epoch 4081: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1389 - accuracy: 0.9360 - val_loss: 0.1919 - val_accuracy: 0.9362\n",
            "Epoch 4082/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9380\n",
            "Epoch 4082: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1358 - accuracy: 0.9380 - val_loss: 0.1758 - val_accuracy: 0.9432\n",
            "Epoch 4083/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9377\n",
            "Epoch 4083: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1326 - accuracy: 0.9377 - val_loss: 0.1631 - val_accuracy: 0.9504\n",
            "Epoch 4084/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1311 - accuracy: 0.9378\n",
            "Epoch 4084: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1309 - accuracy: 0.9382 - val_loss: 0.1347 - val_accuracy: 0.9705\n",
            "Epoch 4085/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9392\n",
            "Epoch 4085: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1316 - accuracy: 0.9392 - val_loss: 0.2092 - val_accuracy: 0.9302\n",
            "Epoch 4086/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9375\n",
            "Epoch 4086: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1358 - accuracy: 0.9373 - val_loss: 0.1582 - val_accuracy: 0.9556\n",
            "Epoch 4087/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1290 - accuracy: 0.9409\n",
            "Epoch 4087: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1300 - accuracy: 0.9398 - val_loss: 0.1661 - val_accuracy: 0.9521\n",
            "Epoch 4088/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9400\n",
            "Epoch 4088: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1313 - accuracy: 0.9399 - val_loss: 0.1653 - val_accuracy: 0.9545\n",
            "Epoch 4089/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1304 - accuracy: 0.9387\n",
            "Epoch 4089: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1304 - accuracy: 0.9395 - val_loss: 0.1458 - val_accuracy: 0.9701\n",
            "Epoch 4090/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1380 - accuracy: 0.9374\n",
            "Epoch 4090: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1396 - accuracy: 0.9366 - val_loss: 0.1723 - val_accuracy: 0.9555\n",
            "Epoch 4091/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1327 - accuracy: 0.9375\n",
            "Epoch 4091: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1329 - accuracy: 0.9373 - val_loss: 0.1799 - val_accuracy: 0.9490\n",
            "Epoch 4092/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1400 - accuracy: 0.9339\n",
            "Epoch 4092: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1398 - accuracy: 0.9341 - val_loss: 0.1353 - val_accuracy: 0.9701\n",
            "Epoch 4093/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1279 - accuracy: 0.9392\n",
            "Epoch 4093: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9393 - val_loss: 0.1553 - val_accuracy: 0.9592\n",
            "Epoch 4094/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9409\n",
            "Epoch 4094: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1269 - accuracy: 0.9409 - val_loss: 0.1373 - val_accuracy: 0.9697\n",
            "Epoch 4095/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9391\n",
            "Epoch 4095: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1317 - accuracy: 0.9391 - val_loss: 0.1609 - val_accuracy: 0.9467\n",
            "Epoch 4096/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9381\n",
            "Epoch 4096: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1348 - accuracy: 0.9381 - val_loss: 0.1830 - val_accuracy: 0.9382\n",
            "Epoch 4097/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1311 - accuracy: 0.9402\n",
            "Epoch 4097: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1324 - accuracy: 0.9392 - val_loss: 0.1257 - val_accuracy: 0.9748\n",
            "Epoch 4098/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9405\n",
            "Epoch 4098: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1299 - accuracy: 0.9397 - val_loss: 0.1517 - val_accuracy: 0.9631\n",
            "Epoch 4099/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9399\n",
            "Epoch 4099: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1294 - accuracy: 0.9399 - val_loss: 0.1900 - val_accuracy: 0.9456\n",
            "Epoch 4100/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1338 - accuracy: 0.9374\n",
            "Epoch 4100: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1341 - accuracy: 0.9372 - val_loss: 0.2118 - val_accuracy: 0.9317\n",
            "Epoch 4101/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9361\n",
            "Epoch 4101: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1367 - accuracy: 0.9361 - val_loss: 0.1774 - val_accuracy: 0.9428\n",
            "Epoch 4102/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1296 - accuracy: 0.9381\n",
            "Epoch 4102: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9389 - val_loss: 0.1808 - val_accuracy: 0.9454\n",
            "Epoch 4103/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9402\n",
            "Epoch 4103: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1297 - accuracy: 0.9402 - val_loss: 0.1518 - val_accuracy: 0.9534\n",
            "Epoch 4104/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1291 - accuracy: 0.9407\n",
            "Epoch 4104: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1294 - accuracy: 0.9404 - val_loss: 0.1874 - val_accuracy: 0.9419\n",
            "Epoch 4105/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1295 - accuracy: 0.9386\n",
            "Epoch 4105: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1294 - accuracy: 0.9388 - val_loss: 0.1199 - val_accuracy: 0.9826\n",
            "Epoch 4106/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9411\n",
            "Epoch 4106: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1279 - accuracy: 0.9403 - val_loss: 0.2441 - val_accuracy: 0.9053\n",
            "Epoch 4107/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9408\n",
            "Epoch 4107: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1279 - accuracy: 0.9404 - val_loss: 0.1931 - val_accuracy: 0.9388\n",
            "Epoch 4108/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9426\n",
            "Epoch 4108: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1264 - accuracy: 0.9413 - val_loss: 0.1403 - val_accuracy: 0.9644\n",
            "Epoch 4109/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.9339\n",
            "Epoch 4109: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1420 - accuracy: 0.9339 - val_loss: 0.1759 - val_accuracy: 0.9484\n",
            "Epoch 4110/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1312 - accuracy: 0.9393\n",
            "Epoch 4110: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9394 - val_loss: 0.1405 - val_accuracy: 0.9633\n",
            "Epoch 4111/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1369 - accuracy: 0.9370\n",
            "Epoch 4111: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1373 - accuracy: 0.9367 - val_loss: 0.2141 - val_accuracy: 0.9272\n",
            "Epoch 4112/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1421 - accuracy: 0.9357\n",
            "Epoch 4112: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1434 - accuracy: 0.9347 - val_loss: 0.1800 - val_accuracy: 0.9425\n",
            "Epoch 4113/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1535 - accuracy: 0.9306\n",
            "Epoch 4113: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1522 - accuracy: 0.9309 - val_loss: 0.2100 - val_accuracy: 0.9358\n",
            "Epoch 4114/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.9351\n",
            "Epoch 4114: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1405 - accuracy: 0.9349 - val_loss: 0.1397 - val_accuracy: 0.9673\n",
            "Epoch 4115/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9390\n",
            "Epoch 4115: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1289 - accuracy: 0.9388 - val_loss: 0.1750 - val_accuracy: 0.9506\n",
            "Epoch 4116/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9411\n",
            "Epoch 4116: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1260 - accuracy: 0.9416 - val_loss: 0.1543 - val_accuracy: 0.9640\n",
            "Epoch 4117/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9407\n",
            "Epoch 4117: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1276 - accuracy: 0.9407 - val_loss: 0.2355 - val_accuracy: 0.9178\n",
            "Epoch 4118/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9394\n",
            "Epoch 4118: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1298 - accuracy: 0.9394 - val_loss: 0.1526 - val_accuracy: 0.9690\n",
            "Epoch 4119/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9403\n",
            "Epoch 4119: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1279 - accuracy: 0.9407 - val_loss: 0.1633 - val_accuracy: 0.9549\n",
            "Epoch 4120/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9420\n",
            "Epoch 4120: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1245 - accuracy: 0.9417 - val_loss: 0.1598 - val_accuracy: 0.9594\n",
            "Epoch 4121/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1272 - accuracy: 0.9410\n",
            "Epoch 4121: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1280 - accuracy: 0.9408 - val_loss: 0.1387 - val_accuracy: 0.9718\n",
            "Epoch 4122/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9382\n",
            "Epoch 4122: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1330 - accuracy: 0.9379 - val_loss: 0.1307 - val_accuracy: 0.9764\n",
            "Epoch 4123/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1377 - accuracy: 0.9365\n",
            "Epoch 4123: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1372 - accuracy: 0.9360 - val_loss: 0.1876 - val_accuracy: 0.9436\n",
            "Epoch 4124/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1335 - accuracy: 0.9388\n",
            "Epoch 4124: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1340 - accuracy: 0.9383 - val_loss: 0.1831 - val_accuracy: 0.9425\n",
            "Epoch 4125/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9356\n",
            "Epoch 4125: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1381 - accuracy: 0.9356 - val_loss: 0.1902 - val_accuracy: 0.9310\n",
            "Epoch 4126/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1333 - accuracy: 0.9377\n",
            "Epoch 4126: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1329 - accuracy: 0.9380 - val_loss: 0.1681 - val_accuracy: 0.9545\n",
            "Epoch 4127/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1278 - accuracy: 0.9411\n",
            "Epoch 4127: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1287 - accuracy: 0.9404 - val_loss: 0.2064 - val_accuracy: 0.9243\n",
            "Epoch 4128/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9390\n",
            "Epoch 4128: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9388 - val_loss: 0.2287 - val_accuracy: 0.9076\n",
            "Epoch 4129/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9413\n",
            "Epoch 4129: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1282 - accuracy: 0.9413 - val_loss: 0.1927 - val_accuracy: 0.9343\n",
            "Epoch 4130/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1299 - accuracy: 0.9400\n",
            "Epoch 4130: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1302 - accuracy: 0.9401 - val_loss: 0.1419 - val_accuracy: 0.9644\n",
            "Epoch 4131/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1261 - accuracy: 0.9394\n",
            "Epoch 4131: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1261 - accuracy: 0.9394 - val_loss: 0.2183 - val_accuracy: 0.9152\n",
            "Epoch 4132/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9338\n",
            "Epoch 4132: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1432 - accuracy: 0.9338 - val_loss: 0.1556 - val_accuracy: 0.9582\n",
            "Epoch 4133/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9388\n",
            "Epoch 4133: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1305 - accuracy: 0.9391 - val_loss: 0.1710 - val_accuracy: 0.9486\n",
            "Epoch 4134/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9426\n",
            "Epoch 4134: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1255 - accuracy: 0.9423 - val_loss: 0.1787 - val_accuracy: 0.9475\n",
            "Epoch 4135/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1262 - accuracy: 0.9416\n",
            "Epoch 4135: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1253 - accuracy: 0.9418 - val_loss: 0.1353 - val_accuracy: 0.9644\n",
            "Epoch 4136/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1267 - accuracy: 0.9427\n",
            "Epoch 4136: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1269 - accuracy: 0.9419 - val_loss: 0.1680 - val_accuracy: 0.9516\n",
            "Epoch 4137/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1341 - accuracy: 0.9378\n",
            "Epoch 4137: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1350 - accuracy: 0.9372 - val_loss: 0.1446 - val_accuracy: 0.9675\n",
            "Epoch 4138/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1319 - accuracy: 0.9401\n",
            "Epoch 4138: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1330 - accuracy: 0.9399 - val_loss: 0.1712 - val_accuracy: 0.9532\n",
            "Epoch 4139/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9382\n",
            "Epoch 4139: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1324 - accuracy: 0.9382 - val_loss: 0.2546 - val_accuracy: 0.9185\n",
            "Epoch 4140/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1350 - accuracy: 0.9374\n",
            "Epoch 4140: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1336 - accuracy: 0.9382 - val_loss: 0.1464 - val_accuracy: 0.9684\n",
            "Epoch 4141/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9394\n",
            "Epoch 4141: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1315 - accuracy: 0.9394 - val_loss: 0.1436 - val_accuracy: 0.9582\n",
            "Epoch 4142/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9379\n",
            "Epoch 4142: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1343 - accuracy: 0.9378 - val_loss: 0.1646 - val_accuracy: 0.9555\n",
            "Epoch 4143/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9423\n",
            "Epoch 4143: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1245 - accuracy: 0.9423 - val_loss: 0.1538 - val_accuracy: 0.9647\n",
            "Epoch 4144/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1302 - accuracy: 0.9389\n",
            "Epoch 4144: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1307 - accuracy: 0.9389 - val_loss: 0.1855 - val_accuracy: 0.9415\n",
            "Epoch 4145/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9377\n",
            "Epoch 4145: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1350 - accuracy: 0.9372 - val_loss: 0.1750 - val_accuracy: 0.9438\n",
            "Epoch 4146/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9409\n",
            "Epoch 4146: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1268 - accuracy: 0.9408 - val_loss: 0.1423 - val_accuracy: 0.9710\n",
            "Epoch 4147/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1235 - accuracy: 0.9422\n",
            "Epoch 4147: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1255 - accuracy: 0.9409 - val_loss: 0.2243 - val_accuracy: 0.9202\n",
            "Epoch 4148/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1290 - accuracy: 0.9398\n",
            "Epoch 4148: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1302 - accuracy: 0.9399 - val_loss: 0.1502 - val_accuracy: 0.9599\n",
            "Epoch 4149/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9406\n",
            "Epoch 4149: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9406 - val_loss: 0.2279 - val_accuracy: 0.9148\n",
            "Epoch 4150/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9386\n",
            "Epoch 4150: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1328 - accuracy: 0.9383 - val_loss: 0.1238 - val_accuracy: 0.9779\n",
            "Epoch 4151/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9395\n",
            "Epoch 4151: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1316 - accuracy: 0.9391 - val_loss: 0.1537 - val_accuracy: 0.9625\n",
            "Epoch 4152/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9374\n",
            "Epoch 4152: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1359 - accuracy: 0.9372 - val_loss: 0.1994 - val_accuracy: 0.9391\n",
            "Epoch 4153/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9364\n",
            "Epoch 4153: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1380 - accuracy: 0.9362 - val_loss: 0.1559 - val_accuracy: 0.9610\n",
            "Epoch 4154/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9393\n",
            "Epoch 4154: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1319 - accuracy: 0.9393 - val_loss: 0.1455 - val_accuracy: 0.9662\n",
            "Epoch 4155/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9388\n",
            "Epoch 4155: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1309 - accuracy: 0.9388 - val_loss: 0.1839 - val_accuracy: 0.9469\n",
            "Epoch 4156/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9395\n",
            "Epoch 4156: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1286 - accuracy: 0.9396 - val_loss: 0.1338 - val_accuracy: 0.9744\n",
            "Epoch 4157/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1284 - accuracy: 0.9414\n",
            "Epoch 4157: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1283 - accuracy: 0.9409 - val_loss: 0.1691 - val_accuracy: 0.9471\n",
            "Epoch 4158/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1253 - accuracy: 0.9420\n",
            "Epoch 4158: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1263 - accuracy: 0.9418 - val_loss: 0.1547 - val_accuracy: 0.9616\n",
            "Epoch 4159/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9393\n",
            "Epoch 4159: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9393 - val_loss: 0.1579 - val_accuracy: 0.9614\n",
            "Epoch 4160/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9407\n",
            "Epoch 4160: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1259 - accuracy: 0.9407 - val_loss: 0.1355 - val_accuracy: 0.9748\n",
            "Epoch 4161/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9403\n",
            "Epoch 4161: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1297 - accuracy: 0.9403 - val_loss: 0.1740 - val_accuracy: 0.9499\n",
            "Epoch 4162/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9378\n",
            "Epoch 4162: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1336 - accuracy: 0.9378 - val_loss: 0.1670 - val_accuracy: 0.9477\n",
            "Epoch 4163/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1249 - accuracy: 0.9418\n",
            "Epoch 4163: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1257 - accuracy: 0.9413 - val_loss: 0.1495 - val_accuracy: 0.9610\n",
            "Epoch 4164/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9400\n",
            "Epoch 4164: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1301 - accuracy: 0.9397 - val_loss: 0.2142 - val_accuracy: 0.9141\n",
            "Epoch 4165/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1316 - accuracy: 0.9391\n",
            "Epoch 4165: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1331 - accuracy: 0.9385 - val_loss: 0.1966 - val_accuracy: 0.9315\n",
            "Epoch 4166/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1426 - accuracy: 0.9347\n",
            "Epoch 4166: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1426 - accuracy: 0.9349 - val_loss: 0.1950 - val_accuracy: 0.9363\n",
            "Epoch 4167/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1376 - accuracy: 0.9359\n",
            "Epoch 4167: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1359 - accuracy: 0.9371 - val_loss: 0.1559 - val_accuracy: 0.9638\n",
            "Epoch 4168/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9352\n",
            "Epoch 4168: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1389 - accuracy: 0.9350 - val_loss: 0.1832 - val_accuracy: 0.9401\n",
            "Epoch 4169/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1346 - accuracy: 0.9384\n",
            "Epoch 4169: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1346 - accuracy: 0.9381 - val_loss: 0.1303 - val_accuracy: 0.9694\n",
            "Epoch 4170/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9396\n",
            "Epoch 4170: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9391 - val_loss: 0.1491 - val_accuracy: 0.9666\n",
            "Epoch 4171/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9411\n",
            "Epoch 4171: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1284 - accuracy: 0.9411 - val_loss: 0.1268 - val_accuracy: 0.9753\n",
            "Epoch 4172/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9400\n",
            "Epoch 4172: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1309 - accuracy: 0.9400 - val_loss: 0.1441 - val_accuracy: 0.9703\n",
            "Epoch 4173/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1369 - accuracy: 0.9373\n",
            "Epoch 4173: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1363 - accuracy: 0.9373 - val_loss: 0.1769 - val_accuracy: 0.9408\n",
            "Epoch 4174/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9381\n",
            "Epoch 4174: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1324 - accuracy: 0.9381 - val_loss: 0.1651 - val_accuracy: 0.9558\n",
            "Epoch 4175/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9409\n",
            "Epoch 4175: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1275 - accuracy: 0.9409 - val_loss: 0.1599 - val_accuracy: 0.9545\n",
            "Epoch 4176/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9389\n",
            "Epoch 4176: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1279 - accuracy: 0.9389 - val_loss: 0.1675 - val_accuracy: 0.9560\n",
            "Epoch 4177/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1338 - accuracy: 0.9383\n",
            "Epoch 4177: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1336 - accuracy: 0.9382 - val_loss: 0.1360 - val_accuracy: 0.9675\n",
            "Epoch 4178/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.9398\n",
            "Epoch 4178: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1329 - accuracy: 0.9397 - val_loss: 0.1963 - val_accuracy: 0.9406\n",
            "Epoch 4179/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1322 - accuracy: 0.9397\n",
            "Epoch 4179: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1327 - accuracy: 0.9389 - val_loss: 0.1562 - val_accuracy: 0.9618\n",
            "Epoch 4180/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1408 - accuracy: 0.9360\n",
            "Epoch 4180: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1407 - accuracy: 0.9362 - val_loss: 0.2237 - val_accuracy: 0.9274\n",
            "Epoch 4181/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9386\n",
            "Epoch 4181: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1334 - accuracy: 0.9388 - val_loss: 0.1854 - val_accuracy: 0.9388\n",
            "Epoch 4182/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1301 - accuracy: 0.9409\n",
            "Epoch 4182: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1302 - accuracy: 0.9406 - val_loss: 0.1338 - val_accuracy: 0.9640\n",
            "Epoch 4183/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1292 - accuracy: 0.9409\n",
            "Epoch 4183: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1294 - accuracy: 0.9406 - val_loss: 0.1594 - val_accuracy: 0.9579\n",
            "Epoch 4184/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9376\n",
            "Epoch 4184: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1352 - accuracy: 0.9379 - val_loss: 0.2041 - val_accuracy: 0.9228\n",
            "Epoch 4185/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9400\n",
            "Epoch 4185: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1321 - accuracy: 0.9400 - val_loss: 0.2013 - val_accuracy: 0.9304\n",
            "Epoch 4186/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9385\n",
            "Epoch 4186: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1314 - accuracy: 0.9387 - val_loss: 0.1905 - val_accuracy: 0.9321\n",
            "Epoch 4187/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1385 - accuracy: 0.9366\n",
            "Epoch 4187: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1394 - accuracy: 0.9360 - val_loss: 0.1349 - val_accuracy: 0.9720\n",
            "Epoch 4188/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.9396\n",
            "Epoch 4188: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1317 - accuracy: 0.9396 - val_loss: 0.1486 - val_accuracy: 0.9627\n",
            "Epoch 4189/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1285 - accuracy: 0.9401\n",
            "Epoch 4189: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1280 - accuracy: 0.9401 - val_loss: 0.1217 - val_accuracy: 0.9753\n",
            "Epoch 4190/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9432\n",
            "Epoch 4190: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1259 - accuracy: 0.9427 - val_loss: 0.1422 - val_accuracy: 0.9707\n",
            "Epoch 4191/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9410\n",
            "Epoch 4191: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1284 - accuracy: 0.9410 - val_loss: 0.1830 - val_accuracy: 0.9352\n",
            "Epoch 4192/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9396\n",
            "Epoch 4192: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1310 - accuracy: 0.9396 - val_loss: 0.1463 - val_accuracy: 0.9659\n",
            "Epoch 4193/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9376\n",
            "Epoch 4193: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1332 - accuracy: 0.9376 - val_loss: 0.1449 - val_accuracy: 0.9649\n",
            "Epoch 4194/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1339 - accuracy: 0.9376\n",
            "Epoch 4194: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1331 - accuracy: 0.9384 - val_loss: 0.1810 - val_accuracy: 0.9417\n",
            "Epoch 4195/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9407\n",
            "Epoch 4195: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1279 - accuracy: 0.9408 - val_loss: 0.1272 - val_accuracy: 0.9705\n",
            "Epoch 4196/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1258 - accuracy: 0.9429\n",
            "Epoch 4196: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1264 - accuracy: 0.9421 - val_loss: 0.2011 - val_accuracy: 0.9363\n",
            "Epoch 4197/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1265 - accuracy: 0.9414\n",
            "Epoch 4197: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1282 - accuracy: 0.9408 - val_loss: 0.2119 - val_accuracy: 0.9219\n",
            "Epoch 4198/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1382 - accuracy: 0.9364\n",
            "Epoch 4198: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1376 - accuracy: 0.9366 - val_loss: 0.1503 - val_accuracy: 0.9651\n",
            "Epoch 4199/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9390\n",
            "Epoch 4199: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1332 - accuracy: 0.9390 - val_loss: 0.1758 - val_accuracy: 0.9439\n",
            "Epoch 4200/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1299 - accuracy: 0.9396\n",
            "Epoch 4200: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1310 - accuracy: 0.9390 - val_loss: 0.1567 - val_accuracy: 0.9553\n",
            "Epoch 4201/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9388\n",
            "Epoch 4201: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1328 - accuracy: 0.9384 - val_loss: 0.2069 - val_accuracy: 0.9178\n",
            "Epoch 4202/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1357 - accuracy: 0.9378\n",
            "Epoch 4202: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1350 - accuracy: 0.9381 - val_loss: 0.1559 - val_accuracy: 0.9581\n",
            "Epoch 4203/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1285 - accuracy: 0.9404\n",
            "Epoch 4203: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1294 - accuracy: 0.9398 - val_loss: 0.1481 - val_accuracy: 0.9616\n",
            "Epoch 4204/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1264 - accuracy: 0.9393\n",
            "Epoch 4204: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1265 - accuracy: 0.9398 - val_loss: 0.1828 - val_accuracy: 0.9401\n",
            "Epoch 4205/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1275 - accuracy: 0.9405\n",
            "Epoch 4205: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1262 - accuracy: 0.9413 - val_loss: 0.1450 - val_accuracy: 0.9657\n",
            "Epoch 4206/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1286 - accuracy: 0.9402\n",
            "Epoch 4206: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1288 - accuracy: 0.9398 - val_loss: 0.1764 - val_accuracy: 0.9490\n",
            "Epoch 4207/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1256 - accuracy: 0.9423\n",
            "Epoch 4207: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1273 - accuracy: 0.9415 - val_loss: 0.1048 - val_accuracy: 0.9868\n",
            "Epoch 4208/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9346\n",
            "Epoch 4208: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1439 - accuracy: 0.9346 - val_loss: 0.2042 - val_accuracy: 0.9421\n",
            "Epoch 4209/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1341 - accuracy: 0.9383\n",
            "Epoch 4209: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1339 - accuracy: 0.9381 - val_loss: 0.1596 - val_accuracy: 0.9490\n",
            "Epoch 4210/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9400\n",
            "Epoch 4210: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1305 - accuracy: 0.9400 - val_loss: 0.2092 - val_accuracy: 0.9306\n",
            "Epoch 4211/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.9385\n",
            "Epoch 4211: loss did not improve from 0.12216\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1313 - accuracy: 0.9387 - val_loss: 0.1645 - val_accuracy: 0.9529\n",
            "Epoch 4212/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9435\n",
            "Epoch 4212: loss improved from 0.12216 to 0.12179, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.1218 - accuracy: 0.9438 - val_loss: 0.1268 - val_accuracy: 0.9779\n",
            "Epoch 4213/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1243 - accuracy: 0.9408\n",
            "Epoch 4213: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1241 - accuracy: 0.9414 - val_loss: 0.1898 - val_accuracy: 0.9421\n",
            "Epoch 4214/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9394\n",
            "Epoch 4214: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1288 - accuracy: 0.9390 - val_loss: 0.1865 - val_accuracy: 0.9382\n",
            "Epoch 4215/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1332 - accuracy: 0.9385\n",
            "Epoch 4215: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1331 - accuracy: 0.9384 - val_loss: 0.1427 - val_accuracy: 0.9716\n",
            "Epoch 4216/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9397\n",
            "Epoch 4216: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1310 - accuracy: 0.9397 - val_loss: 0.1462 - val_accuracy: 0.9634\n",
            "Epoch 4217/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9386\n",
            "Epoch 4217: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1294 - accuracy: 0.9394 - val_loss: 0.1804 - val_accuracy: 0.9373\n",
            "Epoch 4218/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1391 - accuracy: 0.9366\n",
            "Epoch 4218: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1394 - accuracy: 0.9363 - val_loss: 0.1847 - val_accuracy: 0.9404\n",
            "Epoch 4219/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1384 - accuracy: 0.9371\n",
            "Epoch 4219: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1393 - accuracy: 0.9365 - val_loss: 0.1566 - val_accuracy: 0.9543\n",
            "Epoch 4220/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9400\n",
            "Epoch 4220: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1309 - accuracy: 0.9405 - val_loss: 0.1346 - val_accuracy: 0.9738\n",
            "Epoch 4221/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1308 - accuracy: 0.9405\n",
            "Epoch 4221: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1322 - accuracy: 0.9404 - val_loss: 0.1821 - val_accuracy: 0.9443\n",
            "Epoch 4222/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9362\n",
            "Epoch 4222: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1387 - accuracy: 0.9362 - val_loss: 0.1665 - val_accuracy: 0.9538\n",
            "Epoch 4223/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9390\n",
            "Epoch 4223: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1332 - accuracy: 0.9388 - val_loss: 0.1660 - val_accuracy: 0.9584\n",
            "Epoch 4224/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1333 - accuracy: 0.9392\n",
            "Epoch 4224: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1327 - accuracy: 0.9398 - val_loss: 0.1517 - val_accuracy: 0.9594\n",
            "Epoch 4225/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9416\n",
            "Epoch 4225: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1257 - accuracy: 0.9416 - val_loss: 0.1365 - val_accuracy: 0.9749\n",
            "Epoch 4226/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1313 - accuracy: 0.9386\n",
            "Epoch 4226: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1313 - accuracy: 0.9391 - val_loss: 0.1424 - val_accuracy: 0.9720\n",
            "Epoch 4227/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1384 - accuracy: 0.9359\n",
            "Epoch 4227: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1386 - accuracy: 0.9353 - val_loss: 0.1711 - val_accuracy: 0.9538\n",
            "Epoch 4228/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.9389\n",
            "Epoch 4228: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1352 - accuracy: 0.9386 - val_loss: 0.1403 - val_accuracy: 0.9614\n",
            "Epoch 4229/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1403 - accuracy: 0.9342\n",
            "Epoch 4229: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1409 - accuracy: 0.9339 - val_loss: 0.1254 - val_accuracy: 0.9772\n",
            "Epoch 4230/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1410 - accuracy: 0.9341\n",
            "Epoch 4230: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1403 - accuracy: 0.9346 - val_loss: 0.1793 - val_accuracy: 0.9499\n",
            "Epoch 4231/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1299 - accuracy: 0.9399\n",
            "Epoch 4231: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9394 - val_loss: 0.1592 - val_accuracy: 0.9543\n",
            "Epoch 4232/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1263 - accuracy: 0.9405\n",
            "Epoch 4232: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1274 - accuracy: 0.9401 - val_loss: 0.1815 - val_accuracy: 0.9456\n",
            "Epoch 4233/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1259 - accuracy: 0.9417\n",
            "Epoch 4233: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1268 - accuracy: 0.9409 - val_loss: 0.1816 - val_accuracy: 0.9432\n",
            "Epoch 4234/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9412\n",
            "Epoch 4234: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1268 - accuracy: 0.9410 - val_loss: 0.1739 - val_accuracy: 0.9452\n",
            "Epoch 4235/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9419\n",
            "Epoch 4235: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1280 - accuracy: 0.9410 - val_loss: 0.1007 - val_accuracy: 0.9861\n",
            "Epoch 4236/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1322 - accuracy: 0.9385\n",
            "Epoch 4236: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1326 - accuracy: 0.9383 - val_loss: 0.2165 - val_accuracy: 0.9235\n",
            "Epoch 4237/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1265 - accuracy: 0.9412\n",
            "Epoch 4237: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1267 - accuracy: 0.9414 - val_loss: 0.1535 - val_accuracy: 0.9603\n",
            "Epoch 4238/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1303 - accuracy: 0.9399\n",
            "Epoch 4238: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1295 - accuracy: 0.9403 - val_loss: 0.1282 - val_accuracy: 0.9740\n",
            "Epoch 4239/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9407\n",
            "Epoch 4239: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1316 - accuracy: 0.9402 - val_loss: 0.1775 - val_accuracy: 0.9465\n",
            "Epoch 4240/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9419\n",
            "Epoch 4240: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1262 - accuracy: 0.9418 - val_loss: 0.1279 - val_accuracy: 0.9746\n",
            "Epoch 4241/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1333 - accuracy: 0.9366\n",
            "Epoch 4241: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1336 - accuracy: 0.9362 - val_loss: 0.2323 - val_accuracy: 0.9141\n",
            "Epoch 4242/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1315 - accuracy: 0.9397\n",
            "Epoch 4242: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1334 - accuracy: 0.9384 - val_loss: 0.1390 - val_accuracy: 0.9696\n",
            "Epoch 4243/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9401\n",
            "Epoch 4243: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1282 - accuracy: 0.9407 - val_loss: 0.1461 - val_accuracy: 0.9616\n",
            "Epoch 4244/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9407\n",
            "Epoch 4244: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1279 - accuracy: 0.9406 - val_loss: 0.2029 - val_accuracy: 0.9291\n",
            "Epoch 4245/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9424\n",
            "Epoch 4245: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1256 - accuracy: 0.9420 - val_loss: 0.1604 - val_accuracy: 0.9484\n",
            "Epoch 4246/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9384\n",
            "Epoch 4246: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1305 - accuracy: 0.9384 - val_loss: 0.1454 - val_accuracy: 0.9642\n",
            "Epoch 4247/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9402\n",
            "Epoch 4247: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1268 - accuracy: 0.9403 - val_loss: 0.1399 - val_accuracy: 0.9683\n",
            "Epoch 4248/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1234 - accuracy: 0.9440\n",
            "Epoch 4248: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9429 - val_loss: 0.1484 - val_accuracy: 0.9588\n",
            "Epoch 4249/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9427\n",
            "Epoch 4249: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1236 - accuracy: 0.9427 - val_loss: 0.1066 - val_accuracy: 0.9837\n",
            "Epoch 4250/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9398\n",
            "Epoch 4250: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1313 - accuracy: 0.9397 - val_loss: 0.1720 - val_accuracy: 0.9419\n",
            "Epoch 4251/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9385\n",
            "Epoch 4251: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9387 - val_loss: 0.1754 - val_accuracy: 0.9508\n",
            "Epoch 4252/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9408\n",
            "Epoch 4252: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1276 - accuracy: 0.9406 - val_loss: 0.1872 - val_accuracy: 0.9365\n",
            "Epoch 4253/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9416\n",
            "Epoch 4253: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1266 - accuracy: 0.9420 - val_loss: 0.1763 - val_accuracy: 0.9475\n",
            "Epoch 4254/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9424\n",
            "Epoch 4254: loss did not improve from 0.12179\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1238 - accuracy: 0.9424 - val_loss: 0.1462 - val_accuracy: 0.9607\n",
            "Epoch 4255/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9438\n",
            "Epoch 4255: loss improved from 0.12179 to 0.12174, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 0.1217 - accuracy: 0.9438 - val_loss: 0.2082 - val_accuracy: 0.9291\n",
            "Epoch 4256/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9396\n",
            "Epoch 4256: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1267 - accuracy: 0.9396 - val_loss: 0.1718 - val_accuracy: 0.9376\n",
            "Epoch 4257/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9383\n",
            "Epoch 4257: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1322 - accuracy: 0.9381 - val_loss: 0.2270 - val_accuracy: 0.9085\n",
            "Epoch 4258/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9377\n",
            "Epoch 4258: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1384 - accuracy: 0.9368 - val_loss: 0.1874 - val_accuracy: 0.9363\n",
            "Epoch 4259/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9398\n",
            "Epoch 4259: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1286 - accuracy: 0.9398 - val_loss: 0.1883 - val_accuracy: 0.9343\n",
            "Epoch 4260/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9401\n",
            "Epoch 4260: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1354 - accuracy: 0.9398 - val_loss: 0.1790 - val_accuracy: 0.9488\n",
            "Epoch 4261/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1360 - accuracy: 0.9373\n",
            "Epoch 4261: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1350 - accuracy: 0.9376 - val_loss: 0.1406 - val_accuracy: 0.9668\n",
            "Epoch 4262/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9395\n",
            "Epoch 4262: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9395 - val_loss: 0.1683 - val_accuracy: 0.9543\n",
            "Epoch 4263/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1269 - accuracy: 0.9419\n",
            "Epoch 4263: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9408 - val_loss: 0.1118 - val_accuracy: 0.9818\n",
            "Epoch 4264/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9333\n",
            "Epoch 4264: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1420 - accuracy: 0.9331 - val_loss: 0.1599 - val_accuracy: 0.9568\n",
            "Epoch 4265/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9358\n",
            "Epoch 4265: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1376 - accuracy: 0.9358 - val_loss: 0.1496 - val_accuracy: 0.9649\n",
            "Epoch 4266/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1254 - accuracy: 0.9420\n",
            "Epoch 4266: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1267 - accuracy: 0.9414 - val_loss: 0.1439 - val_accuracy: 0.9659\n",
            "Epoch 4267/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1328 - accuracy: 0.9381\n",
            "Epoch 4267: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1323 - accuracy: 0.9386 - val_loss: 0.1669 - val_accuracy: 0.9477\n",
            "Epoch 4268/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1297 - accuracy: 0.9398\n",
            "Epoch 4268: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1299 - accuracy: 0.9396 - val_loss: 0.2084 - val_accuracy: 0.9198\n",
            "Epoch 4269/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9380\n",
            "Epoch 4269: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1341 - accuracy: 0.9380 - val_loss: 0.1796 - val_accuracy: 0.9414\n",
            "Epoch 4270/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1363 - accuracy: 0.9381\n",
            "Epoch 4270: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1363 - accuracy: 0.9386 - val_loss: 0.1805 - val_accuracy: 0.9491\n",
            "Epoch 4271/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9371\n",
            "Epoch 4271: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1357 - accuracy: 0.9377 - val_loss: 0.1579 - val_accuracy: 0.9599\n",
            "Epoch 4272/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1395 - accuracy: 0.9361\n",
            "Epoch 4272: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1406 - accuracy: 0.9358 - val_loss: 0.1309 - val_accuracy: 0.9738\n",
            "Epoch 4273/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1396 - accuracy: 0.9355\n",
            "Epoch 4273: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1394 - accuracy: 0.9358 - val_loss: 0.1788 - val_accuracy: 0.9464\n",
            "Epoch 4274/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9388\n",
            "Epoch 4274: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1324 - accuracy: 0.9388 - val_loss: 0.1644 - val_accuracy: 0.9543\n",
            "Epoch 4275/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1249 - accuracy: 0.9421\n",
            "Epoch 4275: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1263 - accuracy: 0.9415 - val_loss: 0.1592 - val_accuracy: 0.9555\n",
            "Epoch 4276/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9397\n",
            "Epoch 4276: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1280 - accuracy: 0.9397 - val_loss: 0.1434 - val_accuracy: 0.9631\n",
            "Epoch 4277/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1293 - accuracy: 0.9409\n",
            "Epoch 4277: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1304 - accuracy: 0.9406 - val_loss: 0.1622 - val_accuracy: 0.9521\n",
            "Epoch 4278/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1337 - accuracy: 0.9370\n",
            "Epoch 4278: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1329 - accuracy: 0.9375 - val_loss: 0.2370 - val_accuracy: 0.9182\n",
            "Epoch 4279/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.9378\n",
            "Epoch 4279: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1315 - accuracy: 0.9371 - val_loss: 0.1787 - val_accuracy: 0.9436\n",
            "Epoch 4280/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1329 - accuracy: 0.9370\n",
            "Epoch 4280: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1326 - accuracy: 0.9375 - val_loss: 0.1341 - val_accuracy: 0.9716\n",
            "Epoch 4281/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1288 - accuracy: 0.9394\n",
            "Epoch 4281: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1297 - accuracy: 0.9391 - val_loss: 0.1804 - val_accuracy: 0.9482\n",
            "Epoch 4282/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9368\n",
            "Epoch 4282: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1307 - accuracy: 0.9368 - val_loss: 0.1385 - val_accuracy: 0.9621\n",
            "Epoch 4283/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1337 - accuracy: 0.9375\n",
            "Epoch 4283: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1335 - accuracy: 0.9380 - val_loss: 0.1562 - val_accuracy: 0.9575\n",
            "Epoch 4284/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1272 - accuracy: 0.9395\n",
            "Epoch 4284: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9391 - val_loss: 0.1536 - val_accuracy: 0.9631\n",
            "Epoch 4285/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1251 - accuracy: 0.9420\n",
            "Epoch 4285: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1263 - accuracy: 0.9414 - val_loss: 0.1922 - val_accuracy: 0.9369\n",
            "Epoch 4286/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1260 - accuracy: 0.9410\n",
            "Epoch 4286: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1280 - accuracy: 0.9394 - val_loss: 0.1549 - val_accuracy: 0.9595\n",
            "Epoch 4287/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9404\n",
            "Epoch 4287: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1258 - accuracy: 0.9407 - val_loss: 0.1631 - val_accuracy: 0.9499\n",
            "Epoch 4288/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9409\n",
            "Epoch 4288: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1256 - accuracy: 0.9406 - val_loss: 0.2058 - val_accuracy: 0.9280\n",
            "Epoch 4289/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1330 - accuracy: 0.9384\n",
            "Epoch 4289: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1326 - accuracy: 0.9385 - val_loss: 0.2083 - val_accuracy: 0.9217\n",
            "Epoch 4290/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9399\n",
            "Epoch 4290: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1299 - accuracy: 0.9399 - val_loss: 0.2167 - val_accuracy: 0.9161\n",
            "Epoch 4291/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1288 - accuracy: 0.9399\n",
            "Epoch 4291: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1283 - accuracy: 0.9402 - val_loss: 0.1501 - val_accuracy: 0.9595\n",
            "Epoch 4292/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9392\n",
            "Epoch 4292: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9387 - val_loss: 0.1709 - val_accuracy: 0.9488\n",
            "Epoch 4293/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1274 - accuracy: 0.9411\n",
            "Epoch 4293: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 0.9409 - val_loss: 0.1668 - val_accuracy: 0.9577\n",
            "Epoch 4294/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1249 - accuracy: 0.9419\n",
            "Epoch 4294: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1270 - accuracy: 0.9410 - val_loss: 0.1224 - val_accuracy: 0.9744\n",
            "Epoch 4295/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1544 - accuracy: 0.9308\n",
            "Epoch 4295: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1545 - accuracy: 0.9310 - val_loss: 0.1638 - val_accuracy: 0.9573\n",
            "Epoch 4296/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1464 - accuracy: 0.9327\n",
            "Epoch 4296: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1469 - accuracy: 0.9326 - val_loss: 0.1392 - val_accuracy: 0.9697\n",
            "Epoch 4297/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1379 - accuracy: 0.9362\n",
            "Epoch 4297: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1379 - accuracy: 0.9362 - val_loss: 0.1291 - val_accuracy: 0.9749\n",
            "Epoch 4298/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9393\n",
            "Epoch 4298: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1316 - accuracy: 0.9391 - val_loss: 0.1290 - val_accuracy: 0.9755\n",
            "Epoch 4299/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9409\n",
            "Epoch 4299: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1271 - accuracy: 0.9409 - val_loss: 0.1798 - val_accuracy: 0.9417\n",
            "Epoch 4300/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1250 - accuracy: 0.9417\n",
            "Epoch 4300: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1240 - accuracy: 0.9423 - val_loss: 0.1525 - val_accuracy: 0.9579\n",
            "Epoch 4301/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1220 - accuracy: 0.9428\n",
            "Epoch 4301: loss did not improve from 0.12174\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1219 - accuracy: 0.9428 - val_loss: 0.1232 - val_accuracy: 0.9753\n",
            "Epoch 4302/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9448\n",
            "Epoch 4302: loss improved from 0.12174 to 0.12146, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.1215 - accuracy: 0.9443 - val_loss: 0.1585 - val_accuracy: 0.9536\n",
            "Epoch 4303/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9419\n",
            "Epoch 4303: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1225 - accuracy: 0.9417 - val_loss: 0.1616 - val_accuracy: 0.9568\n",
            "Epoch 4304/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9423\n",
            "Epoch 4304: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1246 - accuracy: 0.9418 - val_loss: 0.1801 - val_accuracy: 0.9445\n",
            "Epoch 4305/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1435 - accuracy: 0.9366\n",
            "Epoch 4305: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1442 - accuracy: 0.9361 - val_loss: 0.1988 - val_accuracy: 0.9441\n",
            "Epoch 4306/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.9383\n",
            "Epoch 4306: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1334 - accuracy: 0.9383 - val_loss: 0.1485 - val_accuracy: 0.9590\n",
            "Epoch 4307/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9408\n",
            "Epoch 4307: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1262 - accuracy: 0.9410 - val_loss: 0.1363 - val_accuracy: 0.9716\n",
            "Epoch 4308/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.9393\n",
            "Epoch 4308: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1318 - accuracy: 0.9394 - val_loss: 0.2310 - val_accuracy: 0.9157\n",
            "Epoch 4309/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1296 - accuracy: 0.9398\n",
            "Epoch 4309: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1304 - accuracy: 0.9394 - val_loss: 0.1783 - val_accuracy: 0.9475\n",
            "Epoch 4310/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1317 - accuracy: 0.9393\n",
            "Epoch 4310: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1322 - accuracy: 0.9391 - val_loss: 0.1862 - val_accuracy: 0.9428\n",
            "Epoch 4311/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1252 - accuracy: 0.9412\n",
            "Epoch 4311: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1256 - accuracy: 0.9410 - val_loss: 0.1598 - val_accuracy: 0.9571\n",
            "Epoch 4312/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9427\n",
            "Epoch 4312: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1271 - accuracy: 0.9423 - val_loss: 0.2152 - val_accuracy: 0.9304\n",
            "Epoch 4313/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1311 - accuracy: 0.9390\n",
            "Epoch 4313: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1308 - accuracy: 0.9388 - val_loss: 0.1563 - val_accuracy: 0.9532\n",
            "Epoch 4314/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9383\n",
            "Epoch 4314: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1324 - accuracy: 0.9387 - val_loss: 0.1497 - val_accuracy: 0.9651\n",
            "Epoch 4315/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9377\n",
            "Epoch 4315: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1371 - accuracy: 0.9377 - val_loss: 0.2785 - val_accuracy: 0.9057\n",
            "Epoch 4316/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9345\n",
            "Epoch 4316: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1414 - accuracy: 0.9344 - val_loss: 0.1432 - val_accuracy: 0.9634\n",
            "Epoch 4317/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1379 - accuracy: 0.9376\n",
            "Epoch 4317: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1369 - accuracy: 0.9382 - val_loss: 0.1733 - val_accuracy: 0.9514\n",
            "Epoch 4318/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.9358\n",
            "Epoch 4318: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1386 - accuracy: 0.9360 - val_loss: 0.3119 - val_accuracy: 0.8755\n",
            "Epoch 4319/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9311\n",
            "Epoch 4319: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1470 - accuracy: 0.9314 - val_loss: 0.1616 - val_accuracy: 0.9579\n",
            "Epoch 4320/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9385\n",
            "Epoch 4320: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1347 - accuracy: 0.9385 - val_loss: 0.1807 - val_accuracy: 0.9456\n",
            "Epoch 4321/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1324 - accuracy: 0.9382\n",
            "Epoch 4321: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1324 - accuracy: 0.9383 - val_loss: 0.1605 - val_accuracy: 0.9538\n",
            "Epoch 4322/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9418\n",
            "Epoch 4322: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9419 - val_loss: 0.1633 - val_accuracy: 0.9530\n",
            "Epoch 4323/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1232 - accuracy: 0.9428\n",
            "Epoch 4323: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1232 - accuracy: 0.9427 - val_loss: 0.1574 - val_accuracy: 0.9582\n",
            "Epoch 4324/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9416\n",
            "Epoch 4324: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1304 - accuracy: 0.9417 - val_loss: 0.1301 - val_accuracy: 0.9692\n",
            "Epoch 4325/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1290 - accuracy: 0.9405\n",
            "Epoch 4325: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1295 - accuracy: 0.9405 - val_loss: 0.2000 - val_accuracy: 0.9306\n",
            "Epoch 4326/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9415\n",
            "Epoch 4326: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 0.9415 - val_loss: 0.1874 - val_accuracy: 0.9386\n",
            "Epoch 4327/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9422\n",
            "Epoch 4327: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1225 - accuracy: 0.9420 - val_loss: 0.1853 - val_accuracy: 0.9373\n",
            "Epoch 4328/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9414\n",
            "Epoch 4328: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1249 - accuracy: 0.9414 - val_loss: 0.1561 - val_accuracy: 0.9527\n",
            "Epoch 4329/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9420\n",
            "Epoch 4329: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1240 - accuracy: 0.9416 - val_loss: 0.1836 - val_accuracy: 0.9393\n",
            "Epoch 4330/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9427\n",
            "Epoch 4330: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 0.9427 - val_loss: 0.1571 - val_accuracy: 0.9577\n",
            "Epoch 4331/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1219 - accuracy: 0.9417\n",
            "Epoch 4331: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 0.9417 - val_loss: 0.1612 - val_accuracy: 0.9595\n",
            "Epoch 4332/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1218 - accuracy: 0.9431\n",
            "Epoch 4332: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1225 - accuracy: 0.9428 - val_loss: 0.1497 - val_accuracy: 0.9571\n",
            "Epoch 4333/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9411\n",
            "Epoch 4333: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1263 - accuracy: 0.9406 - val_loss: 0.1715 - val_accuracy: 0.9501\n",
            "Epoch 4334/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9410\n",
            "Epoch 4334: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1274 - accuracy: 0.9411 - val_loss: 0.1565 - val_accuracy: 0.9616\n",
            "Epoch 4335/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9394\n",
            "Epoch 4335: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1298 - accuracy: 0.9394 - val_loss: 0.1491 - val_accuracy: 0.9631\n",
            "Epoch 4336/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9380\n",
            "Epoch 4336: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1362 - accuracy: 0.9377 - val_loss: 0.1258 - val_accuracy: 0.9775\n",
            "Epoch 4337/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1327 - accuracy: 0.9379\n",
            "Epoch 4337: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1331 - accuracy: 0.9375 - val_loss: 0.2341 - val_accuracy: 0.9076\n",
            "Epoch 4338/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1305 - accuracy: 0.9394\n",
            "Epoch 4338: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1310 - accuracy: 0.9396 - val_loss: 0.1406 - val_accuracy: 0.9634\n",
            "Epoch 4339/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1336 - accuracy: 0.9389\n",
            "Epoch 4339: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1341 - accuracy: 0.9384 - val_loss: 0.2253 - val_accuracy: 0.9185\n",
            "Epoch 4340/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1287 - accuracy: 0.9419\n",
            "Epoch 4340: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1282 - accuracy: 0.9421 - val_loss: 0.1697 - val_accuracy: 0.9529\n",
            "Epoch 4341/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9387\n",
            "Epoch 4341: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1325 - accuracy: 0.9386 - val_loss: 0.1665 - val_accuracy: 0.9521\n",
            "Epoch 4342/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1346 - accuracy: 0.9366\n",
            "Epoch 4342: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1335 - accuracy: 0.9371 - val_loss: 0.1530 - val_accuracy: 0.9601\n",
            "Epoch 4343/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9417\n",
            "Epoch 4343: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1294 - accuracy: 0.9415 - val_loss: 0.1841 - val_accuracy: 0.9336\n",
            "Epoch 4344/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1319 - accuracy: 0.9393\n",
            "Epoch 4344: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1327 - accuracy: 0.9387 - val_loss: 0.2026 - val_accuracy: 0.9336\n",
            "Epoch 4345/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1275 - accuracy: 0.9410\n",
            "Epoch 4345: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1281 - accuracy: 0.9406 - val_loss: 0.1628 - val_accuracy: 0.9603\n",
            "Epoch 4346/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9394\n",
            "Epoch 4346: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1309 - accuracy: 0.9394 - val_loss: 0.1916 - val_accuracy: 0.9410\n",
            "Epoch 4347/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1303 - accuracy: 0.9402\n",
            "Epoch 4347: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1309 - accuracy: 0.9401 - val_loss: 0.1567 - val_accuracy: 0.9530\n",
            "Epoch 4348/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9398\n",
            "Epoch 4348: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9395 - val_loss: 0.1580 - val_accuracy: 0.9558\n",
            "Epoch 4349/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9422\n",
            "Epoch 4349: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1277 - accuracy: 0.9422 - val_loss: 0.1229 - val_accuracy: 0.9731\n",
            "Epoch 4350/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1228 - accuracy: 0.9426\n",
            "Epoch 4350: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1230 - accuracy: 0.9427 - val_loss: 0.1460 - val_accuracy: 0.9668\n",
            "Epoch 4351/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9417\n",
            "Epoch 4351: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1269 - accuracy: 0.9417 - val_loss: 0.1928 - val_accuracy: 0.9384\n",
            "Epoch 4352/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1294 - accuracy: 0.9414\n",
            "Epoch 4352: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1297 - accuracy: 0.9414 - val_loss: 0.1815 - val_accuracy: 0.9434\n",
            "Epoch 4353/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9413\n",
            "Epoch 4353: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1259 - accuracy: 0.9413 - val_loss: 0.1542 - val_accuracy: 0.9558\n",
            "Epoch 4354/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1298 - accuracy: 0.9398\n",
            "Epoch 4354: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1312 - accuracy: 0.9393 - val_loss: 0.1334 - val_accuracy: 0.9703\n",
            "Epoch 4355/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.9370\n",
            "Epoch 4355: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1326 - accuracy: 0.9371 - val_loss: 0.1784 - val_accuracy: 0.9402\n",
            "Epoch 4356/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9400\n",
            "Epoch 4356: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1284 - accuracy: 0.9400 - val_loss: 0.1322 - val_accuracy: 0.9749\n",
            "Epoch 4357/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9374\n",
            "Epoch 4357: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1341 - accuracy: 0.9374 - val_loss: 0.1623 - val_accuracy: 0.9555\n",
            "Epoch 4358/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9396\n",
            "Epoch 4358: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1330 - accuracy: 0.9392 - val_loss: 0.2010 - val_accuracy: 0.9304\n",
            "Epoch 4359/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9384\n",
            "Epoch 4359: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1330 - accuracy: 0.9379 - val_loss: 0.2241 - val_accuracy: 0.9135\n",
            "Epoch 4360/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9387\n",
            "Epoch 4360: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9384 - val_loss: 0.1455 - val_accuracy: 0.9623\n",
            "Epoch 4361/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1252 - accuracy: 0.9415\n",
            "Epoch 4361: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9411 - val_loss: 0.1456 - val_accuracy: 0.9679\n",
            "Epoch 4362/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9415\n",
            "Epoch 4362: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1233 - accuracy: 0.9422 - val_loss: 0.1352 - val_accuracy: 0.9670\n",
            "Epoch 4363/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9434\n",
            "Epoch 4363: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1217 - accuracy: 0.9434 - val_loss: 0.1414 - val_accuracy: 0.9696\n",
            "Epoch 4364/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1303 - accuracy: 0.9398\n",
            "Epoch 4364: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1298 - accuracy: 0.9401 - val_loss: 0.1638 - val_accuracy: 0.9545\n",
            "Epoch 4365/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9396\n",
            "Epoch 4365: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1292 - accuracy: 0.9396 - val_loss: 0.1604 - val_accuracy: 0.9553\n",
            "Epoch 4366/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1324 - accuracy: 0.9398\n",
            "Epoch 4366: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1343 - accuracy: 0.9387 - val_loss: 0.3124 - val_accuracy: 0.8805\n",
            "Epoch 4367/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9340\n",
            "Epoch 4367: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1435 - accuracy: 0.9343 - val_loss: 0.1951 - val_accuracy: 0.9441\n",
            "Epoch 4368/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1320 - accuracy: 0.9386\n",
            "Epoch 4368: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1324 - accuracy: 0.9387 - val_loss: 0.1364 - val_accuracy: 0.9699\n",
            "Epoch 4369/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1281 - accuracy: 0.9404\n",
            "Epoch 4369: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1298 - accuracy: 0.9385 - val_loss: 0.1631 - val_accuracy: 0.9516\n",
            "Epoch 4370/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1365 - accuracy: 0.9364\n",
            "Epoch 4370: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1368 - accuracy: 0.9360 - val_loss: 0.2189 - val_accuracy: 0.9263\n",
            "Epoch 4371/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9343\n",
            "Epoch 4371: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1450 - accuracy: 0.9342 - val_loss: 0.2630 - val_accuracy: 0.9161\n",
            "Epoch 4372/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1703 - accuracy: 0.9240\n",
            "Epoch 4372: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1688 - accuracy: 0.9242 - val_loss: 0.2740 - val_accuracy: 0.8994\n",
            "Epoch 4373/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1375 - accuracy: 0.9369\n",
            "Epoch 4373: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1362 - accuracy: 0.9378 - val_loss: 0.1686 - val_accuracy: 0.9503\n",
            "Epoch 4374/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9432\n",
            "Epoch 4374: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1237 - accuracy: 0.9433 - val_loss: 0.1568 - val_accuracy: 0.9560\n",
            "Epoch 4375/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1247 - accuracy: 0.9422\n",
            "Epoch 4375: loss did not improve from 0.12146\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1229 - accuracy: 0.9428 - val_loss: 0.1431 - val_accuracy: 0.9651\n",
            "Epoch 4376/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1194 - accuracy: 0.9451\n",
            "Epoch 4376: loss improved from 0.12146 to 0.11985, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 0.1198 - accuracy: 0.9450 - val_loss: 0.1423 - val_accuracy: 0.9666\n",
            "Epoch 4377/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.9394\n",
            "Epoch 4377: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1316 - accuracy: 0.9391 - val_loss: 0.1933 - val_accuracy: 0.9449\n",
            "Epoch 4378/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1257 - accuracy: 0.9415\n",
            "Epoch 4378: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1258 - accuracy: 0.9413 - val_loss: 0.1771 - val_accuracy: 0.9503\n",
            "Epoch 4379/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9431\n",
            "Epoch 4379: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1249 - accuracy: 0.9427 - val_loss: 0.1684 - val_accuracy: 0.9484\n",
            "Epoch 4380/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1219 - accuracy: 0.9438\n",
            "Epoch 4380: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1233 - accuracy: 0.9432 - val_loss: 0.1666 - val_accuracy: 0.9568\n",
            "Epoch 4381/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1267 - accuracy: 0.9409\n",
            "Epoch 4381: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1270 - accuracy: 0.9410 - val_loss: 0.1337 - val_accuracy: 0.9701\n",
            "Epoch 4382/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9409\n",
            "Epoch 4382: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1250 - accuracy: 0.9409 - val_loss: 0.1611 - val_accuracy: 0.9517\n",
            "Epoch 4383/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1249 - accuracy: 0.9413\n",
            "Epoch 4383: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1247 - accuracy: 0.9414 - val_loss: 0.1855 - val_accuracy: 0.9399\n",
            "Epoch 4384/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1279 - accuracy: 0.9404\n",
            "Epoch 4384: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 0.9411 - val_loss: 0.1664 - val_accuracy: 0.9521\n",
            "Epoch 4385/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1229 - accuracy: 0.9427\n",
            "Epoch 4385: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1246 - accuracy: 0.9418 - val_loss: 0.1492 - val_accuracy: 0.9616\n",
            "Epoch 4386/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9433\n",
            "Epoch 4386: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9428 - val_loss: 0.1749 - val_accuracy: 0.9482\n",
            "Epoch 4387/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9379\n",
            "Epoch 4387: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1341 - accuracy: 0.9379 - val_loss: 0.1422 - val_accuracy: 0.9709\n",
            "Epoch 4388/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1378 - accuracy: 0.9369\n",
            "Epoch 4388: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1395 - accuracy: 0.9358 - val_loss: 0.2613 - val_accuracy: 0.9098\n",
            "Epoch 4389/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.9368\n",
            "Epoch 4389: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1357 - accuracy: 0.9374 - val_loss: 0.1869 - val_accuracy: 0.9356\n",
            "Epoch 4390/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9399\n",
            "Epoch 4390: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1275 - accuracy: 0.9398 - val_loss: 0.1830 - val_accuracy: 0.9399\n",
            "Epoch 4391/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.9419\n",
            "Epoch 4391: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1232 - accuracy: 0.9418 - val_loss: 0.1664 - val_accuracy: 0.9519\n",
            "Epoch 4392/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1221 - accuracy: 0.9438\n",
            "Epoch 4392: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1223 - accuracy: 0.9438 - val_loss: 0.2044 - val_accuracy: 0.9389\n",
            "Epoch 4393/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9407\n",
            "Epoch 4393: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1286 - accuracy: 0.9406 - val_loss: 0.0969 - val_accuracy: 0.9905\n",
            "Epoch 4394/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9388\n",
            "Epoch 4394: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1304 - accuracy: 0.9388 - val_loss: 0.1498 - val_accuracy: 0.9627\n",
            "Epoch 4395/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9381\n",
            "Epoch 4395: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1298 - accuracy: 0.9384 - val_loss: 0.1702 - val_accuracy: 0.9486\n",
            "Epoch 4396/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9419\n",
            "Epoch 4396: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1243 - accuracy: 0.9419 - val_loss: 0.1212 - val_accuracy: 0.9774\n",
            "Epoch 4397/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9421\n",
            "Epoch 4397: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1241 - accuracy: 0.9421 - val_loss: 0.1705 - val_accuracy: 0.9451\n",
            "Epoch 4398/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9404\n",
            "Epoch 4398: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1275 - accuracy: 0.9404 - val_loss: 0.1286 - val_accuracy: 0.9749\n",
            "Epoch 4399/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9347\n",
            "Epoch 4399: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1435 - accuracy: 0.9347 - val_loss: 0.1951 - val_accuracy: 0.9367\n",
            "Epoch 4400/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1494 - accuracy: 0.9309\n",
            "Epoch 4400: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1490 - accuracy: 0.9307 - val_loss: 0.1543 - val_accuracy: 0.9543\n",
            "Epoch 4401/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1313 - accuracy: 0.9400\n",
            "Epoch 4401: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1316 - accuracy: 0.9399 - val_loss: 0.1626 - val_accuracy: 0.9612\n",
            "Epoch 4402/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1384 - accuracy: 0.9355\n",
            "Epoch 4402: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1380 - accuracy: 0.9360 - val_loss: 0.1583 - val_accuracy: 0.9538\n",
            "Epoch 4403/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9414\n",
            "Epoch 4403: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1290 - accuracy: 0.9415 - val_loss: 0.1718 - val_accuracy: 0.9519\n",
            "Epoch 4404/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9422\n",
            "Epoch 4404: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1259 - accuracy: 0.9416 - val_loss: 0.1660 - val_accuracy: 0.9519\n",
            "Epoch 4405/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9394\n",
            "Epoch 4405: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1304 - accuracy: 0.9394 - val_loss: 0.1252 - val_accuracy: 0.9709\n",
            "Epoch 4406/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1284 - accuracy: 0.9404\n",
            "Epoch 4406: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1282 - accuracy: 0.9404 - val_loss: 0.1414 - val_accuracy: 0.9657\n",
            "Epoch 4407/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9390\n",
            "Epoch 4407: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1291 - accuracy: 0.9390 - val_loss: 0.1301 - val_accuracy: 0.9764\n",
            "Epoch 4408/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1342 - accuracy: 0.9390\n",
            "Epoch 4408: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1369 - accuracy: 0.9379 - val_loss: 0.2491 - val_accuracy: 0.9219\n",
            "Epoch 4409/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9360\n",
            "Epoch 4409: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1395 - accuracy: 0.9360 - val_loss: 0.2754 - val_accuracy: 0.9087\n",
            "Epoch 4410/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1335 - accuracy: 0.9394\n",
            "Epoch 4410: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1340 - accuracy: 0.9390 - val_loss: 0.1731 - val_accuracy: 0.9475\n",
            "Epoch 4411/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9390\n",
            "Epoch 4411: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1317 - accuracy: 0.9390 - val_loss: 0.1740 - val_accuracy: 0.9460\n",
            "Epoch 4412/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1232 - accuracy: 0.9426\n",
            "Epoch 4412: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1231 - accuracy: 0.9425 - val_loss: 0.1514 - val_accuracy: 0.9581\n",
            "Epoch 4413/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9429\n",
            "Epoch 4413: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 0.9428 - val_loss: 0.1758 - val_accuracy: 0.9421\n",
            "Epoch 4414/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9395\n",
            "Epoch 4414: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1295 - accuracy: 0.9395 - val_loss: 0.1890 - val_accuracy: 0.9425\n",
            "Epoch 4415/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1301 - accuracy: 0.9397\n",
            "Epoch 4415: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1326 - accuracy: 0.9387 - val_loss: 0.1101 - val_accuracy: 0.9840\n",
            "Epoch 4416/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9376\n",
            "Epoch 4416: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1396 - accuracy: 0.9375 - val_loss: 0.1898 - val_accuracy: 0.9449\n",
            "Epoch 4417/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9399\n",
            "Epoch 4417: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1286 - accuracy: 0.9400 - val_loss: 0.1684 - val_accuracy: 0.9523\n",
            "Epoch 4418/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1259 - accuracy: 0.9413\n",
            "Epoch 4418: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1266 - accuracy: 0.9409 - val_loss: 0.1900 - val_accuracy: 0.9393\n",
            "Epoch 4419/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1263 - accuracy: 0.9405\n",
            "Epoch 4419: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1268 - accuracy: 0.9402 - val_loss: 0.1197 - val_accuracy: 0.9794\n",
            "Epoch 4420/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1270 - accuracy: 0.9397\n",
            "Epoch 4420: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1272 - accuracy: 0.9394 - val_loss: 0.1624 - val_accuracy: 0.9532\n",
            "Epoch 4421/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9427\n",
            "Epoch 4421: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1231 - accuracy: 0.9427 - val_loss: 0.1662 - val_accuracy: 0.9458\n",
            "Epoch 4422/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1281 - accuracy: 0.9417\n",
            "Epoch 4422: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9408 - val_loss: 0.1430 - val_accuracy: 0.9651\n",
            "Epoch 4423/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9401\n",
            "Epoch 4423: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1300 - accuracy: 0.9401 - val_loss: 0.1399 - val_accuracy: 0.9634\n",
            "Epoch 4424/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1345 - accuracy: 0.9379\n",
            "Epoch 4424: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1342 - accuracy: 0.9384 - val_loss: 0.1827 - val_accuracy: 0.9432\n",
            "Epoch 4425/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9385\n",
            "Epoch 4425: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9385 - val_loss: 0.1275 - val_accuracy: 0.9772\n",
            "Epoch 4426/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9415\n",
            "Epoch 4426: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1266 - accuracy: 0.9414 - val_loss: 0.2162 - val_accuracy: 0.9215\n",
            "Epoch 4427/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9408\n",
            "Epoch 4427: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1294 - accuracy: 0.9406 - val_loss: 0.1577 - val_accuracy: 0.9579\n",
            "Epoch 4428/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1228 - accuracy: 0.9417\n",
            "Epoch 4428: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1221 - accuracy: 0.9421 - val_loss: 0.1504 - val_accuracy: 0.9577\n",
            "Epoch 4429/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1203 - accuracy: 0.9445\n",
            "Epoch 4429: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1222 - accuracy: 0.9438 - val_loss: 0.1516 - val_accuracy: 0.9605\n",
            "Epoch 4430/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1314 - accuracy: 0.9373\n",
            "Epoch 4430: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1307 - accuracy: 0.9379 - val_loss: 0.1394 - val_accuracy: 0.9666\n",
            "Epoch 4431/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1266 - accuracy: 0.9408\n",
            "Epoch 4431: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1264 - accuracy: 0.9412 - val_loss: 0.2144 - val_accuracy: 0.9252\n",
            "Epoch 4432/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9404\n",
            "Epoch 4432: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9404 - val_loss: 0.2970 - val_accuracy: 0.8701\n",
            "Epoch 4433/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1361 - accuracy: 0.9359\n",
            "Epoch 4433: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1368 - accuracy: 0.9351 - val_loss: 0.1752 - val_accuracy: 0.9517\n",
            "Epoch 4434/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9410\n",
            "Epoch 4434: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1271 - accuracy: 0.9410 - val_loss: 0.1950 - val_accuracy: 0.9421\n",
            "Epoch 4435/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9416\n",
            "Epoch 4435: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1291 - accuracy: 0.9409 - val_loss: 0.1527 - val_accuracy: 0.9614\n",
            "Epoch 4436/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9407\n",
            "Epoch 4436: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9407 - val_loss: 0.2171 - val_accuracy: 0.9146\n",
            "Epoch 4437/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1312 - accuracy: 0.9383\n",
            "Epoch 4437: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1322 - accuracy: 0.9378 - val_loss: 0.2713 - val_accuracy: 0.9102\n",
            "Epoch 4438/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9390\n",
            "Epoch 4438: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1327 - accuracy: 0.9390 - val_loss: 0.1563 - val_accuracy: 0.9564\n",
            "Epoch 4439/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9354\n",
            "Epoch 4439: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1380 - accuracy: 0.9356 - val_loss: 0.1634 - val_accuracy: 0.9543\n",
            "Epoch 4440/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1222 - accuracy: 0.9419\n",
            "Epoch 4440: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1233 - accuracy: 0.9414 - val_loss: 0.1413 - val_accuracy: 0.9668\n",
            "Epoch 4441/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9420\n",
            "Epoch 4441: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1226 - accuracy: 0.9418 - val_loss: 0.1922 - val_accuracy: 0.9336\n",
            "Epoch 4442/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1262 - accuracy: 0.9415\n",
            "Epoch 4442: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1253 - accuracy: 0.9421 - val_loss: 0.1454 - val_accuracy: 0.9627\n",
            "Epoch 4443/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9396\n",
            "Epoch 4443: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1279 - accuracy: 0.9386 - val_loss: 0.1410 - val_accuracy: 0.9664\n",
            "Epoch 4444/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9402\n",
            "Epoch 4444: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 0.9406 - val_loss: 0.1423 - val_accuracy: 0.9642\n",
            "Epoch 4445/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9400\n",
            "Epoch 4445: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1305 - accuracy: 0.9399 - val_loss: 0.2068 - val_accuracy: 0.9315\n",
            "Epoch 4446/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9419\n",
            "Epoch 4446: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1296 - accuracy: 0.9419 - val_loss: 0.1763 - val_accuracy: 0.9532\n",
            "Epoch 4447/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9393\n",
            "Epoch 4447: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1300 - accuracy: 0.9396 - val_loss: 0.1611 - val_accuracy: 0.9623\n",
            "Epoch 4448/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9430\n",
            "Epoch 4448: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1227 - accuracy: 0.9427 - val_loss: 0.1658 - val_accuracy: 0.9599\n",
            "Epoch 4449/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1271 - accuracy: 0.9407\n",
            "Epoch 4449: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1280 - accuracy: 0.9395 - val_loss: 0.1284 - val_accuracy: 0.9736\n",
            "Epoch 4450/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9414\n",
            "Epoch 4450: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9412 - val_loss: 0.1447 - val_accuracy: 0.9631\n",
            "Epoch 4451/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1254 - accuracy: 0.9410\n",
            "Epoch 4451: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1249 - accuracy: 0.9414 - val_loss: 0.1686 - val_accuracy: 0.9504\n",
            "Epoch 4452/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9431\n",
            "Epoch 4452: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1212 - accuracy: 0.9433 - val_loss: 0.1632 - val_accuracy: 0.9514\n",
            "Epoch 4453/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9415\n",
            "Epoch 4453: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1247 - accuracy: 0.9412 - val_loss: 0.2074 - val_accuracy: 0.9336\n",
            "Epoch 4454/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1408 - accuracy: 0.9340\n",
            "Epoch 4454: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1424 - accuracy: 0.9328 - val_loss: 0.2159 - val_accuracy: 0.9250\n",
            "Epoch 4455/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1406 - accuracy: 0.9361\n",
            "Epoch 4455: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1405 - accuracy: 0.9355 - val_loss: 0.1899 - val_accuracy: 0.9404\n",
            "Epoch 4456/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9389\n",
            "Epoch 4456: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1306 - accuracy: 0.9391 - val_loss: 0.1102 - val_accuracy: 0.9837\n",
            "Epoch 4457/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9411\n",
            "Epoch 4457: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1263 - accuracy: 0.9411 - val_loss: 0.1601 - val_accuracy: 0.9529\n",
            "Epoch 4458/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1270 - accuracy: 0.9419\n",
            "Epoch 4458: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1271 - accuracy: 0.9414 - val_loss: 0.1387 - val_accuracy: 0.9636\n",
            "Epoch 4459/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9406\n",
            "Epoch 4459: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1279 - accuracy: 0.9406 - val_loss: 0.1880 - val_accuracy: 0.9389\n",
            "Epoch 4460/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9400\n",
            "Epoch 4460: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1306 - accuracy: 0.9400 - val_loss: 0.1793 - val_accuracy: 0.9456\n",
            "Epoch 4461/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1268 - accuracy: 0.9410\n",
            "Epoch 4461: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1267 - accuracy: 0.9412 - val_loss: 0.1611 - val_accuracy: 0.9538\n",
            "Epoch 4462/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1264 - accuracy: 0.9408\n",
            "Epoch 4462: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1262 - accuracy: 0.9412 - val_loss: 0.1718 - val_accuracy: 0.9438\n",
            "Epoch 4463/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9407\n",
            "Epoch 4463: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9400 - val_loss: 0.1688 - val_accuracy: 0.9447\n",
            "Epoch 4464/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9387\n",
            "Epoch 4464: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9387 - val_loss: 0.1796 - val_accuracy: 0.9480\n",
            "Epoch 4465/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9402\n",
            "Epoch 4465: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1295 - accuracy: 0.9402 - val_loss: 0.1363 - val_accuracy: 0.9660\n",
            "Epoch 4466/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9423\n",
            "Epoch 4466: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1261 - accuracy: 0.9414 - val_loss: 0.1396 - val_accuracy: 0.9675\n",
            "Epoch 4467/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1277 - accuracy: 0.9408\n",
            "Epoch 4467: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1269 - accuracy: 0.9414 - val_loss: 0.1633 - val_accuracy: 0.9478\n",
            "Epoch 4468/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1311 - accuracy: 0.9392\n",
            "Epoch 4468: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1310 - accuracy: 0.9396 - val_loss: 0.2843 - val_accuracy: 0.8827\n",
            "Epoch 4469/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9425\n",
            "Epoch 4469: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1253 - accuracy: 0.9425 - val_loss: 0.1853 - val_accuracy: 0.9414\n",
            "Epoch 4470/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1288 - accuracy: 0.9404\n",
            "Epoch 4470: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1296 - accuracy: 0.9404 - val_loss: 0.2018 - val_accuracy: 0.9432\n",
            "Epoch 4471/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1329 - accuracy: 0.9380\n",
            "Epoch 4471: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1340 - accuracy: 0.9378 - val_loss: 0.1622 - val_accuracy: 0.9575\n",
            "Epoch 4472/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1503 - accuracy: 0.9313\n",
            "Epoch 4472: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1501 - accuracy: 0.9311 - val_loss: 0.2500 - val_accuracy: 0.9003\n",
            "Epoch 4473/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1732 - accuracy: 0.9243\n",
            "Epoch 4473: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1730 - accuracy: 0.9239 - val_loss: 0.2168 - val_accuracy: 0.9313\n",
            "Epoch 4474/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9316\n",
            "Epoch 4474: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1497 - accuracy: 0.9316 - val_loss: 0.1897 - val_accuracy: 0.9410\n",
            "Epoch 4475/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9386\n",
            "Epoch 4475: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1322 - accuracy: 0.9386 - val_loss: 0.1281 - val_accuracy: 0.9785\n",
            "Epoch 4476/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9429\n",
            "Epoch 4476: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1238 - accuracy: 0.9427 - val_loss: 0.2140 - val_accuracy: 0.9276\n",
            "Epoch 4477/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9435\n",
            "Epoch 4477: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1243 - accuracy: 0.9435 - val_loss: 0.1433 - val_accuracy: 0.9688\n",
            "Epoch 4478/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1201 - accuracy: 0.9432\n",
            "Epoch 4478: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1214 - accuracy: 0.9429 - val_loss: 0.1517 - val_accuracy: 0.9599\n",
            "Epoch 4479/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1248 - accuracy: 0.9424\n",
            "Epoch 4479: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9425 - val_loss: 0.1606 - val_accuracy: 0.9573\n",
            "Epoch 4480/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1211 - accuracy: 0.9435\n",
            "Epoch 4480: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1223 - accuracy: 0.9426 - val_loss: 0.3038 - val_accuracy: 0.8727\n",
            "Epoch 4481/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1337 - accuracy: 0.9381\n",
            "Epoch 4481: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1338 - accuracy: 0.9378 - val_loss: 0.1583 - val_accuracy: 0.9495\n",
            "Epoch 4482/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1289 - accuracy: 0.9382\n",
            "Epoch 4482: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1289 - accuracy: 0.9387 - val_loss: 0.1565 - val_accuracy: 0.9564\n",
            "Epoch 4483/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9421\n",
            "Epoch 4483: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1254 - accuracy: 0.9419 - val_loss: 0.1744 - val_accuracy: 0.9471\n",
            "Epoch 4484/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1294 - accuracy: 0.9403\n",
            "Epoch 4484: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1305 - accuracy: 0.9396 - val_loss: 0.1394 - val_accuracy: 0.9664\n",
            "Epoch 4485/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9393\n",
            "Epoch 4485: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1288 - accuracy: 0.9394 - val_loss: 0.1838 - val_accuracy: 0.9336\n",
            "Epoch 4486/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9397\n",
            "Epoch 4486: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1290 - accuracy: 0.9397 - val_loss: 0.1397 - val_accuracy: 0.9755\n",
            "Epoch 4487/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1290 - accuracy: 0.9402\n",
            "Epoch 4487: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9394 - val_loss: 0.2045 - val_accuracy: 0.9336\n",
            "Epoch 4488/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9393\n",
            "Epoch 4488: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9393 - val_loss: 0.1666 - val_accuracy: 0.9497\n",
            "Epoch 4489/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9369\n",
            "Epoch 4489: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1349 - accuracy: 0.9365 - val_loss: 0.1448 - val_accuracy: 0.9644\n",
            "Epoch 4490/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9376\n",
            "Epoch 4490: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1331 - accuracy: 0.9376 - val_loss: 0.1353 - val_accuracy: 0.9653\n",
            "Epoch 4491/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1297 - accuracy: 0.9405\n",
            "Epoch 4491: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9402 - val_loss: 0.1580 - val_accuracy: 0.9558\n",
            "Epoch 4492/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1311 - accuracy: 0.9407\n",
            "Epoch 4492: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9412 - val_loss: 0.2450 - val_accuracy: 0.9050\n",
            "Epoch 4493/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1249 - accuracy: 0.9426\n",
            "Epoch 4493: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1273 - accuracy: 0.9408 - val_loss: 0.1696 - val_accuracy: 0.9465\n",
            "Epoch 4494/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1251 - accuracy: 0.9417\n",
            "Epoch 4494: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1270 - accuracy: 0.9403 - val_loss: 0.1586 - val_accuracy: 0.9527\n",
            "Epoch 4495/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1267 - accuracy: 0.9401\n",
            "Epoch 4495: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9401 - val_loss: 0.1591 - val_accuracy: 0.9547\n",
            "Epoch 4496/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9427\n",
            "Epoch 4496: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1238 - accuracy: 0.9427 - val_loss: 0.1581 - val_accuracy: 0.9529\n",
            "Epoch 4497/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9432\n",
            "Epoch 4497: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 0.9432 - val_loss: 0.1655 - val_accuracy: 0.9516\n",
            "Epoch 4498/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9440\n",
            "Epoch 4498: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1216 - accuracy: 0.9440 - val_loss: 0.1492 - val_accuracy: 0.9629\n",
            "Epoch 4499/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9436\n",
            "Epoch 4499: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1200 - accuracy: 0.9436 - val_loss: 0.1398 - val_accuracy: 0.9653\n",
            "Epoch 4500/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9420\n",
            "Epoch 4500: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1232 - accuracy: 0.9420 - val_loss: 0.1565 - val_accuracy: 0.9599\n",
            "Epoch 4501/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9421\n",
            "Epoch 4501: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1239 - accuracy: 0.9420 - val_loss: 0.1492 - val_accuracy: 0.9629\n",
            "Epoch 4502/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1239 - accuracy: 0.9430\n",
            "Epoch 4502: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1240 - accuracy: 0.9430 - val_loss: 0.1863 - val_accuracy: 0.9458\n",
            "Epoch 4503/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1262 - accuracy: 0.9403\n",
            "Epoch 4503: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1289 - accuracy: 0.9389 - val_loss: 0.2004 - val_accuracy: 0.9337\n",
            "Epoch 4504/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1366 - accuracy: 0.9378\n",
            "Epoch 4504: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1383 - accuracy: 0.9368 - val_loss: 0.1822 - val_accuracy: 0.9376\n",
            "Epoch 4505/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9384\n",
            "Epoch 4505: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1348 - accuracy: 0.9379 - val_loss: 0.1857 - val_accuracy: 0.9384\n",
            "Epoch 4506/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9325\n",
            "Epoch 4506: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1466 - accuracy: 0.9325 - val_loss: 0.1344 - val_accuracy: 0.9634\n",
            "Epoch 4507/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1373 - accuracy: 0.9372\n",
            "Epoch 4507: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1383 - accuracy: 0.9362 - val_loss: 0.1913 - val_accuracy: 0.9376\n",
            "Epoch 4508/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9399\n",
            "Epoch 4508: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1308 - accuracy: 0.9398 - val_loss: 0.2189 - val_accuracy: 0.9178\n",
            "Epoch 4509/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1309 - accuracy: 0.9391\n",
            "Epoch 4509: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1320 - accuracy: 0.9387 - val_loss: 0.1876 - val_accuracy: 0.9423\n",
            "Epoch 4510/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9395\n",
            "Epoch 4510: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1306 - accuracy: 0.9398 - val_loss: 0.1389 - val_accuracy: 0.9690\n",
            "Epoch 4511/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1303 - accuracy: 0.9390\n",
            "Epoch 4511: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1305 - accuracy: 0.9392 - val_loss: 0.2245 - val_accuracy: 0.9228\n",
            "Epoch 4512/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9401\n",
            "Epoch 4512: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1276 - accuracy: 0.9401 - val_loss: 0.1411 - val_accuracy: 0.9647\n",
            "Epoch 4513/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1231 - accuracy: 0.9430\n",
            "Epoch 4513: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1248 - accuracy: 0.9419 - val_loss: 0.1919 - val_accuracy: 0.9319\n",
            "Epoch 4514/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9418\n",
            "Epoch 4514: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1290 - accuracy: 0.9414 - val_loss: 0.1489 - val_accuracy: 0.9655\n",
            "Epoch 4515/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9380\n",
            "Epoch 4515: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1329 - accuracy: 0.9373 - val_loss: 0.1129 - val_accuracy: 0.9833\n",
            "Epoch 4516/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9400\n",
            "Epoch 4516: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1329 - accuracy: 0.9389 - val_loss: 0.1816 - val_accuracy: 0.9484\n",
            "Epoch 4517/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1399 - accuracy: 0.9361\n",
            "Epoch 4517: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1406 - accuracy: 0.9359 - val_loss: 0.1158 - val_accuracy: 0.9790\n",
            "Epoch 4518/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9337\n",
            "Epoch 4518: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1413 - accuracy: 0.9337 - val_loss: 0.1377 - val_accuracy: 0.9657\n",
            "Epoch 4519/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9377\n",
            "Epoch 4519: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1337 - accuracy: 0.9377 - val_loss: 0.1783 - val_accuracy: 0.9458\n",
            "Epoch 4520/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1261 - accuracy: 0.9415\n",
            "Epoch 4520: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1268 - accuracy: 0.9411 - val_loss: 0.1350 - val_accuracy: 0.9675\n",
            "Epoch 4521/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1215 - accuracy: 0.9426\n",
            "Epoch 4521: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1223 - accuracy: 0.9422 - val_loss: 0.1323 - val_accuracy: 0.9668\n",
            "Epoch 4522/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1232 - accuracy: 0.9432\n",
            "Epoch 4522: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1240 - accuracy: 0.9428 - val_loss: 0.1536 - val_accuracy: 0.9553\n",
            "Epoch 4523/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9420\n",
            "Epoch 4523: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9415 - val_loss: 0.1253 - val_accuracy: 0.9751\n",
            "Epoch 4524/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1270 - accuracy: 0.9406\n",
            "Epoch 4524: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1270 - accuracy: 0.9407 - val_loss: 0.1732 - val_accuracy: 0.9425\n",
            "Epoch 4525/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1345 - accuracy: 0.9371\n",
            "Epoch 4525: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1359 - accuracy: 0.9374 - val_loss: 0.2162 - val_accuracy: 0.9289\n",
            "Epoch 4526/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.9379\n",
            "Epoch 4526: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1374 - accuracy: 0.9376 - val_loss: 0.1868 - val_accuracy: 0.9436\n",
            "Epoch 4527/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1299 - accuracy: 0.9403\n",
            "Epoch 4527: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1330 - accuracy: 0.9388 - val_loss: 0.1510 - val_accuracy: 0.9660\n",
            "Epoch 4528/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1338 - accuracy: 0.9371\n",
            "Epoch 4528: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1330 - accuracy: 0.9377 - val_loss: 0.1691 - val_accuracy: 0.9488\n",
            "Epoch 4529/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1334 - accuracy: 0.9378\n",
            "Epoch 4529: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1330 - accuracy: 0.9378 - val_loss: 0.1728 - val_accuracy: 0.9595\n",
            "Epoch 4530/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1272 - accuracy: 0.9420\n",
            "Epoch 4530: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1286 - accuracy: 0.9411 - val_loss: 0.2004 - val_accuracy: 0.9272\n",
            "Epoch 4531/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1300 - accuracy: 0.9406\n",
            "Epoch 4531: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1298 - accuracy: 0.9407 - val_loss: 0.1556 - val_accuracy: 0.9575\n",
            "Epoch 4532/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9427\n",
            "Epoch 4532: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1246 - accuracy: 0.9427 - val_loss: 0.1483 - val_accuracy: 0.9670\n",
            "Epoch 4533/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9393\n",
            "Epoch 4533: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1303 - accuracy: 0.9390 - val_loss: 0.1822 - val_accuracy: 0.9556\n",
            "Epoch 4534/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1277 - accuracy: 0.9416\n",
            "Epoch 4534: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1272 - accuracy: 0.9416 - val_loss: 0.1835 - val_accuracy: 0.9471\n",
            "Epoch 4535/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1305 - accuracy: 0.9387\n",
            "Epoch 4535: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9390 - val_loss: 0.1807 - val_accuracy: 0.9415\n",
            "Epoch 4536/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9423\n",
            "Epoch 4536: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1242 - accuracy: 0.9422 - val_loss: 0.1637 - val_accuracy: 0.9504\n",
            "Epoch 4537/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9429\n",
            "Epoch 4537: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1218 - accuracy: 0.9427 - val_loss: 0.1756 - val_accuracy: 0.9425\n",
            "Epoch 4538/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9429\n",
            "Epoch 4538: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1214 - accuracy: 0.9431 - val_loss: 0.1521 - val_accuracy: 0.9608\n",
            "Epoch 4539/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9420\n",
            "Epoch 4539: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1226 - accuracy: 0.9420 - val_loss: 0.1492 - val_accuracy: 0.9594\n",
            "Epoch 4540/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1218 - accuracy: 0.9415\n",
            "Epoch 4540: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1240 - accuracy: 0.9406 - val_loss: 0.1257 - val_accuracy: 0.9707\n",
            "Epoch 4541/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1357 - accuracy: 0.9376\n",
            "Epoch 4541: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1350 - accuracy: 0.9381 - val_loss: 0.2127 - val_accuracy: 0.9213\n",
            "Epoch 4542/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9395\n",
            "Epoch 4542: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1298 - accuracy: 0.9392 - val_loss: 0.1784 - val_accuracy: 0.9417\n",
            "Epoch 4543/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1315 - accuracy: 0.9390\n",
            "Epoch 4543: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1304 - accuracy: 0.9395 - val_loss: 0.1836 - val_accuracy: 0.9506\n",
            "Epoch 4544/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1375 - accuracy: 0.9367\n",
            "Epoch 4544: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1392 - accuracy: 0.9357 - val_loss: 0.1514 - val_accuracy: 0.9623\n",
            "Epoch 4545/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.9356\n",
            "Epoch 4545: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1394 - accuracy: 0.9353 - val_loss: 0.1799 - val_accuracy: 0.9386\n",
            "Epoch 4546/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1341 - accuracy: 0.9367\n",
            "Epoch 4546: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1340 - accuracy: 0.9368 - val_loss: 0.1568 - val_accuracy: 0.9581\n",
            "Epoch 4547/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9354\n",
            "Epoch 4547: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1389 - accuracy: 0.9356 - val_loss: 0.3024 - val_accuracy: 0.8925\n",
            "Epoch 4548/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1313 - accuracy: 0.9397\n",
            "Epoch 4548: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9396 - val_loss: 0.1782 - val_accuracy: 0.9389\n",
            "Epoch 4549/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9394\n",
            "Epoch 4549: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1292 - accuracy: 0.9397 - val_loss: 0.1473 - val_accuracy: 0.9627\n",
            "Epoch 4550/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1271 - accuracy: 0.9405\n",
            "Epoch 4550: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1273 - accuracy: 0.9407 - val_loss: 0.1743 - val_accuracy: 0.9441\n",
            "Epoch 4551/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9420\n",
            "Epoch 4551: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9422 - val_loss: 0.1560 - val_accuracy: 0.9616\n",
            "Epoch 4552/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9406\n",
            "Epoch 4552: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1247 - accuracy: 0.9402 - val_loss: 0.1767 - val_accuracy: 0.9527\n",
            "Epoch 4553/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1262 - accuracy: 0.9415\n",
            "Epoch 4553: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1262 - accuracy: 0.9414 - val_loss: 0.1639 - val_accuracy: 0.9542\n",
            "Epoch 4554/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1228 - accuracy: 0.9436\n",
            "Epoch 4554: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1229 - accuracy: 0.9436 - val_loss: 0.1706 - val_accuracy: 0.9493\n",
            "Epoch 4555/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9434\n",
            "Epoch 4555: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1220 - accuracy: 0.9434 - val_loss: 0.1378 - val_accuracy: 0.9666\n",
            "Epoch 4556/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1241 - accuracy: 0.9428\n",
            "Epoch 4556: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9421 - val_loss: 0.2380 - val_accuracy: 0.9308\n",
            "Epoch 4557/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1287 - accuracy: 0.9407\n",
            "Epoch 4557: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9399 - val_loss: 0.1843 - val_accuracy: 0.9432\n",
            "Epoch 4558/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1406 - accuracy: 0.9342\n",
            "Epoch 4558: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1402 - accuracy: 0.9344 - val_loss: 0.1846 - val_accuracy: 0.9427\n",
            "Epoch 4559/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9410\n",
            "Epoch 4559: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9410 - val_loss: 0.2078 - val_accuracy: 0.9259\n",
            "Epoch 4560/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9413\n",
            "Epoch 4560: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1268 - accuracy: 0.9413 - val_loss: 0.1783 - val_accuracy: 0.9451\n",
            "Epoch 4561/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9419\n",
            "Epoch 4561: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1264 - accuracy: 0.9418 - val_loss: 0.1627 - val_accuracy: 0.9605\n",
            "Epoch 4562/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9395\n",
            "Epoch 4562: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1322 - accuracy: 0.9395 - val_loss: 0.1835 - val_accuracy: 0.9471\n",
            "Epoch 4563/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1307 - accuracy: 0.9399\n",
            "Epoch 4563: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1316 - accuracy: 0.9392 - val_loss: 0.1535 - val_accuracy: 0.9577\n",
            "Epoch 4564/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1260 - accuracy: 0.9418\n",
            "Epoch 4564: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9406 - val_loss: 0.1788 - val_accuracy: 0.9414\n",
            "Epoch 4565/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1325 - accuracy: 0.9382\n",
            "Epoch 4565: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1308 - accuracy: 0.9394 - val_loss: 0.1111 - val_accuracy: 0.9787\n",
            "Epoch 4566/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9392\n",
            "Epoch 4566: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1330 - accuracy: 0.9392 - val_loss: 0.1748 - val_accuracy: 0.9441\n",
            "Epoch 4567/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9410\n",
            "Epoch 4567: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9409 - val_loss: 0.1322 - val_accuracy: 0.9705\n",
            "Epoch 4568/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1268 - accuracy: 0.9392\n",
            "Epoch 4568: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 0.9388 - val_loss: 0.2226 - val_accuracy: 0.9167\n",
            "Epoch 4569/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1252 - accuracy: 0.9420\n",
            "Epoch 4569: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1256 - accuracy: 0.9419 - val_loss: 0.2211 - val_accuracy: 0.9217\n",
            "Epoch 4570/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9430\n",
            "Epoch 4570: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1243 - accuracy: 0.9433 - val_loss: 0.1543 - val_accuracy: 0.9534\n",
            "Epoch 4571/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9407\n",
            "Epoch 4571: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1267 - accuracy: 0.9408 - val_loss: 0.1526 - val_accuracy: 0.9651\n",
            "Epoch 4572/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9414\n",
            "Epoch 4572: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1264 - accuracy: 0.9414 - val_loss: 0.1476 - val_accuracy: 0.9569\n",
            "Epoch 4573/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1258 - accuracy: 0.9423\n",
            "Epoch 4573: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1263 - accuracy: 0.9421 - val_loss: 0.1717 - val_accuracy: 0.9471\n",
            "Epoch 4574/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9408\n",
            "Epoch 4574: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1287 - accuracy: 0.9408 - val_loss: 0.1402 - val_accuracy: 0.9640\n",
            "Epoch 4575/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1296 - accuracy: 0.9403\n",
            "Epoch 4575: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1302 - accuracy: 0.9401 - val_loss: 0.2019 - val_accuracy: 0.9367\n",
            "Epoch 4576/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1295 - accuracy: 0.9408\n",
            "Epoch 4576: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1289 - accuracy: 0.9407 - val_loss: 0.2400 - val_accuracy: 0.9046\n",
            "Epoch 4577/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9404\n",
            "Epoch 4577: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1268 - accuracy: 0.9404 - val_loss: 0.1293 - val_accuracy: 0.9731\n",
            "Epoch 4578/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9429\n",
            "Epoch 4578: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1227 - accuracy: 0.9429 - val_loss: 0.1492 - val_accuracy: 0.9636\n",
            "Epoch 4579/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9425\n",
            "Epoch 4579: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1261 - accuracy: 0.9414 - val_loss: 0.1692 - val_accuracy: 0.9436\n",
            "Epoch 4580/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1267 - accuracy: 0.9409\n",
            "Epoch 4580: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1270 - accuracy: 0.9405 - val_loss: 0.1520 - val_accuracy: 0.9657\n",
            "Epoch 4581/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9393\n",
            "Epoch 4581: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1296 - accuracy: 0.9393 - val_loss: 0.1491 - val_accuracy: 0.9620\n",
            "Epoch 4582/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9375\n",
            "Epoch 4582: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9375 - val_loss: 0.1709 - val_accuracy: 0.9516\n",
            "Epoch 4583/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1269 - accuracy: 0.9409\n",
            "Epoch 4583: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9406 - val_loss: 0.2509 - val_accuracy: 0.8983\n",
            "Epoch 4584/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9412\n",
            "Epoch 4584: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1239 - accuracy: 0.9412 - val_loss: 0.1479 - val_accuracy: 0.9627\n",
            "Epoch 4585/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1255 - accuracy: 0.9417\n",
            "Epoch 4585: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9415 - val_loss: 0.1623 - val_accuracy: 0.9495\n",
            "Epoch 4586/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1328 - accuracy: 0.9378\n",
            "Epoch 4586: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1335 - accuracy: 0.9375 - val_loss: 0.1618 - val_accuracy: 0.9516\n",
            "Epoch 4587/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.9375\n",
            "Epoch 4587: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1329 - accuracy: 0.9375 - val_loss: 0.1508 - val_accuracy: 0.9573\n",
            "Epoch 4588/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1281 - accuracy: 0.9401\n",
            "Epoch 4588: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1289 - accuracy: 0.9402 - val_loss: 0.1702 - val_accuracy: 0.9493\n",
            "Epoch 4589/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1280 - accuracy: 0.9409\n",
            "Epoch 4589: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1286 - accuracy: 0.9408 - val_loss: 0.2539 - val_accuracy: 0.8922\n",
            "Epoch 4590/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9390\n",
            "Epoch 4590: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1326 - accuracy: 0.9388 - val_loss: 0.1274 - val_accuracy: 0.9705\n",
            "Epoch 4591/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1289 - accuracy: 0.9398\n",
            "Epoch 4591: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1290 - accuracy: 0.9398 - val_loss: 0.1281 - val_accuracy: 0.9751\n",
            "Epoch 4592/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9407\n",
            "Epoch 4592: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1265 - accuracy: 0.9412 - val_loss: 0.1625 - val_accuracy: 0.9452\n",
            "Epoch 4593/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9429\n",
            "Epoch 4593: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1209 - accuracy: 0.9429 - val_loss: 0.1548 - val_accuracy: 0.9582\n",
            "Epoch 4594/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1248 - accuracy: 0.9420\n",
            "Epoch 4594: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9417 - val_loss: 0.1313 - val_accuracy: 0.9671\n",
            "Epoch 4595/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9406\n",
            "Epoch 4595: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1273 - accuracy: 0.9406 - val_loss: 0.1340 - val_accuracy: 0.9727\n",
            "Epoch 4596/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9418\n",
            "Epoch 4596: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1240 - accuracy: 0.9414 - val_loss: 0.1985 - val_accuracy: 0.9332\n",
            "Epoch 4597/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1291 - accuracy: 0.9401\n",
            "Epoch 4597: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1302 - accuracy: 0.9392 - val_loss: 0.1431 - val_accuracy: 0.9660\n",
            "Epoch 4598/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1340 - accuracy: 0.9375\n",
            "Epoch 4598: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1345 - accuracy: 0.9373 - val_loss: 0.1692 - val_accuracy: 0.9482\n",
            "Epoch 4599/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9407\n",
            "Epoch 4599: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1285 - accuracy: 0.9407 - val_loss: 0.2076 - val_accuracy: 0.9285\n",
            "Epoch 4600/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9408\n",
            "Epoch 4600: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1273 - accuracy: 0.9409 - val_loss: 0.1492 - val_accuracy: 0.9625\n",
            "Epoch 4601/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1290 - accuracy: 0.9397\n",
            "Epoch 4601: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1286 - accuracy: 0.9399 - val_loss: 0.2613 - val_accuracy: 0.9014\n",
            "Epoch 4602/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1303 - accuracy: 0.9392\n",
            "Epoch 4602: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1311 - accuracy: 0.9388 - val_loss: 0.1297 - val_accuracy: 0.9720\n",
            "Epoch 4603/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1276 - accuracy: 0.9392\n",
            "Epoch 4603: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1288 - accuracy: 0.9384 - val_loss: 0.1549 - val_accuracy: 0.9595\n",
            "Epoch 4604/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1333 - accuracy: 0.9380\n",
            "Epoch 4604: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1338 - accuracy: 0.9375 - val_loss: 0.1742 - val_accuracy: 0.9430\n",
            "Epoch 4605/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1316 - accuracy: 0.9391\n",
            "Epoch 4605: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9394 - val_loss: 0.1138 - val_accuracy: 0.9833\n",
            "Epoch 4606/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9430\n",
            "Epoch 4606: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1229 - accuracy: 0.9426 - val_loss: 0.1530 - val_accuracy: 0.9592\n",
            "Epoch 4607/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1247 - accuracy: 0.9422\n",
            "Epoch 4607: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1254 - accuracy: 0.9413 - val_loss: 0.1751 - val_accuracy: 0.9521\n",
            "Epoch 4608/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.9389\n",
            "Epoch 4608: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1304 - accuracy: 0.9388 - val_loss: 0.1279 - val_accuracy: 0.9703\n",
            "Epoch 4609/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1263 - accuracy: 0.9420\n",
            "Epoch 4609: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1255 - accuracy: 0.9426 - val_loss: 0.1519 - val_accuracy: 0.9527\n",
            "Epoch 4610/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9414\n",
            "Epoch 4610: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1258 - accuracy: 0.9414 - val_loss: 0.1668 - val_accuracy: 0.9510\n",
            "Epoch 4611/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9400\n",
            "Epoch 4611: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9399 - val_loss: 0.1800 - val_accuracy: 0.9497\n",
            "Epoch 4612/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9368\n",
            "Epoch 4612: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1379 - accuracy: 0.9368 - val_loss: 0.2537 - val_accuracy: 0.9072\n",
            "Epoch 4613/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1367 - accuracy: 0.9365\n",
            "Epoch 4613: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1354 - accuracy: 0.9372 - val_loss: 0.1663 - val_accuracy: 0.9504\n",
            "Epoch 4614/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1267 - accuracy: 0.9411\n",
            "Epoch 4614: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1275 - accuracy: 0.9407 - val_loss: 0.1768 - val_accuracy: 0.9404\n",
            "Epoch 4615/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9368\n",
            "Epoch 4615: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1362 - accuracy: 0.9368 - val_loss: 0.2870 - val_accuracy: 0.8866\n",
            "Epoch 4616/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1351 - accuracy: 0.9387\n",
            "Epoch 4616: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1343 - accuracy: 0.9389 - val_loss: 0.1839 - val_accuracy: 0.9406\n",
            "Epoch 4617/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9378\n",
            "Epoch 4617: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1328 - accuracy: 0.9380 - val_loss: 0.1831 - val_accuracy: 0.9439\n",
            "Epoch 4618/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1330 - accuracy: 0.9389\n",
            "Epoch 4618: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1334 - accuracy: 0.9385 - val_loss: 0.1948 - val_accuracy: 0.9360\n",
            "Epoch 4619/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9381\n",
            "Epoch 4619: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1341 - accuracy: 0.9382 - val_loss: 0.1750 - val_accuracy: 0.9510\n",
            "Epoch 4620/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9382\n",
            "Epoch 4620: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1357 - accuracy: 0.9382 - val_loss: 0.1482 - val_accuracy: 0.9601\n",
            "Epoch 4621/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1342 - accuracy: 0.9391\n",
            "Epoch 4621: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1351 - accuracy: 0.9389 - val_loss: 0.2267 - val_accuracy: 0.9215\n",
            "Epoch 4622/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9411\n",
            "Epoch 4622: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1299 - accuracy: 0.9411 - val_loss: 0.1942 - val_accuracy: 0.9304\n",
            "Epoch 4623/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9416\n",
            "Epoch 4623: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1247 - accuracy: 0.9414 - val_loss: 0.1950 - val_accuracy: 0.9306\n",
            "Epoch 4624/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9402\n",
            "Epoch 4624: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1290 - accuracy: 0.9401 - val_loss: 0.2241 - val_accuracy: 0.9169\n",
            "Epoch 4625/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9424\n",
            "Epoch 4625: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9423 - val_loss: 0.1647 - val_accuracy: 0.9586\n",
            "Epoch 4626/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9438\n",
            "Epoch 4626: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1230 - accuracy: 0.9429 - val_loss: 0.1652 - val_accuracy: 0.9519\n",
            "Epoch 4627/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1240 - accuracy: 0.9413\n",
            "Epoch 4627: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1240 - accuracy: 0.9417 - val_loss: 0.1632 - val_accuracy: 0.9605\n",
            "Epoch 4628/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9392\n",
            "Epoch 4628: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1300 - accuracy: 0.9382 - val_loss: 0.1758 - val_accuracy: 0.9484\n",
            "Epoch 4629/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9369\n",
            "Epoch 4629: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1353 - accuracy: 0.9369 - val_loss: 0.3379 - val_accuracy: 0.8692\n",
            "Epoch 4630/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1427 - accuracy: 0.9345\n",
            "Epoch 4630: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1426 - accuracy: 0.9344 - val_loss: 0.1560 - val_accuracy: 0.9586\n",
            "Epoch 4631/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1362 - accuracy: 0.9355\n",
            "Epoch 4631: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1361 - accuracy: 0.9362 - val_loss: 0.2063 - val_accuracy: 0.9284\n",
            "Epoch 4632/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1323 - accuracy: 0.9387\n",
            "Epoch 4632: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1328 - accuracy: 0.9382 - val_loss: 0.1670 - val_accuracy: 0.9477\n",
            "Epoch 4633/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1213 - accuracy: 0.9434\n",
            "Epoch 4633: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1219 - accuracy: 0.9434 - val_loss: 0.1555 - val_accuracy: 0.9538\n",
            "Epoch 4634/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9424\n",
            "Epoch 4634: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1224 - accuracy: 0.9427 - val_loss: 0.1499 - val_accuracy: 0.9581\n",
            "Epoch 4635/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9438\n",
            "Epoch 4635: loss did not improve from 0.11985\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1209 - accuracy: 0.9430 - val_loss: 0.1444 - val_accuracy: 0.9631\n",
            "Epoch 4636/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1186 - accuracy: 0.9445\n",
            "Epoch 4636: loss improved from 0.11985 to 0.11888, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.1189 - accuracy: 0.9444 - val_loss: 0.1589 - val_accuracy: 0.9519\n",
            "Epoch 4637/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1211 - accuracy: 0.9442\n",
            "Epoch 4637: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1217 - accuracy: 0.9440 - val_loss: 0.1635 - val_accuracy: 0.9566\n",
            "Epoch 4638/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9429\n",
            "Epoch 4638: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1234 - accuracy: 0.9424 - val_loss: 0.1131 - val_accuracy: 0.9744\n",
            "Epoch 4639/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9352\n",
            "Epoch 4639: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1387 - accuracy: 0.9352 - val_loss: 0.1782 - val_accuracy: 0.9449\n",
            "Epoch 4640/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1348 - accuracy: 0.9374\n",
            "Epoch 4640: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1352 - accuracy: 0.9374 - val_loss: 0.1457 - val_accuracy: 0.9631\n",
            "Epoch 4641/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9416\n",
            "Epoch 4641: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1286 - accuracy: 0.9411 - val_loss: 0.1820 - val_accuracy: 0.9460\n",
            "Epoch 4642/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9392\n",
            "Epoch 4642: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1294 - accuracy: 0.9394 - val_loss: 0.1885 - val_accuracy: 0.9401\n",
            "Epoch 4643/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9393\n",
            "Epoch 4643: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1367 - accuracy: 0.9389 - val_loss: 0.2161 - val_accuracy: 0.9297\n",
            "Epoch 4644/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1331 - accuracy: 0.9384\n",
            "Epoch 4644: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1333 - accuracy: 0.9381 - val_loss: 0.1291 - val_accuracy: 0.9772\n",
            "Epoch 4645/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1269 - accuracy: 0.9425\n",
            "Epoch 4645: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1289 - accuracy: 0.9415 - val_loss: 0.2537 - val_accuracy: 0.9033\n",
            "Epoch 4646/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9398\n",
            "Epoch 4646: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9398 - val_loss: 0.2169 - val_accuracy: 0.9200\n",
            "Epoch 4647/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9399\n",
            "Epoch 4647: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9399 - val_loss: 0.1923 - val_accuracy: 0.9311\n",
            "Epoch 4648/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9424\n",
            "Epoch 4648: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1235 - accuracy: 0.9427 - val_loss: 0.1624 - val_accuracy: 0.9529\n",
            "Epoch 4649/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1193 - accuracy: 0.9438\n",
            "Epoch 4649: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1212 - accuracy: 0.9427 - val_loss: 0.1446 - val_accuracy: 0.9633\n",
            "Epoch 4650/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1200 - accuracy: 0.9443\n",
            "Epoch 4650: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1194 - accuracy: 0.9446 - val_loss: 0.1167 - val_accuracy: 0.9785\n",
            "Epoch 4651/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9429\n",
            "Epoch 4651: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1214 - accuracy: 0.9430 - val_loss: 0.1479 - val_accuracy: 0.9644\n",
            "Epoch 4652/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1196 - accuracy: 0.9446\n",
            "Epoch 4652: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1206 - accuracy: 0.9443 - val_loss: 0.1410 - val_accuracy: 0.9620\n",
            "Epoch 4653/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9403\n",
            "Epoch 4653: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1287 - accuracy: 0.9396 - val_loss: 0.1747 - val_accuracy: 0.9469\n",
            "Epoch 4654/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1305 - accuracy: 0.9394\n",
            "Epoch 4654: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1312 - accuracy: 0.9392 - val_loss: 0.1560 - val_accuracy: 0.9584\n",
            "Epoch 4655/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1305 - accuracy: 0.9393\n",
            "Epoch 4655: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1295 - accuracy: 0.9401 - val_loss: 0.1008 - val_accuracy: 0.9898\n",
            "Epoch 4656/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.9328\n",
            "Epoch 4656: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1407 - accuracy: 0.9336 - val_loss: 0.2409 - val_accuracy: 0.9039\n",
            "Epoch 4657/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1321 - accuracy: 0.9388\n",
            "Epoch 4657: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1317 - accuracy: 0.9388 - val_loss: 0.1888 - val_accuracy: 0.9406\n",
            "Epoch 4658/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1291 - accuracy: 0.9397\n",
            "Epoch 4658: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1294 - accuracy: 0.9396 - val_loss: 0.1500 - val_accuracy: 0.9623\n",
            "Epoch 4659/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9407\n",
            "Epoch 4659: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1269 - accuracy: 0.9404 - val_loss: 0.1784 - val_accuracy: 0.9410\n",
            "Epoch 4660/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1253 - accuracy: 0.9432\n",
            "Epoch 4660: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9420 - val_loss: 0.1914 - val_accuracy: 0.9284\n",
            "Epoch 4661/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9431\n",
            "Epoch 4661: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1220 - accuracy: 0.9430 - val_loss: 0.1336 - val_accuracy: 0.9714\n",
            "Epoch 4662/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1205 - accuracy: 0.9443\n",
            "Epoch 4662: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 0.9432 - val_loss: 0.1583 - val_accuracy: 0.9536\n",
            "Epoch 4663/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9422\n",
            "Epoch 4663: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1254 - accuracy: 0.9422 - val_loss: 0.1481 - val_accuracy: 0.9677\n",
            "Epoch 4664/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9428\n",
            "Epoch 4664: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9416 - val_loss: 0.1734 - val_accuracy: 0.9467\n",
            "Epoch 4665/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1384 - accuracy: 0.9356\n",
            "Epoch 4665: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1398 - accuracy: 0.9352 - val_loss: 0.1173 - val_accuracy: 0.9848\n",
            "Epoch 4666/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9367\n",
            "Epoch 4666: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1375 - accuracy: 0.9367 - val_loss: 0.2345 - val_accuracy: 0.9014\n",
            "Epoch 4667/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9378\n",
            "Epoch 4667: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1340 - accuracy: 0.9376 - val_loss: 0.1541 - val_accuracy: 0.9510\n",
            "Epoch 4668/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1299 - accuracy: 0.9400\n",
            "Epoch 4668: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1298 - accuracy: 0.9399 - val_loss: 0.1926 - val_accuracy: 0.9350\n",
            "Epoch 4669/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9398\n",
            "Epoch 4669: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9393 - val_loss: 0.1786 - val_accuracy: 0.9395\n",
            "Epoch 4670/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1340 - accuracy: 0.9363\n",
            "Epoch 4670: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1330 - accuracy: 0.9373 - val_loss: 0.1725 - val_accuracy: 0.9525\n",
            "Epoch 4671/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9400\n",
            "Epoch 4671: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9401 - val_loss: 0.1326 - val_accuracy: 0.9723\n",
            "Epoch 4672/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1281 - accuracy: 0.9417\n",
            "Epoch 4672: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1286 - accuracy: 0.9416 - val_loss: 0.1621 - val_accuracy: 0.9551\n",
            "Epoch 4673/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1249 - accuracy: 0.9432\n",
            "Epoch 4673: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1259 - accuracy: 0.9432 - val_loss: 0.1538 - val_accuracy: 0.9614\n",
            "Epoch 4674/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9415\n",
            "Epoch 4674: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1244 - accuracy: 0.9415 - val_loss: 0.1764 - val_accuracy: 0.9406\n",
            "Epoch 4675/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9418\n",
            "Epoch 4675: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1250 - accuracy: 0.9411 - val_loss: 0.1485 - val_accuracy: 0.9640\n",
            "Epoch 4676/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1265 - accuracy: 0.9405\n",
            "Epoch 4676: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1263 - accuracy: 0.9406 - val_loss: 0.1647 - val_accuracy: 0.9497\n",
            "Epoch 4677/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9412\n",
            "Epoch 4677: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1272 - accuracy: 0.9408 - val_loss: 0.1702 - val_accuracy: 0.9503\n",
            "Epoch 4678/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9441\n",
            "Epoch 4678: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1205 - accuracy: 0.9441 - val_loss: 0.1491 - val_accuracy: 0.9621\n",
            "Epoch 4679/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9355\n",
            "Epoch 4679: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1390 - accuracy: 0.9353 - val_loss: 0.2443 - val_accuracy: 0.9122\n",
            "Epoch 4680/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9370\n",
            "Epoch 4680: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1345 - accuracy: 0.9374 - val_loss: 0.1422 - val_accuracy: 0.9696\n",
            "Epoch 4681/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9406\n",
            "Epoch 4681: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1305 - accuracy: 0.9406 - val_loss: 0.1515 - val_accuracy: 0.9659\n",
            "Epoch 4682/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9397\n",
            "Epoch 4682: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1304 - accuracy: 0.9391 - val_loss: 0.1483 - val_accuracy: 0.9642\n",
            "Epoch 4683/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9395\n",
            "Epoch 4683: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1319 - accuracy: 0.9395 - val_loss: 0.1276 - val_accuracy: 0.9718\n",
            "Epoch 4684/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1304 - accuracy: 0.9390\n",
            "Epoch 4684: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1302 - accuracy: 0.9394 - val_loss: 0.1584 - val_accuracy: 0.9556\n",
            "Epoch 4685/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9440\n",
            "Epoch 4685: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1249 - accuracy: 0.9440 - val_loss: 0.1312 - val_accuracy: 0.9731\n",
            "Epoch 4686/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1234 - accuracy: 0.9422\n",
            "Epoch 4686: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9424 - val_loss: 0.1257 - val_accuracy: 0.9746\n",
            "Epoch 4687/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1295 - accuracy: 0.9403\n",
            "Epoch 4687: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1285 - accuracy: 0.9408 - val_loss: 0.1509 - val_accuracy: 0.9638\n",
            "Epoch 4688/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1256 - accuracy: 0.9424\n",
            "Epoch 4688: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1259 - accuracy: 0.9426 - val_loss: 0.2066 - val_accuracy: 0.9269\n",
            "Epoch 4689/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9379\n",
            "Epoch 4689: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1321 - accuracy: 0.9380 - val_loss: 0.1689 - val_accuracy: 0.9545\n",
            "Epoch 4690/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9362\n",
            "Epoch 4690: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1372 - accuracy: 0.9362 - val_loss: 0.2085 - val_accuracy: 0.9410\n",
            "Epoch 4691/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9420\n",
            "Epoch 4691: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1282 - accuracy: 0.9421 - val_loss: 0.1541 - val_accuracy: 0.9543\n",
            "Epoch 4692/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9393\n",
            "Epoch 4692: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1280 - accuracy: 0.9401 - val_loss: 0.2155 - val_accuracy: 0.9254\n",
            "Epoch 4693/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9368\n",
            "Epoch 4693: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1337 - accuracy: 0.9368 - val_loss: 0.1440 - val_accuracy: 0.9683\n",
            "Epoch 4694/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9421\n",
            "Epoch 4694: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1238 - accuracy: 0.9421 - val_loss: 0.1368 - val_accuracy: 0.9683\n",
            "Epoch 4695/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9413\n",
            "Epoch 4695: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1222 - accuracy: 0.9414 - val_loss: 0.1442 - val_accuracy: 0.9620\n",
            "Epoch 4696/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1207 - accuracy: 0.9430\n",
            "Epoch 4696: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 0.9431 - val_loss: 0.1249 - val_accuracy: 0.9744\n",
            "Epoch 4697/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9443\n",
            "Epoch 4697: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1212 - accuracy: 0.9441 - val_loss: 0.1378 - val_accuracy: 0.9659\n",
            "Epoch 4698/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1226 - accuracy: 0.9434\n",
            "Epoch 4698: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1224 - accuracy: 0.9436 - val_loss: 0.1435 - val_accuracy: 0.9634\n",
            "Epoch 4699/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9431\n",
            "Epoch 4699: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1231 - accuracy: 0.9431 - val_loss: 0.1590 - val_accuracy: 0.9536\n",
            "Epoch 4700/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9409\n",
            "Epoch 4700: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9406 - val_loss: 0.1870 - val_accuracy: 0.9393\n",
            "Epoch 4701/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9418\n",
            "Epoch 4701: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 0.9415 - val_loss: 0.1439 - val_accuracy: 0.9646\n",
            "Epoch 4702/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1212 - accuracy: 0.9445\n",
            "Epoch 4702: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1219 - accuracy: 0.9441 - val_loss: 0.1973 - val_accuracy: 0.9300\n",
            "Epoch 4703/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1274 - accuracy: 0.9413\n",
            "Epoch 4703: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1288 - accuracy: 0.9404 - val_loss: 0.2077 - val_accuracy: 0.9256\n",
            "Epoch 4704/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1994 - accuracy: 0.9146\n",
            "Epoch 4704: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.2002 - accuracy: 0.9133 - val_loss: 0.3628 - val_accuracy: 0.8469\n",
            "Epoch 4705/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9308\n",
            "Epoch 4705: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1519 - accuracy: 0.9304 - val_loss: 0.1687 - val_accuracy: 0.9499\n",
            "Epoch 4706/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9414\n",
            "Epoch 4706: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1265 - accuracy: 0.9414 - val_loss: 0.1496 - val_accuracy: 0.9590\n",
            "Epoch 4707/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9428\n",
            "Epoch 4707: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1224 - accuracy: 0.9425 - val_loss: 0.1274 - val_accuracy: 0.9770\n",
            "Epoch 4708/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9444\n",
            "Epoch 4708: loss did not improve from 0.11888\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1191 - accuracy: 0.9446 - val_loss: 0.1386 - val_accuracy: 0.9601\n",
            "Epoch 4709/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1182 - accuracy: 0.9435\n",
            "Epoch 4709: loss improved from 0.11888 to 0.11833, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 0.1183 - accuracy: 0.9436 - val_loss: 0.1397 - val_accuracy: 0.9640\n",
            "Epoch 4710/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9437\n",
            "Epoch 4710: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1218 - accuracy: 0.9428 - val_loss: 0.1975 - val_accuracy: 0.9263\n",
            "Epoch 4711/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9390\n",
            "Epoch 4711: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1282 - accuracy: 0.9394 - val_loss: 0.1477 - val_accuracy: 0.9644\n",
            "Epoch 4712/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1215 - accuracy: 0.9433\n",
            "Epoch 4712: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1228 - accuracy: 0.9427 - val_loss: 0.1182 - val_accuracy: 0.9790\n",
            "Epoch 4713/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9429\n",
            "Epoch 4713: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 0.9432 - val_loss: 0.1086 - val_accuracy: 0.9813\n",
            "Epoch 4714/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1171 - accuracy: 0.9451\n",
            "Epoch 4714: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1184 - accuracy: 0.9444 - val_loss: 0.1585 - val_accuracy: 0.9495\n",
            "Epoch 4715/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1197 - accuracy: 0.9444\n",
            "Epoch 4715: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1194 - accuracy: 0.9446 - val_loss: 0.1367 - val_accuracy: 0.9660\n",
            "Epoch 4716/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9434\n",
            "Epoch 4716: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1213 - accuracy: 0.9427 - val_loss: 0.1254 - val_accuracy: 0.9807\n",
            "Epoch 4717/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9437\n",
            "Epoch 4717: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1216 - accuracy: 0.9437 - val_loss: 0.1333 - val_accuracy: 0.9675\n",
            "Epoch 4718/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9414\n",
            "Epoch 4718: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9410 - val_loss: 0.1800 - val_accuracy: 0.9406\n",
            "Epoch 4719/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1218 - accuracy: 0.9427\n",
            "Epoch 4719: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1239 - accuracy: 0.9420 - val_loss: 0.1239 - val_accuracy: 0.9740\n",
            "Epoch 4720/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1238 - accuracy: 0.9422\n",
            "Epoch 4720: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1245 - accuracy: 0.9421 - val_loss: 0.1434 - val_accuracy: 0.9608\n",
            "Epoch 4721/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1295 - accuracy: 0.9400\n",
            "Epoch 4721: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9401 - val_loss: 0.1437 - val_accuracy: 0.9683\n",
            "Epoch 4722/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1314 - accuracy: 0.9396\n",
            "Epoch 4722: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9403 - val_loss: 0.1433 - val_accuracy: 0.9621\n",
            "Epoch 4723/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1250 - accuracy: 0.9420\n",
            "Epoch 4723: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1245 - accuracy: 0.9420 - val_loss: 0.1564 - val_accuracy: 0.9623\n",
            "Epoch 4724/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9413\n",
            "Epoch 4724: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1250 - accuracy: 0.9413 - val_loss: 0.1763 - val_accuracy: 0.9408\n",
            "Epoch 4725/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9376\n",
            "Epoch 4725: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1373 - accuracy: 0.9368 - val_loss: 0.2273 - val_accuracy: 0.9198\n",
            "Epoch 4726/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1461 - accuracy: 0.9335\n",
            "Epoch 4726: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1456 - accuracy: 0.9338 - val_loss: 0.1839 - val_accuracy: 0.9491\n",
            "Epoch 4727/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9411\n",
            "Epoch 4727: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1290 - accuracy: 0.9408 - val_loss: 0.2031 - val_accuracy: 0.9321\n",
            "Epoch 4728/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9427\n",
            "Epoch 4728: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1252 - accuracy: 0.9427 - val_loss: 0.1749 - val_accuracy: 0.9510\n",
            "Epoch 4729/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1271 - accuracy: 0.9406\n",
            "Epoch 4729: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1278 - accuracy: 0.9406 - val_loss: 0.1556 - val_accuracy: 0.9590\n",
            "Epoch 4730/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9400\n",
            "Epoch 4730: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1298 - accuracy: 0.9400 - val_loss: 0.1784 - val_accuracy: 0.9419\n",
            "Epoch 4731/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1293 - accuracy: 0.9399\n",
            "Epoch 4731: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1285 - accuracy: 0.9409 - val_loss: 0.1400 - val_accuracy: 0.9684\n",
            "Epoch 4732/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1217 - accuracy: 0.9428\n",
            "Epoch 4732: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1226 - accuracy: 0.9426 - val_loss: 0.1602 - val_accuracy: 0.9469\n",
            "Epoch 4733/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.9427\n",
            "Epoch 4733: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1241 - accuracy: 0.9430 - val_loss: 0.1566 - val_accuracy: 0.9595\n",
            "Epoch 4734/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9410\n",
            "Epoch 4734: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1270 - accuracy: 0.9408 - val_loss: 0.2260 - val_accuracy: 0.9206\n",
            "Epoch 4735/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9433\n",
            "Epoch 4735: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1261 - accuracy: 0.9431 - val_loss: 0.1314 - val_accuracy: 0.9659\n",
            "Epoch 4736/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9420\n",
            "Epoch 4736: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9407 - val_loss: 0.1428 - val_accuracy: 0.9655\n",
            "Epoch 4737/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9395\n",
            "Epoch 4737: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9394 - val_loss: 0.1494 - val_accuracy: 0.9581\n",
            "Epoch 4738/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9420\n",
            "Epoch 4738: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1256 - accuracy: 0.9420 - val_loss: 0.1685 - val_accuracy: 0.9521\n",
            "Epoch 4739/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9405\n",
            "Epoch 4739: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1297 - accuracy: 0.9405 - val_loss: 0.1581 - val_accuracy: 0.9582\n",
            "Epoch 4740/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9412\n",
            "Epoch 4740: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1312 - accuracy: 0.9411 - val_loss: 0.1913 - val_accuracy: 0.9298\n",
            "Epoch 4741/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9426\n",
            "Epoch 4741: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1235 - accuracy: 0.9426 - val_loss: 0.1541 - val_accuracy: 0.9620\n",
            "Epoch 4742/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9421\n",
            "Epoch 4742: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9419 - val_loss: 0.1274 - val_accuracy: 0.9749\n",
            "Epoch 4743/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1273 - accuracy: 0.9405\n",
            "Epoch 4743: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9404 - val_loss: 0.1289 - val_accuracy: 0.9666\n",
            "Epoch 4744/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1225 - accuracy: 0.9430\n",
            "Epoch 4744: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1246 - accuracy: 0.9421 - val_loss: 0.1582 - val_accuracy: 0.9588\n",
            "Epoch 4745/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1271 - accuracy: 0.9415\n",
            "Epoch 4745: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9418 - val_loss: 0.1716 - val_accuracy: 0.9471\n",
            "Epoch 4746/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1223 - accuracy: 0.9429\n",
            "Epoch 4746: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1229 - accuracy: 0.9424 - val_loss: 0.1755 - val_accuracy: 0.9415\n",
            "Epoch 4747/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1237 - accuracy: 0.9426\n",
            "Epoch 4747: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9416 - val_loss: 0.1722 - val_accuracy: 0.9499\n",
            "Epoch 4748/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9394\n",
            "Epoch 4748: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1280 - accuracy: 0.9394 - val_loss: 0.1519 - val_accuracy: 0.9542\n",
            "Epoch 4749/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1269 - accuracy: 0.9423\n",
            "Epoch 4749: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1287 - accuracy: 0.9412 - val_loss: 0.2159 - val_accuracy: 0.9213\n",
            "Epoch 4750/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1390 - accuracy: 0.9385\n",
            "Epoch 4750: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1394 - accuracy: 0.9379 - val_loss: 0.1685 - val_accuracy: 0.9491\n",
            "Epoch 4751/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.9383\n",
            "Epoch 4751: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1339 - accuracy: 0.9383 - val_loss: 0.1390 - val_accuracy: 0.9659\n",
            "Epoch 4752/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1361 - accuracy: 0.9388\n",
            "Epoch 4752: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1370 - accuracy: 0.9383 - val_loss: 0.1339 - val_accuracy: 0.9694\n",
            "Epoch 4753/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1246 - accuracy: 0.9421\n",
            "Epoch 4753: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1250 - accuracy: 0.9419 - val_loss: 0.1523 - val_accuracy: 0.9584\n",
            "Epoch 4754/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9409\n",
            "Epoch 4754: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1298 - accuracy: 0.9408 - val_loss: 0.1731 - val_accuracy: 0.9477\n",
            "Epoch 4755/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1254 - accuracy: 0.9421\n",
            "Epoch 4755: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9416 - val_loss: 0.1508 - val_accuracy: 0.9590\n",
            "Epoch 4756/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1202 - accuracy: 0.9435\n",
            "Epoch 4756: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1204 - accuracy: 0.9433 - val_loss: 0.1364 - val_accuracy: 0.9690\n",
            "Epoch 4757/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1207 - accuracy: 0.9436\n",
            "Epoch 4757: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 0.9433 - val_loss: 0.1612 - val_accuracy: 0.9473\n",
            "Epoch 4758/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9428\n",
            "Epoch 4758: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1230 - accuracy: 0.9426 - val_loss: 0.1275 - val_accuracy: 0.9718\n",
            "Epoch 4759/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1252 - accuracy: 0.9427\n",
            "Epoch 4759: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1247 - accuracy: 0.9428 - val_loss: 0.1560 - val_accuracy: 0.9551\n",
            "Epoch 4760/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1247 - accuracy: 0.9413\n",
            "Epoch 4760: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9413 - val_loss: 0.1489 - val_accuracy: 0.9540\n",
            "Epoch 4761/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1246 - accuracy: 0.9422\n",
            "Epoch 4761: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1243 - accuracy: 0.9428 - val_loss: 0.1506 - val_accuracy: 0.9618\n",
            "Epoch 4762/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9410\n",
            "Epoch 4762: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1284 - accuracy: 0.9410 - val_loss: 0.1501 - val_accuracy: 0.9582\n",
            "Epoch 4763/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9391\n",
            "Epoch 4763: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1368 - accuracy: 0.9383 - val_loss: 0.1477 - val_accuracy: 0.9633\n",
            "Epoch 4764/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1344 - accuracy: 0.9391\n",
            "Epoch 4764: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1355 - accuracy: 0.9382 - val_loss: 0.2414 - val_accuracy: 0.9083\n",
            "Epoch 4765/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9390\n",
            "Epoch 4765: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1322 - accuracy: 0.9390 - val_loss: 0.1438 - val_accuracy: 0.9668\n",
            "Epoch 4766/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9400\n",
            "Epoch 4766: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1288 - accuracy: 0.9397 - val_loss: 0.2251 - val_accuracy: 0.9167\n",
            "Epoch 4767/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9410\n",
            "Epoch 4767: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1258 - accuracy: 0.9410 - val_loss: 0.1734 - val_accuracy: 0.9430\n",
            "Epoch 4768/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9431\n",
            "Epoch 4768: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1200 - accuracy: 0.9434 - val_loss: 0.1628 - val_accuracy: 0.9475\n",
            "Epoch 4769/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9419\n",
            "Epoch 4769: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1224 - accuracy: 0.9414 - val_loss: 0.1592 - val_accuracy: 0.9605\n",
            "Epoch 4770/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9408\n",
            "Epoch 4770: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1257 - accuracy: 0.9410 - val_loss: 0.1745 - val_accuracy: 0.9436\n",
            "Epoch 4771/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9425\n",
            "Epoch 4771: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1208 - accuracy: 0.9425 - val_loss: 0.1928 - val_accuracy: 0.9321\n",
            "Epoch 4772/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9422\n",
            "Epoch 4772: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1231 - accuracy: 0.9421 - val_loss: 0.1126 - val_accuracy: 0.9824\n",
            "Epoch 4773/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9407\n",
            "Epoch 4773: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1238 - accuracy: 0.9411 - val_loss: 0.1412 - val_accuracy: 0.9696\n",
            "Epoch 4774/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1244 - accuracy: 0.9408\n",
            "Epoch 4774: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1266 - accuracy: 0.9399 - val_loss: 0.2020 - val_accuracy: 0.9423\n",
            "Epoch 4775/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9401\n",
            "Epoch 4775: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9395 - val_loss: 0.1462 - val_accuracy: 0.9633\n",
            "Epoch 4776/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9386\n",
            "Epoch 4776: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1336 - accuracy: 0.9386 - val_loss: 0.1683 - val_accuracy: 0.9562\n",
            "Epoch 4777/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1368 - accuracy: 0.9356\n",
            "Epoch 4777: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1368 - accuracy: 0.9357 - val_loss: 0.2038 - val_accuracy: 0.9310\n",
            "Epoch 4778/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1297 - accuracy: 0.9407\n",
            "Epoch 4778: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1320 - accuracy: 0.9399 - val_loss: 0.1819 - val_accuracy: 0.9445\n",
            "Epoch 4779/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9396\n",
            "Epoch 4779: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1331 - accuracy: 0.9396 - val_loss: 0.1365 - val_accuracy: 0.9684\n",
            "Epoch 4780/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1287 - accuracy: 0.9405\n",
            "Epoch 4780: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1278 - accuracy: 0.9410 - val_loss: 0.1905 - val_accuracy: 0.9321\n",
            "Epoch 4781/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9437\n",
            "Epoch 4781: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9433 - val_loss: 0.1336 - val_accuracy: 0.9761\n",
            "Epoch 4782/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9433\n",
            "Epoch 4782: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1237 - accuracy: 0.9431 - val_loss: 0.1440 - val_accuracy: 0.9595\n",
            "Epoch 4783/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1198 - accuracy: 0.9447\n",
            "Epoch 4783: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1203 - accuracy: 0.9443 - val_loss: 0.2039 - val_accuracy: 0.9243\n",
            "Epoch 4784/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1194 - accuracy: 0.9451\n",
            "Epoch 4784: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1192 - accuracy: 0.9451 - val_loss: 0.1429 - val_accuracy: 0.9579\n",
            "Epoch 4785/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9421\n",
            "Epoch 4785: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9418 - val_loss: 0.1427 - val_accuracy: 0.9627\n",
            "Epoch 4786/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9404\n",
            "Epoch 4786: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9400 - val_loss: 0.1798 - val_accuracy: 0.9369\n",
            "Epoch 4787/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9413\n",
            "Epoch 4787: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1257 - accuracy: 0.9413 - val_loss: 0.1977 - val_accuracy: 0.9315\n",
            "Epoch 4788/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9398\n",
            "Epoch 4788: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1276 - accuracy: 0.9395 - val_loss: 0.1641 - val_accuracy: 0.9542\n",
            "Epoch 4789/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1245 - accuracy: 0.9423\n",
            "Epoch 4789: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1257 - accuracy: 0.9418 - val_loss: 0.2048 - val_accuracy: 0.9302\n",
            "Epoch 4790/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1253 - accuracy: 0.9403\n",
            "Epoch 4790: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1270 - accuracy: 0.9394 - val_loss: 0.2161 - val_accuracy: 0.9176\n",
            "Epoch 4791/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1267 - accuracy: 0.9409\n",
            "Epoch 4791: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1285 - accuracy: 0.9398 - val_loss: 0.1471 - val_accuracy: 0.9623\n",
            "Epoch 4792/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9403\n",
            "Epoch 4792: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9402 - val_loss: 0.1447 - val_accuracy: 0.9603\n",
            "Epoch 4793/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1245 - accuracy: 0.9412\n",
            "Epoch 4793: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9414 - val_loss: 0.2069 - val_accuracy: 0.9219\n",
            "Epoch 4794/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9403\n",
            "Epoch 4794: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1302 - accuracy: 0.9403 - val_loss: 0.1600 - val_accuracy: 0.9514\n",
            "Epoch 4795/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9432\n",
            "Epoch 4795: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1252 - accuracy: 0.9433 - val_loss: 0.1479 - val_accuracy: 0.9605\n",
            "Epoch 4796/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9406\n",
            "Epoch 4796: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1299 - accuracy: 0.9407 - val_loss: 0.1531 - val_accuracy: 0.9649\n",
            "Epoch 4797/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.9366\n",
            "Epoch 4797: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1343 - accuracy: 0.9367 - val_loss: 0.1886 - val_accuracy: 0.9349\n",
            "Epoch 4798/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9420\n",
            "Epoch 4798: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1268 - accuracy: 0.9416 - val_loss: 0.1680 - val_accuracy: 0.9458\n",
            "Epoch 4799/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9400\n",
            "Epoch 4799: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1258 - accuracy: 0.9401 - val_loss: 0.1292 - val_accuracy: 0.9733\n",
            "Epoch 4800/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9412\n",
            "Epoch 4800: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1271 - accuracy: 0.9412 - val_loss: 0.1402 - val_accuracy: 0.9697\n",
            "Epoch 4801/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9393\n",
            "Epoch 4801: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1324 - accuracy: 0.9393 - val_loss: 0.1601 - val_accuracy: 0.9579\n",
            "Epoch 4802/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9403\n",
            "Epoch 4802: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1318 - accuracy: 0.9401 - val_loss: 0.1527 - val_accuracy: 0.9590\n",
            "Epoch 4803/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1275 - accuracy: 0.9409\n",
            "Epoch 4803: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1283 - accuracy: 0.9406 - val_loss: 0.2248 - val_accuracy: 0.9083\n",
            "Epoch 4804/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9398\n",
            "Epoch 4804: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1286 - accuracy: 0.9398 - val_loss: 0.1349 - val_accuracy: 0.9701\n",
            "Epoch 4805/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1241 - accuracy: 0.9428\n",
            "Epoch 4805: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1251 - accuracy: 0.9423 - val_loss: 0.2464 - val_accuracy: 0.9055\n",
            "Epoch 4806/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1257 - accuracy: 0.9417\n",
            "Epoch 4806: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1253 - accuracy: 0.9417 - val_loss: 0.1543 - val_accuracy: 0.9584\n",
            "Epoch 4807/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9430\n",
            "Epoch 4807: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1234 - accuracy: 0.9431 - val_loss: 0.1517 - val_accuracy: 0.9534\n",
            "Epoch 4808/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1224 - accuracy: 0.9452\n",
            "Epoch 4808: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1230 - accuracy: 0.9441 - val_loss: 0.1806 - val_accuracy: 0.9395\n",
            "Epoch 4809/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9434\n",
            "Epoch 4809: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1215 - accuracy: 0.9435 - val_loss: 0.1402 - val_accuracy: 0.9701\n",
            "Epoch 4810/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9430\n",
            "Epoch 4810: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1208 - accuracy: 0.9430 - val_loss: 0.1594 - val_accuracy: 0.9542\n",
            "Epoch 4811/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1297 - accuracy: 0.9392\n",
            "Epoch 4811: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9395 - val_loss: 0.2102 - val_accuracy: 0.9304\n",
            "Epoch 4812/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9411\n",
            "Epoch 4812: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1284 - accuracy: 0.9413 - val_loss: 0.1309 - val_accuracy: 0.9751\n",
            "Epoch 4813/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9421\n",
            "Epoch 4813: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1241 - accuracy: 0.9421 - val_loss: 0.1439 - val_accuracy: 0.9592\n",
            "Epoch 4814/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9428\n",
            "Epoch 4814: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1248 - accuracy: 0.9418 - val_loss: 0.2219 - val_accuracy: 0.9239\n",
            "Epoch 4815/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9411\n",
            "Epoch 4815: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1294 - accuracy: 0.9411 - val_loss: 0.1904 - val_accuracy: 0.9321\n",
            "Epoch 4816/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1275 - accuracy: 0.9408\n",
            "Epoch 4816: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1277 - accuracy: 0.9406 - val_loss: 0.1723 - val_accuracy: 0.9491\n",
            "Epoch 4817/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9396\n",
            "Epoch 4817: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1319 - accuracy: 0.9392 - val_loss: 0.1910 - val_accuracy: 0.9367\n",
            "Epoch 4818/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1351 - accuracy: 0.9375\n",
            "Epoch 4818: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1351 - accuracy: 0.9376 - val_loss: 0.1407 - val_accuracy: 0.9696\n",
            "Epoch 4819/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9381\n",
            "Epoch 4819: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1337 - accuracy: 0.9375 - val_loss: 0.2251 - val_accuracy: 0.9220\n",
            "Epoch 4820/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1277 - accuracy: 0.9404\n",
            "Epoch 4820: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1283 - accuracy: 0.9400 - val_loss: 0.1684 - val_accuracy: 0.9493\n",
            "Epoch 4821/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9410\n",
            "Epoch 4821: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1272 - accuracy: 0.9411 - val_loss: 0.2410 - val_accuracy: 0.9007\n",
            "Epoch 4822/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9427\n",
            "Epoch 4822: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1240 - accuracy: 0.9425 - val_loss: 0.1409 - val_accuracy: 0.9625\n",
            "Epoch 4823/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9406\n",
            "Epoch 4823: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1303 - accuracy: 0.9406 - val_loss: 0.1547 - val_accuracy: 0.9497\n",
            "Epoch 4824/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.9373\n",
            "Epoch 4824: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1347 - accuracy: 0.9375 - val_loss: 0.1369 - val_accuracy: 0.9716\n",
            "Epoch 4825/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.9381\n",
            "Epoch 4825: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1333 - accuracy: 0.9388 - val_loss: 0.1429 - val_accuracy: 0.9629\n",
            "Epoch 4826/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9409\n",
            "Epoch 4826: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1284 - accuracy: 0.9410 - val_loss: 0.1602 - val_accuracy: 0.9620\n",
            "Epoch 4827/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9393\n",
            "Epoch 4827: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1279 - accuracy: 0.9393 - val_loss: 0.1481 - val_accuracy: 0.9633\n",
            "Epoch 4828/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1411 - accuracy: 0.9355\n",
            "Epoch 4828: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1408 - accuracy: 0.9358 - val_loss: 0.1826 - val_accuracy: 0.9490\n",
            "Epoch 4829/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1330 - accuracy: 0.9383\n",
            "Epoch 4829: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1339 - accuracy: 0.9381 - val_loss: 0.1701 - val_accuracy: 0.9530\n",
            "Epoch 4830/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1318 - accuracy: 0.9389\n",
            "Epoch 4830: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9393 - val_loss: 0.1335 - val_accuracy: 0.9716\n",
            "Epoch 4831/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9429\n",
            "Epoch 4831: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1221 - accuracy: 0.9432 - val_loss: 0.1900 - val_accuracy: 0.9272\n",
            "Epoch 4832/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9421\n",
            "Epoch 4832: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1245 - accuracy: 0.9428 - val_loss: 0.1325 - val_accuracy: 0.9718\n",
            "Epoch 4833/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1256 - accuracy: 0.9420\n",
            "Epoch 4833: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1259 - accuracy: 0.9418 - val_loss: 0.1766 - val_accuracy: 0.9399\n",
            "Epoch 4834/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9442\n",
            "Epoch 4834: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1209 - accuracy: 0.9435 - val_loss: 0.1562 - val_accuracy: 0.9577\n",
            "Epoch 4835/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1188 - accuracy: 0.9455\n",
            "Epoch 4835: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1216 - accuracy: 0.9443 - val_loss: 0.1359 - val_accuracy: 0.9716\n",
            "Epoch 4836/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1218 - accuracy: 0.9437\n",
            "Epoch 4836: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1229 - accuracy: 0.9427 - val_loss: 0.1776 - val_accuracy: 0.9436\n",
            "Epoch 4837/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9448\n",
            "Epoch 4837: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1197 - accuracy: 0.9448 - val_loss: 0.1389 - val_accuracy: 0.9629\n",
            "Epoch 4838/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1258 - accuracy: 0.9396\n",
            "Epoch 4838: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9407 - val_loss: 0.1247 - val_accuracy: 0.9712\n",
            "Epoch 4839/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1234 - accuracy: 0.9422\n",
            "Epoch 4839: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9413 - val_loss: 0.1502 - val_accuracy: 0.9597\n",
            "Epoch 4840/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9408\n",
            "Epoch 4840: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1297 - accuracy: 0.9410 - val_loss: 0.1971 - val_accuracy: 0.9408\n",
            "Epoch 4841/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9419\n",
            "Epoch 4841: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1260 - accuracy: 0.9419 - val_loss: 0.2188 - val_accuracy: 0.9161\n",
            "Epoch 4842/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9418\n",
            "Epoch 4842: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1260 - accuracy: 0.9418 - val_loss: 0.1666 - val_accuracy: 0.9493\n",
            "Epoch 4843/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1269 - accuracy: 0.9423\n",
            "Epoch 4843: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1268 - accuracy: 0.9422 - val_loss: 0.1258 - val_accuracy: 0.9762\n",
            "Epoch 4844/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1240 - accuracy: 0.9423\n",
            "Epoch 4844: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1250 - accuracy: 0.9416 - val_loss: 0.1467 - val_accuracy: 0.9601\n",
            "Epoch 4845/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9415\n",
            "Epoch 4845: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9411 - val_loss: 0.1724 - val_accuracy: 0.9525\n",
            "Epoch 4846/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9400\n",
            "Epoch 4846: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1289 - accuracy: 0.9400 - val_loss: 0.1757 - val_accuracy: 0.9430\n",
            "Epoch 4847/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9407\n",
            "Epoch 4847: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1256 - accuracy: 0.9402 - val_loss: 0.1996 - val_accuracy: 0.9280\n",
            "Epoch 4848/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9420\n",
            "Epoch 4848: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1267 - accuracy: 0.9420 - val_loss: 0.1401 - val_accuracy: 0.9740\n",
            "Epoch 4849/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9412\n",
            "Epoch 4849: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1280 - accuracy: 0.9408 - val_loss: 0.1280 - val_accuracy: 0.9709\n",
            "Epoch 4850/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1363 - accuracy: 0.9374\n",
            "Epoch 4850: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1362 - accuracy: 0.9377 - val_loss: 0.1250 - val_accuracy: 0.9751\n",
            "Epoch 4851/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1321 - accuracy: 0.9386\n",
            "Epoch 4851: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9389 - val_loss: 0.1612 - val_accuracy: 0.9499\n",
            "Epoch 4852/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1256 - accuracy: 0.9423\n",
            "Epoch 4852: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1260 - accuracy: 0.9426 - val_loss: 0.1900 - val_accuracy: 0.9371\n",
            "Epoch 4853/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9420\n",
            "Epoch 4853: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1252 - accuracy: 0.9420 - val_loss: 0.1963 - val_accuracy: 0.9308\n",
            "Epoch 4854/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9411\n",
            "Epoch 4854: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1275 - accuracy: 0.9411 - val_loss: 0.2286 - val_accuracy: 0.9130\n",
            "Epoch 4855/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1291 - accuracy: 0.9403\n",
            "Epoch 4855: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1285 - accuracy: 0.9407 - val_loss: 0.1761 - val_accuracy: 0.9404\n",
            "Epoch 4856/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1229 - accuracy: 0.9415\n",
            "Epoch 4856: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1211 - accuracy: 0.9427 - val_loss: 0.1852 - val_accuracy: 0.9367\n",
            "Epoch 4857/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1257 - accuracy: 0.9413\n",
            "Epoch 4857: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1285 - accuracy: 0.9401 - val_loss: 0.1359 - val_accuracy: 0.9744\n",
            "Epoch 4858/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1359 - accuracy: 0.9385\n",
            "Epoch 4858: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1369 - accuracy: 0.9379 - val_loss: 0.1783 - val_accuracy: 0.9410\n",
            "Epoch 4859/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1256 - accuracy: 0.9411\n",
            "Epoch 4859: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9408 - val_loss: 0.1304 - val_accuracy: 0.9720\n",
            "Epoch 4860/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1246 - accuracy: 0.9416\n",
            "Epoch 4860: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1249 - accuracy: 0.9418 - val_loss: 0.1902 - val_accuracy: 0.9315\n",
            "Epoch 4861/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1267 - accuracy: 0.9413\n",
            "Epoch 4861: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1282 - accuracy: 0.9403 - val_loss: 0.1351 - val_accuracy: 0.9696\n",
            "Epoch 4862/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1273 - accuracy: 0.9414\n",
            "Epoch 4862: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1263 - accuracy: 0.9419 - val_loss: 0.1536 - val_accuracy: 0.9584\n",
            "Epoch 4863/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9437\n",
            "Epoch 4863: loss did not improve from 0.11833\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1202 - accuracy: 0.9437 - val_loss: 0.1488 - val_accuracy: 0.9579\n",
            "Epoch 4864/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9439\n",
            "Epoch 4864: loss improved from 0.11833 to 0.11781, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 29ms/step - loss: 0.1178 - accuracy: 0.9438 - val_loss: 0.1683 - val_accuracy: 0.9460\n",
            "Epoch 4865/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9439\n",
            "Epoch 4865: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1218 - accuracy: 0.9436 - val_loss: 0.1744 - val_accuracy: 0.9421\n",
            "Epoch 4866/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.9402\n",
            "Epoch 4866: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1237 - accuracy: 0.9405 - val_loss: 0.1727 - val_accuracy: 0.9419\n",
            "Epoch 4867/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9420\n",
            "Epoch 4867: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1227 - accuracy: 0.9420 - val_loss: 0.1812 - val_accuracy: 0.9439\n",
            "Epoch 4868/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1220 - accuracy: 0.9444\n",
            "Epoch 4868: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1231 - accuracy: 0.9436 - val_loss: 0.1376 - val_accuracy: 0.9660\n",
            "Epoch 4869/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1227 - accuracy: 0.9435\n",
            "Epoch 4869: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1233 - accuracy: 0.9429 - val_loss: 0.1620 - val_accuracy: 0.9469\n",
            "Epoch 4870/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1271 - accuracy: 0.9418\n",
            "Epoch 4870: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1273 - accuracy: 0.9417 - val_loss: 0.1404 - val_accuracy: 0.9692\n",
            "Epoch 4871/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1268 - accuracy: 0.9412\n",
            "Epoch 4871: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1282 - accuracy: 0.9407 - val_loss: 0.2233 - val_accuracy: 0.9220\n",
            "Epoch 4872/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9343\n",
            "Epoch 4872: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1454 - accuracy: 0.9343 - val_loss: 0.1505 - val_accuracy: 0.9575\n",
            "Epoch 4873/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1453 - accuracy: 0.9327\n",
            "Epoch 4873: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1433 - accuracy: 0.9338 - val_loss: 0.1424 - val_accuracy: 0.9660\n",
            "Epoch 4874/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1360 - accuracy: 0.9371\n",
            "Epoch 4874: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1360 - accuracy: 0.9369 - val_loss: 0.2022 - val_accuracy: 0.9206\n",
            "Epoch 4875/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1248 - accuracy: 0.9409\n",
            "Epoch 4875: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1263 - accuracy: 0.9408 - val_loss: 0.1818 - val_accuracy: 0.9454\n",
            "Epoch 4876/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9403\n",
            "Epoch 4876: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1276 - accuracy: 0.9407 - val_loss: 0.1464 - val_accuracy: 0.9644\n",
            "Epoch 4877/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9424\n",
            "Epoch 4877: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1249 - accuracy: 0.9424 - val_loss: 0.1268 - val_accuracy: 0.9748\n",
            "Epoch 4878/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9440\n",
            "Epoch 4878: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1222 - accuracy: 0.9440 - val_loss: 0.1683 - val_accuracy: 0.9519\n",
            "Epoch 4879/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1252 - accuracy: 0.9421\n",
            "Epoch 4879: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1256 - accuracy: 0.9420 - val_loss: 0.1461 - val_accuracy: 0.9586\n",
            "Epoch 4880/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9420\n",
            "Epoch 4880: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1265 - accuracy: 0.9420 - val_loss: 0.1318 - val_accuracy: 0.9694\n",
            "Epoch 4881/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9413\n",
            "Epoch 4881: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1270 - accuracy: 0.9413 - val_loss: 0.1707 - val_accuracy: 0.9438\n",
            "Epoch 4882/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9448\n",
            "Epoch 4882: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1202 - accuracy: 0.9446 - val_loss: 0.1705 - val_accuracy: 0.9506\n",
            "Epoch 4883/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9437\n",
            "Epoch 4883: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1194 - accuracy: 0.9438 - val_loss: 0.1400 - val_accuracy: 0.9662\n",
            "Epoch 4884/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9436\n",
            "Epoch 4884: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1187 - accuracy: 0.9436 - val_loss: 0.1423 - val_accuracy: 0.9692\n",
            "Epoch 4885/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1289 - accuracy: 0.9390\n",
            "Epoch 4885: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1294 - accuracy: 0.9385 - val_loss: 0.1767 - val_accuracy: 0.9425\n",
            "Epoch 4886/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1258 - accuracy: 0.9423\n",
            "Epoch 4886: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1261 - accuracy: 0.9421 - val_loss: 0.1429 - val_accuracy: 0.9625\n",
            "Epoch 4887/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1237 - accuracy: 0.9430\n",
            "Epoch 4887: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1249 - accuracy: 0.9422 - val_loss: 0.1965 - val_accuracy: 0.9306\n",
            "Epoch 4888/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9410\n",
            "Epoch 4888: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1281 - accuracy: 0.9410 - val_loss: 0.1592 - val_accuracy: 0.9545\n",
            "Epoch 4889/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9436\n",
            "Epoch 4889: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1232 - accuracy: 0.9436 - val_loss: 0.1491 - val_accuracy: 0.9547\n",
            "Epoch 4890/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1208 - accuracy: 0.9447\n",
            "Epoch 4890: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1217 - accuracy: 0.9442 - val_loss: 0.1730 - val_accuracy: 0.9465\n",
            "Epoch 4891/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1198 - accuracy: 0.9445\n",
            "Epoch 4891: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1189 - accuracy: 0.9450 - val_loss: 0.1464 - val_accuracy: 0.9642\n",
            "Epoch 4892/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1203 - accuracy: 0.9436\n",
            "Epoch 4892: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1224 - accuracy: 0.9428 - val_loss: 0.1030 - val_accuracy: 0.9852\n",
            "Epoch 4893/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9435\n",
            "Epoch 4893: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1255 - accuracy: 0.9419 - val_loss: 0.1224 - val_accuracy: 0.9746\n",
            "Epoch 4894/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9420\n",
            "Epoch 4894: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1276 - accuracy: 0.9417 - val_loss: 0.1419 - val_accuracy: 0.9597\n",
            "Epoch 4895/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1259 - accuracy: 0.9416\n",
            "Epoch 4895: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1254 - accuracy: 0.9418 - val_loss: 0.1382 - val_accuracy: 0.9692\n",
            "Epoch 4896/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9421\n",
            "Epoch 4896: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1261 - accuracy: 0.9421 - val_loss: 0.2174 - val_accuracy: 0.9191\n",
            "Epoch 4897/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9387\n",
            "Epoch 4897: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1352 - accuracy: 0.9387 - val_loss: 0.1669 - val_accuracy: 0.9536\n",
            "Epoch 4898/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1443 - accuracy: 0.9332\n",
            "Epoch 4898: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1436 - accuracy: 0.9337 - val_loss: 0.1879 - val_accuracy: 0.9382\n",
            "Epoch 4899/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1301 - accuracy: 0.9387\n",
            "Epoch 4899: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1298 - accuracy: 0.9387 - val_loss: 0.1455 - val_accuracy: 0.9595\n",
            "Epoch 4900/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9416\n",
            "Epoch 4900: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1265 - accuracy: 0.9417 - val_loss: 0.2138 - val_accuracy: 0.9157\n",
            "Epoch 4901/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9424\n",
            "Epoch 4901: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1256 - accuracy: 0.9421 - val_loss: 0.2126 - val_accuracy: 0.9326\n",
            "Epoch 4902/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1358 - accuracy: 0.9375\n",
            "Epoch 4902: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1387 - accuracy: 0.9358 - val_loss: 0.2086 - val_accuracy: 0.9328\n",
            "Epoch 4903/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1336 - accuracy: 0.9396\n",
            "Epoch 4903: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1345 - accuracy: 0.9384 - val_loss: 0.2004 - val_accuracy: 0.9326\n",
            "Epoch 4904/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1284 - accuracy: 0.9416\n",
            "Epoch 4904: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1296 - accuracy: 0.9409 - val_loss: 0.1408 - val_accuracy: 0.9679\n",
            "Epoch 4905/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1251 - accuracy: 0.9413\n",
            "Epoch 4905: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1244 - accuracy: 0.9417 - val_loss: 0.1388 - val_accuracy: 0.9605\n",
            "Epoch 4906/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1194 - accuracy: 0.9441\n",
            "Epoch 4906: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1197 - accuracy: 0.9440 - val_loss: 0.1658 - val_accuracy: 0.9482\n",
            "Epoch 4907/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9380\n",
            "Epoch 4907: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1318 - accuracy: 0.9380 - val_loss: 0.1438 - val_accuracy: 0.9625\n",
            "Epoch 4908/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9418\n",
            "Epoch 4908: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1274 - accuracy: 0.9418 - val_loss: 0.1834 - val_accuracy: 0.9395\n",
            "Epoch 4909/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9427\n",
            "Epoch 4909: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1241 - accuracy: 0.9427 - val_loss: 0.1734 - val_accuracy: 0.9501\n",
            "Epoch 4910/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9428\n",
            "Epoch 4910: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1210 - accuracy: 0.9428 - val_loss: 0.1579 - val_accuracy: 0.9497\n",
            "Epoch 4911/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1212 - accuracy: 0.9445\n",
            "Epoch 4911: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1225 - accuracy: 0.9436 - val_loss: 0.1546 - val_accuracy: 0.9586\n",
            "Epoch 4912/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9400\n",
            "Epoch 4912: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9400 - val_loss: 0.1052 - val_accuracy: 0.9840\n",
            "Epoch 4913/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9373\n",
            "Epoch 4913: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1352 - accuracy: 0.9373 - val_loss: 0.1624 - val_accuracy: 0.9543\n",
            "Epoch 4914/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9425\n",
            "Epoch 4914: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1238 - accuracy: 0.9428 - val_loss: 0.1921 - val_accuracy: 0.9326\n",
            "Epoch 4915/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9434\n",
            "Epoch 4915: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1234 - accuracy: 0.9425 - val_loss: 0.1936 - val_accuracy: 0.9245\n",
            "Epoch 4916/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9420\n",
            "Epoch 4916: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1268 - accuracy: 0.9420 - val_loss: 0.1474 - val_accuracy: 0.9582\n",
            "Epoch 4917/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9414\n",
            "Epoch 4917: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1260 - accuracy: 0.9414 - val_loss: 0.2782 - val_accuracy: 0.8905\n",
            "Epoch 4918/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1240 - accuracy: 0.9411\n",
            "Epoch 4918: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1256 - accuracy: 0.9403 - val_loss: 0.1167 - val_accuracy: 0.9818\n",
            "Epoch 4919/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9420\n",
            "Epoch 4919: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1247 - accuracy: 0.9420 - val_loss: 0.1268 - val_accuracy: 0.9733\n",
            "Epoch 4920/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1196 - accuracy: 0.9427\n",
            "Epoch 4920: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1211 - accuracy: 0.9426 - val_loss: 0.1411 - val_accuracy: 0.9601\n",
            "Epoch 4921/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1272 - accuracy: 0.9418\n",
            "Epoch 4921: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1265 - accuracy: 0.9423 - val_loss: 0.1789 - val_accuracy: 0.9397\n",
            "Epoch 4922/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1211 - accuracy: 0.9426\n",
            "Epoch 4922: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1218 - accuracy: 0.9426 - val_loss: 0.1326 - val_accuracy: 0.9705\n",
            "Epoch 4923/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9457\n",
            "Epoch 4923: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1199 - accuracy: 0.9451 - val_loss: 0.1614 - val_accuracy: 0.9523\n",
            "Epoch 4924/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9423\n",
            "Epoch 4924: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1223 - accuracy: 0.9419 - val_loss: 0.1811 - val_accuracy: 0.9439\n",
            "Epoch 4925/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9398\n",
            "Epoch 4925: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1302 - accuracy: 0.9398 - val_loss: 0.1626 - val_accuracy: 0.9523\n",
            "Epoch 4926/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1374 - accuracy: 0.9365\n",
            "Epoch 4926: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1376 - accuracy: 0.9365 - val_loss: 0.2483 - val_accuracy: 0.9020\n",
            "Epoch 4927/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1626 - accuracy: 0.9284\n",
            "Epoch 4927: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1624 - accuracy: 0.9282 - val_loss: 0.2056 - val_accuracy: 0.9352\n",
            "Epoch 4928/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9364\n",
            "Epoch 4928: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1396 - accuracy: 0.9372 - val_loss: 0.1546 - val_accuracy: 0.9560\n",
            "Epoch 4929/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9429\n",
            "Epoch 4929: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1235 - accuracy: 0.9425 - val_loss: 0.1094 - val_accuracy: 0.9829\n",
            "Epoch 4930/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1213 - accuracy: 0.9427\n",
            "Epoch 4930: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1212 - accuracy: 0.9428 - val_loss: 0.1449 - val_accuracy: 0.9653\n",
            "Epoch 4931/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9409\n",
            "Epoch 4931: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1262 - accuracy: 0.9406 - val_loss: 0.1810 - val_accuracy: 0.9384\n",
            "Epoch 4932/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9409\n",
            "Epoch 4932: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1303 - accuracy: 0.9395 - val_loss: 0.1724 - val_accuracy: 0.9473\n",
            "Epoch 4933/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9397\n",
            "Epoch 4933: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1306 - accuracy: 0.9399 - val_loss: 0.1274 - val_accuracy: 0.9738\n",
            "Epoch 4934/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9417\n",
            "Epoch 4934: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1241 - accuracy: 0.9417 - val_loss: 0.1349 - val_accuracy: 0.9679\n",
            "Epoch 4935/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1202 - accuracy: 0.9440\n",
            "Epoch 4935: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1210 - accuracy: 0.9440 - val_loss: 0.1302 - val_accuracy: 0.9729\n",
            "Epoch 4936/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9427\n",
            "Epoch 4936: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1240 - accuracy: 0.9427 - val_loss: 0.1588 - val_accuracy: 0.9521\n",
            "Epoch 4937/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1242 - accuracy: 0.9433\n",
            "Epoch 4937: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1248 - accuracy: 0.9429 - val_loss: 0.1560 - val_accuracy: 0.9501\n",
            "Epoch 4938/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1249 - accuracy: 0.9427\n",
            "Epoch 4938: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9425 - val_loss: 0.1512 - val_accuracy: 0.9586\n",
            "Epoch 4939/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9459\n",
            "Epoch 4939: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1181 - accuracy: 0.9451 - val_loss: 0.1489 - val_accuracy: 0.9597\n",
            "Epoch 4940/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9417\n",
            "Epoch 4940: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1251 - accuracy: 0.9414 - val_loss: 0.1562 - val_accuracy: 0.9558\n",
            "Epoch 4941/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1245 - accuracy: 0.9419\n",
            "Epoch 4941: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1252 - accuracy: 0.9414 - val_loss: 0.1737 - val_accuracy: 0.9401\n",
            "Epoch 4942/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1341 - accuracy: 0.9378\n",
            "Epoch 4942: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1339 - accuracy: 0.9384 - val_loss: 0.1388 - val_accuracy: 0.9690\n",
            "Epoch 4943/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9396\n",
            "Epoch 4943: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1304 - accuracy: 0.9396 - val_loss: 0.1875 - val_accuracy: 0.9423\n",
            "Epoch 4944/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1304 - accuracy: 0.9399\n",
            "Epoch 4944: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1311 - accuracy: 0.9395 - val_loss: 0.1881 - val_accuracy: 0.9452\n",
            "Epoch 4945/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1359 - accuracy: 0.9380\n",
            "Epoch 4945: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1356 - accuracy: 0.9378 - val_loss: 0.1306 - val_accuracy: 0.9714\n",
            "Epoch 4946/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9382\n",
            "Epoch 4946: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1330 - accuracy: 0.9382 - val_loss: 0.2446 - val_accuracy: 0.9163\n",
            "Epoch 4947/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1282 - accuracy: 0.9404\n",
            "Epoch 4947: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1285 - accuracy: 0.9401 - val_loss: 0.1464 - val_accuracy: 0.9579\n",
            "Epoch 4948/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9414\n",
            "Epoch 4948: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1277 - accuracy: 0.9414 - val_loss: 0.1541 - val_accuracy: 0.9582\n",
            "Epoch 4949/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9432\n",
            "Epoch 4949: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1251 - accuracy: 0.9432 - val_loss: 0.1677 - val_accuracy: 0.9493\n",
            "Epoch 4950/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1240 - accuracy: 0.9419\n",
            "Epoch 4950: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1240 - accuracy: 0.9421 - val_loss: 0.1808 - val_accuracy: 0.9458\n",
            "Epoch 4951/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1200 - accuracy: 0.9443\n",
            "Epoch 4951: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1201 - accuracy: 0.9444 - val_loss: 0.2063 - val_accuracy: 0.9237\n",
            "Epoch 4952/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1342 - accuracy: 0.9375\n",
            "Epoch 4952: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1349 - accuracy: 0.9376 - val_loss: 0.1474 - val_accuracy: 0.9634\n",
            "Epoch 4953/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1338 - accuracy: 0.9411\n",
            "Epoch 4953: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1338 - accuracy: 0.9414 - val_loss: 0.1259 - val_accuracy: 0.9755\n",
            "Epoch 4954/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9425\n",
            "Epoch 4954: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1291 - accuracy: 0.9419 - val_loss: 0.1861 - val_accuracy: 0.9375\n",
            "Epoch 4955/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9424\n",
            "Epoch 4955: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1235 - accuracy: 0.9421 - val_loss: 0.1482 - val_accuracy: 0.9601\n",
            "Epoch 4956/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9443\n",
            "Epoch 4956: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1211 - accuracy: 0.9443 - val_loss: 0.2086 - val_accuracy: 0.9258\n",
            "Epoch 4957/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1190 - accuracy: 0.9430\n",
            "Epoch 4957: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1194 - accuracy: 0.9430 - val_loss: 0.1964 - val_accuracy: 0.9345\n",
            "Epoch 4958/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9414\n",
            "Epoch 4958: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1274 - accuracy: 0.9414 - val_loss: 0.1637 - val_accuracy: 0.9532\n",
            "Epoch 4959/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9409\n",
            "Epoch 4959: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1273 - accuracy: 0.9412 - val_loss: 0.1439 - val_accuracy: 0.9627\n",
            "Epoch 4960/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9415\n",
            "Epoch 4960: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1250 - accuracy: 0.9415 - val_loss: 0.1545 - val_accuracy: 0.9568\n",
            "Epoch 4961/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9407\n",
            "Epoch 4961: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1276 - accuracy: 0.9407 - val_loss: 0.2356 - val_accuracy: 0.9115\n",
            "Epoch 4962/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1247 - accuracy: 0.9429\n",
            "Epoch 4962: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1257 - accuracy: 0.9423 - val_loss: 0.1498 - val_accuracy: 0.9612\n",
            "Epoch 4963/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9428\n",
            "Epoch 4963: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1254 - accuracy: 0.9424 - val_loss: 0.1705 - val_accuracy: 0.9452\n",
            "Epoch 4964/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9434\n",
            "Epoch 4964: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1228 - accuracy: 0.9434 - val_loss: 0.1351 - val_accuracy: 0.9671\n",
            "Epoch 4965/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1227 - accuracy: 0.9431\n",
            "Epoch 4965: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1231 - accuracy: 0.9432 - val_loss: 0.1195 - val_accuracy: 0.9788\n",
            "Epoch 4966/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1212 - accuracy: 0.9440\n",
            "Epoch 4966: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1230 - accuracy: 0.9423 - val_loss: 0.1246 - val_accuracy: 0.9742\n",
            "Epoch 4967/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1333 - accuracy: 0.9386\n",
            "Epoch 4967: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1335 - accuracy: 0.9387 - val_loss: 0.1743 - val_accuracy: 0.9482\n",
            "Epoch 4968/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9409\n",
            "Epoch 4968: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1264 - accuracy: 0.9411 - val_loss: 0.1715 - val_accuracy: 0.9516\n",
            "Epoch 4969/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9412\n",
            "Epoch 4969: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1253 - accuracy: 0.9412 - val_loss: 0.1530 - val_accuracy: 0.9575\n",
            "Epoch 4970/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1215 - accuracy: 0.9434\n",
            "Epoch 4970: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1218 - accuracy: 0.9432 - val_loss: 0.2106 - val_accuracy: 0.9276\n",
            "Epoch 4971/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1221 - accuracy: 0.9411\n",
            "Epoch 4971: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1221 - accuracy: 0.9414 - val_loss: 0.1542 - val_accuracy: 0.9553\n",
            "Epoch 4972/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9426\n",
            "Epoch 4972: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1243 - accuracy: 0.9420 - val_loss: 0.2062 - val_accuracy: 0.9232\n",
            "Epoch 4973/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9416\n",
            "Epoch 4973: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1260 - accuracy: 0.9414 - val_loss: 0.1410 - val_accuracy: 0.9629\n",
            "Epoch 4974/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9433\n",
            "Epoch 4974: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1211 - accuracy: 0.9435 - val_loss: 0.1352 - val_accuracy: 0.9647\n",
            "Epoch 4975/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9438\n",
            "Epoch 4975: loss did not improve from 0.11781\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1193 - accuracy: 0.9438 - val_loss: 0.1456 - val_accuracy: 0.9664\n",
            "Epoch 4976/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9453\n",
            "Epoch 4976: loss improved from 0.11781 to 0.11701, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 0.1170 - accuracy: 0.9453 - val_loss: 0.1577 - val_accuracy: 0.9558\n",
            "Epoch 4977/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9415\n",
            "Epoch 4977: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1237 - accuracy: 0.9415 - val_loss: 0.1349 - val_accuracy: 0.9736\n",
            "Epoch 4978/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1260 - accuracy: 0.9414\n",
            "Epoch 4978: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1268 - accuracy: 0.9414 - val_loss: 0.1500 - val_accuracy: 0.9620\n",
            "Epoch 4979/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1249 - accuracy: 0.9415\n",
            "Epoch 4979: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1228 - accuracy: 0.9426 - val_loss: 0.1410 - val_accuracy: 0.9647\n",
            "Epoch 4980/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1200 - accuracy: 0.9456\n",
            "Epoch 4980: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1208 - accuracy: 0.9453 - val_loss: 0.2152 - val_accuracy: 0.9157\n",
            "Epoch 4981/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1246 - accuracy: 0.9431\n",
            "Epoch 4981: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9425 - val_loss: 0.1075 - val_accuracy: 0.9827\n",
            "Epoch 4982/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1226 - accuracy: 0.9412\n",
            "Epoch 4982: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1226 - accuracy: 0.9416 - val_loss: 0.1414 - val_accuracy: 0.9594\n",
            "Epoch 4983/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1253 - accuracy: 0.9418\n",
            "Epoch 4983: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9417 - val_loss: 0.1565 - val_accuracy: 0.9545\n",
            "Epoch 4984/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1320 - accuracy: 0.9393\n",
            "Epoch 4984: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1335 - accuracy: 0.9383 - val_loss: 0.1671 - val_accuracy: 0.9508\n",
            "Epoch 4985/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9362\n",
            "Epoch 4985: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1351 - accuracy: 0.9367 - val_loss: 0.1399 - val_accuracy: 0.9720\n",
            "Epoch 4986/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1312 - accuracy: 0.9398\n",
            "Epoch 4986: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1315 - accuracy: 0.9390 - val_loss: 0.2068 - val_accuracy: 0.9293\n",
            "Epoch 4987/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1314 - accuracy: 0.9402\n",
            "Epoch 4987: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1316 - accuracy: 0.9401 - val_loss: 0.1621 - val_accuracy: 0.9577\n",
            "Epoch 4988/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9425\n",
            "Epoch 4988: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1230 - accuracy: 0.9425 - val_loss: 0.1434 - val_accuracy: 0.9653\n",
            "Epoch 4989/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9430\n",
            "Epoch 4989: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1237 - accuracy: 0.9430 - val_loss: 0.2053 - val_accuracy: 0.9297\n",
            "Epoch 4990/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1266 - accuracy: 0.9418\n",
            "Epoch 4990: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1275 - accuracy: 0.9414 - val_loss: 0.1492 - val_accuracy: 0.9556\n",
            "Epoch 4991/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9425\n",
            "Epoch 4991: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1236 - accuracy: 0.9427 - val_loss: 0.1533 - val_accuracy: 0.9517\n",
            "Epoch 4992/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1292 - accuracy: 0.9404\n",
            "Epoch 4992: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1301 - accuracy: 0.9398 - val_loss: 0.1776 - val_accuracy: 0.9471\n",
            "Epoch 4993/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9435\n",
            "Epoch 4993: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1236 - accuracy: 0.9431 - val_loss: 0.1993 - val_accuracy: 0.9207\n",
            "Epoch 4994/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1256 - accuracy: 0.9416\n",
            "Epoch 4994: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1267 - accuracy: 0.9410 - val_loss: 0.1444 - val_accuracy: 0.9666\n",
            "Epoch 4995/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9383\n",
            "Epoch 4995: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1304 - accuracy: 0.9379 - val_loss: 0.1879 - val_accuracy: 0.9458\n",
            "Epoch 4996/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1423 - accuracy: 0.9342\n",
            "Epoch 4996: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1422 - accuracy: 0.9346 - val_loss: 0.1698 - val_accuracy: 0.9508\n",
            "Epoch 4997/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1328 - accuracy: 0.9382\n",
            "Epoch 4997: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1341 - accuracy: 0.9376 - val_loss: 0.1232 - val_accuracy: 0.9770\n",
            "Epoch 4998/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9383\n",
            "Epoch 4998: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1335 - accuracy: 0.9383 - val_loss: 0.2152 - val_accuracy: 0.9196\n",
            "Epoch 4999/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9424\n",
            "Epoch 4999: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1251 - accuracy: 0.9423 - val_loss: 0.1654 - val_accuracy: 0.9571\n",
            "Epoch 5000/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1234 - accuracy: 0.9421\n",
            "Epoch 5000: loss did not improve from 0.11701\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1240 - accuracy: 0.9417 - val_loss: 0.1239 - val_accuracy: 0.9753\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 2.8615 - accuracy: 0.7406\n",
            "[2.861520528793335, 0.7405987977981567]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_1       0.87      0.80      0.83      3369\n",
            "     class 2       0.37      0.49      0.42       806\n",
            "\n",
            "    accuracy                           0.74      4175\n",
            "   macro avg       0.62      0.65      0.63      4175\n",
            "weighted avg       0.77      0.74      0.75      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9506\n",
            "[0.1217246949672699, 0.9505568146705627]\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_1       0.98      0.92      0.95     13470\n",
            "     class 2       0.93      0.98      0.95     13470\n",
            "\n",
            "    accuracy                           0.95     26940\n",
            "   macro avg       0.95      0.95      0.95     26940\n",
            "weighted avg       0.95      0.95      0.95     26940\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Model Continue Learning***#\n",
        "#------------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "history = model.fit(train_features, train_target['Number of Bugs'], batch_size = 600, epochs = 5000,  validation_split = 0.2 , callbacks=[checkpoint])\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\n\\n\")\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_1', 'class 2']\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTest-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int64)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9It7t8n_7Vi",
        "outputId": "20f6e01d-8844-4c8c-8a9a-970d1be62095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1098 - accuracy: 0.9493\n",
            "Epoch 3752: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1104 - accuracy: 0.9491 - val_loss: 0.1310 - val_accuracy: 0.9725\n",
            "Epoch 3753/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9477\n",
            "Epoch 3753: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1121 - accuracy: 0.9475 - val_loss: 0.1346 - val_accuracy: 0.9694\n",
            "Epoch 3754/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9488\n",
            "Epoch 3754: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9491 - val_loss: 0.1716 - val_accuracy: 0.9464\n",
            "Epoch 3755/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1081 - accuracy: 0.9492\n",
            "Epoch 3755: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9489 - val_loss: 0.1465 - val_accuracy: 0.9568\n",
            "Epoch 3756/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9492\n",
            "Epoch 3756: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1080 - accuracy: 0.9492 - val_loss: 0.0982 - val_accuracy: 0.9865\n",
            "Epoch 3757/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9490\n",
            "Epoch 3757: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1103 - accuracy: 0.9490 - val_loss: 0.1571 - val_accuracy: 0.9564\n",
            "Epoch 3758/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9494\n",
            "Epoch 3758: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1099 - accuracy: 0.9486 - val_loss: 0.1560 - val_accuracy: 0.9599\n",
            "Epoch 3759/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9480\n",
            "Epoch 3759: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9480 - val_loss: 0.1145 - val_accuracy: 0.9794\n",
            "Epoch 3760/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9476\n",
            "Epoch 3760: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1134 - accuracy: 0.9475 - val_loss: 0.1306 - val_accuracy: 0.9664\n",
            "Epoch 3761/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9437\n",
            "Epoch 3761: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1199 - accuracy: 0.9437 - val_loss: 0.1543 - val_accuracy: 0.9527\n",
            "Epoch 3762/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1431 - accuracy: 0.9375\n",
            "Epoch 3762: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1428 - accuracy: 0.9388 - val_loss: 0.1409 - val_accuracy: 0.9590\n",
            "Epoch 3763/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1197 - accuracy: 0.9465\n",
            "Epoch 3763: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1207 - accuracy: 0.9456 - val_loss: 0.1318 - val_accuracy: 0.9723\n",
            "Epoch 3764/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1130 - accuracy: 0.9486\n",
            "Epoch 3764: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1135 - accuracy: 0.9477 - val_loss: 0.1442 - val_accuracy: 0.9618\n",
            "Epoch 3765/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9489\n",
            "Epoch 3765: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1114 - accuracy: 0.9488 - val_loss: 0.1608 - val_accuracy: 0.9519\n",
            "Epoch 3766/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1101 - accuracy: 0.9490\n",
            "Epoch 3766: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9486 - val_loss: 0.1365 - val_accuracy: 0.9651\n",
            "Epoch 3767/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9417\n",
            "Epoch 3767: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1287 - accuracy: 0.9412 - val_loss: 0.1339 - val_accuracy: 0.9722\n",
            "Epoch 3768/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9465\n",
            "Epoch 3768: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1167 - accuracy: 0.9462 - val_loss: 0.1706 - val_accuracy: 0.9382\n",
            "Epoch 3769/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1168 - accuracy: 0.9446\n",
            "Epoch 3769: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1162 - accuracy: 0.9450 - val_loss: 0.1619 - val_accuracy: 0.9575\n",
            "Epoch 3770/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1131 - accuracy: 0.9486\n",
            "Epoch 3770: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1127 - accuracy: 0.9491 - val_loss: 0.1591 - val_accuracy: 0.9508\n",
            "Epoch 3771/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9512\n",
            "Epoch 3771: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1084 - accuracy: 0.9512 - val_loss: 0.1503 - val_accuracy: 0.9582\n",
            "Epoch 3772/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9526\n",
            "Epoch 3772: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1046 - accuracy: 0.9526 - val_loss: 0.1577 - val_accuracy: 0.9484\n",
            "Epoch 3773/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9493\n",
            "Epoch 3773: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1094 - accuracy: 0.9486 - val_loss: 0.1303 - val_accuracy: 0.9677\n",
            "Epoch 3774/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1091 - accuracy: 0.9481\n",
            "Epoch 3774: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1091 - accuracy: 0.9486 - val_loss: 0.1390 - val_accuracy: 0.9660\n",
            "Epoch 3775/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1092 - accuracy: 0.9497\n",
            "Epoch 3775: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1095 - accuracy: 0.9491 - val_loss: 0.1750 - val_accuracy: 0.9369\n",
            "Epoch 3776/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9491\n",
            "Epoch 3776: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 0.9489 - val_loss: 0.1405 - val_accuracy: 0.9597\n",
            "Epoch 3777/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9500\n",
            "Epoch 3777: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1078 - accuracy: 0.9493 - val_loss: 0.1151 - val_accuracy: 0.9733\n",
            "Epoch 3778/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1123 - accuracy: 0.9479\n",
            "Epoch 3778: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1122 - accuracy: 0.9477 - val_loss: 0.1600 - val_accuracy: 0.9503\n",
            "Epoch 3779/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9482\n",
            "Epoch 3779: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1109 - accuracy: 0.9480 - val_loss: 0.1226 - val_accuracy: 0.9792\n",
            "Epoch 3780/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9490\n",
            "Epoch 3780: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9492 - val_loss: 0.1501 - val_accuracy: 0.9607\n",
            "Epoch 3781/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9490\n",
            "Epoch 3781: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9491 - val_loss: 0.2079 - val_accuracy: 0.9195\n",
            "Epoch 3782/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1201 - accuracy: 0.9456\n",
            "Epoch 3782: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1227 - accuracy: 0.9442 - val_loss: 0.1437 - val_accuracy: 0.9659\n",
            "Epoch 3783/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1253 - accuracy: 0.9441\n",
            "Epoch 3783: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1254 - accuracy: 0.9438 - val_loss: 0.1699 - val_accuracy: 0.9445\n",
            "Epoch 3784/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9460\n",
            "Epoch 3784: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1155 - accuracy: 0.9461 - val_loss: 0.1529 - val_accuracy: 0.9607\n",
            "Epoch 3785/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1080 - accuracy: 0.9508\n",
            "Epoch 3785: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1079 - accuracy: 0.9507 - val_loss: 0.1112 - val_accuracy: 0.9805\n",
            "Epoch 3786/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1056 - accuracy: 0.9519\n",
            "Epoch 3786: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1070 - accuracy: 0.9510 - val_loss: 0.1528 - val_accuracy: 0.9536\n",
            "Epoch 3787/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1085 - accuracy: 0.9488\n",
            "Epoch 3787: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1099 - accuracy: 0.9485 - val_loss: 0.1816 - val_accuracy: 0.9274\n",
            "Epoch 3788/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9498\n",
            "Epoch 3788: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1104 - accuracy: 0.9500 - val_loss: 0.1278 - val_accuracy: 0.9694\n",
            "Epoch 3789/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1100 - accuracy: 0.9487\n",
            "Epoch 3789: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9483 - val_loss: 0.1200 - val_accuracy: 0.9738\n",
            "Epoch 3790/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1094 - accuracy: 0.9477\n",
            "Epoch 3790: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1098 - accuracy: 0.9475 - val_loss: 0.1530 - val_accuracy: 0.9558\n",
            "Epoch 3791/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9496\n",
            "Epoch 3791: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1082 - accuracy: 0.9494 - val_loss: 0.1635 - val_accuracy: 0.9419\n",
            "Epoch 3792/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1098 - accuracy: 0.9485\n",
            "Epoch 3792: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1105 - accuracy: 0.9481 - val_loss: 0.1554 - val_accuracy: 0.9556\n",
            "Epoch 3793/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1113 - accuracy: 0.9485\n",
            "Epoch 3793: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1120 - accuracy: 0.9480 - val_loss: 0.1764 - val_accuracy: 0.9332\n",
            "Epoch 3794/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1095 - accuracy: 0.9502\n",
            "Epoch 3794: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1104 - accuracy: 0.9497 - val_loss: 0.1170 - val_accuracy: 0.9777\n",
            "Epoch 3795/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1087 - accuracy: 0.9500\n",
            "Epoch 3795: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9500 - val_loss: 0.1158 - val_accuracy: 0.9718\n",
            "Epoch 3796/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1207 - accuracy: 0.9451\n",
            "Epoch 3796: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1225 - accuracy: 0.9444 - val_loss: 0.1888 - val_accuracy: 0.9393\n",
            "Epoch 3797/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1359 - accuracy: 0.9377\n",
            "Epoch 3797: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1347 - accuracy: 0.9386 - val_loss: 0.2227 - val_accuracy: 0.9185\n",
            "Epoch 3798/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9436\n",
            "Epoch 3798: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1207 - accuracy: 0.9441 - val_loss: 0.1383 - val_accuracy: 0.9595\n",
            "Epoch 3799/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1204 - accuracy: 0.9429\n",
            "Epoch 3799: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1200 - accuracy: 0.9434 - val_loss: 0.1311 - val_accuracy: 0.9744\n",
            "Epoch 3800/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9461\n",
            "Epoch 3800: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1187 - accuracy: 0.9461 - val_loss: 0.1229 - val_accuracy: 0.9753\n",
            "Epoch 3801/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9487\n",
            "Epoch 3801: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1103 - accuracy: 0.9487 - val_loss: 0.1363 - val_accuracy: 0.9657\n",
            "Epoch 3802/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1113 - accuracy: 0.9494\n",
            "Epoch 3802: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1135 - accuracy: 0.9476 - val_loss: 0.1353 - val_accuracy: 0.9681\n",
            "Epoch 3803/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1138 - accuracy: 0.9485\n",
            "Epoch 3803: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1139 - accuracy: 0.9479 - val_loss: 0.1305 - val_accuracy: 0.9677\n",
            "Epoch 3804/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1102 - accuracy: 0.9488\n",
            "Epoch 3804: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1104 - accuracy: 0.9483 - val_loss: 0.1628 - val_accuracy: 0.9475\n",
            "Epoch 3805/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1119 - accuracy: 0.9502\n",
            "Epoch 3805: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1112 - accuracy: 0.9498 - val_loss: 0.1328 - val_accuracy: 0.9679\n",
            "Epoch 3806/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9500\n",
            "Epoch 3806: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9493 - val_loss: 0.1139 - val_accuracy: 0.9751\n",
            "Epoch 3807/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9516\n",
            "Epoch 3807: loss did not improve from 0.10360\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1039 - accuracy: 0.9516 - val_loss: 0.1264 - val_accuracy: 0.9744\n",
            "Epoch 3808/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1018 - accuracy: 0.9534\n",
            "Epoch 3808: loss improved from 0.10360 to 0.10306, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 0.1031 - accuracy: 0.9524 - val_loss: 0.1314 - val_accuracy: 0.9671\n",
            "Epoch 3809/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9526\n",
            "Epoch 3809: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1043 - accuracy: 0.9526 - val_loss: 0.1458 - val_accuracy: 0.9551\n",
            "Epoch 3810/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9503\n",
            "Epoch 3810: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1071 - accuracy: 0.9503 - val_loss: 0.1323 - val_accuracy: 0.9670\n",
            "Epoch 3811/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1064 - accuracy: 0.9505\n",
            "Epoch 3811: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1068 - accuracy: 0.9501 - val_loss: 0.1200 - val_accuracy: 0.9742\n",
            "Epoch 3812/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1079 - accuracy: 0.9512\n",
            "Epoch 3812: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1086 - accuracy: 0.9503 - val_loss: 0.1452 - val_accuracy: 0.9640\n",
            "Epoch 3813/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9502\n",
            "Epoch 3813: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1067 - accuracy: 0.9502 - val_loss: 0.1197 - val_accuracy: 0.9766\n",
            "Epoch 3814/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9499\n",
            "Epoch 3814: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1062 - accuracy: 0.9499 - val_loss: 0.1156 - val_accuracy: 0.9764\n",
            "Epoch 3815/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1079 - accuracy: 0.9493\n",
            "Epoch 3815: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1093 - accuracy: 0.9483 - val_loss: 0.1272 - val_accuracy: 0.9683\n",
            "Epoch 3816/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1118 - accuracy: 0.9477\n",
            "Epoch 3816: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1123 - accuracy: 0.9474 - val_loss: 0.1411 - val_accuracy: 0.9605\n",
            "Epoch 3817/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9501\n",
            "Epoch 3817: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1068 - accuracy: 0.9501 - val_loss: 0.2024 - val_accuracy: 0.9131\n",
            "Epoch 3818/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1093 - accuracy: 0.9500\n",
            "Epoch 3818: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1109 - accuracy: 0.9489 - val_loss: 0.1813 - val_accuracy: 0.9345\n",
            "Epoch 3819/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9493\n",
            "Epoch 3819: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1069 - accuracy: 0.9494 - val_loss: 0.1324 - val_accuracy: 0.9670\n",
            "Epoch 3820/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9471\n",
            "Epoch 3820: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1118 - accuracy: 0.9469 - val_loss: 0.1410 - val_accuracy: 0.9633\n",
            "Epoch 3821/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9475\n",
            "Epoch 3821: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1191 - accuracy: 0.9460 - val_loss: 0.1438 - val_accuracy: 0.9579\n",
            "Epoch 3822/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1406 - accuracy: 0.9373\n",
            "Epoch 3822: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1406 - accuracy: 0.9371 - val_loss: 0.1441 - val_accuracy: 0.9556\n",
            "Epoch 3823/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1213 - accuracy: 0.9444\n",
            "Epoch 3823: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1216 - accuracy: 0.9447 - val_loss: 0.1241 - val_accuracy: 0.9716\n",
            "Epoch 3824/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1160 - accuracy: 0.9468\n",
            "Epoch 3824: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1154 - accuracy: 0.9471 - val_loss: 0.1288 - val_accuracy: 0.9749\n",
            "Epoch 3825/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1137 - accuracy: 0.9480\n",
            "Epoch 3825: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1138 - accuracy: 0.9480 - val_loss: 0.1290 - val_accuracy: 0.9701\n",
            "Epoch 3826/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1084 - accuracy: 0.9489\n",
            "Epoch 3826: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9479 - val_loss: 0.1447 - val_accuracy: 0.9556\n",
            "Epoch 3827/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1062 - accuracy: 0.9503\n",
            "Epoch 3827: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1073 - accuracy: 0.9494 - val_loss: 0.1175 - val_accuracy: 0.9757\n",
            "Epoch 3828/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9494\n",
            "Epoch 3828: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9494 - val_loss: 0.1384 - val_accuracy: 0.9629\n",
            "Epoch 3829/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1105 - accuracy: 0.9489\n",
            "Epoch 3829: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1111 - accuracy: 0.9485 - val_loss: 0.1317 - val_accuracy: 0.9677\n",
            "Epoch 3830/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9499\n",
            "Epoch 3830: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1087 - accuracy: 0.9499 - val_loss: 0.1873 - val_accuracy: 0.9376\n",
            "Epoch 3831/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1171 - accuracy: 0.9464\n",
            "Epoch 3831: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1180 - accuracy: 0.9459 - val_loss: 0.1192 - val_accuracy: 0.9762\n",
            "Epoch 3832/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1204 - accuracy: 0.9448\n",
            "Epoch 3832: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1203 - accuracy: 0.9443 - val_loss: 0.1739 - val_accuracy: 0.9438\n",
            "Epoch 3833/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9463\n",
            "Epoch 3833: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1164 - accuracy: 0.9463 - val_loss: 0.1219 - val_accuracy: 0.9744\n",
            "Epoch 3834/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9447\n",
            "Epoch 3834: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1190 - accuracy: 0.9447 - val_loss: 0.1670 - val_accuracy: 0.9542\n",
            "Epoch 3835/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9453\n",
            "Epoch 3835: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1204 - accuracy: 0.9453 - val_loss: 0.1690 - val_accuracy: 0.9547\n",
            "Epoch 3836/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9488\n",
            "Epoch 3836: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1117 - accuracy: 0.9488 - val_loss: 0.1524 - val_accuracy: 0.9564\n",
            "Epoch 3837/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1174 - accuracy: 0.9463\n",
            "Epoch 3837: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1178 - accuracy: 0.9461 - val_loss: 0.1727 - val_accuracy: 0.9493\n",
            "Epoch 3838/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9450\n",
            "Epoch 3838: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1182 - accuracy: 0.9449 - val_loss: 0.1118 - val_accuracy: 0.9822\n",
            "Epoch 3839/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9478\n",
            "Epoch 3839: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1135 - accuracy: 0.9478 - val_loss: 0.1582 - val_accuracy: 0.9501\n",
            "Epoch 3840/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1096 - accuracy: 0.9483\n",
            "Epoch 3840: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1103 - accuracy: 0.9476 - val_loss: 0.1397 - val_accuracy: 0.9588\n",
            "Epoch 3841/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9488\n",
            "Epoch 3841: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1083 - accuracy: 0.9488 - val_loss: 0.1415 - val_accuracy: 0.9634\n",
            "Epoch 3842/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9495\n",
            "Epoch 3842: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1073 - accuracy: 0.9494 - val_loss: 0.1622 - val_accuracy: 0.9642\n",
            "Epoch 3843/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9511\n",
            "Epoch 3843: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1059 - accuracy: 0.9508 - val_loss: 0.1104 - val_accuracy: 0.9762\n",
            "Epoch 3844/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9498\n",
            "Epoch 3844: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9501 - val_loss: 0.1185 - val_accuracy: 0.9768\n",
            "Epoch 3845/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9486\n",
            "Epoch 3845: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1130 - accuracy: 0.9481 - val_loss: 0.0962 - val_accuracy: 0.9872\n",
            "Epoch 3846/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1077 - accuracy: 0.9507\n",
            "Epoch 3846: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1091 - accuracy: 0.9499 - val_loss: 0.1150 - val_accuracy: 0.9779\n",
            "Epoch 3847/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1049 - accuracy: 0.9511\n",
            "Epoch 3847: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1085 - accuracy: 0.9497 - val_loss: 0.2037 - val_accuracy: 0.9250\n",
            "Epoch 3848/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1174 - accuracy: 0.9457\n",
            "Epoch 3848: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1172 - accuracy: 0.9456 - val_loss: 0.1336 - val_accuracy: 0.9694\n",
            "Epoch 3849/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9470\n",
            "Epoch 3849: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1129 - accuracy: 0.9470 - val_loss: 0.1399 - val_accuracy: 0.9631\n",
            "Epoch 3850/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9482\n",
            "Epoch 3850: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1116 - accuracy: 0.9478 - val_loss: 0.1610 - val_accuracy: 0.9523\n",
            "Epoch 3851/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1083 - accuracy: 0.9492\n",
            "Epoch 3851: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1087 - accuracy: 0.9495 - val_loss: 0.1385 - val_accuracy: 0.9627\n",
            "Epoch 3852/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9487\n",
            "Epoch 3852: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1106 - accuracy: 0.9487 - val_loss: 0.1259 - val_accuracy: 0.9675\n",
            "Epoch 3853/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9500\n",
            "Epoch 3853: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1058 - accuracy: 0.9500 - val_loss: 0.1524 - val_accuracy: 0.9508\n",
            "Epoch 3854/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1130 - accuracy: 0.9481\n",
            "Epoch 3854: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1130 - accuracy: 0.9485 - val_loss: 0.1480 - val_accuracy: 0.9558\n",
            "Epoch 3855/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1100 - accuracy: 0.9494\n",
            "Epoch 3855: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1112 - accuracy: 0.9483 - val_loss: 0.1253 - val_accuracy: 0.9722\n",
            "Epoch 3856/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1113 - accuracy: 0.9477\n",
            "Epoch 3856: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1130 - accuracy: 0.9469 - val_loss: 0.1601 - val_accuracy: 0.9530\n",
            "Epoch 3857/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9463\n",
            "Epoch 3857: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1174 - accuracy: 0.9460 - val_loss: 0.1358 - val_accuracy: 0.9692\n",
            "Epoch 3858/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1222 - accuracy: 0.9445\n",
            "Epoch 3858: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1231 - accuracy: 0.9440 - val_loss: 0.1274 - val_accuracy: 0.9709\n",
            "Epoch 3859/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9420\n",
            "Epoch 3859: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1272 - accuracy: 0.9423 - val_loss: 0.1630 - val_accuracy: 0.9534\n",
            "Epoch 3860/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1122 - accuracy: 0.9460\n",
            "Epoch 3860: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1109 - accuracy: 0.9468 - val_loss: 0.1225 - val_accuracy: 0.9718\n",
            "Epoch 3861/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9489\n",
            "Epoch 3861: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1094 - accuracy: 0.9489 - val_loss: 0.1523 - val_accuracy: 0.9547\n",
            "Epoch 3862/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1055 - accuracy: 0.9499\n",
            "Epoch 3862: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1058 - accuracy: 0.9499 - val_loss: 0.1412 - val_accuracy: 0.9625\n",
            "Epoch 3863/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9516\n",
            "Epoch 3863: loss did not improve from 0.10306\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1042 - accuracy: 0.9516 - val_loss: 0.1502 - val_accuracy: 0.9540\n",
            "Epoch 3864/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1008 - accuracy: 0.9529\n",
            "Epoch 3864: loss improved from 0.10306 to 0.10262, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 24ms/step - loss: 0.1026 - accuracy: 0.9520 - val_loss: 0.1241 - val_accuracy: 0.9709\n",
            "Epoch 3865/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9500\n",
            "Epoch 3865: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1057 - accuracy: 0.9503 - val_loss: 0.1412 - val_accuracy: 0.9640\n",
            "Epoch 3866/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1056 - accuracy: 0.9498\n",
            "Epoch 3866: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1060 - accuracy: 0.9495 - val_loss: 0.1413 - val_accuracy: 0.9633\n",
            "Epoch 3867/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1105 - accuracy: 0.9487\n",
            "Epoch 3867: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1114 - accuracy: 0.9483 - val_loss: 0.1357 - val_accuracy: 0.9638\n",
            "Epoch 3868/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1156 - accuracy: 0.9472\n",
            "Epoch 3868: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1159 - accuracy: 0.9466 - val_loss: 0.2108 - val_accuracy: 0.9302\n",
            "Epoch 3869/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9468\n",
            "Epoch 3869: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1173 - accuracy: 0.9466 - val_loss: 0.1282 - val_accuracy: 0.9738\n",
            "Epoch 3870/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1150 - accuracy: 0.9482\n",
            "Epoch 3870: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1148 - accuracy: 0.9480 - val_loss: 0.1383 - val_accuracy: 0.9592\n",
            "Epoch 3871/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1093 - accuracy: 0.9498\n",
            "Epoch 3871: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1095 - accuracy: 0.9497 - val_loss: 0.1346 - val_accuracy: 0.9677\n",
            "Epoch 3872/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1053 - accuracy: 0.9508\n",
            "Epoch 3872: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1069 - accuracy: 0.9499 - val_loss: 0.1535 - val_accuracy: 0.9512\n",
            "Epoch 3873/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1080 - accuracy: 0.9497\n",
            "Epoch 3873: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9492 - val_loss: 0.1729 - val_accuracy: 0.9438\n",
            "Epoch 3874/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9495\n",
            "Epoch 3874: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1084 - accuracy: 0.9494 - val_loss: 0.1361 - val_accuracy: 0.9653\n",
            "Epoch 3875/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1105 - accuracy: 0.9496\n",
            "Epoch 3875: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1102 - accuracy: 0.9495 - val_loss: 0.1510 - val_accuracy: 0.9553\n",
            "Epoch 3876/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9473\n",
            "Epoch 3876: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1128 - accuracy: 0.9477 - val_loss: 0.1204 - val_accuracy: 0.9787\n",
            "Epoch 3877/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9468\n",
            "Epoch 3877: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1170 - accuracy: 0.9461 - val_loss: 0.1338 - val_accuracy: 0.9657\n",
            "Epoch 3878/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9437\n",
            "Epoch 3878: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1256 - accuracy: 0.9436 - val_loss: 0.1609 - val_accuracy: 0.9560\n",
            "Epoch 3879/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1268 - accuracy: 0.9427\n",
            "Epoch 3879: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1264 - accuracy: 0.9430 - val_loss: 0.1762 - val_accuracy: 0.9569\n",
            "Epoch 3880/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9418\n",
            "Epoch 3880: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1293 - accuracy: 0.9417 - val_loss: 0.1821 - val_accuracy: 0.9410\n",
            "Epoch 3881/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9449\n",
            "Epoch 3881: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1205 - accuracy: 0.9449 - val_loss: 0.1262 - val_accuracy: 0.9729\n",
            "Epoch 3882/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9446\n",
            "Epoch 3882: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1181 - accuracy: 0.9446 - val_loss: 0.1027 - val_accuracy: 0.9820\n",
            "Epoch 3883/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1117 - accuracy: 0.9476\n",
            "Epoch 3883: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1118 - accuracy: 0.9477 - val_loss: 0.1427 - val_accuracy: 0.9679\n",
            "Epoch 3884/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1146 - accuracy: 0.9480\n",
            "Epoch 3884: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1155 - accuracy: 0.9480 - val_loss: 0.1661 - val_accuracy: 0.9473\n",
            "Epoch 3885/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1297 - accuracy: 0.9415\n",
            "Epoch 3885: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1295 - accuracy: 0.9414 - val_loss: 0.1949 - val_accuracy: 0.9332\n",
            "Epoch 3886/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9429\n",
            "Epoch 3886: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1233 - accuracy: 0.9427 - val_loss: 0.1345 - val_accuracy: 0.9655\n",
            "Epoch 3887/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9431\n",
            "Epoch 3887: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1243 - accuracy: 0.9431 - val_loss: 0.1250 - val_accuracy: 0.9707\n",
            "Epoch 3888/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1165 - accuracy: 0.9465\n",
            "Epoch 3888: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1164 - accuracy: 0.9461 - val_loss: 0.1451 - val_accuracy: 0.9586\n",
            "Epoch 3889/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1158 - accuracy: 0.9480\n",
            "Epoch 3889: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1166 - accuracy: 0.9473 - val_loss: 0.1579 - val_accuracy: 0.9536\n",
            "Epoch 3890/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9496\n",
            "Epoch 3890: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1095 - accuracy: 0.9496 - val_loss: 0.1281 - val_accuracy: 0.9646\n",
            "Epoch 3891/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9499\n",
            "Epoch 3891: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1062 - accuracy: 0.9499 - val_loss: 0.1284 - val_accuracy: 0.9720\n",
            "Epoch 3892/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1070 - accuracy: 0.9503\n",
            "Epoch 3892: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1075 - accuracy: 0.9501 - val_loss: 0.1047 - val_accuracy: 0.9807\n",
            "Epoch 3893/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9500\n",
            "Epoch 3893: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1058 - accuracy: 0.9500 - val_loss: 0.1372 - val_accuracy: 0.9659\n",
            "Epoch 3894/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9516\n",
            "Epoch 3894: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1036 - accuracy: 0.9516 - val_loss: 0.1247 - val_accuracy: 0.9653\n",
            "Epoch 3895/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1084 - accuracy: 0.9484\n",
            "Epoch 3895: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1081 - accuracy: 0.9489 - val_loss: 0.1460 - val_accuracy: 0.9607\n",
            "Epoch 3896/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1113 - accuracy: 0.9480\n",
            "Epoch 3896: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1126 - accuracy: 0.9472 - val_loss: 0.1187 - val_accuracy: 0.9772\n",
            "Epoch 3897/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9507\n",
            "Epoch 3897: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1099 - accuracy: 0.9490 - val_loss: 0.1513 - val_accuracy: 0.9601\n",
            "Epoch 3898/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1087 - accuracy: 0.9499\n",
            "Epoch 3898: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1090 - accuracy: 0.9496 - val_loss: 0.1613 - val_accuracy: 0.9475\n",
            "Epoch 3899/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1065 - accuracy: 0.9492\n",
            "Epoch 3899: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1071 - accuracy: 0.9491 - val_loss: 0.1413 - val_accuracy: 0.9657\n",
            "Epoch 3900/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1066 - accuracy: 0.9523\n",
            "Epoch 3900: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1079 - accuracy: 0.9515 - val_loss: 0.1413 - val_accuracy: 0.9629\n",
            "Epoch 3901/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1105 - accuracy: 0.9490\n",
            "Epoch 3901: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1102 - accuracy: 0.9487 - val_loss: 0.1309 - val_accuracy: 0.9662\n",
            "Epoch 3902/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9481\n",
            "Epoch 3902: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1109 - accuracy: 0.9482 - val_loss: 0.1379 - val_accuracy: 0.9649\n",
            "Epoch 3903/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9511\n",
            "Epoch 3903: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1071 - accuracy: 0.9510 - val_loss: 0.1207 - val_accuracy: 0.9748\n",
            "Epoch 3904/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9487\n",
            "Epoch 3904: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1086 - accuracy: 0.9488 - val_loss: 0.1324 - val_accuracy: 0.9659\n",
            "Epoch 3905/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9488\n",
            "Epoch 3905: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1097 - accuracy: 0.9488 - val_loss: 0.1311 - val_accuracy: 0.9607\n",
            "Epoch 3906/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9499\n",
            "Epoch 3906: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1079 - accuracy: 0.9499 - val_loss: 0.1560 - val_accuracy: 0.9503\n",
            "Epoch 3907/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1038 - accuracy: 0.9524\n",
            "Epoch 3907: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1042 - accuracy: 0.9523 - val_loss: 0.1414 - val_accuracy: 0.9594\n",
            "Epoch 3908/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9495\n",
            "Epoch 3908: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1086 - accuracy: 0.9495 - val_loss: 0.1828 - val_accuracy: 0.9291\n",
            "Epoch 3909/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9457\n",
            "Epoch 3909: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1165 - accuracy: 0.9453 - val_loss: 0.1323 - val_accuracy: 0.9671\n",
            "Epoch 3910/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9481\n",
            "Epoch 3910: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1119 - accuracy: 0.9481 - val_loss: 0.1601 - val_accuracy: 0.9449\n",
            "Epoch 3911/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9478\n",
            "Epoch 3911: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1110 - accuracy: 0.9477 - val_loss: 0.1331 - val_accuracy: 0.9666\n",
            "Epoch 3912/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9506\n",
            "Epoch 3912: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1059 - accuracy: 0.9506 - val_loss: 0.1069 - val_accuracy: 0.9818\n",
            "Epoch 3913/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9488\n",
            "Epoch 3913: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1108 - accuracy: 0.9488 - val_loss: 0.1487 - val_accuracy: 0.9581\n",
            "Epoch 3914/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9485\n",
            "Epoch 3914: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1114 - accuracy: 0.9485 - val_loss: 0.1400 - val_accuracy: 0.9634\n",
            "Epoch 3915/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1116 - accuracy: 0.9478\n",
            "Epoch 3915: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1124 - accuracy: 0.9471 - val_loss: 0.1213 - val_accuracy: 0.9727\n",
            "Epoch 3916/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1076 - accuracy: 0.9491\n",
            "Epoch 3916: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1069 - accuracy: 0.9494 - val_loss: 0.1141 - val_accuracy: 0.9749\n",
            "Epoch 3917/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1082 - accuracy: 0.9485\n",
            "Epoch 3917: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1083 - accuracy: 0.9485 - val_loss: 0.1386 - val_accuracy: 0.9573\n",
            "Epoch 3918/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9477\n",
            "Epoch 3918: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1126 - accuracy: 0.9477 - val_loss: 0.1561 - val_accuracy: 0.9536\n",
            "Epoch 3919/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9464\n",
            "Epoch 3919: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1140 - accuracy: 0.9464 - val_loss: 0.1343 - val_accuracy: 0.9686\n",
            "Epoch 3920/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1126 - accuracy: 0.9477\n",
            "Epoch 3920: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1133 - accuracy: 0.9474 - val_loss: 0.1419 - val_accuracy: 0.9612\n",
            "Epoch 3921/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9478\n",
            "Epoch 3921: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1114 - accuracy: 0.9478 - val_loss: 0.1192 - val_accuracy: 0.9681\n",
            "Epoch 3922/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9429\n",
            "Epoch 3922: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 0.9429 - val_loss: 0.2395 - val_accuracy: 0.9185\n",
            "Epoch 3923/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9449\n",
            "Epoch 3923: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1201 - accuracy: 0.9451 - val_loss: 0.1298 - val_accuracy: 0.9647\n",
            "Epoch 3924/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1165 - accuracy: 0.9472\n",
            "Epoch 3924: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1176 - accuracy: 0.9466 - val_loss: 0.1671 - val_accuracy: 0.9449\n",
            "Epoch 3925/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9455\n",
            "Epoch 3925: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1171 - accuracy: 0.9455 - val_loss: 0.1575 - val_accuracy: 0.9540\n",
            "Epoch 3926/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9488\n",
            "Epoch 3926: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1137 - accuracy: 0.9488 - val_loss: 0.1039 - val_accuracy: 0.9796\n",
            "Epoch 3927/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1173 - accuracy: 0.9455\n",
            "Epoch 3927: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1198 - accuracy: 0.9439 - val_loss: 0.1230 - val_accuracy: 0.9696\n",
            "Epoch 3928/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9442\n",
            "Epoch 3928: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1241 - accuracy: 0.9439 - val_loss: 0.1334 - val_accuracy: 0.9662\n",
            "Epoch 3929/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1187 - accuracy: 0.9462\n",
            "Epoch 3929: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1187 - accuracy: 0.9460 - val_loss: 0.1420 - val_accuracy: 0.9638\n",
            "Epoch 3930/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9488\n",
            "Epoch 3930: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1089 - accuracy: 0.9488 - val_loss: 0.1420 - val_accuracy: 0.9608\n",
            "Epoch 3931/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9481\n",
            "Epoch 3931: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1130 - accuracy: 0.9481 - val_loss: 0.1596 - val_accuracy: 0.9430\n",
            "Epoch 3932/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1169 - accuracy: 0.9454\n",
            "Epoch 3932: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1170 - accuracy: 0.9453 - val_loss: 0.1351 - val_accuracy: 0.9673\n",
            "Epoch 3933/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1126 - accuracy: 0.9472\n",
            "Epoch 3933: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1128 - accuracy: 0.9473 - val_loss: 0.1325 - val_accuracy: 0.9631\n",
            "Epoch 3934/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1114 - accuracy: 0.9482\n",
            "Epoch 3934: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1114 - accuracy: 0.9480 - val_loss: 0.1361 - val_accuracy: 0.9607\n",
            "Epoch 3935/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9459\n",
            "Epoch 3935: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1187 - accuracy: 0.9459 - val_loss: 0.1061 - val_accuracy: 0.9840\n",
            "Epoch 3936/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9491\n",
            "Epoch 3936: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9495 - val_loss: 0.1286 - val_accuracy: 0.9716\n",
            "Epoch 3937/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9515\n",
            "Epoch 3937: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1039 - accuracy: 0.9515 - val_loss: 0.1443 - val_accuracy: 0.9608\n",
            "Epoch 3938/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9500\n",
            "Epoch 3938: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1077 - accuracy: 0.9500 - val_loss: 0.1338 - val_accuracy: 0.9651\n",
            "Epoch 3939/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9493\n",
            "Epoch 3939: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9493 - val_loss: 0.1341 - val_accuracy: 0.9629\n",
            "Epoch 3940/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9511\n",
            "Epoch 3940: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9511 - val_loss: 0.1388 - val_accuracy: 0.9612\n",
            "Epoch 3941/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9502\n",
            "Epoch 3941: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1073 - accuracy: 0.9502 - val_loss: 0.1069 - val_accuracy: 0.9855\n",
            "Epoch 3942/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9509\n",
            "Epoch 3942: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9508 - val_loss: 0.1365 - val_accuracy: 0.9649\n",
            "Epoch 3943/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1081 - accuracy: 0.9502\n",
            "Epoch 3943: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1083 - accuracy: 0.9501 - val_loss: 0.1436 - val_accuracy: 0.9649\n",
            "Epoch 3944/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9479\n",
            "Epoch 3944: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1102 - accuracy: 0.9478 - val_loss: 0.1276 - val_accuracy: 0.9701\n",
            "Epoch 3945/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1123 - accuracy: 0.9481\n",
            "Epoch 3945: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1126 - accuracy: 0.9477 - val_loss: 0.1011 - val_accuracy: 0.9824\n",
            "Epoch 3946/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9504\n",
            "Epoch 3946: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1085 - accuracy: 0.9498 - val_loss: 0.1478 - val_accuracy: 0.9527\n",
            "Epoch 3947/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9500\n",
            "Epoch 3947: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1067 - accuracy: 0.9498 - val_loss: 0.1113 - val_accuracy: 0.9807\n",
            "Epoch 3948/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1110 - accuracy: 0.9486\n",
            "Epoch 3948: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1121 - accuracy: 0.9482 - val_loss: 0.1460 - val_accuracy: 0.9638\n",
            "Epoch 3949/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9484\n",
            "Epoch 3949: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1155 - accuracy: 0.9482 - val_loss: 0.1207 - val_accuracy: 0.9798\n",
            "Epoch 3950/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9425\n",
            "Epoch 3950: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1301 - accuracy: 0.9426 - val_loss: 0.1384 - val_accuracy: 0.9673\n",
            "Epoch 3951/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1319 - accuracy: 0.9407\n",
            "Epoch 3951: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1330 - accuracy: 0.9401 - val_loss: 0.1668 - val_accuracy: 0.9538\n",
            "Epoch 3952/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1243 - accuracy: 0.9442\n",
            "Epoch 3952: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1245 - accuracy: 0.9433 - val_loss: 0.1353 - val_accuracy: 0.9673\n",
            "Epoch 3953/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1151 - accuracy: 0.9481\n",
            "Epoch 3953: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1158 - accuracy: 0.9477 - val_loss: 0.1324 - val_accuracy: 0.9694\n",
            "Epoch 3954/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1146 - accuracy: 0.9497\n",
            "Epoch 3954: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1148 - accuracy: 0.9491 - val_loss: 0.2094 - val_accuracy: 0.9252\n",
            "Epoch 3955/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9467\n",
            "Epoch 3955: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1124 - accuracy: 0.9467 - val_loss: 0.1276 - val_accuracy: 0.9729\n",
            "Epoch 3956/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9497\n",
            "Epoch 3956: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1078 - accuracy: 0.9496 - val_loss: 0.1270 - val_accuracy: 0.9718\n",
            "Epoch 3957/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1055 - accuracy: 0.9520\n",
            "Epoch 3957: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1076 - accuracy: 0.9505 - val_loss: 0.1713 - val_accuracy: 0.9389\n",
            "Epoch 3958/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9501\n",
            "Epoch 3958: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1092 - accuracy: 0.9501 - val_loss: 0.1506 - val_accuracy: 0.9636\n",
            "Epoch 3959/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9472\n",
            "Epoch 3959: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1121 - accuracy: 0.9472 - val_loss: 0.1463 - val_accuracy: 0.9558\n",
            "Epoch 3960/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9481\n",
            "Epoch 3960: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1110 - accuracy: 0.9481 - val_loss: 0.1607 - val_accuracy: 0.9495\n",
            "Epoch 3961/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9488\n",
            "Epoch 3961: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1084 - accuracy: 0.9488 - val_loss: 0.1942 - val_accuracy: 0.9360\n",
            "Epoch 3962/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9490\n",
            "Epoch 3962: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9489 - val_loss: 0.1474 - val_accuracy: 0.9536\n",
            "Epoch 3963/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1033 - accuracy: 0.9509\n",
            "Epoch 3963: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1033 - accuracy: 0.9513 - val_loss: 0.1362 - val_accuracy: 0.9647\n",
            "Epoch 3964/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9503\n",
            "Epoch 3964: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1048 - accuracy: 0.9503 - val_loss: 0.1152 - val_accuracy: 0.9783\n",
            "Epoch 3965/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9488\n",
            "Epoch 3965: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9489 - val_loss: 0.1344 - val_accuracy: 0.9633\n",
            "Epoch 3966/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1111 - accuracy: 0.9480\n",
            "Epoch 3966: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9480 - val_loss: 0.1443 - val_accuracy: 0.9540\n",
            "Epoch 3967/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9498\n",
            "Epoch 3967: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1056 - accuracy: 0.9500 - val_loss: 0.1458 - val_accuracy: 0.9545\n",
            "Epoch 3968/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9495\n",
            "Epoch 3968: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1102 - accuracy: 0.9495 - val_loss: 0.1377 - val_accuracy: 0.9673\n",
            "Epoch 3969/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9479\n",
            "Epoch 3969: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1123 - accuracy: 0.9479 - val_loss: 0.1502 - val_accuracy: 0.9506\n",
            "Epoch 3970/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9468\n",
            "Epoch 3970: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1112 - accuracy: 0.9473 - val_loss: 0.1314 - val_accuracy: 0.9712\n",
            "Epoch 3971/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1114 - accuracy: 0.9481\n",
            "Epoch 3971: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1109 - accuracy: 0.9484 - val_loss: 0.1370 - val_accuracy: 0.9607\n",
            "Epoch 3972/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9483\n",
            "Epoch 3972: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1110 - accuracy: 0.9479 - val_loss: 0.1354 - val_accuracy: 0.9722\n",
            "Epoch 3973/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1113 - accuracy: 0.9480\n",
            "Epoch 3973: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1120 - accuracy: 0.9474 - val_loss: 0.1793 - val_accuracy: 0.9388\n",
            "Epoch 3974/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9489\n",
            "Epoch 3974: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1109 - accuracy: 0.9485 - val_loss: 0.1596 - val_accuracy: 0.9573\n",
            "Epoch 3975/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9501\n",
            "Epoch 3975: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1077 - accuracy: 0.9500 - val_loss: 0.1464 - val_accuracy: 0.9517\n",
            "Epoch 3976/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1125 - accuracy: 0.9479\n",
            "Epoch 3976: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1126 - accuracy: 0.9478 - val_loss: 0.1784 - val_accuracy: 0.9295\n",
            "Epoch 3977/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9483\n",
            "Epoch 3977: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9482 - val_loss: 0.2404 - val_accuracy: 0.8937\n",
            "Epoch 3978/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1171 - accuracy: 0.9461\n",
            "Epoch 3978: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1183 - accuracy: 0.9455 - val_loss: 0.1852 - val_accuracy: 0.9330\n",
            "Epoch 3979/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1191 - accuracy: 0.9451\n",
            "Epoch 3979: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1199 - accuracy: 0.9450 - val_loss: 0.2121 - val_accuracy: 0.9133\n",
            "Epoch 3980/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9491\n",
            "Epoch 3980: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1117 - accuracy: 0.9491 - val_loss: 0.1604 - val_accuracy: 0.9491\n",
            "Epoch 3981/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1146 - accuracy: 0.9470\n",
            "Epoch 3981: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1134 - accuracy: 0.9474 - val_loss: 0.1428 - val_accuracy: 0.9616\n",
            "Epoch 3982/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1089 - accuracy: 0.9501\n",
            "Epoch 3982: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1097 - accuracy: 0.9495 - val_loss: 0.1381 - val_accuracy: 0.9642\n",
            "Epoch 3983/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9477\n",
            "Epoch 3983: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1126 - accuracy: 0.9477 - val_loss: 0.1139 - val_accuracy: 0.9794\n",
            "Epoch 3984/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9453\n",
            "Epoch 3984: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1195 - accuracy: 0.9453 - val_loss: 0.1480 - val_accuracy: 0.9620\n",
            "Epoch 3985/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9465\n",
            "Epoch 3985: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1152 - accuracy: 0.9463 - val_loss: 0.1806 - val_accuracy: 0.9395\n",
            "Epoch 3986/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9468\n",
            "Epoch 3986: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1132 - accuracy: 0.9468 - val_loss: 0.1077 - val_accuracy: 0.9783\n",
            "Epoch 3987/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1142 - accuracy: 0.9468\n",
            "Epoch 3987: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1164 - accuracy: 0.9464 - val_loss: 0.1315 - val_accuracy: 0.9686\n",
            "Epoch 3988/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9430\n",
            "Epoch 3988: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1235 - accuracy: 0.9430 - val_loss: 0.1218 - val_accuracy: 0.9744\n",
            "Epoch 3989/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1185 - accuracy: 0.9459\n",
            "Epoch 3989: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1198 - accuracy: 0.9453 - val_loss: 0.1729 - val_accuracy: 0.9495\n",
            "Epoch 3990/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9450\n",
            "Epoch 3990: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1167 - accuracy: 0.9450 - val_loss: 0.1952 - val_accuracy: 0.9369\n",
            "Epoch 3991/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1134 - accuracy: 0.9479\n",
            "Epoch 3991: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1143 - accuracy: 0.9472 - val_loss: 0.1219 - val_accuracy: 0.9772\n",
            "Epoch 3992/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1162 - accuracy: 0.9457\n",
            "Epoch 3992: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1155 - accuracy: 0.9464 - val_loss: 0.1306 - val_accuracy: 0.9620\n",
            "Epoch 3993/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1161 - accuracy: 0.9482\n",
            "Epoch 3993: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1173 - accuracy: 0.9474 - val_loss: 0.1880 - val_accuracy: 0.9362\n",
            "Epoch 3994/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9394\n",
            "Epoch 3994: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1377 - accuracy: 0.9394 - val_loss: 0.1912 - val_accuracy: 0.9363\n",
            "Epoch 3995/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1222 - accuracy: 0.9449\n",
            "Epoch 3995: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1224 - accuracy: 0.9450 - val_loss: 0.1309 - val_accuracy: 0.9740\n",
            "Epoch 3996/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1140 - accuracy: 0.9468\n",
            "Epoch 3996: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1151 - accuracy: 0.9465 - val_loss: 0.1407 - val_accuracy: 0.9634\n",
            "Epoch 3997/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9478\n",
            "Epoch 3997: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1140 - accuracy: 0.9476 - val_loss: 0.1224 - val_accuracy: 0.9779\n",
            "Epoch 3998/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9507\n",
            "Epoch 3998: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1063 - accuracy: 0.9508 - val_loss: 0.1502 - val_accuracy: 0.9512\n",
            "Epoch 3999/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9511\n",
            "Epoch 3999: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1042 - accuracy: 0.9511 - val_loss: 0.1326 - val_accuracy: 0.9651\n",
            "Epoch 4000/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9504\n",
            "Epoch 4000: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9504 - val_loss: 0.1393 - val_accuracy: 0.9640\n",
            "Epoch 4001/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1099 - accuracy: 0.9492\n",
            "Epoch 4001: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1099 - accuracy: 0.9491 - val_loss: 0.1311 - val_accuracy: 0.9714\n",
            "Epoch 4002/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1061 - accuracy: 0.9505\n",
            "Epoch 4002: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1059 - accuracy: 0.9504 - val_loss: 0.1398 - val_accuracy: 0.9625\n",
            "Epoch 4003/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9517\n",
            "Epoch 4003: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1054 - accuracy: 0.9514 - val_loss: 0.1836 - val_accuracy: 0.9291\n",
            "Epoch 4004/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1092 - accuracy: 0.9488\n",
            "Epoch 4004: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1102 - accuracy: 0.9483 - val_loss: 0.1151 - val_accuracy: 0.9772\n",
            "Epoch 4005/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9461\n",
            "Epoch 4005: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1145 - accuracy: 0.9461 - val_loss: 0.1879 - val_accuracy: 0.9272\n",
            "Epoch 4006/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1156 - accuracy: 0.9479\n",
            "Epoch 4006: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1177 - accuracy: 0.9467 - val_loss: 0.1143 - val_accuracy: 0.9779\n",
            "Epoch 4007/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1141 - accuracy: 0.9458\n",
            "Epoch 4007: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1147 - accuracy: 0.9453 - val_loss: 0.1761 - val_accuracy: 0.9388\n",
            "Epoch 4008/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9504\n",
            "Epoch 4008: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9502 - val_loss: 0.1819 - val_accuracy: 0.9384\n",
            "Epoch 4009/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1059 - accuracy: 0.9498\n",
            "Epoch 4009: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1060 - accuracy: 0.9498 - val_loss: 0.1195 - val_accuracy: 0.9684\n",
            "Epoch 4010/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1067 - accuracy: 0.9491\n",
            "Epoch 4010: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1075 - accuracy: 0.9487 - val_loss: 0.1391 - val_accuracy: 0.9530\n",
            "Epoch 4011/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1075 - accuracy: 0.9502\n",
            "Epoch 4011: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1077 - accuracy: 0.9504 - val_loss: 0.1131 - val_accuracy: 0.9772\n",
            "Epoch 4012/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1066 - accuracy: 0.9503\n",
            "Epoch 4012: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1069 - accuracy: 0.9501 - val_loss: 0.1221 - val_accuracy: 0.9748\n",
            "Epoch 4013/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1065 - accuracy: 0.9513\n",
            "Epoch 4013: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1076 - accuracy: 0.9503 - val_loss: 0.1324 - val_accuracy: 0.9677\n",
            "Epoch 4014/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9494\n",
            "Epoch 4014: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1108 - accuracy: 0.9494 - val_loss: 0.1183 - val_accuracy: 0.9738\n",
            "Epoch 4015/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9480\n",
            "Epoch 4015: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1122 - accuracy: 0.9479 - val_loss: 0.1199 - val_accuracy: 0.9722\n",
            "Epoch 4016/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9481\n",
            "Epoch 4016: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1070 - accuracy: 0.9481 - val_loss: 0.1672 - val_accuracy: 0.9471\n",
            "Epoch 4017/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9497\n",
            "Epoch 4017: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1070 - accuracy: 0.9497 - val_loss: 0.1414 - val_accuracy: 0.9623\n",
            "Epoch 4018/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9494\n",
            "Epoch 4018: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9494 - val_loss: 0.1355 - val_accuracy: 0.9610\n",
            "Epoch 4019/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9486\n",
            "Epoch 4019: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1095 - accuracy: 0.9472 - val_loss: 0.1400 - val_accuracy: 0.9749\n",
            "Epoch 4020/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1263 - accuracy: 0.9432\n",
            "Epoch 4020: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1259 - accuracy: 0.9434 - val_loss: 0.2137 - val_accuracy: 0.9230\n",
            "Epoch 4021/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1277 - accuracy: 0.9435\n",
            "Epoch 4021: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1278 - accuracy: 0.9432 - val_loss: 0.1249 - val_accuracy: 0.9703\n",
            "Epoch 4022/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9463\n",
            "Epoch 4022: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1176 - accuracy: 0.9463 - val_loss: 0.1645 - val_accuracy: 0.9501\n",
            "Epoch 4023/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1158 - accuracy: 0.9465\n",
            "Epoch 4023: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1155 - accuracy: 0.9466 - val_loss: 0.1738 - val_accuracy: 0.9464\n",
            "Epoch 4024/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1127 - accuracy: 0.9473\n",
            "Epoch 4024: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1138 - accuracy: 0.9465 - val_loss: 0.1381 - val_accuracy: 0.9575\n",
            "Epoch 4025/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9478\n",
            "Epoch 4025: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1140 - accuracy: 0.9478 - val_loss: 0.1658 - val_accuracy: 0.9414\n",
            "Epoch 4026/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9491\n",
            "Epoch 4026: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1096 - accuracy: 0.9491 - val_loss: 0.1269 - val_accuracy: 0.9701\n",
            "Epoch 4027/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1062 - accuracy: 0.9498\n",
            "Epoch 4027: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1063 - accuracy: 0.9498 - val_loss: 0.0993 - val_accuracy: 0.9835\n",
            "Epoch 4028/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1077 - accuracy: 0.9508\n",
            "Epoch 4028: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1080 - accuracy: 0.9505 - val_loss: 0.1398 - val_accuracy: 0.9664\n",
            "Epoch 4029/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1065 - accuracy: 0.9516\n",
            "Epoch 4029: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9517 - val_loss: 0.1080 - val_accuracy: 0.9814\n",
            "Epoch 4030/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9490\n",
            "Epoch 4030: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1100 - accuracy: 0.9492 - val_loss: 0.1159 - val_accuracy: 0.9807\n",
            "Epoch 4031/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1049 - accuracy: 0.9520\n",
            "Epoch 4031: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9510 - val_loss: 0.1341 - val_accuracy: 0.9647\n",
            "Epoch 4032/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9495\n",
            "Epoch 4032: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1084 - accuracy: 0.9497 - val_loss: 0.1442 - val_accuracy: 0.9597\n",
            "Epoch 4033/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1116 - accuracy: 0.9486\n",
            "Epoch 4033: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1126 - accuracy: 0.9482 - val_loss: 0.1179 - val_accuracy: 0.9798\n",
            "Epoch 4034/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9466\n",
            "Epoch 4034: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1145 - accuracy: 0.9466 - val_loss: 0.1413 - val_accuracy: 0.9610\n",
            "Epoch 4035/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9510\n",
            "Epoch 4035: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1077 - accuracy: 0.9508 - val_loss: 0.1380 - val_accuracy: 0.9616\n",
            "Epoch 4036/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9505\n",
            "Epoch 4036: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1091 - accuracy: 0.9504 - val_loss: 0.1383 - val_accuracy: 0.9675\n",
            "Epoch 4037/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1056 - accuracy: 0.9509\n",
            "Epoch 4037: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1057 - accuracy: 0.9507 - val_loss: 0.1229 - val_accuracy: 0.9714\n",
            "Epoch 4038/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9513\n",
            "Epoch 4038: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9511 - val_loss: 0.1477 - val_accuracy: 0.9536\n",
            "Epoch 4039/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9504\n",
            "Epoch 4039: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9497 - val_loss: 0.1782 - val_accuracy: 0.9378\n",
            "Epoch 4040/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1080 - accuracy: 0.9491\n",
            "Epoch 4040: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1080 - accuracy: 0.9489 - val_loss: 0.1562 - val_accuracy: 0.9501\n",
            "Epoch 4041/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9480\n",
            "Epoch 4041: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1085 - accuracy: 0.9479 - val_loss: 0.1459 - val_accuracy: 0.9516\n",
            "Epoch 4042/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9460\n",
            "Epoch 4042: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1154 - accuracy: 0.9460 - val_loss: 0.2231 - val_accuracy: 0.9213\n",
            "Epoch 4043/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1188 - accuracy: 0.9444\n",
            "Epoch 4043: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1192 - accuracy: 0.9439 - val_loss: 0.1277 - val_accuracy: 0.9684\n",
            "Epoch 4044/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9423\n",
            "Epoch 4044: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1285 - accuracy: 0.9423 - val_loss: 0.1553 - val_accuracy: 0.9542\n",
            "Epoch 4045/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1276 - accuracy: 0.9419\n",
            "Epoch 4045: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1291 - accuracy: 0.9412 - val_loss: 0.1150 - val_accuracy: 0.9820\n",
            "Epoch 4046/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9460\n",
            "Epoch 4046: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1186 - accuracy: 0.9460 - val_loss: 0.1494 - val_accuracy: 0.9575\n",
            "Epoch 4047/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9442\n",
            "Epoch 4047: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1194 - accuracy: 0.9444 - val_loss: 0.1091 - val_accuracy: 0.9816\n",
            "Epoch 4048/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1193 - accuracy: 0.9452\n",
            "Epoch 4048: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1190 - accuracy: 0.9453 - val_loss: 0.1708 - val_accuracy: 0.9542\n",
            "Epoch 4049/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1202 - accuracy: 0.9443\n",
            "Epoch 4049: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1196 - accuracy: 0.9439 - val_loss: 0.1756 - val_accuracy: 0.9475\n",
            "Epoch 4050/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1088 - accuracy: 0.9497\n",
            "Epoch 4050: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1096 - accuracy: 0.9495 - val_loss: 0.1350 - val_accuracy: 0.9694\n",
            "Epoch 4051/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1036 - accuracy: 0.9520\n",
            "Epoch 4051: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1048 - accuracy: 0.9514 - val_loss: 0.1432 - val_accuracy: 0.9555\n",
            "Epoch 4052/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1043 - accuracy: 0.9509\n",
            "Epoch 4052: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1056 - accuracy: 0.9503 - val_loss: 0.1197 - val_accuracy: 0.9805\n",
            "Epoch 4053/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1072 - accuracy: 0.9502\n",
            "Epoch 4053: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9497 - val_loss: 0.1277 - val_accuracy: 0.9753\n",
            "Epoch 4054/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1099 - accuracy: 0.9504\n",
            "Epoch 4054: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1115 - accuracy: 0.9491 - val_loss: 0.1178 - val_accuracy: 0.9744\n",
            "Epoch 4055/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1098 - accuracy: 0.9491\n",
            "Epoch 4055: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1096 - accuracy: 0.9492 - val_loss: 0.1626 - val_accuracy: 0.9451\n",
            "Epoch 4056/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9466\n",
            "Epoch 4056: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1145 - accuracy: 0.9466 - val_loss: 0.1833 - val_accuracy: 0.9389\n",
            "Epoch 4057/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1111 - accuracy: 0.9473\n",
            "Epoch 4057: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1123 - accuracy: 0.9466 - val_loss: 0.1113 - val_accuracy: 0.9820\n",
            "Epoch 4058/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1117 - accuracy: 0.9499\n",
            "Epoch 4058: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1128 - accuracy: 0.9490 - val_loss: 0.1261 - val_accuracy: 0.9738\n",
            "Epoch 4059/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1118 - accuracy: 0.9490\n",
            "Epoch 4059: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1129 - accuracy: 0.9486 - val_loss: 0.1442 - val_accuracy: 0.9634\n",
            "Epoch 4060/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9466\n",
            "Epoch 4060: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1170 - accuracy: 0.9465 - val_loss: 0.1848 - val_accuracy: 0.9371\n",
            "Epoch 4061/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9448\n",
            "Epoch 4061: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1191 - accuracy: 0.9439 - val_loss: 0.1927 - val_accuracy: 0.9293\n",
            "Epoch 4062/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1075 - accuracy: 0.9497\n",
            "Epoch 4062: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9498 - val_loss: 0.1051 - val_accuracy: 0.9814\n",
            "Epoch 4063/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9494\n",
            "Epoch 4063: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1090 - accuracy: 0.9494 - val_loss: 0.1487 - val_accuracy: 0.9536\n",
            "Epoch 4064/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1104 - accuracy: 0.9494\n",
            "Epoch 4064: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1104 - accuracy: 0.9498 - val_loss: 0.1280 - val_accuracy: 0.9666\n",
            "Epoch 4065/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1102 - accuracy: 0.9492\n",
            "Epoch 4065: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1108 - accuracy: 0.9487 - val_loss: 0.1324 - val_accuracy: 0.9703\n",
            "Epoch 4066/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9484\n",
            "Epoch 4066: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1116 - accuracy: 0.9484 - val_loss: 0.1398 - val_accuracy: 0.9601\n",
            "Epoch 4067/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1074 - accuracy: 0.9495\n",
            "Epoch 4067: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1081 - accuracy: 0.9494 - val_loss: 0.1199 - val_accuracy: 0.9746\n",
            "Epoch 4068/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1087 - accuracy: 0.9490\n",
            "Epoch 4068: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9485 - val_loss: 0.1401 - val_accuracy: 0.9607\n",
            "Epoch 4069/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9461\n",
            "Epoch 4069: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1129 - accuracy: 0.9465 - val_loss: 0.1286 - val_accuracy: 0.9623\n",
            "Epoch 4070/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9481\n",
            "Epoch 4070: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1135 - accuracy: 0.9481 - val_loss: 0.1358 - val_accuracy: 0.9701\n",
            "Epoch 4071/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9478\n",
            "Epoch 4071: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1139 - accuracy: 0.9478 - val_loss: 0.1287 - val_accuracy: 0.9696\n",
            "Epoch 4072/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1160 - accuracy: 0.9466\n",
            "Epoch 4072: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1155 - accuracy: 0.9467 - val_loss: 0.1817 - val_accuracy: 0.9391\n",
            "Epoch 4073/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1098 - accuracy: 0.9488\n",
            "Epoch 4073: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1093 - accuracy: 0.9487 - val_loss: 0.1182 - val_accuracy: 0.9738\n",
            "Epoch 4074/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1108 - accuracy: 0.9490\n",
            "Epoch 4074: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1116 - accuracy: 0.9486 - val_loss: 0.1280 - val_accuracy: 0.9671\n",
            "Epoch 4075/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9483\n",
            "Epoch 4075: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1122 - accuracy: 0.9483 - val_loss: 0.1278 - val_accuracy: 0.9668\n",
            "Epoch 4076/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1169 - accuracy: 0.9460\n",
            "Epoch 4076: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1169 - accuracy: 0.9457 - val_loss: 0.1171 - val_accuracy: 0.9761\n",
            "Epoch 4077/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1117 - accuracy: 0.9484\n",
            "Epoch 4077: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1125 - accuracy: 0.9480 - val_loss: 0.1180 - val_accuracy: 0.9768\n",
            "Epoch 4078/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1118 - accuracy: 0.9473\n",
            "Epoch 4078: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1126 - accuracy: 0.9468 - val_loss: 0.1192 - val_accuracy: 0.9775\n",
            "Epoch 4079/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9469\n",
            "Epoch 4079: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1147 - accuracy: 0.9466 - val_loss: 0.1096 - val_accuracy: 0.9826\n",
            "Epoch 4080/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1124 - accuracy: 0.9496\n",
            "Epoch 4080: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1130 - accuracy: 0.9489 - val_loss: 0.1519 - val_accuracy: 0.9530\n",
            "Epoch 4081/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9471\n",
            "Epoch 4081: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1131 - accuracy: 0.9472 - val_loss: 0.2140 - val_accuracy: 0.9336\n",
            "Epoch 4082/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9454\n",
            "Epoch 4082: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1179 - accuracy: 0.9454 - val_loss: 0.1740 - val_accuracy: 0.9414\n",
            "Epoch 4083/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1185 - accuracy: 0.9449\n",
            "Epoch 4083: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1185 - accuracy: 0.9449 - val_loss: 0.1713 - val_accuracy: 0.9393\n",
            "Epoch 4084/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1135 - accuracy: 0.9459\n",
            "Epoch 4084: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1133 - accuracy: 0.9464 - val_loss: 0.1386 - val_accuracy: 0.9668\n",
            "Epoch 4085/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1099 - accuracy: 0.9478\n",
            "Epoch 4085: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1083 - accuracy: 0.9487 - val_loss: 0.1397 - val_accuracy: 0.9631\n",
            "Epoch 4086/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9490\n",
            "Epoch 4086: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9490 - val_loss: 0.1168 - val_accuracy: 0.9742\n",
            "Epoch 4087/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9482\n",
            "Epoch 4087: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1131 - accuracy: 0.9482 - val_loss: 0.1509 - val_accuracy: 0.9553\n",
            "Epoch 4088/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1081 - accuracy: 0.9491\n",
            "Epoch 4088: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9489 - val_loss: 0.1332 - val_accuracy: 0.9662\n",
            "Epoch 4089/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9510\n",
            "Epoch 4089: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9504 - val_loss: 0.1510 - val_accuracy: 0.9545\n",
            "Epoch 4090/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1043 - accuracy: 0.9522\n",
            "Epoch 4090: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9514 - val_loss: 0.1001 - val_accuracy: 0.9833\n",
            "Epoch 4091/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9476\n",
            "Epoch 4091: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1110 - accuracy: 0.9472 - val_loss: 0.1119 - val_accuracy: 0.9781\n",
            "Epoch 4092/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1067 - accuracy: 0.9492\n",
            "Epoch 4092: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1078 - accuracy: 0.9490 - val_loss: 0.1545 - val_accuracy: 0.9560\n",
            "Epoch 4093/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9479\n",
            "Epoch 4093: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 0.9479 - val_loss: 0.1561 - val_accuracy: 0.9562\n",
            "Epoch 4094/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1165 - accuracy: 0.9456\n",
            "Epoch 4094: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1158 - accuracy: 0.9463 - val_loss: 0.1358 - val_accuracy: 0.9655\n",
            "Epoch 4095/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1141 - accuracy: 0.9457\n",
            "Epoch 4095: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1134 - accuracy: 0.9463 - val_loss: 0.1538 - val_accuracy: 0.9525\n",
            "Epoch 4096/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1110 - accuracy: 0.9482\n",
            "Epoch 4096: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1101 - accuracy: 0.9489 - val_loss: 0.1424 - val_accuracy: 0.9597\n",
            "Epoch 4097/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9487\n",
            "Epoch 4097: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1102 - accuracy: 0.9488 - val_loss: 0.1057 - val_accuracy: 0.9870\n",
            "Epoch 4098/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9493\n",
            "Epoch 4098: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9493 - val_loss: 0.1677 - val_accuracy: 0.9540\n",
            "Epoch 4099/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1087 - accuracy: 0.9495\n",
            "Epoch 4099: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1093 - accuracy: 0.9490 - val_loss: 0.1125 - val_accuracy: 0.9770\n",
            "Epoch 4100/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9479\n",
            "Epoch 4100: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9481 - val_loss: 0.1263 - val_accuracy: 0.9694\n",
            "Epoch 4101/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1108 - accuracy: 0.9477\n",
            "Epoch 4101: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9479 - val_loss: 0.1266 - val_accuracy: 0.9675\n",
            "Epoch 4102/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9484\n",
            "Epoch 4102: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1081 - accuracy: 0.9481 - val_loss: 0.1364 - val_accuracy: 0.9614\n",
            "Epoch 4103/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1128 - accuracy: 0.9485\n",
            "Epoch 4103: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1130 - accuracy: 0.9483 - val_loss: 0.1558 - val_accuracy: 0.9530\n",
            "Epoch 4104/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1175 - accuracy: 0.9448\n",
            "Epoch 4104: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1177 - accuracy: 0.9449 - val_loss: 0.1894 - val_accuracy: 0.9306\n",
            "Epoch 4105/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9456\n",
            "Epoch 4105: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1154 - accuracy: 0.9456 - val_loss: 0.1901 - val_accuracy: 0.9358\n",
            "Epoch 4106/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1122 - accuracy: 0.9481\n",
            "Epoch 4106: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1143 - accuracy: 0.9478 - val_loss: 0.1228 - val_accuracy: 0.9690\n",
            "Epoch 4107/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1206 - accuracy: 0.9441\n",
            "Epoch 4107: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1203 - accuracy: 0.9444 - val_loss: 0.1358 - val_accuracy: 0.9671\n",
            "Epoch 4108/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9446\n",
            "Epoch 4108: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1193 - accuracy: 0.9446 - val_loss: 0.1205 - val_accuracy: 0.9720\n",
            "Epoch 4109/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9451\n",
            "Epoch 4109: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1183 - accuracy: 0.9452 - val_loss: 0.1264 - val_accuracy: 0.9671\n",
            "Epoch 4110/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9433\n",
            "Epoch 4110: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1250 - accuracy: 0.9433 - val_loss: 0.1031 - val_accuracy: 0.9857\n",
            "Epoch 4111/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1188 - accuracy: 0.9453\n",
            "Epoch 4111: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1183 - accuracy: 0.9457 - val_loss: 0.1179 - val_accuracy: 0.9731\n",
            "Epoch 4112/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1139 - accuracy: 0.9465\n",
            "Epoch 4112: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9463 - val_loss: 0.1204 - val_accuracy: 0.9738\n",
            "Epoch 4113/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9499\n",
            "Epoch 4113: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1074 - accuracy: 0.9501 - val_loss: 0.1389 - val_accuracy: 0.9642\n",
            "Epoch 4114/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1039 - accuracy: 0.9506\n",
            "Epoch 4114: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1050 - accuracy: 0.9501 - val_loss: 0.1496 - val_accuracy: 0.9517\n",
            "Epoch 4115/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9473\n",
            "Epoch 4115: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1123 - accuracy: 0.9476 - val_loss: 0.1811 - val_accuracy: 0.9462\n",
            "Epoch 4116/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1101 - accuracy: 0.9477\n",
            "Epoch 4116: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1111 - accuracy: 0.9478 - val_loss: 0.1192 - val_accuracy: 0.9731\n",
            "Epoch 4117/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1114 - accuracy: 0.9482\n",
            "Epoch 4117: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1118 - accuracy: 0.9480 - val_loss: 0.1600 - val_accuracy: 0.9491\n",
            "Epoch 4118/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9466\n",
            "Epoch 4118: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1174 - accuracy: 0.9460 - val_loss: 0.1481 - val_accuracy: 0.9562\n",
            "Epoch 4119/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1185 - accuracy: 0.9442\n",
            "Epoch 4119: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1203 - accuracy: 0.9436 - val_loss: 0.1401 - val_accuracy: 0.9631\n",
            "Epoch 4120/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9465\n",
            "Epoch 4120: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1131 - accuracy: 0.9465 - val_loss: 0.1496 - val_accuracy: 0.9590\n",
            "Epoch 4121/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9503\n",
            "Epoch 4121: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1067 - accuracy: 0.9503 - val_loss: 0.1271 - val_accuracy: 0.9688\n",
            "Epoch 4122/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9505\n",
            "Epoch 4122: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1069 - accuracy: 0.9502 - val_loss: 0.1200 - val_accuracy: 0.9744\n",
            "Epoch 4123/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1143 - accuracy: 0.9477\n",
            "Epoch 4123: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1149 - accuracy: 0.9476 - val_loss: 0.1044 - val_accuracy: 0.9863\n",
            "Epoch 4124/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1142 - accuracy: 0.9474\n",
            "Epoch 4124: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9467 - val_loss: 0.1573 - val_accuracy: 0.9529\n",
            "Epoch 4125/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1185 - accuracy: 0.9459\n",
            "Epoch 4125: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1203 - accuracy: 0.9445 - val_loss: 0.1016 - val_accuracy: 0.9861\n",
            "Epoch 4126/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1117 - accuracy: 0.9477\n",
            "Epoch 4126: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1138 - accuracy: 0.9464 - val_loss: 0.0961 - val_accuracy: 0.9865\n",
            "Epoch 4127/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1078 - accuracy: 0.9493\n",
            "Epoch 4127: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1087 - accuracy: 0.9487 - val_loss: 0.1655 - val_accuracy: 0.9425\n",
            "Epoch 4128/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1054 - accuracy: 0.9515\n",
            "Epoch 4128: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1068 - accuracy: 0.9512 - val_loss: 0.1431 - val_accuracy: 0.9521\n",
            "Epoch 4129/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9437\n",
            "Epoch 4129: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1212 - accuracy: 0.9437 - val_loss: 0.2085 - val_accuracy: 0.9165\n",
            "Epoch 4130/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9420\n",
            "Epoch 4130: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1268 - accuracy: 0.9420 - val_loss: 0.1743 - val_accuracy: 0.9477\n",
            "Epoch 4131/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9447\n",
            "Epoch 4131: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1214 - accuracy: 0.9439 - val_loss: 0.1443 - val_accuracy: 0.9629\n",
            "Epoch 4132/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1132 - accuracy: 0.9481\n",
            "Epoch 4132: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1133 - accuracy: 0.9479 - val_loss: 0.1630 - val_accuracy: 0.9501\n",
            "Epoch 4133/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9503\n",
            "Epoch 4133: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1088 - accuracy: 0.9503 - val_loss: 0.1302 - val_accuracy: 0.9697\n",
            "Epoch 4134/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9499\n",
            "Epoch 4134: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9499 - val_loss: 0.1294 - val_accuracy: 0.9681\n",
            "Epoch 4135/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1063 - accuracy: 0.9501\n",
            "Epoch 4135: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1070 - accuracy: 0.9503 - val_loss: 0.1555 - val_accuracy: 0.9575\n",
            "Epoch 4136/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1076 - accuracy: 0.9503\n",
            "Epoch 4136: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1081 - accuracy: 0.9499 - val_loss: 0.1263 - val_accuracy: 0.9690\n",
            "Epoch 4137/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9492\n",
            "Epoch 4137: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1058 - accuracy: 0.9489 - val_loss: 0.1377 - val_accuracy: 0.9651\n",
            "Epoch 4138/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1040 - accuracy: 0.9514\n",
            "Epoch 4138: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1044 - accuracy: 0.9511 - val_loss: 0.1648 - val_accuracy: 0.9480\n",
            "Epoch 4139/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9497\n",
            "Epoch 4139: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1080 - accuracy: 0.9497 - val_loss: 0.1211 - val_accuracy: 0.9729\n",
            "Epoch 4140/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1082 - accuracy: 0.9493\n",
            "Epoch 4140: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1093 - accuracy: 0.9488 - val_loss: 0.1349 - val_accuracy: 0.9640\n",
            "Epoch 4141/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9478\n",
            "Epoch 4141: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1134 - accuracy: 0.9478 - val_loss: 0.1639 - val_accuracy: 0.9523\n",
            "Epoch 4142/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1308 - accuracy: 0.9413\n",
            "Epoch 4142: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1303 - accuracy: 0.9418 - val_loss: 0.1141 - val_accuracy: 0.9811\n",
            "Epoch 4143/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1307 - accuracy: 0.9413\n",
            "Epoch 4143: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1309 - accuracy: 0.9411 - val_loss: 0.1867 - val_accuracy: 0.9395\n",
            "Epoch 4144/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9450\n",
            "Epoch 4144: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1195 - accuracy: 0.9454 - val_loss: 0.1325 - val_accuracy: 0.9696\n",
            "Epoch 4145/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9492\n",
            "Epoch 4145: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1107 - accuracy: 0.9492 - val_loss: 0.1222 - val_accuracy: 0.9746\n",
            "Epoch 4146/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1092 - accuracy: 0.9494\n",
            "Epoch 4146: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1094 - accuracy: 0.9490 - val_loss: 0.1238 - val_accuracy: 0.9705\n",
            "Epoch 4147/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9517\n",
            "Epoch 4147: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1055 - accuracy: 0.9517 - val_loss: 0.1372 - val_accuracy: 0.9620\n",
            "Epoch 4148/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9485\n",
            "Epoch 4148: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1100 - accuracy: 0.9485 - val_loss: 0.1598 - val_accuracy: 0.9478\n",
            "Epoch 4149/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1032 - accuracy: 0.9518\n",
            "Epoch 4149: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1041 - accuracy: 0.9514 - val_loss: 0.1459 - val_accuracy: 0.9575\n",
            "Epoch 4150/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1070 - accuracy: 0.9493\n",
            "Epoch 4150: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1080 - accuracy: 0.9486 - val_loss: 0.1624 - val_accuracy: 0.9519\n",
            "Epoch 4151/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9479\n",
            "Epoch 4151: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1106 - accuracy: 0.9474 - val_loss: 0.1659 - val_accuracy: 0.9425\n",
            "Epoch 4152/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9497\n",
            "Epoch 4152: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9497 - val_loss: 0.1248 - val_accuracy: 0.9660\n",
            "Epoch 4153/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1070 - accuracy: 0.9508\n",
            "Epoch 4153: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1092 - accuracy: 0.9497 - val_loss: 0.1577 - val_accuracy: 0.9510\n",
            "Epoch 4154/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1103 - accuracy: 0.9499\n",
            "Epoch 4154: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1110 - accuracy: 0.9487 - val_loss: 0.1804 - val_accuracy: 0.9382\n",
            "Epoch 4155/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9474\n",
            "Epoch 4155: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1134 - accuracy: 0.9473 - val_loss: 0.1598 - val_accuracy: 0.9508\n",
            "Epoch 4156/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1113 - accuracy: 0.9489\n",
            "Epoch 4156: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1113 - accuracy: 0.9490 - val_loss: 0.1336 - val_accuracy: 0.9703\n",
            "Epoch 4157/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9508\n",
            "Epoch 4157: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1046 - accuracy: 0.9505 - val_loss: 0.1271 - val_accuracy: 0.9738\n",
            "Epoch 4158/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9511\n",
            "Epoch 4158: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1045 - accuracy: 0.9511 - val_loss: 0.1769 - val_accuracy: 0.9452\n",
            "Epoch 4159/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1064 - accuracy: 0.9505\n",
            "Epoch 4159: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9503 - val_loss: 0.1372 - val_accuracy: 0.9662\n",
            "Epoch 4160/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9512\n",
            "Epoch 4160: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9501 - val_loss: 0.1013 - val_accuracy: 0.9801\n",
            "Epoch 4161/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1048 - accuracy: 0.9505\n",
            "Epoch 4161: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9506 - val_loss: 0.1219 - val_accuracy: 0.9762\n",
            "Epoch 4162/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1064 - accuracy: 0.9501\n",
            "Epoch 4162: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1063 - accuracy: 0.9502 - val_loss: 0.1149 - val_accuracy: 0.9751\n",
            "Epoch 4163/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9483\n",
            "Epoch 4163: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1107 - accuracy: 0.9482 - val_loss: 0.1280 - val_accuracy: 0.9660\n",
            "Epoch 4164/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1157 - accuracy: 0.9470\n",
            "Epoch 4164: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1156 - accuracy: 0.9466 - val_loss: 0.1547 - val_accuracy: 0.9486\n",
            "Epoch 4165/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1161 - accuracy: 0.9462\n",
            "Epoch 4165: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1160 - accuracy: 0.9458 - val_loss: 0.1942 - val_accuracy: 0.9269\n",
            "Epoch 4166/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9465\n",
            "Epoch 4166: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1146 - accuracy: 0.9465 - val_loss: 0.1622 - val_accuracy: 0.9469\n",
            "Epoch 4167/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1178 - accuracy: 0.9453\n",
            "Epoch 4167: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1193 - accuracy: 0.9450 - val_loss: 0.1841 - val_accuracy: 0.9428\n",
            "Epoch 4168/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9448\n",
            "Epoch 4168: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1182 - accuracy: 0.9450 - val_loss: 0.1523 - val_accuracy: 0.9601\n",
            "Epoch 4169/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1129 - accuracy: 0.9482\n",
            "Epoch 4169: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1139 - accuracy: 0.9478 - val_loss: 0.1218 - val_accuracy: 0.9800\n",
            "Epoch 4170/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1086 - accuracy: 0.9498\n",
            "Epoch 4170: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9502 - val_loss: 0.1211 - val_accuracy: 0.9768\n",
            "Epoch 4171/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1051 - accuracy: 0.9520\n",
            "Epoch 4171: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1073 - accuracy: 0.9509 - val_loss: 0.1505 - val_accuracy: 0.9504\n",
            "Epoch 4172/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9518\n",
            "Epoch 4172: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9518 - val_loss: 0.1227 - val_accuracy: 0.9733\n",
            "Epoch 4173/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9465\n",
            "Epoch 4173: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1193 - accuracy: 0.9461 - val_loss: 0.1721 - val_accuracy: 0.9412\n",
            "Epoch 4174/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1164 - accuracy: 0.9455\n",
            "Epoch 4174: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1170 - accuracy: 0.9454 - val_loss: 0.1138 - val_accuracy: 0.9748\n",
            "Epoch 4175/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1137 - accuracy: 0.9484\n",
            "Epoch 4175: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1151 - accuracy: 0.9471 - val_loss: 0.1386 - val_accuracy: 0.9655\n",
            "Epoch 4176/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9501\n",
            "Epoch 4176: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1076 - accuracy: 0.9501 - val_loss: 0.1301 - val_accuracy: 0.9735\n",
            "Epoch 4177/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1050 - accuracy: 0.9500\n",
            "Epoch 4177: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1055 - accuracy: 0.9495 - val_loss: 0.1866 - val_accuracy: 0.9319\n",
            "Epoch 4178/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9483\n",
            "Epoch 4178: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1104 - accuracy: 0.9483 - val_loss: 0.1600 - val_accuracy: 0.9497\n",
            "Epoch 4179/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1081 - accuracy: 0.9489\n",
            "Epoch 4179: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1100 - accuracy: 0.9476 - val_loss: 0.1382 - val_accuracy: 0.9659\n",
            "Epoch 4180/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9467\n",
            "Epoch 4180: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1154 - accuracy: 0.9467 - val_loss: 0.1340 - val_accuracy: 0.9659\n",
            "Epoch 4181/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1142 - accuracy: 0.9468\n",
            "Epoch 4181: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1138 - accuracy: 0.9473 - val_loss: 0.1232 - val_accuracy: 0.9699\n",
            "Epoch 4182/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1130 - accuracy: 0.9476\n",
            "Epoch 4182: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1155 - accuracy: 0.9467 - val_loss: 0.1670 - val_accuracy: 0.9443\n",
            "Epoch 4183/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1187 - accuracy: 0.9457\n",
            "Epoch 4183: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1188 - accuracy: 0.9455 - val_loss: 0.2200 - val_accuracy: 0.9204\n",
            "Epoch 4184/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1179 - accuracy: 0.9465\n",
            "Epoch 4184: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1195 - accuracy: 0.9457 - val_loss: 0.1605 - val_accuracy: 0.9588\n",
            "Epoch 4185/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9427\n",
            "Epoch 4185: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1272 - accuracy: 0.9427 - val_loss: 0.1217 - val_accuracy: 0.9690\n",
            "Epoch 4186/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9467\n",
            "Epoch 4186: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1171 - accuracy: 0.9461 - val_loss: 0.1702 - val_accuracy: 0.9382\n",
            "Epoch 4187/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9483\n",
            "Epoch 4187: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9478 - val_loss: 0.1507 - val_accuracy: 0.9577\n",
            "Epoch 4188/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1089 - accuracy: 0.9510\n",
            "Epoch 4188: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9504 - val_loss: 0.1390 - val_accuracy: 0.9692\n",
            "Epoch 4189/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1072 - accuracy: 0.9501\n",
            "Epoch 4189: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1072 - accuracy: 0.9501 - val_loss: 0.1486 - val_accuracy: 0.9545\n",
            "Epoch 4190/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1054 - accuracy: 0.9496\n",
            "Epoch 4190: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1047 - accuracy: 0.9497 - val_loss: 0.1617 - val_accuracy: 0.9517\n",
            "Epoch 4191/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1067 - accuracy: 0.9501\n",
            "Epoch 4191: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1070 - accuracy: 0.9502 - val_loss: 0.1417 - val_accuracy: 0.9627\n",
            "Epoch 4192/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9472\n",
            "Epoch 4192: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1137 - accuracy: 0.9472 - val_loss: 0.1177 - val_accuracy: 0.9740\n",
            "Epoch 4193/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9485\n",
            "Epoch 4193: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1109 - accuracy: 0.9485 - val_loss: 0.1499 - val_accuracy: 0.9543\n",
            "Epoch 4194/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9500\n",
            "Epoch 4194: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9495 - val_loss: 0.1622 - val_accuracy: 0.9499\n",
            "Epoch 4195/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9510\n",
            "Epoch 4195: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9503 - val_loss: 0.1451 - val_accuracy: 0.9569\n",
            "Epoch 4196/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1091 - accuracy: 0.9508\n",
            "Epoch 4196: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1087 - accuracy: 0.9510 - val_loss: 0.1218 - val_accuracy: 0.9755\n",
            "Epoch 4197/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9502\n",
            "Epoch 4197: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9497 - val_loss: 0.1601 - val_accuracy: 0.9571\n",
            "Epoch 4198/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1107 - accuracy: 0.9481\n",
            "Epoch 4198: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1122 - accuracy: 0.9469 - val_loss: 0.1221 - val_accuracy: 0.9751\n",
            "Epoch 4199/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1088 - accuracy: 0.9493\n",
            "Epoch 4199: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1085 - accuracy: 0.9494 - val_loss: 0.1277 - val_accuracy: 0.9703\n",
            "Epoch 4200/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9504\n",
            "Epoch 4200: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1088 - accuracy: 0.9504 - val_loss: 0.2167 - val_accuracy: 0.9165\n",
            "Epoch 4201/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9486\n",
            "Epoch 4201: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1135 - accuracy: 0.9480 - val_loss: 0.1521 - val_accuracy: 0.9543\n",
            "Epoch 4202/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1099 - accuracy: 0.9488\n",
            "Epoch 4202: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1095 - accuracy: 0.9491 - val_loss: 0.1438 - val_accuracy: 0.9603\n",
            "Epoch 4203/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1129 - accuracy: 0.9486\n",
            "Epoch 4203: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1132 - accuracy: 0.9482 - val_loss: 0.1299 - val_accuracy: 0.9670\n",
            "Epoch 4204/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1134 - accuracy: 0.9458\n",
            "Epoch 4204: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1123 - accuracy: 0.9467 - val_loss: 0.1531 - val_accuracy: 0.9564\n",
            "Epoch 4205/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9492\n",
            "Epoch 4205: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1081 - accuracy: 0.9492 - val_loss: 0.1339 - val_accuracy: 0.9644\n",
            "Epoch 4206/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1064 - accuracy: 0.9493\n",
            "Epoch 4206: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1073 - accuracy: 0.9491 - val_loss: 0.1497 - val_accuracy: 0.9651\n",
            "Epoch 4207/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9489\n",
            "Epoch 4207: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1102 - accuracy: 0.9489 - val_loss: 0.1518 - val_accuracy: 0.9614\n",
            "Epoch 4208/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1135 - accuracy: 0.9478\n",
            "Epoch 4208: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1124 - accuracy: 0.9479 - val_loss: 0.1314 - val_accuracy: 0.9664\n",
            "Epoch 4209/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1114 - accuracy: 0.9480\n",
            "Epoch 4209: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1122 - accuracy: 0.9477 - val_loss: 0.1630 - val_accuracy: 0.9397\n",
            "Epoch 4210/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1137 - accuracy: 0.9475\n",
            "Epoch 4210: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1132 - accuracy: 0.9477 - val_loss: 0.1281 - val_accuracy: 0.9697\n",
            "Epoch 4211/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9483\n",
            "Epoch 4211: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1113 - accuracy: 0.9483 - val_loss: 0.1586 - val_accuracy: 0.9534\n",
            "Epoch 4212/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1109 - accuracy: 0.9483\n",
            "Epoch 4212: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1095 - accuracy: 0.9492 - val_loss: 0.1275 - val_accuracy: 0.9723\n",
            "Epoch 4213/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9494\n",
            "Epoch 4213: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1073 - accuracy: 0.9495 - val_loss: 0.1507 - val_accuracy: 0.9501\n",
            "Epoch 4214/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1059 - accuracy: 0.9506\n",
            "Epoch 4214: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1055 - accuracy: 0.9507 - val_loss: 0.1382 - val_accuracy: 0.9646\n",
            "Epoch 4215/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9488\n",
            "Epoch 4215: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1091 - accuracy: 0.9489 - val_loss: 0.1978 - val_accuracy: 0.9425\n",
            "Epoch 4216/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9480\n",
            "Epoch 4216: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1134 - accuracy: 0.9474 - val_loss: 0.1585 - val_accuracy: 0.9490\n",
            "Epoch 4217/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9485\n",
            "Epoch 4217: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1104 - accuracy: 0.9485 - val_loss: 0.1551 - val_accuracy: 0.9491\n",
            "Epoch 4218/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9505\n",
            "Epoch 4218: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1084 - accuracy: 0.9494 - val_loss: 0.1487 - val_accuracy: 0.9469\n",
            "Epoch 4219/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1124 - accuracy: 0.9484\n",
            "Epoch 4219: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1123 - accuracy: 0.9487 - val_loss: 0.1327 - val_accuracy: 0.9664\n",
            "Epoch 4220/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1188 - accuracy: 0.9446\n",
            "Epoch 4220: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1185 - accuracy: 0.9446 - val_loss: 0.1651 - val_accuracy: 0.9532\n",
            "Epoch 4221/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9400\n",
            "Epoch 4221: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1302 - accuracy: 0.9400 - val_loss: 0.1300 - val_accuracy: 0.9727\n",
            "Epoch 4222/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1174 - accuracy: 0.9474\n",
            "Epoch 4222: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1180 - accuracy: 0.9472 - val_loss: 0.1733 - val_accuracy: 0.9473\n",
            "Epoch 4223/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9469\n",
            "Epoch 4223: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1160 - accuracy: 0.9471 - val_loss: 0.1502 - val_accuracy: 0.9545\n",
            "Epoch 4224/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1151 - accuracy: 0.9480\n",
            "Epoch 4224: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1165 - accuracy: 0.9472 - val_loss: 0.1649 - val_accuracy: 0.9423\n",
            "Epoch 4225/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1094 - accuracy: 0.9498\n",
            "Epoch 4225: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1127 - accuracy: 0.9485 - val_loss: 0.1319 - val_accuracy: 0.9646\n",
            "Epoch 4226/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1122 - accuracy: 0.9481\n",
            "Epoch 4226: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1121 - accuracy: 0.9482 - val_loss: 0.1916 - val_accuracy: 0.9415\n",
            "Epoch 4227/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9471\n",
            "Epoch 4227: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1147 - accuracy: 0.9471 - val_loss: 0.1144 - val_accuracy: 0.9822\n",
            "Epoch 4228/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9487\n",
            "Epoch 4228: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1117 - accuracy: 0.9487 - val_loss: 0.1463 - val_accuracy: 0.9636\n",
            "Epoch 4229/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1167 - accuracy: 0.9455\n",
            "Epoch 4229: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1159 - accuracy: 0.9461 - val_loss: 0.1243 - val_accuracy: 0.9618\n",
            "Epoch 4230/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9463\n",
            "Epoch 4230: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1154 - accuracy: 0.9463 - val_loss: 0.1367 - val_accuracy: 0.9612\n",
            "Epoch 4231/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9492\n",
            "Epoch 4231: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1102 - accuracy: 0.9492 - val_loss: 0.1326 - val_accuracy: 0.9684\n",
            "Epoch 4232/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9502\n",
            "Epoch 4232: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1067 - accuracy: 0.9502 - val_loss: 0.1541 - val_accuracy: 0.9478\n",
            "Epoch 4233/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1029 - accuracy: 0.9508\n",
            "Epoch 4233: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1035 - accuracy: 0.9504 - val_loss: 0.1533 - val_accuracy: 0.9553\n",
            "Epoch 4234/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1042 - accuracy: 0.9524\n",
            "Epoch 4234: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1060 - accuracy: 0.9513 - val_loss: 0.1230 - val_accuracy: 0.9696\n",
            "Epoch 4235/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1067 - accuracy: 0.9503\n",
            "Epoch 4235: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1074 - accuracy: 0.9497 - val_loss: 0.1624 - val_accuracy: 0.9354\n",
            "Epoch 4236/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9496\n",
            "Epoch 4236: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1122 - accuracy: 0.9496 - val_loss: 0.1891 - val_accuracy: 0.9324\n",
            "Epoch 4237/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9461\n",
            "Epoch 4237: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1211 - accuracy: 0.9461 - val_loss: 0.1079 - val_accuracy: 0.9800\n",
            "Epoch 4238/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1118 - accuracy: 0.9483\n",
            "Epoch 4238: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1121 - accuracy: 0.9483 - val_loss: 0.1455 - val_accuracy: 0.9586\n",
            "Epoch 4239/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1131 - accuracy: 0.9494\n",
            "Epoch 4239: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1135 - accuracy: 0.9491 - val_loss: 0.1741 - val_accuracy: 0.9341\n",
            "Epoch 4240/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1096 - accuracy: 0.9488\n",
            "Epoch 4240: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9482 - val_loss: 0.1251 - val_accuracy: 0.9781\n",
            "Epoch 4241/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9479\n",
            "Epoch 4241: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1160 - accuracy: 0.9478 - val_loss: 0.1384 - val_accuracy: 0.9584\n",
            "Epoch 4242/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9479\n",
            "Epoch 4242: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1153 - accuracy: 0.9473 - val_loss: 0.1317 - val_accuracy: 0.9631\n",
            "Epoch 4243/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9471\n",
            "Epoch 4243: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1103 - accuracy: 0.9480 - val_loss: 0.1096 - val_accuracy: 0.9790\n",
            "Epoch 4244/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1113 - accuracy: 0.9472\n",
            "Epoch 4244: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1112 - accuracy: 0.9473 - val_loss: 0.1449 - val_accuracy: 0.9597\n",
            "Epoch 4245/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9468\n",
            "Epoch 4245: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1099 - accuracy: 0.9468 - val_loss: 0.1684 - val_accuracy: 0.9427\n",
            "Epoch 4246/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1126 - accuracy: 0.9471\n",
            "Epoch 4246: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1135 - accuracy: 0.9467 - val_loss: 0.1211 - val_accuracy: 0.9738\n",
            "Epoch 4247/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9491\n",
            "Epoch 4247: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1087 - accuracy: 0.9486 - val_loss: 0.1137 - val_accuracy: 0.9746\n",
            "Epoch 4248/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9492\n",
            "Epoch 4248: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1085 - accuracy: 0.9494 - val_loss: 0.1419 - val_accuracy: 0.9616\n",
            "Epoch 4249/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1081 - accuracy: 0.9488\n",
            "Epoch 4249: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1083 - accuracy: 0.9489 - val_loss: 0.1225 - val_accuracy: 0.9725\n",
            "Epoch 4250/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9502\n",
            "Epoch 4250: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1061 - accuracy: 0.9502 - val_loss: 0.1494 - val_accuracy: 0.9579\n",
            "Epoch 4251/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9504\n",
            "Epoch 4251: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9504 - val_loss: 0.1615 - val_accuracy: 0.9477\n",
            "Epoch 4252/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1131 - accuracy: 0.9471\n",
            "Epoch 4252: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1134 - accuracy: 0.9473 - val_loss: 0.1180 - val_accuracy: 0.9699\n",
            "Epoch 4253/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1129 - accuracy: 0.9475\n",
            "Epoch 4253: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1142 - accuracy: 0.9467 - val_loss: 0.1369 - val_accuracy: 0.9625\n",
            "Epoch 4254/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1118 - accuracy: 0.9486\n",
            "Epoch 4254: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1119 - accuracy: 0.9486 - val_loss: 0.1322 - val_accuracy: 0.9705\n",
            "Epoch 4255/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9481\n",
            "Epoch 4255: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1104 - accuracy: 0.9486 - val_loss: 0.1456 - val_accuracy: 0.9529\n",
            "Epoch 4256/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9477\n",
            "Epoch 4256: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1136 - accuracy: 0.9472 - val_loss: 0.1447 - val_accuracy: 0.9629\n",
            "Epoch 4257/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9422\n",
            "Epoch 4257: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1268 - accuracy: 0.9422 - val_loss: 0.1803 - val_accuracy: 0.9434\n",
            "Epoch 4258/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1343 - accuracy: 0.9379\n",
            "Epoch 4258: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1333 - accuracy: 0.9389 - val_loss: 0.1742 - val_accuracy: 0.9415\n",
            "Epoch 4259/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1279 - accuracy: 0.9417\n",
            "Epoch 4259: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1282 - accuracy: 0.9414 - val_loss: 0.1911 - val_accuracy: 0.9399\n",
            "Epoch 4260/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9458\n",
            "Epoch 4260: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1181 - accuracy: 0.9458 - val_loss: 0.1268 - val_accuracy: 0.9740\n",
            "Epoch 4261/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9476\n",
            "Epoch 4261: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1114 - accuracy: 0.9478 - val_loss: 0.1508 - val_accuracy: 0.9545\n",
            "Epoch 4262/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1029 - accuracy: 0.9526\n",
            "Epoch 4262: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1051 - accuracy: 0.9510 - val_loss: 0.1821 - val_accuracy: 0.9323\n",
            "Epoch 4263/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1047 - accuracy: 0.9511\n",
            "Epoch 4263: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1064 - accuracy: 0.9496 - val_loss: 0.1220 - val_accuracy: 0.9768\n",
            "Epoch 4264/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9480\n",
            "Epoch 4264: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1130 - accuracy: 0.9480 - val_loss: 0.1318 - val_accuracy: 0.9692\n",
            "Epoch 4265/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1077 - accuracy: 0.9493\n",
            "Epoch 4265: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1076 - accuracy: 0.9500 - val_loss: 0.1176 - val_accuracy: 0.9735\n",
            "Epoch 4266/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1075 - accuracy: 0.9506\n",
            "Epoch 4266: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1081 - accuracy: 0.9500 - val_loss: 0.1254 - val_accuracy: 0.9712\n",
            "Epoch 4267/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9488\n",
            "Epoch 4267: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1108 - accuracy: 0.9488 - val_loss: 0.1591 - val_accuracy: 0.9553\n",
            "Epoch 4268/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1178 - accuracy: 0.9456\n",
            "Epoch 4268: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1177 - accuracy: 0.9454 - val_loss: 0.1586 - val_accuracy: 0.9482\n",
            "Epoch 4269/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1110 - accuracy: 0.9479\n",
            "Epoch 4269: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1108 - accuracy: 0.9479 - val_loss: 0.1287 - val_accuracy: 0.9707\n",
            "Epoch 4270/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9513\n",
            "Epoch 4270: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1046 - accuracy: 0.9510 - val_loss: 0.1165 - val_accuracy: 0.9779\n",
            "Epoch 4271/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1044 - accuracy: 0.9502\n",
            "Epoch 4271: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1055 - accuracy: 0.9499 - val_loss: 0.1040 - val_accuracy: 0.9859\n",
            "Epoch 4272/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9450\n",
            "Epoch 4272: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1201 - accuracy: 0.9450 - val_loss: 0.1844 - val_accuracy: 0.9352\n",
            "Epoch 4273/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1125 - accuracy: 0.9476\n",
            "Epoch 4273: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1128 - accuracy: 0.9471 - val_loss: 0.1292 - val_accuracy: 0.9748\n",
            "Epoch 4274/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9491\n",
            "Epoch 4274: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1069 - accuracy: 0.9491 - val_loss: 0.1790 - val_accuracy: 0.9321\n",
            "Epoch 4275/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1092 - accuracy: 0.9487\n",
            "Epoch 4275: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1098 - accuracy: 0.9487 - val_loss: 0.1449 - val_accuracy: 0.9579\n",
            "Epoch 4276/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1149 - accuracy: 0.9491\n",
            "Epoch 4276: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1149 - accuracy: 0.9488 - val_loss: 0.1466 - val_accuracy: 0.9560\n",
            "Epoch 4277/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1061 - accuracy: 0.9501\n",
            "Epoch 4277: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1057 - accuracy: 0.9507 - val_loss: 0.1334 - val_accuracy: 0.9625\n",
            "Epoch 4278/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9521\n",
            "Epoch 4278: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1063 - accuracy: 0.9519 - val_loss: 0.1371 - val_accuracy: 0.9581\n",
            "Epoch 4279/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9500\n",
            "Epoch 4279: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1039 - accuracy: 0.9500 - val_loss: 0.1338 - val_accuracy: 0.9573\n",
            "Epoch 4280/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1074 - accuracy: 0.9490\n",
            "Epoch 4280: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1087 - accuracy: 0.9486 - val_loss: 0.1192 - val_accuracy: 0.9707\n",
            "Epoch 4281/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1145 - accuracy: 0.9476\n",
            "Epoch 4281: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1163 - accuracy: 0.9465 - val_loss: 0.1501 - val_accuracy: 0.9493\n",
            "Epoch 4282/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1126 - accuracy: 0.9477\n",
            "Epoch 4282: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1125 - accuracy: 0.9474 - val_loss: 0.1254 - val_accuracy: 0.9649\n",
            "Epoch 4283/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1060 - accuracy: 0.9506\n",
            "Epoch 4283: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1062 - accuracy: 0.9508 - val_loss: 0.1630 - val_accuracy: 0.9465\n",
            "Epoch 4284/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9491\n",
            "Epoch 4284: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1073 - accuracy: 0.9491 - val_loss: 0.1576 - val_accuracy: 0.9460\n",
            "Epoch 4285/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9486\n",
            "Epoch 4285: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1103 - accuracy: 0.9486 - val_loss: 0.1181 - val_accuracy: 0.9738\n",
            "Epoch 4286/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9498\n",
            "Epoch 4286: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1085 - accuracy: 0.9497 - val_loss: 0.1261 - val_accuracy: 0.9688\n",
            "Epoch 4287/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9481\n",
            "Epoch 4287: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1101 - accuracy: 0.9481 - val_loss: 0.1164 - val_accuracy: 0.9751\n",
            "Epoch 4288/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9507\n",
            "Epoch 4288: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1067 - accuracy: 0.9507 - val_loss: 0.1168 - val_accuracy: 0.9790\n",
            "Epoch 4289/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9495\n",
            "Epoch 4289: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1097 - accuracy: 0.9493 - val_loss: 0.1518 - val_accuracy: 0.9534\n",
            "Epoch 4290/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1130 - accuracy: 0.9475\n",
            "Epoch 4290: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1137 - accuracy: 0.9468 - val_loss: 0.1252 - val_accuracy: 0.9748\n",
            "Epoch 4291/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9478\n",
            "Epoch 4291: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1157 - accuracy: 0.9478 - val_loss: 0.0866 - val_accuracy: 0.9898\n",
            "Epoch 4292/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9450\n",
            "Epoch 4292: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1197 - accuracy: 0.9450 - val_loss: 0.1289 - val_accuracy: 0.9710\n",
            "Epoch 4293/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1146 - accuracy: 0.9464\n",
            "Epoch 4293: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1145 - accuracy: 0.9464 - val_loss: 0.1569 - val_accuracy: 0.9590\n",
            "Epoch 4294/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9484\n",
            "Epoch 4294: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1117 - accuracy: 0.9484 - val_loss: 0.1286 - val_accuracy: 0.9681\n",
            "Epoch 4295/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1134 - accuracy: 0.9467\n",
            "Epoch 4295: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1127 - accuracy: 0.9473 - val_loss: 0.1469 - val_accuracy: 0.9575\n",
            "Epoch 4296/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1085 - accuracy: 0.9485\n",
            "Epoch 4296: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9486 - val_loss: 0.1510 - val_accuracy: 0.9644\n",
            "Epoch 4297/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9505\n",
            "Epoch 4297: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1073 - accuracy: 0.9499 - val_loss: 0.1326 - val_accuracy: 0.9653\n",
            "Epoch 4298/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9503\n",
            "Epoch 4298: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1075 - accuracy: 0.9503 - val_loss: 0.1352 - val_accuracy: 0.9640\n",
            "Epoch 4299/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1046 - accuracy: 0.9492\n",
            "Epoch 4299: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1044 - accuracy: 0.9495 - val_loss: 0.1321 - val_accuracy: 0.9679\n",
            "Epoch 4300/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1062 - accuracy: 0.9506\n",
            "Epoch 4300: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1065 - accuracy: 0.9505 - val_loss: 0.1060 - val_accuracy: 0.9827\n",
            "Epoch 4301/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1144 - accuracy: 0.9466\n",
            "Epoch 4301: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9470 - val_loss: 0.1437 - val_accuracy: 0.9627\n",
            "Epoch 4302/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1213 - accuracy: 0.9453\n",
            "Epoch 4302: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1273 - accuracy: 0.9423 - val_loss: 0.1990 - val_accuracy: 0.9269\n",
            "Epoch 4303/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1245 - accuracy: 0.9432\n",
            "Epoch 4303: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1254 - accuracy: 0.9425 - val_loss: 0.1456 - val_accuracy: 0.9620\n",
            "Epoch 4304/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9453\n",
            "Epoch 4304: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 20ms/step - loss: 0.1191 - accuracy: 0.9453 - val_loss: 0.1812 - val_accuracy: 0.9391\n",
            "Epoch 4305/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1226 - accuracy: 0.9429\n",
            "Epoch 4305: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1235 - accuracy: 0.9426 - val_loss: 0.1417 - val_accuracy: 0.9577\n",
            "Epoch 4306/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1163 - accuracy: 0.9460\n",
            "Epoch 4306: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1168 - accuracy: 0.9461 - val_loss: 0.1364 - val_accuracy: 0.9688\n",
            "Epoch 4307/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1155 - accuracy: 0.9466\n",
            "Epoch 4307: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1153 - accuracy: 0.9469 - val_loss: 0.0961 - val_accuracy: 0.9900\n",
            "Epoch 4308/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1146 - accuracy: 0.9477\n",
            "Epoch 4308: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1155 - accuracy: 0.9473 - val_loss: 0.1322 - val_accuracy: 0.9662\n",
            "Epoch 4309/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1144 - accuracy: 0.9488\n",
            "Epoch 4309: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1140 - accuracy: 0.9485 - val_loss: 0.1696 - val_accuracy: 0.9473\n",
            "Epoch 4310/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1084 - accuracy: 0.9491\n",
            "Epoch 4310: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1089 - accuracy: 0.9487 - val_loss: 0.1324 - val_accuracy: 0.9735\n",
            "Epoch 4311/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1114 - accuracy: 0.9478\n",
            "Epoch 4311: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1111 - accuracy: 0.9482 - val_loss: 0.1147 - val_accuracy: 0.9766\n",
            "Epoch 4312/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9484\n",
            "Epoch 4312: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1119 - accuracy: 0.9484 - val_loss: 0.1602 - val_accuracy: 0.9529\n",
            "Epoch 4313/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9483\n",
            "Epoch 4313: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1091 - accuracy: 0.9483 - val_loss: 0.1190 - val_accuracy: 0.9690\n",
            "Epoch 4314/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1064 - accuracy: 0.9504\n",
            "Epoch 4314: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1082 - accuracy: 0.9496 - val_loss: 0.1319 - val_accuracy: 0.9662\n",
            "Epoch 4315/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9485\n",
            "Epoch 4315: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1101 - accuracy: 0.9485 - val_loss: 0.1063 - val_accuracy: 0.9775\n",
            "Epoch 4316/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1240 - accuracy: 0.9427\n",
            "Epoch 4316: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1264 - accuracy: 0.9419 - val_loss: 0.1828 - val_accuracy: 0.9386\n",
            "Epoch 4317/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1211 - accuracy: 0.9438\n",
            "Epoch 4317: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1209 - accuracy: 0.9439 - val_loss: 0.1744 - val_accuracy: 0.9451\n",
            "Epoch 4318/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9485\n",
            "Epoch 4318: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1129 - accuracy: 0.9483 - val_loss: 0.1845 - val_accuracy: 0.9365\n",
            "Epoch 4319/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1150 - accuracy: 0.9481\n",
            "Epoch 4319: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1157 - accuracy: 0.9475 - val_loss: 0.1307 - val_accuracy: 0.9660\n",
            "Epoch 4320/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9453\n",
            "Epoch 4320: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1181 - accuracy: 0.9446 - val_loss: 0.1213 - val_accuracy: 0.9725\n",
            "Epoch 4321/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9465\n",
            "Epoch 4321: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1157 - accuracy: 0.9465 - val_loss: 0.1335 - val_accuracy: 0.9660\n",
            "Epoch 4322/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1049 - accuracy: 0.9506\n",
            "Epoch 4322: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1061 - accuracy: 0.9498 - val_loss: 0.1354 - val_accuracy: 0.9584\n",
            "Epoch 4323/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9492\n",
            "Epoch 4323: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9494 - val_loss: 0.1509 - val_accuracy: 0.9516\n",
            "Epoch 4324/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9508\n",
            "Epoch 4324: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1074 - accuracy: 0.9501 - val_loss: 0.1588 - val_accuracy: 0.9508\n",
            "Epoch 4325/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9485\n",
            "Epoch 4325: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1115 - accuracy: 0.9485 - val_loss: 0.1145 - val_accuracy: 0.9761\n",
            "Epoch 4326/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9494\n",
            "Epoch 4326: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9494 - val_loss: 0.0943 - val_accuracy: 0.9846\n",
            "Epoch 4327/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9506\n",
            "Epoch 4327: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1084 - accuracy: 0.9506 - val_loss: 0.1305 - val_accuracy: 0.9701\n",
            "Epoch 4328/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9475\n",
            "Epoch 4328: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1130 - accuracy: 0.9475 - val_loss: 0.1299 - val_accuracy: 0.9664\n",
            "Epoch 4329/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9491\n",
            "Epoch 4329: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1109 - accuracy: 0.9491 - val_loss: 0.1189 - val_accuracy: 0.9738\n",
            "Epoch 4330/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9497\n",
            "Epoch 4330: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1081 - accuracy: 0.9497 - val_loss: 0.1307 - val_accuracy: 0.9646\n",
            "Epoch 4331/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1044 - accuracy: 0.9510\n",
            "Epoch 4331: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1053 - accuracy: 0.9505 - val_loss: 0.1504 - val_accuracy: 0.9525\n",
            "Epoch 4332/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1031 - accuracy: 0.9517\n",
            "Epoch 4332: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1044 - accuracy: 0.9510 - val_loss: 0.1235 - val_accuracy: 0.9718\n",
            "Epoch 4333/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1066 - accuracy: 0.9500\n",
            "Epoch 4333: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1065 - accuracy: 0.9503 - val_loss: 0.1411 - val_accuracy: 0.9607\n",
            "Epoch 4334/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9498\n",
            "Epoch 4334: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1080 - accuracy: 0.9496 - val_loss: 0.1292 - val_accuracy: 0.9666\n",
            "Epoch 4335/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1044 - accuracy: 0.9513\n",
            "Epoch 4335: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1046 - accuracy: 0.9512 - val_loss: 0.1323 - val_accuracy: 0.9651\n",
            "Epoch 4336/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9522\n",
            "Epoch 4336: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1043 - accuracy: 0.9519 - val_loss: 0.1680 - val_accuracy: 0.9456\n",
            "Epoch 4337/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1085 - accuracy: 0.9493\n",
            "Epoch 4337: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1079 - accuracy: 0.9495 - val_loss: 0.1339 - val_accuracy: 0.9655\n",
            "Epoch 4338/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9471\n",
            "Epoch 4338: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1133 - accuracy: 0.9470 - val_loss: 0.1683 - val_accuracy: 0.9406\n",
            "Epoch 4339/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9450\n",
            "Epoch 4339: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1215 - accuracy: 0.9446 - val_loss: 0.1438 - val_accuracy: 0.9657\n",
            "Epoch 4340/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1192 - accuracy: 0.9458\n",
            "Epoch 4340: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1195 - accuracy: 0.9457 - val_loss: 0.1125 - val_accuracy: 0.9783\n",
            "Epoch 4341/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9480\n",
            "Epoch 4341: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1142 - accuracy: 0.9478 - val_loss: 0.1700 - val_accuracy: 0.9547\n",
            "Epoch 4342/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1421 - accuracy: 0.9367\n",
            "Epoch 4342: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1414 - accuracy: 0.9369 - val_loss: 0.2800 - val_accuracy: 0.8799\n",
            "Epoch 4343/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9440\n",
            "Epoch 4343: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1225 - accuracy: 0.9440 - val_loss: 0.1255 - val_accuracy: 0.9740\n",
            "Epoch 4344/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1099 - accuracy: 0.9480\n",
            "Epoch 4344: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1089 - accuracy: 0.9493 - val_loss: 0.1307 - val_accuracy: 0.9677\n",
            "Epoch 4345/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1045 - accuracy: 0.9504\n",
            "Epoch 4345: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1055 - accuracy: 0.9497 - val_loss: 0.1218 - val_accuracy: 0.9699\n",
            "Epoch 4346/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1033 - accuracy: 0.9512\n",
            "Epoch 4346: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1032 - accuracy: 0.9512 - val_loss: 0.1128 - val_accuracy: 0.9814\n",
            "Epoch 4347/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1063 - accuracy: 0.9504\n",
            "Epoch 4347: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1073 - accuracy: 0.9502 - val_loss: 0.1286 - val_accuracy: 0.9679\n",
            "Epoch 4348/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1123 - accuracy: 0.9480\n",
            "Epoch 4348: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1127 - accuracy: 0.9479 - val_loss: 0.0999 - val_accuracy: 0.9824\n",
            "Epoch 4349/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9495\n",
            "Epoch 4349: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9495 - val_loss: 0.0897 - val_accuracy: 0.9900\n",
            "Epoch 4350/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9474\n",
            "Epoch 4350: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1118 - accuracy: 0.9476 - val_loss: 0.1347 - val_accuracy: 0.9664\n",
            "Epoch 4351/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9479\n",
            "Epoch 4351: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1111 - accuracy: 0.9480 - val_loss: 0.1300 - val_accuracy: 0.9731\n",
            "Epoch 4352/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1099 - accuracy: 0.9486\n",
            "Epoch 4352: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1111 - accuracy: 0.9476 - val_loss: 0.1755 - val_accuracy: 0.9302\n",
            "Epoch 4353/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1087 - accuracy: 0.9505\n",
            "Epoch 4353: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9506 - val_loss: 0.1563 - val_accuracy: 0.9525\n",
            "Epoch 4354/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9494\n",
            "Epoch 4354: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1086 - accuracy: 0.9494 - val_loss: 0.1697 - val_accuracy: 0.9443\n",
            "Epoch 4355/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1083 - accuracy: 0.9492\n",
            "Epoch 4355: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1089 - accuracy: 0.9493 - val_loss: 0.1213 - val_accuracy: 0.9705\n",
            "Epoch 4356/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9476\n",
            "Epoch 4356: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9473 - val_loss: 0.1662 - val_accuracy: 0.9465\n",
            "Epoch 4357/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1079 - accuracy: 0.9498\n",
            "Epoch 4357: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1084 - accuracy: 0.9496 - val_loss: 0.1479 - val_accuracy: 0.9595\n",
            "Epoch 4358/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9491\n",
            "Epoch 4358: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 0.9488 - val_loss: 0.1936 - val_accuracy: 0.9239\n",
            "Epoch 4359/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1096 - accuracy: 0.9494\n",
            "Epoch 4359: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1097 - accuracy: 0.9496 - val_loss: 0.1344 - val_accuracy: 0.9670\n",
            "Epoch 4360/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1100 - accuracy: 0.9488\n",
            "Epoch 4360: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1093 - accuracy: 0.9494 - val_loss: 0.1336 - val_accuracy: 0.9701\n",
            "Epoch 4361/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1084 - accuracy: 0.9500\n",
            "Epoch 4361: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9494 - val_loss: 0.1703 - val_accuracy: 0.9501\n",
            "Epoch 4362/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1157 - accuracy: 0.9456\n",
            "Epoch 4362: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1168 - accuracy: 0.9454 - val_loss: 0.1780 - val_accuracy: 0.9419\n",
            "Epoch 4363/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1161 - accuracy: 0.9458\n",
            "Epoch 4363: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1162 - accuracy: 0.9460 - val_loss: 0.1215 - val_accuracy: 0.9736\n",
            "Epoch 4364/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1154 - accuracy: 0.9458\n",
            "Epoch 4364: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1156 - accuracy: 0.9460 - val_loss: 0.2036 - val_accuracy: 0.9269\n",
            "Epoch 4365/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9480\n",
            "Epoch 4365: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1146 - accuracy: 0.9477 - val_loss: 0.1391 - val_accuracy: 0.9646\n",
            "Epoch 4366/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1091 - accuracy: 0.9495\n",
            "Epoch 4366: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1096 - accuracy: 0.9491 - val_loss: 0.1755 - val_accuracy: 0.9434\n",
            "Epoch 4367/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9498\n",
            "Epoch 4367: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9498 - val_loss: 0.1042 - val_accuracy: 0.9809\n",
            "Epoch 4368/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9499\n",
            "Epoch 4368: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1076 - accuracy: 0.9499 - val_loss: 0.1303 - val_accuracy: 0.9662\n",
            "Epoch 4369/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9507\n",
            "Epoch 4369: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9512 - val_loss: 0.1313 - val_accuracy: 0.9618\n",
            "Epoch 4370/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9500\n",
            "Epoch 4370: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1068 - accuracy: 0.9501 - val_loss: 0.1162 - val_accuracy: 0.9751\n",
            "Epoch 4371/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9510\n",
            "Epoch 4371: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1054 - accuracy: 0.9510 - val_loss: 0.1601 - val_accuracy: 0.9438\n",
            "Epoch 4372/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9506\n",
            "Epoch 4372: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1071 - accuracy: 0.9506 - val_loss: 0.1555 - val_accuracy: 0.9473\n",
            "Epoch 4373/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9495\n",
            "Epoch 4373: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1062 - accuracy: 0.9495 - val_loss: 0.1350 - val_accuracy: 0.9668\n",
            "Epoch 4374/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1053 - accuracy: 0.9512\n",
            "Epoch 4374: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1047 - accuracy: 0.9520 - val_loss: 0.1190 - val_accuracy: 0.9742\n",
            "Epoch 4375/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9510\n",
            "Epoch 4375: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9508 - val_loss: 0.1563 - val_accuracy: 0.9441\n",
            "Epoch 4376/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9492\n",
            "Epoch 4376: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1074 - accuracy: 0.9492 - val_loss: 0.1744 - val_accuracy: 0.9486\n",
            "Epoch 4377/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1067 - accuracy: 0.9503\n",
            "Epoch 4377: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1060 - accuracy: 0.9504 - val_loss: 0.1162 - val_accuracy: 0.9766\n",
            "Epoch 4378/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9481\n",
            "Epoch 4378: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9481 - val_loss: 0.1400 - val_accuracy: 0.9616\n",
            "Epoch 4379/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1181 - accuracy: 0.9449\n",
            "Epoch 4379: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1193 - accuracy: 0.9441 - val_loss: 0.1424 - val_accuracy: 0.9577\n",
            "Epoch 4380/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9393\n",
            "Epoch 4380: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1372 - accuracy: 0.9394 - val_loss: 0.1862 - val_accuracy: 0.9310\n",
            "Epoch 4381/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9434\n",
            "Epoch 4381: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1202 - accuracy: 0.9441 - val_loss: 0.1317 - val_accuracy: 0.9686\n",
            "Epoch 4382/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9487\n",
            "Epoch 4382: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9481 - val_loss: 0.1293 - val_accuracy: 0.9655\n",
            "Epoch 4383/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9485\n",
            "Epoch 4383: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1118 - accuracy: 0.9485 - val_loss: 0.1545 - val_accuracy: 0.9465\n",
            "Epoch 4384/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9506\n",
            "Epoch 4384: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1064 - accuracy: 0.9504 - val_loss: 0.1520 - val_accuracy: 0.9495\n",
            "Epoch 4385/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1059 - accuracy: 0.9515\n",
            "Epoch 4385: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1077 - accuracy: 0.9505 - val_loss: 0.1288 - val_accuracy: 0.9699\n",
            "Epoch 4386/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1078 - accuracy: 0.9502\n",
            "Epoch 4386: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9502 - val_loss: 0.1320 - val_accuracy: 0.9664\n",
            "Epoch 4387/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9486\n",
            "Epoch 4387: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1091 - accuracy: 0.9485 - val_loss: 0.1699 - val_accuracy: 0.9408\n",
            "Epoch 4388/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1116 - accuracy: 0.9491\n",
            "Epoch 4388: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9484 - val_loss: 0.1372 - val_accuracy: 0.9647\n",
            "Epoch 4389/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9494\n",
            "Epoch 4389: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1083 - accuracy: 0.9494 - val_loss: 0.1076 - val_accuracy: 0.9801\n",
            "Epoch 4390/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1132 - accuracy: 0.9488\n",
            "Epoch 4390: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1130 - accuracy: 0.9485 - val_loss: 0.1351 - val_accuracy: 0.9660\n",
            "Epoch 4391/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1158 - accuracy: 0.9477\n",
            "Epoch 4391: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1160 - accuracy: 0.9474 - val_loss: 0.1339 - val_accuracy: 0.9692\n",
            "Epoch 4392/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1113 - accuracy: 0.9485\n",
            "Epoch 4392: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1106 - accuracy: 0.9490 - val_loss: 0.1539 - val_accuracy: 0.9569\n",
            "Epoch 4393/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1088 - accuracy: 0.9498\n",
            "Epoch 4393: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1101 - accuracy: 0.9485 - val_loss: 0.1287 - val_accuracy: 0.9738\n",
            "Epoch 4394/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1105 - accuracy: 0.9484\n",
            "Epoch 4394: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1108 - accuracy: 0.9478 - val_loss: 0.1496 - val_accuracy: 0.9527\n",
            "Epoch 4395/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1219 - accuracy: 0.9459\n",
            "Epoch 4395: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1229 - accuracy: 0.9446 - val_loss: 0.1506 - val_accuracy: 0.9573\n",
            "Epoch 4396/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1233 - accuracy: 0.9441\n",
            "Epoch 4396: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1228 - accuracy: 0.9441 - val_loss: 0.1600 - val_accuracy: 0.9501\n",
            "Epoch 4397/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9466\n",
            "Epoch 4397: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1147 - accuracy: 0.9466 - val_loss: 0.1662 - val_accuracy: 0.9504\n",
            "Epoch 4398/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1155 - accuracy: 0.9470\n",
            "Epoch 4398: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1148 - accuracy: 0.9475 - val_loss: 0.1488 - val_accuracy: 0.9608\n",
            "Epoch 4399/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9487\n",
            "Epoch 4399: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1125 - accuracy: 0.9487 - val_loss: 0.1585 - val_accuracy: 0.9477\n",
            "Epoch 4400/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9504\n",
            "Epoch 4400: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1103 - accuracy: 0.9503 - val_loss: 0.1708 - val_accuracy: 0.9423\n",
            "Epoch 4401/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1071 - accuracy: 0.9501\n",
            "Epoch 4401: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1082 - accuracy: 0.9495 - val_loss: 0.1443 - val_accuracy: 0.9599\n",
            "Epoch 4402/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1086 - accuracy: 0.9491\n",
            "Epoch 4402: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1089 - accuracy: 0.9494 - val_loss: 0.1424 - val_accuracy: 0.9566\n",
            "Epoch 4403/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9497\n",
            "Epoch 4403: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1071 - accuracy: 0.9497 - val_loss: 0.1187 - val_accuracy: 0.9729\n",
            "Epoch 4404/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1059 - accuracy: 0.9520\n",
            "Epoch 4404: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1068 - accuracy: 0.9511 - val_loss: 0.1691 - val_accuracy: 0.9395\n",
            "Epoch 4405/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9514\n",
            "Epoch 4405: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1051 - accuracy: 0.9512 - val_loss: 0.1333 - val_accuracy: 0.9659\n",
            "Epoch 4406/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9518\n",
            "Epoch 4406: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1052 - accuracy: 0.9513 - val_loss: 0.1552 - val_accuracy: 0.9475\n",
            "Epoch 4407/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1036 - accuracy: 0.9518\n",
            "Epoch 4407: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1054 - accuracy: 0.9507 - val_loss: 0.0986 - val_accuracy: 0.9801\n",
            "Epoch 4408/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9473\n",
            "Epoch 4408: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1162 - accuracy: 0.9472 - val_loss: 0.1719 - val_accuracy: 0.9384\n",
            "Epoch 4409/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1130 - accuracy: 0.9470\n",
            "Epoch 4409: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1133 - accuracy: 0.9470 - val_loss: 0.1280 - val_accuracy: 0.9688\n",
            "Epoch 4410/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9489\n",
            "Epoch 4410: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1125 - accuracy: 0.9489 - val_loss: 0.1234 - val_accuracy: 0.9723\n",
            "Epoch 4411/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9466\n",
            "Epoch 4411: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1139 - accuracy: 0.9463 - val_loss: 0.1730 - val_accuracy: 0.9393\n",
            "Epoch 4412/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1351 - accuracy: 0.9407\n",
            "Epoch 4412: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1349 - accuracy: 0.9402 - val_loss: 0.1686 - val_accuracy: 0.9473\n",
            "Epoch 4413/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9418\n",
            "Epoch 4413: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9422 - val_loss: 0.2116 - val_accuracy: 0.9219\n",
            "Epoch 4414/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1216 - accuracy: 0.9439\n",
            "Epoch 4414: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1224 - accuracy: 0.9434 - val_loss: 0.2454 - val_accuracy: 0.9061\n",
            "Epoch 4415/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9442\n",
            "Epoch 4415: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1182 - accuracy: 0.9442 - val_loss: 0.1407 - val_accuracy: 0.9610\n",
            "Epoch 4416/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1069 - accuracy: 0.9507\n",
            "Epoch 4416: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1067 - accuracy: 0.9505 - val_loss: 0.1246 - val_accuracy: 0.9688\n",
            "Epoch 4417/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1035 - accuracy: 0.9513\n",
            "Epoch 4417: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1041 - accuracy: 0.9512 - val_loss: 0.1292 - val_accuracy: 0.9736\n",
            "Epoch 4418/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1039 - accuracy: 0.9504\n",
            "Epoch 4418: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1044 - accuracy: 0.9504 - val_loss: 0.1117 - val_accuracy: 0.9762\n",
            "Epoch 4419/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9502\n",
            "Epoch 4419: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9498 - val_loss: 0.1277 - val_accuracy: 0.9660\n",
            "Epoch 4420/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1070 - accuracy: 0.9499\n",
            "Epoch 4420: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1071 - accuracy: 0.9499 - val_loss: 0.1370 - val_accuracy: 0.9577\n",
            "Epoch 4421/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9501\n",
            "Epoch 4421: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1074 - accuracy: 0.9501 - val_loss: 0.1045 - val_accuracy: 0.9777\n",
            "Epoch 4422/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1119 - accuracy: 0.9494\n",
            "Epoch 4422: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1133 - accuracy: 0.9486 - val_loss: 0.1361 - val_accuracy: 0.9659\n",
            "Epoch 4423/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9469\n",
            "Epoch 4423: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1143 - accuracy: 0.9465 - val_loss: 0.1602 - val_accuracy: 0.9458\n",
            "Epoch 4424/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1111 - accuracy: 0.9477\n",
            "Epoch 4424: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1113 - accuracy: 0.9478 - val_loss: 0.1101 - val_accuracy: 0.9811\n",
            "Epoch 4425/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1074 - accuracy: 0.9505\n",
            "Epoch 4425: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1088 - accuracy: 0.9499 - val_loss: 0.1471 - val_accuracy: 0.9599\n",
            "Epoch 4426/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9518\n",
            "Epoch 4426: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1056 - accuracy: 0.9518 - val_loss: 0.1329 - val_accuracy: 0.9657\n",
            "Epoch 4427/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1037 - accuracy: 0.9530\n",
            "Epoch 4427: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1040 - accuracy: 0.9531 - val_loss: 0.1407 - val_accuracy: 0.9627\n",
            "Epoch 4428/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1026 - accuracy: 0.9519\n",
            "Epoch 4428: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1037 - accuracy: 0.9514 - val_loss: 0.1261 - val_accuracy: 0.9705\n",
            "Epoch 4429/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9509\n",
            "Epoch 4429: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1059 - accuracy: 0.9511 - val_loss: 0.0984 - val_accuracy: 0.9814\n",
            "Epoch 4430/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9500\n",
            "Epoch 4430: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1052 - accuracy: 0.9500 - val_loss: 0.1356 - val_accuracy: 0.9599\n",
            "Epoch 4431/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9526\n",
            "Epoch 4431: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1031 - accuracy: 0.9524 - val_loss: 0.1129 - val_accuracy: 0.9781\n",
            "Epoch 4432/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1044 - accuracy: 0.9525\n",
            "Epoch 4432: loss did not improve from 0.10262\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1050 - accuracy: 0.9517 - val_loss: 0.1577 - val_accuracy: 0.9445\n",
            "Epoch 4433/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9529\n",
            "Epoch 4433: loss improved from 0.10262 to 0.10212, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.1021 - accuracy: 0.9527 - val_loss: 0.1170 - val_accuracy: 0.9751\n",
            "Epoch 4434/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1010 - accuracy: 0.9528\n",
            "Epoch 4434: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1033 - accuracy: 0.9514 - val_loss: 0.1412 - val_accuracy: 0.9592\n",
            "Epoch 4435/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9500\n",
            "Epoch 4435: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1081 - accuracy: 0.9500 - val_loss: 0.1428 - val_accuracy: 0.9625\n",
            "Epoch 4436/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1088 - accuracy: 0.9487\n",
            "Epoch 4436: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1094 - accuracy: 0.9483 - val_loss: 0.1181 - val_accuracy: 0.9757\n",
            "Epoch 4437/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1065 - accuracy: 0.9501\n",
            "Epoch 4437: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1066 - accuracy: 0.9500 - val_loss: 0.1276 - val_accuracy: 0.9770\n",
            "Epoch 4438/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9508\n",
            "Epoch 4438: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1070 - accuracy: 0.9506 - val_loss: 0.1269 - val_accuracy: 0.9735\n",
            "Epoch 4439/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9501\n",
            "Epoch 4439: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1064 - accuracy: 0.9504 - val_loss: 0.1099 - val_accuracy: 0.9772\n",
            "Epoch 4440/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1082 - accuracy: 0.9493\n",
            "Epoch 4440: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1075 - accuracy: 0.9498 - val_loss: 0.1231 - val_accuracy: 0.9740\n",
            "Epoch 4441/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9493\n",
            "Epoch 4441: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1097 - accuracy: 0.9493 - val_loss: 0.1597 - val_accuracy: 0.9514\n",
            "Epoch 4442/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9486\n",
            "Epoch 4442: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1122 - accuracy: 0.9487 - val_loss: 0.1498 - val_accuracy: 0.9607\n",
            "Epoch 4443/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1200 - accuracy: 0.9456\n",
            "Epoch 4443: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1197 - accuracy: 0.9459 - val_loss: 0.1485 - val_accuracy: 0.9683\n",
            "Epoch 4444/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9443\n",
            "Epoch 4444: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1216 - accuracy: 0.9443 - val_loss: 0.1610 - val_accuracy: 0.9517\n",
            "Epoch 4445/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9478\n",
            "Epoch 4445: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1157 - accuracy: 0.9472 - val_loss: 0.1958 - val_accuracy: 0.9337\n",
            "Epoch 4446/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1222 - accuracy: 0.9444\n",
            "Epoch 4446: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1217 - accuracy: 0.9445 - val_loss: 0.1948 - val_accuracy: 0.9308\n",
            "Epoch 4447/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1167 - accuracy: 0.9448\n",
            "Epoch 4447: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1183 - accuracy: 0.9442 - val_loss: 0.1063 - val_accuracy: 0.9790\n",
            "Epoch 4448/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9406\n",
            "Epoch 4448: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1302 - accuracy: 0.9406 - val_loss: 0.1433 - val_accuracy: 0.9651\n",
            "Epoch 4449/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9491\n",
            "Epoch 4449: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1108 - accuracy: 0.9491 - val_loss: 0.1275 - val_accuracy: 0.9642\n",
            "Epoch 4450/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9498\n",
            "Epoch 4450: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1071 - accuracy: 0.9496 - val_loss: 0.1397 - val_accuracy: 0.9581\n",
            "Epoch 4451/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1086 - accuracy: 0.9493\n",
            "Epoch 4451: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1090 - accuracy: 0.9491 - val_loss: 0.1054 - val_accuracy: 0.9827\n",
            "Epoch 4452/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1094 - accuracy: 0.9503\n",
            "Epoch 4452: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1099 - accuracy: 0.9492 - val_loss: 0.1477 - val_accuracy: 0.9560\n",
            "Epoch 4453/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1065 - accuracy: 0.9499\n",
            "Epoch 4453: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9498 - val_loss: 0.1367 - val_accuracy: 0.9642\n",
            "Epoch 4454/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1104 - accuracy: 0.9487\n",
            "Epoch 4454: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9488 - val_loss: 0.1185 - val_accuracy: 0.9772\n",
            "Epoch 4455/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9507\n",
            "Epoch 4455: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1066 - accuracy: 0.9508 - val_loss: 0.1235 - val_accuracy: 0.9712\n",
            "Epoch 4456/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9515\n",
            "Epoch 4456: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1053 - accuracy: 0.9508 - val_loss: 0.1304 - val_accuracy: 0.9675\n",
            "Epoch 4457/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9497\n",
            "Epoch 4457: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1080 - accuracy: 0.9497 - val_loss: 0.1323 - val_accuracy: 0.9673\n",
            "Epoch 4458/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1101 - accuracy: 0.9495\n",
            "Epoch 4458: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1099 - accuracy: 0.9495 - val_loss: 0.1600 - val_accuracy: 0.9586\n",
            "Epoch 4459/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9503\n",
            "Epoch 4459: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1069 - accuracy: 0.9505 - val_loss: 0.1322 - val_accuracy: 0.9646\n",
            "Epoch 4460/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1075 - accuracy: 0.9498\n",
            "Epoch 4460: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1079 - accuracy: 0.9496 - val_loss: 0.1048 - val_accuracy: 0.9859\n",
            "Epoch 4461/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9505\n",
            "Epoch 4461: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1075 - accuracy: 0.9505 - val_loss: 0.1731 - val_accuracy: 0.9376\n",
            "Epoch 4462/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9505\n",
            "Epoch 4462: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1070 - accuracy: 0.9507 - val_loss: 0.1159 - val_accuracy: 0.9738\n",
            "Epoch 4463/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1091 - accuracy: 0.9482\n",
            "Epoch 4463: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1091 - accuracy: 0.9483 - val_loss: 0.1146 - val_accuracy: 0.9735\n",
            "Epoch 4464/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1061 - accuracy: 0.9512\n",
            "Epoch 4464: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9501 - val_loss: 0.1932 - val_accuracy: 0.9274\n",
            "Epoch 4465/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1177 - accuracy: 0.9464\n",
            "Epoch 4465: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1175 - accuracy: 0.9467 - val_loss: 0.1608 - val_accuracy: 0.9529\n",
            "Epoch 4466/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9442\n",
            "Epoch 4466: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1201 - accuracy: 0.9440 - val_loss: 0.1110 - val_accuracy: 0.9831\n",
            "Epoch 4467/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9416\n",
            "Epoch 4467: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1292 - accuracy: 0.9416 - val_loss: 0.1650 - val_accuracy: 0.9519\n",
            "Epoch 4468/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9472\n",
            "Epoch 4468: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1161 - accuracy: 0.9472 - val_loss: 0.1764 - val_accuracy: 0.9460\n",
            "Epoch 4469/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9439\n",
            "Epoch 4469: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1231 - accuracy: 0.9439 - val_loss: 0.1477 - val_accuracy: 0.9581\n",
            "Epoch 4470/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1233 - accuracy: 0.9446\n",
            "Epoch 4470: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1229 - accuracy: 0.9447 - val_loss: 0.1895 - val_accuracy: 0.9285\n",
            "Epoch 4471/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1145 - accuracy: 0.9477\n",
            "Epoch 4471: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1141 - accuracy: 0.9479 - val_loss: 0.1783 - val_accuracy: 0.9414\n",
            "Epoch 4472/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1087 - accuracy: 0.9497\n",
            "Epoch 4472: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1092 - accuracy: 0.9492 - val_loss: 0.1456 - val_accuracy: 0.9649\n",
            "Epoch 4473/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9489\n",
            "Epoch 4473: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1092 - accuracy: 0.9491 - val_loss: 0.1899 - val_accuracy: 0.9291\n",
            "Epoch 4474/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1063 - accuracy: 0.9501\n",
            "Epoch 4474: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1062 - accuracy: 0.9499 - val_loss: 0.1269 - val_accuracy: 0.9670\n",
            "Epoch 4475/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9503\n",
            "Epoch 4475: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1074 - accuracy: 0.9502 - val_loss: 0.1344 - val_accuracy: 0.9638\n",
            "Epoch 4476/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1063 - accuracy: 0.9506\n",
            "Epoch 4476: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1067 - accuracy: 0.9501 - val_loss: 0.1075 - val_accuracy: 0.9803\n",
            "Epoch 4477/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1149 - accuracy: 0.9475\n",
            "Epoch 4477: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1139 - accuracy: 0.9481 - val_loss: 0.1240 - val_accuracy: 0.9725\n",
            "Epoch 4478/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9502\n",
            "Epoch 4478: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1067 - accuracy: 0.9502 - val_loss: 0.1123 - val_accuracy: 0.9794\n",
            "Epoch 4479/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1098 - accuracy: 0.9495\n",
            "Epoch 4479: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1097 - accuracy: 0.9492 - val_loss: 0.1427 - val_accuracy: 0.9555\n",
            "Epoch 4480/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9497\n",
            "Epoch 4480: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1077 - accuracy: 0.9495 - val_loss: 0.1407 - val_accuracy: 0.9555\n",
            "Epoch 4481/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1067 - accuracy: 0.9490\n",
            "Epoch 4481: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9493 - val_loss: 0.1222 - val_accuracy: 0.9696\n",
            "Epoch 4482/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9508\n",
            "Epoch 4482: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1069 - accuracy: 0.9508 - val_loss: 0.1112 - val_accuracy: 0.9801\n",
            "Epoch 4483/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1086 - accuracy: 0.9495\n",
            "Epoch 4483: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9493 - val_loss: 0.1110 - val_accuracy: 0.9749\n",
            "Epoch 4484/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1126 - accuracy: 0.9468\n",
            "Epoch 4484: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1115 - accuracy: 0.9475 - val_loss: 0.1582 - val_accuracy: 0.9493\n",
            "Epoch 4485/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1075 - accuracy: 0.9500\n",
            "Epoch 4485: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9497 - val_loss: 0.1287 - val_accuracy: 0.9623\n",
            "Epoch 4486/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1097 - accuracy: 0.9481\n",
            "Epoch 4486: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1096 - accuracy: 0.9484 - val_loss: 0.1495 - val_accuracy: 0.9473\n",
            "Epoch 4487/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9502\n",
            "Epoch 4487: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1074 - accuracy: 0.9502 - val_loss: 0.1221 - val_accuracy: 0.9688\n",
            "Epoch 4488/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1128 - accuracy: 0.9470\n",
            "Epoch 4488: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1125 - accuracy: 0.9473 - val_loss: 0.1353 - val_accuracy: 0.9620\n",
            "Epoch 4489/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9489\n",
            "Epoch 4489: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1110 - accuracy: 0.9488 - val_loss: 0.1319 - val_accuracy: 0.9670\n",
            "Epoch 4490/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9492\n",
            "Epoch 4490: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1117 - accuracy: 0.9490 - val_loss: 0.1218 - val_accuracy: 0.9722\n",
            "Epoch 4491/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9470\n",
            "Epoch 4491: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9470 - val_loss: 0.1352 - val_accuracy: 0.9629\n",
            "Epoch 4492/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9460\n",
            "Epoch 4492: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1190 - accuracy: 0.9460 - val_loss: 0.1259 - val_accuracy: 0.9729\n",
            "Epoch 4493/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1219 - accuracy: 0.9453\n",
            "Epoch 4493: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1229 - accuracy: 0.9440 - val_loss: 0.1603 - val_accuracy: 0.9464\n",
            "Epoch 4494/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9460\n",
            "Epoch 4494: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1195 - accuracy: 0.9460 - val_loss: 0.1207 - val_accuracy: 0.9772\n",
            "Epoch 4495/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1163 - accuracy: 0.9458\n",
            "Epoch 4495: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1169 - accuracy: 0.9453 - val_loss: 0.1320 - val_accuracy: 0.9673\n",
            "Epoch 4496/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1126 - accuracy: 0.9481\n",
            "Epoch 4496: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1132 - accuracy: 0.9477 - val_loss: 0.1576 - val_accuracy: 0.9499\n",
            "Epoch 4497/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1138 - accuracy: 0.9469\n",
            "Epoch 4497: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1143 - accuracy: 0.9465 - val_loss: 0.1498 - val_accuracy: 0.9527\n",
            "Epoch 4498/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1141 - accuracy: 0.9466\n",
            "Epoch 4498: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9469 - val_loss: 0.1181 - val_accuracy: 0.9748\n",
            "Epoch 4499/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9448\n",
            "Epoch 4499: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1199 - accuracy: 0.9448 - val_loss: 0.1641 - val_accuracy: 0.9350\n",
            "Epoch 4500/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9458\n",
            "Epoch 4500: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1182 - accuracy: 0.9460 - val_loss: 0.1653 - val_accuracy: 0.9443\n",
            "Epoch 4501/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9485\n",
            "Epoch 4501: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1109 - accuracy: 0.9481 - val_loss: 0.1235 - val_accuracy: 0.9722\n",
            "Epoch 4502/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1093 - accuracy: 0.9492\n",
            "Epoch 4502: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1105 - accuracy: 0.9485 - val_loss: 0.1767 - val_accuracy: 0.9471\n",
            "Epoch 4503/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9513\n",
            "Epoch 4503: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1071 - accuracy: 0.9503 - val_loss: 0.1488 - val_accuracy: 0.9512\n",
            "Epoch 4504/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1064 - accuracy: 0.9506\n",
            "Epoch 4504: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9504 - val_loss: 0.1440 - val_accuracy: 0.9642\n",
            "Epoch 4505/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1155 - accuracy: 0.9472\n",
            "Epoch 4505: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1143 - accuracy: 0.9476 - val_loss: 0.1509 - val_accuracy: 0.9571\n",
            "Epoch 4506/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9502\n",
            "Epoch 4506: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1096 - accuracy: 0.9502 - val_loss: 0.1498 - val_accuracy: 0.9579\n",
            "Epoch 4507/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9498\n",
            "Epoch 4507: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1075 - accuracy: 0.9498 - val_loss: 0.1032 - val_accuracy: 0.9798\n",
            "Epoch 4508/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9496\n",
            "Epoch 4508: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9499 - val_loss: 0.1589 - val_accuracy: 0.9519\n",
            "Epoch 4509/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1072 - accuracy: 0.9501\n",
            "Epoch 4509: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1054 - accuracy: 0.9512 - val_loss: 0.1158 - val_accuracy: 0.9753\n",
            "Epoch 4510/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1064 - accuracy: 0.9501\n",
            "Epoch 4510: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1071 - accuracy: 0.9498 - val_loss: 0.1593 - val_accuracy: 0.9543\n",
            "Epoch 4511/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9484\n",
            "Epoch 4511: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9483 - val_loss: 0.1234 - val_accuracy: 0.9718\n",
            "Epoch 4512/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1037 - accuracy: 0.9523\n",
            "Epoch 4512: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1049 - accuracy: 0.9517 - val_loss: 0.1332 - val_accuracy: 0.9671\n",
            "Epoch 4513/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9503\n",
            "Epoch 4513: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1071 - accuracy: 0.9503 - val_loss: 0.1739 - val_accuracy: 0.9439\n",
            "Epoch 4514/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1079 - accuracy: 0.9499\n",
            "Epoch 4514: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1063 - accuracy: 0.9503 - val_loss: 0.1369 - val_accuracy: 0.9601\n",
            "Epoch 4515/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1072 - accuracy: 0.9502\n",
            "Epoch 4515: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1078 - accuracy: 0.9497 - val_loss: 0.1577 - val_accuracy: 0.9427\n",
            "Epoch 4516/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9483\n",
            "Epoch 4516: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1108 - accuracy: 0.9484 - val_loss: 0.2157 - val_accuracy: 0.9124\n",
            "Epoch 4517/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9468\n",
            "Epoch 4517: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1123 - accuracy: 0.9468 - val_loss: 0.1286 - val_accuracy: 0.9646\n",
            "Epoch 4518/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9463\n",
            "Epoch 4518: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1133 - accuracy: 0.9463 - val_loss: 0.1171 - val_accuracy: 0.9701\n",
            "Epoch 4519/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1119 - accuracy: 0.9481\n",
            "Epoch 4519: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1123 - accuracy: 0.9479 - val_loss: 0.1168 - val_accuracy: 0.9640\n",
            "Epoch 4520/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1087 - accuracy: 0.9494\n",
            "Epoch 4520: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1082 - accuracy: 0.9493 - val_loss: 0.1400 - val_accuracy: 0.9601\n",
            "Epoch 4521/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9497\n",
            "Epoch 4521: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1076 - accuracy: 0.9493 - val_loss: 0.1304 - val_accuracy: 0.9647\n",
            "Epoch 4522/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1120 - accuracy: 0.9484\n",
            "Epoch 4522: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9477 - val_loss: 0.1361 - val_accuracy: 0.9636\n",
            "Epoch 4523/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9496\n",
            "Epoch 4523: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9496 - val_loss: 0.1535 - val_accuracy: 0.9610\n",
            "Epoch 4524/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1122 - accuracy: 0.9495\n",
            "Epoch 4524: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1130 - accuracy: 0.9495 - val_loss: 0.1716 - val_accuracy: 0.9399\n",
            "Epoch 4525/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1105 - accuracy: 0.9479\n",
            "Epoch 4525: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9462 - val_loss: 0.1679 - val_accuracy: 0.9506\n",
            "Epoch 4526/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9480\n",
            "Epoch 4526: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1122 - accuracy: 0.9480 - val_loss: 0.2116 - val_accuracy: 0.9313\n",
            "Epoch 4527/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9448\n",
            "Epoch 4527: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1181 - accuracy: 0.9448 - val_loss: 0.1464 - val_accuracy: 0.9636\n",
            "Epoch 4528/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1170 - accuracy: 0.9467\n",
            "Epoch 4528: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1167 - accuracy: 0.9468 - val_loss: 0.1030 - val_accuracy: 0.9822\n",
            "Epoch 4529/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1120 - accuracy: 0.9482\n",
            "Epoch 4529: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1128 - accuracy: 0.9479 - val_loss: 0.1327 - val_accuracy: 0.9714\n",
            "Epoch 4530/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1218 - accuracy: 0.9442\n",
            "Epoch 4530: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1206 - accuracy: 0.9452 - val_loss: 0.1478 - val_accuracy: 0.9542\n",
            "Epoch 4531/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1148 - accuracy: 0.9468\n",
            "Epoch 4531: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1152 - accuracy: 0.9466 - val_loss: 0.1393 - val_accuracy: 0.9646\n",
            "Epoch 4532/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9500\n",
            "Epoch 4532: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1072 - accuracy: 0.9500 - val_loss: 0.1478 - val_accuracy: 0.9582\n",
            "Epoch 4533/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9496\n",
            "Epoch 4533: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1078 - accuracy: 0.9496 - val_loss: 0.1571 - val_accuracy: 0.9490\n",
            "Epoch 4534/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9500\n",
            "Epoch 4534: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1060 - accuracy: 0.9503 - val_loss: 0.1046 - val_accuracy: 0.9848\n",
            "Epoch 4535/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9507\n",
            "Epoch 4535: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1047 - accuracy: 0.9507 - val_loss: 0.1523 - val_accuracy: 0.9490\n",
            "Epoch 4536/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9509\n",
            "Epoch 4536: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1051 - accuracy: 0.9504 - val_loss: 0.1493 - val_accuracy: 0.9592\n",
            "Epoch 4537/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1049 - accuracy: 0.9502\n",
            "Epoch 4537: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1041 - accuracy: 0.9505 - val_loss: 0.1281 - val_accuracy: 0.9684\n",
            "Epoch 4538/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1096 - accuracy: 0.9503\n",
            "Epoch 4538: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9498 - val_loss: 0.1235 - val_accuracy: 0.9671\n",
            "Epoch 4539/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9472\n",
            "Epoch 4539: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1124 - accuracy: 0.9472 - val_loss: 0.1243 - val_accuracy: 0.9729\n",
            "Epoch 4540/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9486\n",
            "Epoch 4540: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1101 - accuracy: 0.9486 - val_loss: 0.1128 - val_accuracy: 0.9770\n",
            "Epoch 4541/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9516\n",
            "Epoch 4541: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1067 - accuracy: 0.9515 - val_loss: 0.1580 - val_accuracy: 0.9447\n",
            "Epoch 4542/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1054 - accuracy: 0.9515\n",
            "Epoch 4542: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1068 - accuracy: 0.9504 - val_loss: 0.1142 - val_accuracy: 0.9770\n",
            "Epoch 4543/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9487\n",
            "Epoch 4543: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9490 - val_loss: 0.1731 - val_accuracy: 0.9399\n",
            "Epoch 4544/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1078 - accuracy: 0.9513\n",
            "Epoch 4544: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1090 - accuracy: 0.9501 - val_loss: 0.1321 - val_accuracy: 0.9586\n",
            "Epoch 4545/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1067 - accuracy: 0.9497\n",
            "Epoch 4545: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1075 - accuracy: 0.9494 - val_loss: 0.1369 - val_accuracy: 0.9646\n",
            "Epoch 4546/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1118 - accuracy: 0.9478\n",
            "Epoch 4546: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1097 - accuracy: 0.9491 - val_loss: 0.1612 - val_accuracy: 0.9454\n",
            "Epoch 4547/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1111 - accuracy: 0.9488\n",
            "Epoch 4547: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1129 - accuracy: 0.9479 - val_loss: 0.1751 - val_accuracy: 0.9432\n",
            "Epoch 4548/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1313 - accuracy: 0.9401\n",
            "Epoch 4548: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1321 - accuracy: 0.9403 - val_loss: 0.1628 - val_accuracy: 0.9490\n",
            "Epoch 4549/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.9406\n",
            "Epoch 4549: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9414 - val_loss: 0.1481 - val_accuracy: 0.9590\n",
            "Epoch 4550/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9428\n",
            "Epoch 4550: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1257 - accuracy: 0.9428 - val_loss: 0.1347 - val_accuracy: 0.9664\n",
            "Epoch 4551/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1232 - accuracy: 0.9427\n",
            "Epoch 4551: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1239 - accuracy: 0.9425 - val_loss: 0.1905 - val_accuracy: 0.9406\n",
            "Epoch 4552/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1144 - accuracy: 0.9467\n",
            "Epoch 4552: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1140 - accuracy: 0.9472 - val_loss: 0.1442 - val_accuracy: 0.9573\n",
            "Epoch 4553/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9510\n",
            "Epoch 4553: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1065 - accuracy: 0.9510 - val_loss: 0.1414 - val_accuracy: 0.9527\n",
            "Epoch 4554/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1032 - accuracy: 0.9523\n",
            "Epoch 4554: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1044 - accuracy: 0.9518 - val_loss: 0.1161 - val_accuracy: 0.9788\n",
            "Epoch 4555/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1065 - accuracy: 0.9495\n",
            "Epoch 4555: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9491 - val_loss: 0.1750 - val_accuracy: 0.9438\n",
            "Epoch 4556/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9492\n",
            "Epoch 4556: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1111 - accuracy: 0.9491 - val_loss: 0.1856 - val_accuracy: 0.9337\n",
            "Epoch 4557/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9485\n",
            "Epoch 4557: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1120 - accuracy: 0.9485 - val_loss: 0.1223 - val_accuracy: 0.9690\n",
            "Epoch 4558/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1170 - accuracy: 0.9469\n",
            "Epoch 4558: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1175 - accuracy: 0.9465 - val_loss: 0.1402 - val_accuracy: 0.9653\n",
            "Epoch 4559/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9446\n",
            "Epoch 4559: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1191 - accuracy: 0.9446 - val_loss: 0.2021 - val_accuracy: 0.9315\n",
            "Epoch 4560/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9496\n",
            "Epoch 4560: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1111 - accuracy: 0.9496 - val_loss: 0.1523 - val_accuracy: 0.9538\n",
            "Epoch 4561/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1122 - accuracy: 0.9473\n",
            "Epoch 4561: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1124 - accuracy: 0.9475 - val_loss: 0.1142 - val_accuracy: 0.9735\n",
            "Epoch 4562/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1093 - accuracy: 0.9490\n",
            "Epoch 4562: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1112 - accuracy: 0.9484 - val_loss: 0.1180 - val_accuracy: 0.9774\n",
            "Epoch 4563/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1088 - accuracy: 0.9494\n",
            "Epoch 4563: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1090 - accuracy: 0.9495 - val_loss: 0.1651 - val_accuracy: 0.9447\n",
            "Epoch 4564/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9462\n",
            "Epoch 4564: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1149 - accuracy: 0.9462 - val_loss: 0.2003 - val_accuracy: 0.9350\n",
            "Epoch 4565/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1383 - accuracy: 0.9396\n",
            "Epoch 4565: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1417 - accuracy: 0.9377 - val_loss: 0.1331 - val_accuracy: 0.9642\n",
            "Epoch 4566/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9356\n",
            "Epoch 4566: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1430 - accuracy: 0.9361 - val_loss: 0.1560 - val_accuracy: 0.9521\n",
            "Epoch 4567/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9454\n",
            "Epoch 4567: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1187 - accuracy: 0.9454 - val_loss: 0.1471 - val_accuracy: 0.9614\n",
            "Epoch 4568/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9480\n",
            "Epoch 4568: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1100 - accuracy: 0.9482 - val_loss: 0.1251 - val_accuracy: 0.9660\n",
            "Epoch 4569/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1098 - accuracy: 0.9494\n",
            "Epoch 4569: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1102 - accuracy: 0.9490 - val_loss: 0.1292 - val_accuracy: 0.9725\n",
            "Epoch 4570/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9501\n",
            "Epoch 4570: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1060 - accuracy: 0.9500 - val_loss: 0.1131 - val_accuracy: 0.9783\n",
            "Epoch 4571/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1028 - accuracy: 0.9510\n",
            "Epoch 4571: loss did not improve from 0.10212\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1030 - accuracy: 0.9510 - val_loss: 0.1634 - val_accuracy: 0.9445\n",
            "Epoch 4572/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1015 - accuracy: 0.9527\n",
            "Epoch 4572: loss improved from 0.10212 to 0.10153, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 0.1015 - accuracy: 0.9530 - val_loss: 0.1442 - val_accuracy: 0.9631\n",
            "Epoch 4573/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1051 - accuracy: 0.9503\n",
            "Epoch 4573: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1050 - accuracy: 0.9503 - val_loss: 0.1270 - val_accuracy: 0.9720\n",
            "Epoch 4574/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1035 - accuracy: 0.9515\n",
            "Epoch 4574: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1042 - accuracy: 0.9511 - val_loss: 0.1867 - val_accuracy: 0.9293\n",
            "Epoch 4575/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9501\n",
            "Epoch 4575: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9502 - val_loss: 0.1240 - val_accuracy: 0.9710\n",
            "Epoch 4576/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9479\n",
            "Epoch 4576: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9479 - val_loss: 0.1172 - val_accuracy: 0.9766\n",
            "Epoch 4577/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9478\n",
            "Epoch 4577: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1103 - accuracy: 0.9478 - val_loss: 0.1109 - val_accuracy: 0.9783\n",
            "Epoch 4578/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9486\n",
            "Epoch 4578: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1113 - accuracy: 0.9490 - val_loss: 0.1147 - val_accuracy: 0.9855\n",
            "Epoch 4579/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1051 - accuracy: 0.9500\n",
            "Epoch 4579: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9501 - val_loss: 0.1568 - val_accuracy: 0.9469\n",
            "Epoch 4580/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9509\n",
            "Epoch 4580: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1065 - accuracy: 0.9507 - val_loss: 0.1285 - val_accuracy: 0.9710\n",
            "Epoch 4581/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9492\n",
            "Epoch 4581: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1112 - accuracy: 0.9492 - val_loss: 0.1479 - val_accuracy: 0.9592\n",
            "Epoch 4582/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1277 - accuracy: 0.9449\n",
            "Epoch 4582: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1299 - accuracy: 0.9431 - val_loss: 0.1749 - val_accuracy: 0.9427\n",
            "Epoch 4583/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9420\n",
            "Epoch 4583: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1318 - accuracy: 0.9402 - val_loss: 0.1721 - val_accuracy: 0.9404\n",
            "Epoch 4584/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1427 - accuracy: 0.9363\n",
            "Epoch 4584: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1423 - accuracy: 0.9366 - val_loss: 0.2212 - val_accuracy: 0.9176\n",
            "Epoch 4585/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1265 - accuracy: 0.9429\n",
            "Epoch 4585: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1279 - accuracy: 0.9420 - val_loss: 0.1985 - val_accuracy: 0.9363\n",
            "Epoch 4586/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9446\n",
            "Epoch 4586: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1185 - accuracy: 0.9448 - val_loss: 0.1310 - val_accuracy: 0.9729\n",
            "Epoch 4587/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9506\n",
            "Epoch 4587: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9506 - val_loss: 0.1721 - val_accuracy: 0.9451\n",
            "Epoch 4588/5000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1031 - accuracy: 0.9516\n",
            "Epoch 4588: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1051 - accuracy: 0.9509 - val_loss: 0.1319 - val_accuracy: 0.9629\n",
            "Epoch 4589/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9494\n",
            "Epoch 4589: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1074 - accuracy: 0.9500 - val_loss: 0.1442 - val_accuracy: 0.9564\n",
            "Epoch 4590/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1019 - accuracy: 0.9528\n",
            "Epoch 4590: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1029 - accuracy: 0.9524 - val_loss: 0.1130 - val_accuracy: 0.9757\n",
            "Epoch 4591/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9507\n",
            "Epoch 4591: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1072 - accuracy: 0.9507 - val_loss: 0.1682 - val_accuracy: 0.9540\n",
            "Epoch 4592/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9480\n",
            "Epoch 4592: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1128 - accuracy: 0.9470 - val_loss: 0.1441 - val_accuracy: 0.9608\n",
            "Epoch 4593/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1101 - accuracy: 0.9498\n",
            "Epoch 4593: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1097 - accuracy: 0.9501 - val_loss: 0.1122 - val_accuracy: 0.9772\n",
            "Epoch 4594/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9495\n",
            "Epoch 4594: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1079 - accuracy: 0.9495 - val_loss: 0.1503 - val_accuracy: 0.9532\n",
            "Epoch 4595/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9519\n",
            "Epoch 4595: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1027 - accuracy: 0.9517 - val_loss: 0.1288 - val_accuracy: 0.9612\n",
            "Epoch 4596/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9528\n",
            "Epoch 4596: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1038 - accuracy: 0.9525 - val_loss: 0.1315 - val_accuracy: 0.9686\n",
            "Epoch 4597/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9510\n",
            "Epoch 4597: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1052 - accuracy: 0.9510 - val_loss: 0.1360 - val_accuracy: 0.9647\n",
            "Epoch 4598/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1039 - accuracy: 0.9512\n",
            "Epoch 4598: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1041 - accuracy: 0.9513 - val_loss: 0.1315 - val_accuracy: 0.9657\n",
            "Epoch 4599/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9509\n",
            "Epoch 4599: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1030 - accuracy: 0.9508 - val_loss: 0.1129 - val_accuracy: 0.9774\n",
            "Epoch 4600/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1081 - accuracy: 0.9498\n",
            "Epoch 4600: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9501 - val_loss: 0.1330 - val_accuracy: 0.9640\n",
            "Epoch 4601/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9512\n",
            "Epoch 4601: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1046 - accuracy: 0.9512 - val_loss: 0.1253 - val_accuracy: 0.9757\n",
            "Epoch 4602/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9496\n",
            "Epoch 4602: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1075 - accuracy: 0.9496 - val_loss: 0.1832 - val_accuracy: 0.9298\n",
            "Epoch 4603/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 0.9480\n",
            "Epoch 4603: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9476 - val_loss: 0.1436 - val_accuracy: 0.9575\n",
            "Epoch 4604/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9483\n",
            "Epoch 4604: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1113 - accuracy: 0.9484 - val_loss: 0.1472 - val_accuracy: 0.9607\n",
            "Epoch 4605/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1160 - accuracy: 0.9481\n",
            "Epoch 4605: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1163 - accuracy: 0.9477 - val_loss: 0.1772 - val_accuracy: 0.9350\n",
            "Epoch 4606/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1071 - accuracy: 0.9504\n",
            "Epoch 4606: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1080 - accuracy: 0.9499 - val_loss: 0.1057 - val_accuracy: 0.9772\n",
            "Epoch 4607/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1045 - accuracy: 0.9512\n",
            "Epoch 4607: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1058 - accuracy: 0.9509 - val_loss: 0.1490 - val_accuracy: 0.9530\n",
            "Epoch 4608/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9491\n",
            "Epoch 4608: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1092 - accuracy: 0.9491 - val_loss: 0.1175 - val_accuracy: 0.9707\n",
            "Epoch 4609/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9502\n",
            "Epoch 4609: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9500 - val_loss: 0.1664 - val_accuracy: 0.9508\n",
            "Epoch 4610/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9493\n",
            "Epoch 4610: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1100 - accuracy: 0.9490 - val_loss: 0.1403 - val_accuracy: 0.9599\n",
            "Epoch 4611/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1073 - accuracy: 0.9501\n",
            "Epoch 4611: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9500 - val_loss: 0.1050 - val_accuracy: 0.9814\n",
            "Epoch 4612/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9499\n",
            "Epoch 4612: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1075 - accuracy: 0.9501 - val_loss: 0.1712 - val_accuracy: 0.9454\n",
            "Epoch 4613/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1053 - accuracy: 0.9509\n",
            "Epoch 4613: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1059 - accuracy: 0.9510 - val_loss: 0.1192 - val_accuracy: 0.9746\n",
            "Epoch 4614/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9492\n",
            "Epoch 4614: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1100 - accuracy: 0.9494 - val_loss: 0.1360 - val_accuracy: 0.9612\n",
            "Epoch 4615/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9487\n",
            "Epoch 4615: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9485 - val_loss: 0.1357 - val_accuracy: 0.9649\n",
            "Epoch 4616/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9480\n",
            "Epoch 4616: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1134 - accuracy: 0.9480 - val_loss: 0.1782 - val_accuracy: 0.9406\n",
            "Epoch 4617/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9431\n",
            "Epoch 4617: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1244 - accuracy: 0.9432 - val_loss: 0.1248 - val_accuracy: 0.9740\n",
            "Epoch 4618/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9463\n",
            "Epoch 4618: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9463 - val_loss: 0.1283 - val_accuracy: 0.9697\n",
            "Epoch 4619/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9477\n",
            "Epoch 4619: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1116 - accuracy: 0.9477 - val_loss: 0.1427 - val_accuracy: 0.9566\n",
            "Epoch 4620/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9501\n",
            "Epoch 4620: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1082 - accuracy: 0.9500 - val_loss: 0.1448 - val_accuracy: 0.9579\n",
            "Epoch 4621/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9471\n",
            "Epoch 4621: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9466 - val_loss: 0.1989 - val_accuracy: 0.9180\n",
            "Epoch 4622/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9454\n",
            "Epoch 4622: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1183 - accuracy: 0.9456 - val_loss: 0.1114 - val_accuracy: 0.9807\n",
            "Epoch 4623/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1109 - accuracy: 0.9484\n",
            "Epoch 4623: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1115 - accuracy: 0.9486 - val_loss: 0.1336 - val_accuracy: 0.9714\n",
            "Epoch 4624/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1056 - accuracy: 0.9504\n",
            "Epoch 4624: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9495 - val_loss: 0.1242 - val_accuracy: 0.9720\n",
            "Epoch 4625/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9487\n",
            "Epoch 4625: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1124 - accuracy: 0.9487 - val_loss: 0.1859 - val_accuracy: 0.9350\n",
            "Epoch 4626/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1041 - accuracy: 0.9538\n",
            "Epoch 4626: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9526 - val_loss: 0.1158 - val_accuracy: 0.9761\n",
            "Epoch 4627/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1080 - accuracy: 0.9497\n",
            "Epoch 4627: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1079 - accuracy: 0.9496 - val_loss: 0.1230 - val_accuracy: 0.9722\n",
            "Epoch 4628/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1160 - accuracy: 0.9460\n",
            "Epoch 4628: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1161 - accuracy: 0.9459 - val_loss: 0.1392 - val_accuracy: 0.9575\n",
            "Epoch 4629/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1147 - accuracy: 0.9475\n",
            "Epoch 4629: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1146 - accuracy: 0.9476 - val_loss: 0.1358 - val_accuracy: 0.9627\n",
            "Epoch 4630/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9494\n",
            "Epoch 4630: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9493 - val_loss: 0.1500 - val_accuracy: 0.9521\n",
            "Epoch 4631/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9493\n",
            "Epoch 4631: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1083 - accuracy: 0.9491 - val_loss: 0.1312 - val_accuracy: 0.9644\n",
            "Epoch 4632/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1088 - accuracy: 0.9489\n",
            "Epoch 4632: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1100 - accuracy: 0.9490 - val_loss: 0.1277 - val_accuracy: 0.9677\n",
            "Epoch 4633/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1136 - accuracy: 0.9470\n",
            "Epoch 4633: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1134 - accuracy: 0.9470 - val_loss: 0.1403 - val_accuracy: 0.9633\n",
            "Epoch 4634/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1138 - accuracy: 0.9470\n",
            "Epoch 4634: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1136 - accuracy: 0.9472 - val_loss: 0.1331 - val_accuracy: 0.9670\n",
            "Epoch 4635/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1176 - accuracy: 0.9466\n",
            "Epoch 4635: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1161 - accuracy: 0.9477 - val_loss: 0.1457 - val_accuracy: 0.9582\n",
            "Epoch 4636/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9491\n",
            "Epoch 4636: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1090 - accuracy: 0.9491 - val_loss: 0.1742 - val_accuracy: 0.9404\n",
            "Epoch 4637/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1078 - accuracy: 0.9490\n",
            "Epoch 4637: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1079 - accuracy: 0.9495 - val_loss: 0.1571 - val_accuracy: 0.9467\n",
            "Epoch 4638/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9488\n",
            "Epoch 4638: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1070 - accuracy: 0.9488 - val_loss: 0.1401 - val_accuracy: 0.9610\n",
            "Epoch 4639/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1052 - accuracy: 0.9506\n",
            "Epoch 4639: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9507 - val_loss: 0.1324 - val_accuracy: 0.9677\n",
            "Epoch 4640/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9478\n",
            "Epoch 4640: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1123 - accuracy: 0.9478 - val_loss: 0.1713 - val_accuracy: 0.9369\n",
            "Epoch 4641/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1127 - accuracy: 0.9470\n",
            "Epoch 4641: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9472 - val_loss: 0.1964 - val_accuracy: 0.9362\n",
            "Epoch 4642/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1144 - accuracy: 0.9466\n",
            "Epoch 4642: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1151 - accuracy: 0.9464 - val_loss: 0.1546 - val_accuracy: 0.9501\n",
            "Epoch 4643/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9503\n",
            "Epoch 4643: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9502 - val_loss: 0.1212 - val_accuracy: 0.9681\n",
            "Epoch 4644/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9489\n",
            "Epoch 4644: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1129 - accuracy: 0.9489 - val_loss: 0.1699 - val_accuracy: 0.9447\n",
            "Epoch 4645/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9442\n",
            "Epoch 4645: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1238 - accuracy: 0.9442 - val_loss: 0.1142 - val_accuracy: 0.9762\n",
            "Epoch 4646/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1156 - accuracy: 0.9466\n",
            "Epoch 4646: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1153 - accuracy: 0.9466 - val_loss: 0.1920 - val_accuracy: 0.9276\n",
            "Epoch 4647/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9492\n",
            "Epoch 4647: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1083 - accuracy: 0.9492 - val_loss: 0.1064 - val_accuracy: 0.9809\n",
            "Epoch 4648/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9525\n",
            "Epoch 4648: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1041 - accuracy: 0.9525 - val_loss: 0.1353 - val_accuracy: 0.9636\n",
            "Epoch 4649/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9506\n",
            "Epoch 4649: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1069 - accuracy: 0.9498 - val_loss: 0.1767 - val_accuracy: 0.9341\n",
            "Epoch 4650/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9487\n",
            "Epoch 4650: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1093 - accuracy: 0.9491 - val_loss: 0.1440 - val_accuracy: 0.9653\n",
            "Epoch 4651/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9442\n",
            "Epoch 4651: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1236 - accuracy: 0.9444 - val_loss: 0.1437 - val_accuracy: 0.9614\n",
            "Epoch 4652/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1207 - accuracy: 0.9456\n",
            "Epoch 4652: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1220 - accuracy: 0.9446 - val_loss: 0.1294 - val_accuracy: 0.9657\n",
            "Epoch 4653/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9458\n",
            "Epoch 4653: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1171 - accuracy: 0.9458 - val_loss: 0.1442 - val_accuracy: 0.9657\n",
            "Epoch 4654/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9478\n",
            "Epoch 4654: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1128 - accuracy: 0.9478 - val_loss: 0.1426 - val_accuracy: 0.9620\n",
            "Epoch 4655/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9515\n",
            "Epoch 4655: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1068 - accuracy: 0.9515 - val_loss: 0.1267 - val_accuracy: 0.9701\n",
            "Epoch 4656/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9501\n",
            "Epoch 4656: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1069 - accuracy: 0.9501 - val_loss: 0.1917 - val_accuracy: 0.9300\n",
            "Epoch 4657/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1076 - accuracy: 0.9493\n",
            "Epoch 4657: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1085 - accuracy: 0.9486 - val_loss: 0.1209 - val_accuracy: 0.9772\n",
            "Epoch 4658/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1015 - accuracy: 0.9521\n",
            "Epoch 4658: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1022 - accuracy: 0.9521 - val_loss: 0.1309 - val_accuracy: 0.9649\n",
            "Epoch 4659/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9515\n",
            "Epoch 4659: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1028 - accuracy: 0.9515 - val_loss: 0.2177 - val_accuracy: 0.9195\n",
            "Epoch 4660/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1122 - accuracy: 0.9478\n",
            "Epoch 4660: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1124 - accuracy: 0.9476 - val_loss: 0.1663 - val_accuracy: 0.9428\n",
            "Epoch 4661/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1068 - accuracy: 0.9495\n",
            "Epoch 4661: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1069 - accuracy: 0.9494 - val_loss: 0.1610 - val_accuracy: 0.9510\n",
            "Epoch 4662/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1121 - accuracy: 0.9484\n",
            "Epoch 4662: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1128 - accuracy: 0.9480 - val_loss: 0.1750 - val_accuracy: 0.9462\n",
            "Epoch 4663/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1128 - accuracy: 0.9479\n",
            "Epoch 4663: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1147 - accuracy: 0.9473 - val_loss: 0.1680 - val_accuracy: 0.9389\n",
            "Epoch 4664/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1106 - accuracy: 0.9491\n",
            "Epoch 4664: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1123 - accuracy: 0.9485 - val_loss: 0.1278 - val_accuracy: 0.9690\n",
            "Epoch 4665/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9501\n",
            "Epoch 4665: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1061 - accuracy: 0.9501 - val_loss: 0.1340 - val_accuracy: 0.9629\n",
            "Epoch 4666/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1001 - accuracy: 0.9532\n",
            "Epoch 4666: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1015 - accuracy: 0.9527 - val_loss: 0.1430 - val_accuracy: 0.9584\n",
            "Epoch 4667/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1013 - accuracy: 0.9530\n",
            "Epoch 4667: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1023 - accuracy: 0.9523 - val_loss: 0.1157 - val_accuracy: 0.9777\n",
            "Epoch 4668/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9512\n",
            "Epoch 4668: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1016 - accuracy: 0.9516 - val_loss: 0.1356 - val_accuracy: 0.9586\n",
            "Epoch 4669/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9513\n",
            "Epoch 4669: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1057 - accuracy: 0.9513 - val_loss: 0.1661 - val_accuracy: 0.9438\n",
            "Epoch 4670/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9472\n",
            "Epoch 4670: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1142 - accuracy: 0.9472 - val_loss: 0.2761 - val_accuracy: 0.8998\n",
            "Epoch 4671/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9442\n",
            "Epoch 4671: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1197 - accuracy: 0.9444 - val_loss: 0.1224 - val_accuracy: 0.9742\n",
            "Epoch 4672/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1119 - accuracy: 0.9476\n",
            "Epoch 4672: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1118 - accuracy: 0.9476 - val_loss: 0.1931 - val_accuracy: 0.9245\n",
            "Epoch 4673/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9504\n",
            "Epoch 4673: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1080 - accuracy: 0.9498 - val_loss: 0.1268 - val_accuracy: 0.9668\n",
            "Epoch 4674/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9512\n",
            "Epoch 4674: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1037 - accuracy: 0.9512 - val_loss: 0.1305 - val_accuracy: 0.9670\n",
            "Epoch 4675/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1094 - accuracy: 0.9495\n",
            "Epoch 4675: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1089 - accuracy: 0.9494 - val_loss: 0.1141 - val_accuracy: 0.9766\n",
            "Epoch 4676/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9504\n",
            "Epoch 4676: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1062 - accuracy: 0.9504 - val_loss: 0.1580 - val_accuracy: 0.9458\n",
            "Epoch 4677/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1049 - accuracy: 0.9506\n",
            "Epoch 4677: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1052 - accuracy: 0.9507 - val_loss: 0.1003 - val_accuracy: 0.9829\n",
            "Epoch 4678/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1076 - accuracy: 0.9501\n",
            "Epoch 4678: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1079 - accuracy: 0.9501 - val_loss: 0.1278 - val_accuracy: 0.9709\n",
            "Epoch 4679/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1046 - accuracy: 0.9507\n",
            "Epoch 4679: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1049 - accuracy: 0.9505 - val_loss: 0.1548 - val_accuracy: 0.9478\n",
            "Epoch 4680/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1048 - accuracy: 0.9506\n",
            "Epoch 4680: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9502 - val_loss: 0.1850 - val_accuracy: 0.9317\n",
            "Epoch 4681/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9507\n",
            "Epoch 4681: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1087 - accuracy: 0.9507 - val_loss: 0.1461 - val_accuracy: 0.9735\n",
            "Epoch 4682/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9440\n",
            "Epoch 4682: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 0.9440 - val_loss: 0.1374 - val_accuracy: 0.9620\n",
            "Epoch 4683/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1219 - accuracy: 0.9433\n",
            "Epoch 4683: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1212 - accuracy: 0.9438 - val_loss: 0.1617 - val_accuracy: 0.9564\n",
            "Epoch 4684/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1067 - accuracy: 0.9494\n",
            "Epoch 4684: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1088 - accuracy: 0.9485 - val_loss: 0.1303 - val_accuracy: 0.9720\n",
            "Epoch 4685/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9472\n",
            "Epoch 4685: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1161 - accuracy: 0.9472 - val_loss: 0.1861 - val_accuracy: 0.9336\n",
            "Epoch 4686/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9450\n",
            "Epoch 4686: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1184 - accuracy: 0.9450 - val_loss: 0.1584 - val_accuracy: 0.9475\n",
            "Epoch 4687/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9476\n",
            "Epoch 4687: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1118 - accuracy: 0.9476 - val_loss: 0.2086 - val_accuracy: 0.9250\n",
            "Epoch 4688/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1151 - accuracy: 0.9470\n",
            "Epoch 4688: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1157 - accuracy: 0.9471 - val_loss: 0.1383 - val_accuracy: 0.9631\n",
            "Epoch 4689/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1139 - accuracy: 0.9467\n",
            "Epoch 4689: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9465 - val_loss: 0.1451 - val_accuracy: 0.9560\n",
            "Epoch 4690/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1197 - accuracy: 0.9445\n",
            "Epoch 4690: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1192 - accuracy: 0.9448 - val_loss: 0.1736 - val_accuracy: 0.9503\n",
            "Epoch 4691/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1194 - accuracy: 0.9447\n",
            "Epoch 4691: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1188 - accuracy: 0.9447 - val_loss: 0.1330 - val_accuracy: 0.9697\n",
            "Epoch 4692/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1086 - accuracy: 0.9494\n",
            "Epoch 4692: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1117 - accuracy: 0.9483 - val_loss: 0.1251 - val_accuracy: 0.9735\n",
            "Epoch 4693/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1085 - accuracy: 0.9505\n",
            "Epoch 4693: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1089 - accuracy: 0.9498 - val_loss: 0.1351 - val_accuracy: 0.9636\n",
            "Epoch 4694/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1044 - accuracy: 0.9506\n",
            "Epoch 4694: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1043 - accuracy: 0.9505 - val_loss: 0.1489 - val_accuracy: 0.9521\n",
            "Epoch 4695/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9510\n",
            "Epoch 4695: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1045 - accuracy: 0.9514 - val_loss: 0.1673 - val_accuracy: 0.9451\n",
            "Epoch 4696/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1086 - accuracy: 0.9502\n",
            "Epoch 4696: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1086 - accuracy: 0.9503 - val_loss: 0.1091 - val_accuracy: 0.9798\n",
            "Epoch 4697/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9486\n",
            "Epoch 4697: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1114 - accuracy: 0.9481 - val_loss: 0.1355 - val_accuracy: 0.9707\n",
            "Epoch 4698/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9487\n",
            "Epoch 4698: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1109 - accuracy: 0.9484 - val_loss: 0.1307 - val_accuracy: 0.9671\n",
            "Epoch 4699/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1056 - accuracy: 0.9503\n",
            "Epoch 4699: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1069 - accuracy: 0.9497 - val_loss: 0.1944 - val_accuracy: 0.9272\n",
            "Epoch 4700/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9498\n",
            "Epoch 4700: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1122 - accuracy: 0.9490 - val_loss: 0.1430 - val_accuracy: 0.9610\n",
            "Epoch 4701/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9499\n",
            "Epoch 4701: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9499 - val_loss: 0.1122 - val_accuracy: 0.9824\n",
            "Epoch 4702/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1044 - accuracy: 0.9509\n",
            "Epoch 4702: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1046 - accuracy: 0.9510 - val_loss: 0.1351 - val_accuracy: 0.9725\n",
            "Epoch 4703/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1088 - accuracy: 0.9486\n",
            "Epoch 4703: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1102 - accuracy: 0.9483 - val_loss: 0.1414 - val_accuracy: 0.9647\n",
            "Epoch 4704/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9495\n",
            "Epoch 4704: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1101 - accuracy: 0.9496 - val_loss: 0.1606 - val_accuracy: 0.9538\n",
            "Epoch 4705/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1069 - accuracy: 0.9507\n",
            "Epoch 4705: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1091 - accuracy: 0.9497 - val_loss: 0.1203 - val_accuracy: 0.9788\n",
            "Epoch 4706/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9489\n",
            "Epoch 4706: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1112 - accuracy: 0.9489 - val_loss: 0.1514 - val_accuracy: 0.9592\n",
            "Epoch 4707/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9487\n",
            "Epoch 4707: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1105 - accuracy: 0.9485 - val_loss: 0.1310 - val_accuracy: 0.9688\n",
            "Epoch 4708/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9500\n",
            "Epoch 4708: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1083 - accuracy: 0.9494 - val_loss: 0.1080 - val_accuracy: 0.9796\n",
            "Epoch 4709/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9471\n",
            "Epoch 4709: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1150 - accuracy: 0.9471 - val_loss: 0.1356 - val_accuracy: 0.9636\n",
            "Epoch 4710/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9470\n",
            "Epoch 4710: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1163 - accuracy: 0.9470 - val_loss: 0.1576 - val_accuracy: 0.9447\n",
            "Epoch 4711/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9486\n",
            "Epoch 4711: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1099 - accuracy: 0.9480 - val_loss: 0.1587 - val_accuracy: 0.9512\n",
            "Epoch 4712/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1099 - accuracy: 0.9470\n",
            "Epoch 4712: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1106 - accuracy: 0.9472 - val_loss: 0.1098 - val_accuracy: 0.9872\n",
            "Epoch 4713/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1110 - accuracy: 0.9478\n",
            "Epoch 4713: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1114 - accuracy: 0.9480 - val_loss: 0.1323 - val_accuracy: 0.9692\n",
            "Epoch 4714/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1143 - accuracy: 0.9463\n",
            "Epoch 4714: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1132 - accuracy: 0.9467 - val_loss: 0.1572 - val_accuracy: 0.9523\n",
            "Epoch 4715/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9483\n",
            "Epoch 4715: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1131 - accuracy: 0.9483 - val_loss: 0.1338 - val_accuracy: 0.9610\n",
            "Epoch 4716/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1134 - accuracy: 0.9472\n",
            "Epoch 4716: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1164 - accuracy: 0.9461 - val_loss: 0.1396 - val_accuracy: 0.9614\n",
            "Epoch 4717/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1283 - accuracy: 0.9420\n",
            "Epoch 4717: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9418 - val_loss: 0.1433 - val_accuracy: 0.9631\n",
            "Epoch 4718/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1203 - accuracy: 0.9438\n",
            "Epoch 4718: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1217 - accuracy: 0.9431 - val_loss: 0.1954 - val_accuracy: 0.9289\n",
            "Epoch 4719/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1188 - accuracy: 0.9454\n",
            "Epoch 4719: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1205 - accuracy: 0.9450 - val_loss: 0.1839 - val_accuracy: 0.9378\n",
            "Epoch 4720/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1184 - accuracy: 0.9470\n",
            "Epoch 4720: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1186 - accuracy: 0.9465 - val_loss: 0.1359 - val_accuracy: 0.9677\n",
            "Epoch 4721/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9467\n",
            "Epoch 4721: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1161 - accuracy: 0.9467 - val_loss: 0.0982 - val_accuracy: 0.9861\n",
            "Epoch 4722/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1103 - accuracy: 0.9483\n",
            "Epoch 4722: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1120 - accuracy: 0.9475 - val_loss: 0.1539 - val_accuracy: 0.9503\n",
            "Epoch 4723/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1177 - accuracy: 0.9475\n",
            "Epoch 4723: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1169 - accuracy: 0.9476 - val_loss: 0.1310 - val_accuracy: 0.9673\n",
            "Epoch 4724/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1119 - accuracy: 0.9482\n",
            "Epoch 4724: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1124 - accuracy: 0.9479 - val_loss: 0.1360 - val_accuracy: 0.9620\n",
            "Epoch 4725/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1071 - accuracy: 0.9503\n",
            "Epoch 4725: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1072 - accuracy: 0.9500 - val_loss: 0.1114 - val_accuracy: 0.9805\n",
            "Epoch 4726/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1047 - accuracy: 0.9516\n",
            "Epoch 4726: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1048 - accuracy: 0.9517 - val_loss: 0.1502 - val_accuracy: 0.9536\n",
            "Epoch 4727/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1047 - accuracy: 0.9499\n",
            "Epoch 4727: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1056 - accuracy: 0.9493 - val_loss: 0.1510 - val_accuracy: 0.9501\n",
            "Epoch 4728/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1104 - accuracy: 0.9488\n",
            "Epoch 4728: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1114 - accuracy: 0.9481 - val_loss: 0.1699 - val_accuracy: 0.9375\n",
            "Epoch 4729/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9479\n",
            "Epoch 4729: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1099 - accuracy: 0.9483 - val_loss: 0.1494 - val_accuracy: 0.9573\n",
            "Epoch 4730/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9480\n",
            "Epoch 4730: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1090 - accuracy: 0.9481 - val_loss: 0.1405 - val_accuracy: 0.9612\n",
            "Epoch 4731/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1049 - accuracy: 0.9508\n",
            "Epoch 4731: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1065 - accuracy: 0.9497 - val_loss: 0.1453 - val_accuracy: 0.9577\n",
            "Epoch 4732/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1045 - accuracy: 0.9517\n",
            "Epoch 4732: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1055 - accuracy: 0.9512 - val_loss: 0.2082 - val_accuracy: 0.9156\n",
            "Epoch 4733/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9492\n",
            "Epoch 4733: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1121 - accuracy: 0.9490 - val_loss: 0.1374 - val_accuracy: 0.9582\n",
            "Epoch 4734/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9501\n",
            "Epoch 4734: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1070 - accuracy: 0.9501 - val_loss: 0.1642 - val_accuracy: 0.9464\n",
            "Epoch 4735/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1082 - accuracy: 0.9498\n",
            "Epoch 4735: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1084 - accuracy: 0.9497 - val_loss: 0.1289 - val_accuracy: 0.9657\n",
            "Epoch 4736/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1116 - accuracy: 0.9486\n",
            "Epoch 4736: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1114 - accuracy: 0.9484 - val_loss: 0.1633 - val_accuracy: 0.9428\n",
            "Epoch 4737/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1030 - accuracy: 0.9516\n",
            "Epoch 4737: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1045 - accuracy: 0.9511 - val_loss: 0.1133 - val_accuracy: 0.9809\n",
            "Epoch 4738/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9513\n",
            "Epoch 4738: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1045 - accuracy: 0.9517 - val_loss: 0.1139 - val_accuracy: 0.9779\n",
            "Epoch 4739/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1041 - accuracy: 0.9524\n",
            "Epoch 4739: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1048 - accuracy: 0.9516 - val_loss: 0.1282 - val_accuracy: 0.9703\n",
            "Epoch 4740/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1071 - accuracy: 0.9501\n",
            "Epoch 4740: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1079 - accuracy: 0.9499 - val_loss: 0.1060 - val_accuracy: 0.9805\n",
            "Epoch 4741/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1060 - accuracy: 0.9511\n",
            "Epoch 4741: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1067 - accuracy: 0.9506 - val_loss: 0.1530 - val_accuracy: 0.9553\n",
            "Epoch 4742/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1041 - accuracy: 0.9517\n",
            "Epoch 4742: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1046 - accuracy: 0.9515 - val_loss: 0.1306 - val_accuracy: 0.9673\n",
            "Epoch 4743/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1082 - accuracy: 0.9493\n",
            "Epoch 4743: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9493 - val_loss: 0.1400 - val_accuracy: 0.9568\n",
            "Epoch 4744/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1082 - accuracy: 0.9494\n",
            "Epoch 4744: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9498 - val_loss: 0.1461 - val_accuracy: 0.9523\n",
            "Epoch 4745/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9491\n",
            "Epoch 4745: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1092 - accuracy: 0.9491 - val_loss: 0.1443 - val_accuracy: 0.9540\n",
            "Epoch 4746/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1082 - accuracy: 0.9498\n",
            "Epoch 4746: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1085 - accuracy: 0.9497 - val_loss: 0.1181 - val_accuracy: 0.9781\n",
            "Epoch 4747/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9504\n",
            "Epoch 4747: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1044 - accuracy: 0.9504 - val_loss: 0.1251 - val_accuracy: 0.9709\n",
            "Epoch 4748/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9456\n",
            "Epoch 4748: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1188 - accuracy: 0.9452 - val_loss: 0.2766 - val_accuracy: 0.8911\n",
            "Epoch 4749/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1306 - accuracy: 0.9410\n",
            "Epoch 4749: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1321 - accuracy: 0.9400 - val_loss: 0.1868 - val_accuracy: 0.9378\n",
            "Epoch 4750/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1233 - accuracy: 0.9429\n",
            "Epoch 4750: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1262 - accuracy: 0.9427 - val_loss: 0.1600 - val_accuracy: 0.9517\n",
            "Epoch 4751/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1303 - accuracy: 0.9409\n",
            "Epoch 4751: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1298 - accuracy: 0.9411 - val_loss: 0.1160 - val_accuracy: 0.9777\n",
            "Epoch 4752/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9463\n",
            "Epoch 4752: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1180 - accuracy: 0.9458 - val_loss: 0.1510 - val_accuracy: 0.9560\n",
            "Epoch 4753/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1138 - accuracy: 0.9479\n",
            "Epoch 4753: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1141 - accuracy: 0.9477 - val_loss: 0.1390 - val_accuracy: 0.9616\n",
            "Epoch 4754/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1065 - accuracy: 0.9512\n",
            "Epoch 4754: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1071 - accuracy: 0.9510 - val_loss: 0.1566 - val_accuracy: 0.9480\n",
            "Epoch 4755/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9514\n",
            "Epoch 4755: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1058 - accuracy: 0.9511 - val_loss: 0.1358 - val_accuracy: 0.9634\n",
            "Epoch 4756/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9493\n",
            "Epoch 4756: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1099 - accuracy: 0.9489 - val_loss: 0.1249 - val_accuracy: 0.9642\n",
            "Epoch 4757/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9491\n",
            "Epoch 4757: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1078 - accuracy: 0.9491 - val_loss: 0.1211 - val_accuracy: 0.9709\n",
            "Epoch 4758/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9514\n",
            "Epoch 4758: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1058 - accuracy: 0.9509 - val_loss: 0.1206 - val_accuracy: 0.9720\n",
            "Epoch 4759/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9511\n",
            "Epoch 4759: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1042 - accuracy: 0.9511 - val_loss: 0.1420 - val_accuracy: 0.9582\n",
            "Epoch 4760/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1099 - accuracy: 0.9495\n",
            "Epoch 4760: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1097 - accuracy: 0.9495 - val_loss: 0.1386 - val_accuracy: 0.9614\n",
            "Epoch 4761/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1096 - accuracy: 0.9492\n",
            "Epoch 4761: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1107 - accuracy: 0.9486 - val_loss: 0.1284 - val_accuracy: 0.9684\n",
            "Epoch 4762/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9468\n",
            "Epoch 4762: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1164 - accuracy: 0.9468 - val_loss: 0.1482 - val_accuracy: 0.9560\n",
            "Epoch 4763/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9488\n",
            "Epoch 4763: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1112 - accuracy: 0.9488 - val_loss: 0.1253 - val_accuracy: 0.9697\n",
            "Epoch 4764/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9470\n",
            "Epoch 4764: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1134 - accuracy: 0.9470 - val_loss: 0.1319 - val_accuracy: 0.9640\n",
            "Epoch 4765/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9485\n",
            "Epoch 4765: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1114 - accuracy: 0.9485 - val_loss: 0.1251 - val_accuracy: 0.9714\n",
            "Epoch 4766/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9443\n",
            "Epoch 4766: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1194 - accuracy: 0.9443 - val_loss: 0.1023 - val_accuracy: 0.9848\n",
            "Epoch 4767/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1155 - accuracy: 0.9482\n",
            "Epoch 4767: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1157 - accuracy: 0.9481 - val_loss: 0.1527 - val_accuracy: 0.9499\n",
            "Epoch 4768/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1064 - accuracy: 0.9517\n",
            "Epoch 4768: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9509 - val_loss: 0.1542 - val_accuracy: 0.9543\n",
            "Epoch 4769/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9486\n",
            "Epoch 4769: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1076 - accuracy: 0.9479 - val_loss: 0.1269 - val_accuracy: 0.9742\n",
            "Epoch 4770/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1076 - accuracy: 0.9504\n",
            "Epoch 4770: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1076 - accuracy: 0.9501 - val_loss: 0.1383 - val_accuracy: 0.9616\n",
            "Epoch 4771/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9504\n",
            "Epoch 4771: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1063 - accuracy: 0.9507 - val_loss: 0.1102 - val_accuracy: 0.9762\n",
            "Epoch 4772/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9490\n",
            "Epoch 4772: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1081 - accuracy: 0.9487 - val_loss: 0.1792 - val_accuracy: 0.9293\n",
            "Epoch 4773/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1114 - accuracy: 0.9478\n",
            "Epoch 4773: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1111 - accuracy: 0.9481 - val_loss: 0.1291 - val_accuracy: 0.9718\n",
            "Epoch 4774/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9477\n",
            "Epoch 4774: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9479 - val_loss: 0.1244 - val_accuracy: 0.9634\n",
            "Epoch 4775/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1093 - accuracy: 0.9491\n",
            "Epoch 4775: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1089 - accuracy: 0.9492 - val_loss: 0.1089 - val_accuracy: 0.9785\n",
            "Epoch 4776/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9509\n",
            "Epoch 4776: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1041 - accuracy: 0.9508 - val_loss: 0.1441 - val_accuracy: 0.9582\n",
            "Epoch 4777/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1051 - accuracy: 0.9512\n",
            "Epoch 4777: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1058 - accuracy: 0.9508 - val_loss: 0.1418 - val_accuracy: 0.9568\n",
            "Epoch 4778/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1039 - accuracy: 0.9503\n",
            "Epoch 4778: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1029 - accuracy: 0.9513 - val_loss: 0.1117 - val_accuracy: 0.9803\n",
            "Epoch 4779/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9506\n",
            "Epoch 4779: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1055 - accuracy: 0.9503 - val_loss: 0.1646 - val_accuracy: 0.9484\n",
            "Epoch 4780/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1091 - accuracy: 0.9494\n",
            "Epoch 4780: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1091 - accuracy: 0.9494 - val_loss: 0.1428 - val_accuracy: 0.9660\n",
            "Epoch 4781/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9450\n",
            "Epoch 4781: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1191 - accuracy: 0.9452 - val_loss: 0.1267 - val_accuracy: 0.9779\n",
            "Epoch 4782/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1229 - accuracy: 0.9441\n",
            "Epoch 4782: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1218 - accuracy: 0.9443 - val_loss: 0.1209 - val_accuracy: 0.9746\n",
            "Epoch 4783/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9503\n",
            "Epoch 4783: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1087 - accuracy: 0.9503 - val_loss: 0.1361 - val_accuracy: 0.9696\n",
            "Epoch 4784/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1062 - accuracy: 0.9510\n",
            "Epoch 4784: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1063 - accuracy: 0.9506 - val_loss: 0.1831 - val_accuracy: 0.9354\n",
            "Epoch 4785/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9495\n",
            "Epoch 4785: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1117 - accuracy: 0.9485 - val_loss: 0.1366 - val_accuracy: 0.9614\n",
            "Epoch 4786/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1118 - accuracy: 0.9481\n",
            "Epoch 4786: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1122 - accuracy: 0.9478 - val_loss: 0.1230 - val_accuracy: 0.9742\n",
            "Epoch 4787/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1095 - accuracy: 0.9491\n",
            "Epoch 4787: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1103 - accuracy: 0.9486 - val_loss: 0.1706 - val_accuracy: 0.9397\n",
            "Epoch 4788/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9469\n",
            "Epoch 4788: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1167 - accuracy: 0.9469 - val_loss: 0.1232 - val_accuracy: 0.9709\n",
            "Epoch 4789/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1162 - accuracy: 0.9459\n",
            "Epoch 4789: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1159 - accuracy: 0.9459 - val_loss: 0.1311 - val_accuracy: 0.9666\n",
            "Epoch 4790/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9503\n",
            "Epoch 4790: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1081 - accuracy: 0.9499 - val_loss: 0.2566 - val_accuracy: 0.8998\n",
            "Epoch 4791/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1149 - accuracy: 0.9468\n",
            "Epoch 4791: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1166 - accuracy: 0.9457 - val_loss: 0.1620 - val_accuracy: 0.9530\n",
            "Epoch 4792/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9481\n",
            "Epoch 4792: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1134 - accuracy: 0.9483 - val_loss: 0.1366 - val_accuracy: 0.9640\n",
            "Epoch 4793/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1191 - accuracy: 0.9470\n",
            "Epoch 4793: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1191 - accuracy: 0.9467 - val_loss: 0.1426 - val_accuracy: 0.9582\n",
            "Epoch 4794/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1106 - accuracy: 0.9491\n",
            "Epoch 4794: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1107 - accuracy: 0.9491 - val_loss: 0.1285 - val_accuracy: 0.9731\n",
            "Epoch 4795/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9522\n",
            "Epoch 4795: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1044 - accuracy: 0.9515 - val_loss: 0.1293 - val_accuracy: 0.9675\n",
            "Epoch 4796/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9529\n",
            "Epoch 4796: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1026 - accuracy: 0.9526 - val_loss: 0.1294 - val_accuracy: 0.9735\n",
            "Epoch 4797/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1075 - accuracy: 0.9513\n",
            "Epoch 4797: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9515 - val_loss: 0.1194 - val_accuracy: 0.9688\n",
            "Epoch 4798/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9501\n",
            "Epoch 4798: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1062 - accuracy: 0.9504 - val_loss: 0.1250 - val_accuracy: 0.9677\n",
            "Epoch 4799/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1073 - accuracy: 0.9493\n",
            "Epoch 4799: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1088 - accuracy: 0.9490 - val_loss: 0.1000 - val_accuracy: 0.9846\n",
            "Epoch 4800/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1116 - accuracy: 0.9485\n",
            "Epoch 4800: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1119 - accuracy: 0.9483 - val_loss: 0.1300 - val_accuracy: 0.9614\n",
            "Epoch 4801/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9501\n",
            "Epoch 4801: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1075 - accuracy: 0.9501 - val_loss: 0.1213 - val_accuracy: 0.9710\n",
            "Epoch 4802/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9507\n",
            "Epoch 4802: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1048 - accuracy: 0.9507 - val_loss: 0.1545 - val_accuracy: 0.9556\n",
            "Epoch 4803/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9504\n",
            "Epoch 4803: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1034 - accuracy: 0.9505 - val_loss: 0.1527 - val_accuracy: 0.9616\n",
            "Epoch 4804/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9499\n",
            "Epoch 4804: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1085 - accuracy: 0.9499 - val_loss: 0.1302 - val_accuracy: 0.9701\n",
            "Epoch 4805/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1120 - accuracy: 0.9483\n",
            "Epoch 4805: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1122 - accuracy: 0.9483 - val_loss: 0.0975 - val_accuracy: 0.9839\n",
            "Epoch 4806/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9487\n",
            "Epoch 4806: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1112 - accuracy: 0.9487 - val_loss: 0.1386 - val_accuracy: 0.9601\n",
            "Epoch 4807/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1156 - accuracy: 0.9475\n",
            "Epoch 4807: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1166 - accuracy: 0.9470 - val_loss: 0.1518 - val_accuracy: 0.9556\n",
            "Epoch 4808/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9437\n",
            "Epoch 4808: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1227 - accuracy: 0.9439 - val_loss: 0.1399 - val_accuracy: 0.9620\n",
            "Epoch 4809/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9477\n",
            "Epoch 4809: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1136 - accuracy: 0.9477 - val_loss: 0.1254 - val_accuracy: 0.9716\n",
            "Epoch 4810/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9478\n",
            "Epoch 4810: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1119 - accuracy: 0.9478 - val_loss: 0.1335 - val_accuracy: 0.9677\n",
            "Epoch 4811/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1040 - accuracy: 0.9511\n",
            "Epoch 4811: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1054 - accuracy: 0.9504 - val_loss: 0.1737 - val_accuracy: 0.9438\n",
            "Epoch 4812/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9501\n",
            "Epoch 4812: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1078 - accuracy: 0.9496 - val_loss: 0.1414 - val_accuracy: 0.9568\n",
            "Epoch 4813/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1079 - accuracy: 0.9510\n",
            "Epoch 4813: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1084 - accuracy: 0.9505 - val_loss: 0.1331 - val_accuracy: 0.9684\n",
            "Epoch 4814/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9521\n",
            "Epoch 4814: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1042 - accuracy: 0.9521 - val_loss: 0.1500 - val_accuracy: 0.9556\n",
            "Epoch 4815/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1061 - accuracy: 0.9503\n",
            "Epoch 4815: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1071 - accuracy: 0.9499 - val_loss: 0.1789 - val_accuracy: 0.9438\n",
            "Epoch 4816/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9480\n",
            "Epoch 4816: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1124 - accuracy: 0.9480 - val_loss: 0.2257 - val_accuracy: 0.9278\n",
            "Epoch 4817/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1137 - accuracy: 0.9484\n",
            "Epoch 4817: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1149 - accuracy: 0.9478 - val_loss: 0.1369 - val_accuracy: 0.9644\n",
            "Epoch 4818/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1135 - accuracy: 0.9469\n",
            "Epoch 4818: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1137 - accuracy: 0.9471 - val_loss: 0.1722 - val_accuracy: 0.9464\n",
            "Epoch 4819/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9490\n",
            "Epoch 4819: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1107 - accuracy: 0.9490 - val_loss: 0.1823 - val_accuracy: 0.9456\n",
            "Epoch 4820/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1118 - accuracy: 0.9482\n",
            "Epoch 4820: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1110 - accuracy: 0.9482 - val_loss: 0.1561 - val_accuracy: 0.9566\n",
            "Epoch 4821/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1094 - accuracy: 0.9506\n",
            "Epoch 4821: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1092 - accuracy: 0.9505 - val_loss: 0.1337 - val_accuracy: 0.9657\n",
            "Epoch 4822/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9492\n",
            "Epoch 4822: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1062 - accuracy: 0.9486 - val_loss: 0.1162 - val_accuracy: 0.9748\n",
            "Epoch 4823/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9513\n",
            "Epoch 4823: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1043 - accuracy: 0.9513 - val_loss: 0.1494 - val_accuracy: 0.9519\n",
            "Epoch 4824/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9508\n",
            "Epoch 4824: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1062 - accuracy: 0.9508 - val_loss: 0.1500 - val_accuracy: 0.9569\n",
            "Epoch 4825/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9490\n",
            "Epoch 4825: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1105 - accuracy: 0.9490 - val_loss: 0.1067 - val_accuracy: 0.9811\n",
            "Epoch 4826/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1124 - accuracy: 0.9486\n",
            "Epoch 4826: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1133 - accuracy: 0.9485 - val_loss: 0.1710 - val_accuracy: 0.9477\n",
            "Epoch 4827/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1157 - accuracy: 0.9472\n",
            "Epoch 4827: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1163 - accuracy: 0.9465 - val_loss: 0.1408 - val_accuracy: 0.9532\n",
            "Epoch 4828/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9449\n",
            "Epoch 4828: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1218 - accuracy: 0.9443 - val_loss: 0.1412 - val_accuracy: 0.9545\n",
            "Epoch 4829/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1245 - accuracy: 0.9420\n",
            "Epoch 4829: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1243 - accuracy: 0.9422 - val_loss: 0.1809 - val_accuracy: 0.9471\n",
            "Epoch 4830/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9472\n",
            "Epoch 4830: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1140 - accuracy: 0.9475 - val_loss: 0.1110 - val_accuracy: 0.9813\n",
            "Epoch 4831/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1127 - accuracy: 0.9474\n",
            "Epoch 4831: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1128 - accuracy: 0.9473 - val_loss: 0.1165 - val_accuracy: 0.9679\n",
            "Epoch 4832/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1156 - accuracy: 0.9458\n",
            "Epoch 4832: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1150 - accuracy: 0.9462 - val_loss: 0.1381 - val_accuracy: 0.9584\n",
            "Epoch 4833/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9471\n",
            "Epoch 4833: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1175 - accuracy: 0.9471 - val_loss: 0.1237 - val_accuracy: 0.9722\n",
            "Epoch 4834/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1098 - accuracy: 0.9497\n",
            "Epoch 4834: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9498 - val_loss: 0.1405 - val_accuracy: 0.9538\n",
            "Epoch 4835/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1044 - accuracy: 0.9517\n",
            "Epoch 4835: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1046 - accuracy: 0.9510 - val_loss: 0.1488 - val_accuracy: 0.9542\n",
            "Epoch 4836/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9513\n",
            "Epoch 4836: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1040 - accuracy: 0.9513 - val_loss: 0.1075 - val_accuracy: 0.9742\n",
            "Epoch 4837/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1052 - accuracy: 0.9529\n",
            "Epoch 4837: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1051 - accuracy: 0.9530 - val_loss: 0.1141 - val_accuracy: 0.9729\n",
            "Epoch 4838/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1061 - accuracy: 0.9495\n",
            "Epoch 4838: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1055 - accuracy: 0.9499 - val_loss: 0.1215 - val_accuracy: 0.9688\n",
            "Epoch 4839/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9496\n",
            "Epoch 4839: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1061 - accuracy: 0.9496 - val_loss: 0.1318 - val_accuracy: 0.9683\n",
            "Epoch 4840/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9506\n",
            "Epoch 4840: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1042 - accuracy: 0.9505 - val_loss: 0.1339 - val_accuracy: 0.9657\n",
            "Epoch 4841/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1014 - accuracy: 0.9517\n",
            "Epoch 4841: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1016 - accuracy: 0.9516 - val_loss: 0.1188 - val_accuracy: 0.9723\n",
            "Epoch 4842/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1013 - accuracy: 0.9527\n",
            "Epoch 4842: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1023 - accuracy: 0.9521 - val_loss: 0.1068 - val_accuracy: 0.9794\n",
            "Epoch 4843/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1029 - accuracy: 0.9519\n",
            "Epoch 4843: loss did not improve from 0.10153\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1026 - accuracy: 0.9521 - val_loss: 0.1401 - val_accuracy: 0.9582\n",
            "Epoch 4844/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9535\n",
            "Epoch 4844: loss improved from 0.10153 to 0.10134, saving model to /content/drive/MyDrive/new_df/best_model_by_class2.hdf5\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.1013 - accuracy: 0.9532 - val_loss: 0.1767 - val_accuracy: 0.9345\n",
            "Epoch 4845/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1032 - accuracy: 0.9522\n",
            "Epoch 4845: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1042 - accuracy: 0.9517 - val_loss: 0.1302 - val_accuracy: 0.9664\n",
            "Epoch 4846/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1078 - accuracy: 0.9498\n",
            "Epoch 4846: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1092 - accuracy: 0.9494 - val_loss: 0.1409 - val_accuracy: 0.9599\n",
            "Epoch 4847/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1112 - accuracy: 0.9497\n",
            "Epoch 4847: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1116 - accuracy: 0.9496 - val_loss: 0.1425 - val_accuracy: 0.9616\n",
            "Epoch 4848/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9483\n",
            "Epoch 4848: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1130 - accuracy: 0.9478 - val_loss: 0.1408 - val_accuracy: 0.9599\n",
            "Epoch 4849/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1115 - accuracy: 0.9482\n",
            "Epoch 4849: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1111 - accuracy: 0.9487 - val_loss: 0.1403 - val_accuracy: 0.9631\n",
            "Epoch 4850/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1133 - accuracy: 0.9488\n",
            "Epoch 4850: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1130 - accuracy: 0.9487 - val_loss: 0.1916 - val_accuracy: 0.9274\n",
            "Epoch 4851/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9493\n",
            "Epoch 4851: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1071 - accuracy: 0.9495 - val_loss: 0.1456 - val_accuracy: 0.9642\n",
            "Epoch 4852/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1083 - accuracy: 0.9496\n",
            "Epoch 4852: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1088 - accuracy: 0.9494 - val_loss: 0.1547 - val_accuracy: 0.9499\n",
            "Epoch 4853/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9477\n",
            "Epoch 4853: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1101 - accuracy: 0.9479 - val_loss: 0.1492 - val_accuracy: 0.9534\n",
            "Epoch 4854/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9513\n",
            "Epoch 4854: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9504 - val_loss: 0.1431 - val_accuracy: 0.9579\n",
            "Epoch 4855/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9493\n",
            "Epoch 4855: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1071 - accuracy: 0.9495 - val_loss: 0.1224 - val_accuracy: 0.9720\n",
            "Epoch 4856/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9500\n",
            "Epoch 4856: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1059 - accuracy: 0.9499 - val_loss: 0.1216 - val_accuracy: 0.9722\n",
            "Epoch 4857/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.9496\n",
            "Epoch 4857: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1084 - accuracy: 0.9487 - val_loss: 0.1885 - val_accuracy: 0.9289\n",
            "Epoch 4858/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9484\n",
            "Epoch 4858: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1161 - accuracy: 0.9484 - val_loss: 0.2389 - val_accuracy: 0.9050\n",
            "Epoch 4859/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1162 - accuracy: 0.9461\n",
            "Epoch 4859: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1157 - accuracy: 0.9465 - val_loss: 0.1545 - val_accuracy: 0.9501\n",
            "Epoch 4860/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1157 - accuracy: 0.9491\n",
            "Epoch 4860: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1163 - accuracy: 0.9480 - val_loss: 0.1042 - val_accuracy: 0.9866\n",
            "Epoch 4861/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9450\n",
            "Epoch 4861: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1193 - accuracy: 0.9453 - val_loss: 0.1487 - val_accuracy: 0.9614\n",
            "Epoch 4862/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9447\n",
            "Epoch 4862: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1237 - accuracy: 0.9446 - val_loss: 0.1568 - val_accuracy: 0.9503\n",
            "Epoch 4863/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1133 - accuracy: 0.9463\n",
            "Epoch 4863: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1133 - accuracy: 0.9463 - val_loss: 0.1330 - val_accuracy: 0.9642\n",
            "Epoch 4864/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9515\n",
            "Epoch 4864: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1061 - accuracy: 0.9510 - val_loss: 0.1633 - val_accuracy: 0.9462\n",
            "Epoch 4865/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9503\n",
            "Epoch 4865: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1071 - accuracy: 0.9501 - val_loss: 0.1614 - val_accuracy: 0.9480\n",
            "Epoch 4866/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9474\n",
            "Epoch 4866: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9473 - val_loss: 0.1390 - val_accuracy: 0.9627\n",
            "Epoch 4867/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 0.9472\n",
            "Epoch 4867: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1129 - accuracy: 0.9471 - val_loss: 0.1428 - val_accuracy: 0.9646\n",
            "Epoch 4868/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9482\n",
            "Epoch 4868: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9487 - val_loss: 0.1261 - val_accuracy: 0.9677\n",
            "Epoch 4869/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1110 - accuracy: 0.9482\n",
            "Epoch 4869: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1113 - accuracy: 0.9484 - val_loss: 0.1311 - val_accuracy: 0.9647\n",
            "Epoch 4870/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9495\n",
            "Epoch 4870: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1130 - accuracy: 0.9495 - val_loss: 0.1586 - val_accuracy: 0.9449\n",
            "Epoch 4871/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1082 - accuracy: 0.9508\n",
            "Epoch 4871: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9498 - val_loss: 0.1465 - val_accuracy: 0.9571\n",
            "Epoch 4872/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1184 - accuracy: 0.9458\n",
            "Epoch 4872: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1183 - accuracy: 0.9459 - val_loss: 0.1412 - val_accuracy: 0.9601\n",
            "Epoch 4873/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1139 - accuracy: 0.9484\n",
            "Epoch 4873: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1146 - accuracy: 0.9481 - val_loss: 0.1164 - val_accuracy: 0.9777\n",
            "Epoch 4874/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9453\n",
            "Epoch 4874: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1198 - accuracy: 0.9446 - val_loss: 0.2143 - val_accuracy: 0.9222\n",
            "Epoch 4875/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9466\n",
            "Epoch 4875: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1170 - accuracy: 0.9465 - val_loss: 0.1305 - val_accuracy: 0.9729\n",
            "Epoch 4876/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9477\n",
            "Epoch 4876: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1113 - accuracy: 0.9479 - val_loss: 0.1841 - val_accuracy: 0.9375\n",
            "Epoch 4877/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1082 - accuracy: 0.9487\n",
            "Epoch 4877: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1081 - accuracy: 0.9487 - val_loss: 0.1181 - val_accuracy: 0.9788\n",
            "Epoch 4878/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9498\n",
            "Epoch 4878: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1073 - accuracy: 0.9498 - val_loss: 0.1452 - val_accuracy: 0.9486\n",
            "Epoch 4879/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1095 - accuracy: 0.9485\n",
            "Epoch 4879: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9488 - val_loss: 0.1080 - val_accuracy: 0.9811\n",
            "Epoch 4880/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9483\n",
            "Epoch 4880: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1102 - accuracy: 0.9483 - val_loss: 0.1408 - val_accuracy: 0.9638\n",
            "Epoch 4881/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1048 - accuracy: 0.9511\n",
            "Epoch 4881: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1060 - accuracy: 0.9504 - val_loss: 0.1027 - val_accuracy: 0.9794\n",
            "Epoch 4882/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1033 - accuracy: 0.9516\n",
            "Epoch 4882: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1044 - accuracy: 0.9507 - val_loss: 0.1460 - val_accuracy: 0.9573\n",
            "Epoch 4883/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1041 - accuracy: 0.9512\n",
            "Epoch 4883: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9505 - val_loss: 0.1371 - val_accuracy: 0.9668\n",
            "Epoch 4884/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9496\n",
            "Epoch 4884: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1068 - accuracy: 0.9496 - val_loss: 0.1453 - val_accuracy: 0.9482\n",
            "Epoch 4885/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9489\n",
            "Epoch 4885: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1080 - accuracy: 0.9491 - val_loss: 0.1379 - val_accuracy: 0.9560\n",
            "Epoch 4886/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1127 - accuracy: 0.9490\n",
            "Epoch 4886: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1123 - accuracy: 0.9492 - val_loss: 0.1132 - val_accuracy: 0.9722\n",
            "Epoch 4887/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1087 - accuracy: 0.9486\n",
            "Epoch 4887: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1096 - accuracy: 0.9478 - val_loss: 0.1684 - val_accuracy: 0.9367\n",
            "Epoch 4888/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9487\n",
            "Epoch 4888: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1121 - accuracy: 0.9487 - val_loss: 0.1750 - val_accuracy: 0.9406\n",
            "Epoch 4889/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9477\n",
            "Epoch 4889: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1133 - accuracy: 0.9477 - val_loss: 0.1543 - val_accuracy: 0.9451\n",
            "Epoch 4890/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9475\n",
            "Epoch 4890: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1154 - accuracy: 0.9473 - val_loss: 0.1502 - val_accuracy: 0.9493\n",
            "Epoch 4891/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1091 - accuracy: 0.9493\n",
            "Epoch 4891: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1086 - accuracy: 0.9496 - val_loss: 0.1795 - val_accuracy: 0.9367\n",
            "Epoch 4892/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9508\n",
            "Epoch 4892: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1050 - accuracy: 0.9510 - val_loss: 0.1320 - val_accuracy: 0.9662\n",
            "Epoch 4893/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1005 - accuracy: 0.9529\n",
            "Epoch 4893: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1018 - accuracy: 0.9523 - val_loss: 0.1307 - val_accuracy: 0.9675\n",
            "Epoch 4894/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9509\n",
            "Epoch 4894: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1041 - accuracy: 0.9510 - val_loss: 0.1688 - val_accuracy: 0.9380\n",
            "Epoch 4895/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9500\n",
            "Epoch 4895: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1053 - accuracy: 0.9500 - val_loss: 0.1323 - val_accuracy: 0.9647\n",
            "Epoch 4896/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1052 - accuracy: 0.9518\n",
            "Epoch 4896: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1053 - accuracy: 0.9517 - val_loss: 0.1327 - val_accuracy: 0.9599\n",
            "Epoch 4897/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1031 - accuracy: 0.9516\n",
            "Epoch 4897: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1037 - accuracy: 0.9515 - val_loss: 0.1343 - val_accuracy: 0.9666\n",
            "Epoch 4898/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9511\n",
            "Epoch 4898: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1044 - accuracy: 0.9511 - val_loss: 0.1494 - val_accuracy: 0.9529\n",
            "Epoch 4899/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1098 - accuracy: 0.9487\n",
            "Epoch 4899: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1095 - accuracy: 0.9488 - val_loss: 0.1725 - val_accuracy: 0.9464\n",
            "Epoch 4900/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1038 - accuracy: 0.9508\n",
            "Epoch 4900: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1038 - accuracy: 0.9511 - val_loss: 0.1232 - val_accuracy: 0.9735\n",
            "Epoch 4901/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1033 - accuracy: 0.9523\n",
            "Epoch 4901: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1068 - accuracy: 0.9506 - val_loss: 0.1829 - val_accuracy: 0.9412\n",
            "Epoch 4902/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1115 - accuracy: 0.9485\n",
            "Epoch 4902: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1118 - accuracy: 0.9483 - val_loss: 0.1298 - val_accuracy: 0.9634\n",
            "Epoch 4903/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9449\n",
            "Epoch 4903: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1183 - accuracy: 0.9448 - val_loss: 0.1242 - val_accuracy: 0.9703\n",
            "Epoch 4904/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 0.9482\n",
            "Epoch 4904: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1131 - accuracy: 0.9479 - val_loss: 0.1258 - val_accuracy: 0.9681\n",
            "Epoch 4905/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1124 - accuracy: 0.9490\n",
            "Epoch 4905: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1140 - accuracy: 0.9477 - val_loss: 0.1586 - val_accuracy: 0.9467\n",
            "Epoch 4906/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9473\n",
            "Epoch 4906: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1144 - accuracy: 0.9474 - val_loss: 0.1016 - val_accuracy: 0.9814\n",
            "Epoch 4907/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1134 - accuracy: 0.9479\n",
            "Epoch 4907: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1128 - accuracy: 0.9482 - val_loss: 0.1285 - val_accuracy: 0.9662\n",
            "Epoch 4908/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1081 - accuracy: 0.9495\n",
            "Epoch 4908: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1091 - accuracy: 0.9488 - val_loss: 0.1052 - val_accuracy: 0.9826\n",
            "Epoch 4909/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9457\n",
            "Epoch 4909: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1204 - accuracy: 0.9456 - val_loss: 0.1953 - val_accuracy: 0.9334\n",
            "Epoch 4910/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9458\n",
            "Epoch 4910: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1196 - accuracy: 0.9455 - val_loss: 0.1358 - val_accuracy: 0.9664\n",
            "Epoch 4911/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9455\n",
            "Epoch 4911: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1172 - accuracy: 0.9452 - val_loss: 0.1496 - val_accuracy: 0.9551\n",
            "Epoch 4912/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1126 - accuracy: 0.9489\n",
            "Epoch 4912: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1132 - accuracy: 0.9484 - val_loss: 0.1538 - val_accuracy: 0.9488\n",
            "Epoch 4913/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1108 - accuracy: 0.9483\n",
            "Epoch 4913: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1126 - accuracy: 0.9476 - val_loss: 0.1219 - val_accuracy: 0.9679\n",
            "Epoch 4914/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9474\n",
            "Epoch 4914: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1155 - accuracy: 0.9474 - val_loss: 0.1182 - val_accuracy: 0.9779\n",
            "Epoch 4915/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9505\n",
            "Epoch 4915: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9498 - val_loss: 0.1246 - val_accuracy: 0.9731\n",
            "Epoch 4916/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1028 - accuracy: 0.9515\n",
            "Epoch 4916: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1026 - accuracy: 0.9517 - val_loss: 0.1212 - val_accuracy: 0.9742\n",
            "Epoch 4917/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9487\n",
            "Epoch 4917: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1132 - accuracy: 0.9485 - val_loss: 0.1896 - val_accuracy: 0.9558\n",
            "Epoch 4918/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9435\n",
            "Epoch 4918: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1247 - accuracy: 0.9435 - val_loss: 0.1756 - val_accuracy: 0.9408\n",
            "Epoch 4919/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9459\n",
            "Epoch 4919: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1149 - accuracy: 0.9459 - val_loss: 0.2154 - val_accuracy: 0.9128\n",
            "Epoch 4920/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1102 - accuracy: 0.9491\n",
            "Epoch 4920: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1105 - accuracy: 0.9488 - val_loss: 0.1485 - val_accuracy: 0.9545\n",
            "Epoch 4921/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9506\n",
            "Epoch 4921: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9504 - val_loss: 0.1708 - val_accuracy: 0.9417\n",
            "Epoch 4922/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9491\n",
            "Epoch 4922: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1090 - accuracy: 0.9491 - val_loss: 0.1351 - val_accuracy: 0.9625\n",
            "Epoch 4923/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1024 - accuracy: 0.9522\n",
            "Epoch 4923: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1033 - accuracy: 0.9519 - val_loss: 0.1442 - val_accuracy: 0.9573\n",
            "Epoch 4924/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1046 - accuracy: 0.9513\n",
            "Epoch 4924: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1053 - accuracy: 0.9510 - val_loss: 0.1479 - val_accuracy: 0.9536\n",
            "Epoch 4925/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9509\n",
            "Epoch 4925: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1067 - accuracy: 0.9506 - val_loss: 0.1597 - val_accuracy: 0.9415\n",
            "Epoch 4926/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1056 - accuracy: 0.9505\n",
            "Epoch 4926: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1056 - accuracy: 0.9505 - val_loss: 0.1112 - val_accuracy: 0.9748\n",
            "Epoch 4927/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1079 - accuracy: 0.9484\n",
            "Epoch 4927: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1080 - accuracy: 0.9485 - val_loss: 0.1519 - val_accuracy: 0.9560\n",
            "Epoch 4928/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9491\n",
            "Epoch 4928: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1106 - accuracy: 0.9491 - val_loss: 0.1359 - val_accuracy: 0.9662\n",
            "Epoch 4929/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1055 - accuracy: 0.9506\n",
            "Epoch 4929: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1065 - accuracy: 0.9503 - val_loss: 0.1003 - val_accuracy: 0.9853\n",
            "Epoch 4930/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9500\n",
            "Epoch 4930: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1090 - accuracy: 0.9500 - val_loss: 0.1508 - val_accuracy: 0.9506\n",
            "Epoch 4931/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1084 - accuracy: 0.9491\n",
            "Epoch 4931: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9492 - val_loss: 0.1125 - val_accuracy: 0.9774\n",
            "Epoch 4932/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9516\n",
            "Epoch 4932: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1035 - accuracy: 0.9516 - val_loss: 0.1161 - val_accuracy: 0.9744\n",
            "Epoch 4933/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1046 - accuracy: 0.9514\n",
            "Epoch 4933: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1049 - accuracy: 0.9511 - val_loss: 0.1402 - val_accuracy: 0.9621\n",
            "Epoch 4934/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9499\n",
            "Epoch 4934: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1085 - accuracy: 0.9497 - val_loss: 0.1910 - val_accuracy: 0.9252\n",
            "Epoch 4935/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9498\n",
            "Epoch 4935: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1076 - accuracy: 0.9498 - val_loss: 0.1228 - val_accuracy: 0.9697\n",
            "Epoch 4936/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9496\n",
            "Epoch 4936: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1076 - accuracy: 0.9496 - val_loss: 0.1410 - val_accuracy: 0.9608\n",
            "Epoch 4937/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1104 - accuracy: 0.9496\n",
            "Epoch 4937: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1109 - accuracy: 0.9491 - val_loss: 0.1130 - val_accuracy: 0.9772\n",
            "Epoch 4938/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9484\n",
            "Epoch 4938: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1099 - accuracy: 0.9486 - val_loss: 0.1355 - val_accuracy: 0.9694\n",
            "Epoch 4939/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1115 - accuracy: 0.9481\n",
            "Epoch 4939: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1121 - accuracy: 0.9479 - val_loss: 0.1645 - val_accuracy: 0.9478\n",
            "Epoch 4940/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1154 - accuracy: 0.9471\n",
            "Epoch 4940: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1156 - accuracy: 0.9465 - val_loss: 0.1542 - val_accuracy: 0.9508\n",
            "Epoch 4941/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9515\n",
            "Epoch 4941: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1060 - accuracy: 0.9515 - val_loss: 0.1334 - val_accuracy: 0.9625\n",
            "Epoch 4942/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1036 - accuracy: 0.9520\n",
            "Epoch 4942: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1045 - accuracy: 0.9515 - val_loss: 0.1187 - val_accuracy: 0.9736\n",
            "Epoch 4943/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1046 - accuracy: 0.9518\n",
            "Epoch 4943: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1075 - accuracy: 0.9502 - val_loss: 0.1813 - val_accuracy: 0.9393\n",
            "Epoch 4944/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1148 - accuracy: 0.9478\n",
            "Epoch 4944: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1144 - accuracy: 0.9477 - val_loss: 0.1367 - val_accuracy: 0.9651\n",
            "Epoch 4945/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1150 - accuracy: 0.9466\n",
            "Epoch 4945: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1168 - accuracy: 0.9453 - val_loss: 0.1612 - val_accuracy: 0.9501\n",
            "Epoch 4946/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9452\n",
            "Epoch 4946: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1187 - accuracy: 0.9452 - val_loss: 0.1470 - val_accuracy: 0.9597\n",
            "Epoch 4947/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9431\n",
            "Epoch 4947: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1232 - accuracy: 0.9437 - val_loss: 0.1637 - val_accuracy: 0.9467\n",
            "Epoch 4948/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9440\n",
            "Epoch 4948: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1218 - accuracy: 0.9439 - val_loss: 0.1511 - val_accuracy: 0.9516\n",
            "Epoch 4949/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1150 - accuracy: 0.9468\n",
            "Epoch 4949: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1143 - accuracy: 0.9472 - val_loss: 0.1099 - val_accuracy: 0.9807\n",
            "Epoch 4950/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1081 - accuracy: 0.9499\n",
            "Epoch 4950: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1082 - accuracy: 0.9492 - val_loss: 0.1672 - val_accuracy: 0.9491\n",
            "Epoch 4951/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9500\n",
            "Epoch 4951: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1058 - accuracy: 0.9501 - val_loss: 0.1533 - val_accuracy: 0.9503\n",
            "Epoch 4952/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1090 - accuracy: 0.9496\n",
            "Epoch 4952: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1093 - accuracy: 0.9494 - val_loss: 0.1260 - val_accuracy: 0.9731\n",
            "Epoch 4953/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1070 - accuracy: 0.9502\n",
            "Epoch 4953: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1075 - accuracy: 0.9498 - val_loss: 0.1385 - val_accuracy: 0.9586\n",
            "Epoch 4954/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9486\n",
            "Epoch 4954: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1104 - accuracy: 0.9479 - val_loss: 0.1526 - val_accuracy: 0.9551\n",
            "Epoch 4955/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1138 - accuracy: 0.9483\n",
            "Epoch 4955: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1145 - accuracy: 0.9480 - val_loss: 0.1508 - val_accuracy: 0.9571\n",
            "Epoch 4956/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9488\n",
            "Epoch 4956: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1073 - accuracy: 0.9489 - val_loss: 0.1278 - val_accuracy: 0.9694\n",
            "Epoch 4957/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9501\n",
            "Epoch 4957: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1056 - accuracy: 0.9502 - val_loss: 0.1279 - val_accuracy: 0.9653\n",
            "Epoch 4958/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1035 - accuracy: 0.9512\n",
            "Epoch 4958: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1031 - accuracy: 0.9513 - val_loss: 0.1334 - val_accuracy: 0.9571\n",
            "Epoch 4959/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9493\n",
            "Epoch 4959: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1071 - accuracy: 0.9492 - val_loss: 0.1184 - val_accuracy: 0.9742\n",
            "Epoch 4960/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9498\n",
            "Epoch 4960: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1121 - accuracy: 0.9494 - val_loss: 0.1466 - val_accuracy: 0.9573\n",
            "Epoch 4961/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9486\n",
            "Epoch 4961: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1119 - accuracy: 0.9480 - val_loss: 0.1282 - val_accuracy: 0.9684\n",
            "Epoch 4962/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1073 - accuracy: 0.9499\n",
            "Epoch 4962: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1071 - accuracy: 0.9500 - val_loss: 0.1311 - val_accuracy: 0.9649\n",
            "Epoch 4963/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9499\n",
            "Epoch 4963: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1072 - accuracy: 0.9499 - val_loss: 0.1877 - val_accuracy: 0.9308\n",
            "Epoch 4964/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1092 - accuracy: 0.9504\n",
            "Epoch 4964: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1109 - accuracy: 0.9494 - val_loss: 0.1259 - val_accuracy: 0.9722\n",
            "Epoch 4965/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1135 - accuracy: 0.9459\n",
            "Epoch 4965: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1139 - accuracy: 0.9460 - val_loss: 0.1879 - val_accuracy: 0.9406\n",
            "Epoch 4966/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1288 - accuracy: 0.9426\n",
            "Epoch 4966: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1289 - accuracy: 0.9426 - val_loss: 0.1386 - val_accuracy: 0.9631\n",
            "Epoch 4967/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9470\n",
            "Epoch 4967: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1163 - accuracy: 0.9470 - val_loss: 0.1597 - val_accuracy: 0.9445\n",
            "Epoch 4968/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9451\n",
            "Epoch 4968: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1200 - accuracy: 0.9451 - val_loss: 0.1583 - val_accuracy: 0.9525\n",
            "Epoch 4969/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1146 - accuracy: 0.9469\n",
            "Epoch 4969: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1142 - accuracy: 0.9474 - val_loss: 0.1136 - val_accuracy: 0.9820\n",
            "Epoch 4970/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9487\n",
            "Epoch 4970: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9488 - val_loss: 0.1388 - val_accuracy: 0.9620\n",
            "Epoch 4971/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1063 - accuracy: 0.9512\n",
            "Epoch 4971: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1067 - accuracy: 0.9510 - val_loss: 0.1292 - val_accuracy: 0.9697\n",
            "Epoch 4972/5000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1024 - accuracy: 0.9523\n",
            "Epoch 4972: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1051 - accuracy: 0.9514 - val_loss: 0.1368 - val_accuracy: 0.9653\n",
            "Epoch 4973/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9501\n",
            "Epoch 4973: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1116 - accuracy: 0.9495 - val_loss: 0.1455 - val_accuracy: 0.9610\n",
            "Epoch 4974/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9502\n",
            "Epoch 4974: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1056 - accuracy: 0.9504 - val_loss: 0.1136 - val_accuracy: 0.9800\n",
            "Epoch 4975/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1023 - accuracy: 0.9519\n",
            "Epoch 4975: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1035 - accuracy: 0.9509 - val_loss: 0.1391 - val_accuracy: 0.9675\n",
            "Epoch 4976/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1025 - accuracy: 0.9518\n",
            "Epoch 4976: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1034 - accuracy: 0.9512 - val_loss: 0.1328 - val_accuracy: 0.9662\n",
            "Epoch 4977/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1032 - accuracy: 0.9510\n",
            "Epoch 4977: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1041 - accuracy: 0.9506 - val_loss: 0.1233 - val_accuracy: 0.9729\n",
            "Epoch 4978/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9497\n",
            "Epoch 4978: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1087 - accuracy: 0.9497 - val_loss: 0.1662 - val_accuracy: 0.9401\n",
            "Epoch 4979/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9490\n",
            "Epoch 4979: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 16ms/step - loss: 0.1109 - accuracy: 0.9483 - val_loss: 0.1583 - val_accuracy: 0.9547\n",
            "Epoch 4980/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1115 - accuracy: 0.9483\n",
            "Epoch 4980: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1121 - accuracy: 0.9481 - val_loss: 0.1361 - val_accuracy: 0.9631\n",
            "Epoch 4981/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9477\n",
            "Epoch 4981: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1128 - accuracy: 0.9475 - val_loss: 0.1184 - val_accuracy: 0.9738\n",
            "Epoch 4982/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1046 - accuracy: 0.9515\n",
            "Epoch 4982: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 18ms/step - loss: 0.1045 - accuracy: 0.9515 - val_loss: 0.1725 - val_accuracy: 0.9380\n",
            "Epoch 4983/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9490\n",
            "Epoch 4983: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 17ms/step - loss: 0.1087 - accuracy: 0.9489 - val_loss: 0.1464 - val_accuracy: 0.9568\n",
            "Epoch 4984/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1061 - accuracy: 0.9491\n",
            "Epoch 4984: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 15ms/step - loss: 0.1076 - accuracy: 0.9486 - val_loss: 0.1353 - val_accuracy: 0.9634\n",
            "Epoch 4985/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1169 - accuracy: 0.9460\n",
            "Epoch 4985: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1162 - accuracy: 0.9465 - val_loss: 0.1109 - val_accuracy: 0.9781\n",
            "Epoch 4986/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1083 - accuracy: 0.9482\n",
            "Epoch 4986: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1081 - accuracy: 0.9481 - val_loss: 0.1526 - val_accuracy: 0.9581\n",
            "Epoch 4987/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1114 - accuracy: 0.9487\n",
            "Epoch 4987: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1109 - accuracy: 0.9487 - val_loss: 0.1118 - val_accuracy: 0.9785\n",
            "Epoch 4988/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9491\n",
            "Epoch 4988: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1098 - accuracy: 0.9491 - val_loss: 0.1303 - val_accuracy: 0.9707\n",
            "Epoch 4989/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1054 - accuracy: 0.9498\n",
            "Epoch 4989: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1046 - accuracy: 0.9507 - val_loss: 0.1261 - val_accuracy: 0.9683\n",
            "Epoch 4990/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1071 - accuracy: 0.9506\n",
            "Epoch 4990: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1072 - accuracy: 0.9505 - val_loss: 0.1262 - val_accuracy: 0.9677\n",
            "Epoch 4991/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1066 - accuracy: 0.9506\n",
            "Epoch 4991: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1074 - accuracy: 0.9500 - val_loss: 0.1365 - val_accuracy: 0.9651\n",
            "Epoch 4992/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9505\n",
            "Epoch 4992: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1069 - accuracy: 0.9504 - val_loss: 0.1514 - val_accuracy: 0.9490\n",
            "Epoch 4993/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9487\n",
            "Epoch 4993: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1112 - accuracy: 0.9487 - val_loss: 0.1370 - val_accuracy: 0.9660\n",
            "Epoch 4994/5000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9478\n",
            "Epoch 4994: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 1s 14ms/step - loss: 0.1100 - accuracy: 0.9478 - val_loss: 0.2323 - val_accuracy: 0.9159\n",
            "Epoch 4995/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9468\n",
            "Epoch 4995: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 14ms/step - loss: 0.1135 - accuracy: 0.9470 - val_loss: 0.1562 - val_accuracy: 0.9542\n",
            "Epoch 4996/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9482\n",
            "Epoch 4996: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1126 - accuracy: 0.9481 - val_loss: 0.1512 - val_accuracy: 0.9529\n",
            "Epoch 4997/5000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1087 - accuracy: 0.9500\n",
            "Epoch 4997: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1087 - accuracy: 0.9496 - val_loss: 0.1298 - val_accuracy: 0.9699\n",
            "Epoch 4998/5000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9464\n",
            "Epoch 4998: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 13ms/step - loss: 0.1152 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9298\n",
            "Epoch 4999/5000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1146 - accuracy: 0.9465\n",
            "Epoch 4999: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1142 - accuracy: 0.9462 - val_loss: 0.1648 - val_accuracy: 0.9482\n",
            "Epoch 5000/5000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1108 - accuracy: 0.9485\n",
            "Epoch 5000: loss did not improve from 0.10134\n",
            "36/36 [==============================] - 0s 12ms/step - loss: 0.1121 - accuracy: 0.9478 - val_loss: 0.1474 - val_accuracy: 0.9527\n",
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 3.4159 - accuracy: 0.7578\n",
            "[3.415919780731201, 0.7578443288803101]\n",
            "131/131 [==============================] - 0s 2ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_1       0.87      0.83      0.85      3369\n",
            "     class 2       0.39      0.47      0.43       806\n",
            "\n",
            "    accuracy                           0.76      4175\n",
            "   macro avg       0.63      0.65      0.64      4175\n",
            "weighted avg       0.77      0.76      0.77      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "842/842 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.9530\n",
            "[0.11174020916223526, 0.9530066847801208]\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_1       0.96      0.94      0.95     13470\n",
            "     class 2       0.94      0.96      0.95     13470\n",
            "\n",
            "    accuracy                           0.95     26940\n",
            "   macro avg       0.95      0.95      0.95     26940\n",
            "weighted avg       0.95      0.95      0.95     26940\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.best"
      ],
      "metadata": {
        "id": "IJmZyGZbSJvj",
        "outputId": "f0623b7b-62d4-4621-d628-796d341650f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10133551806211472"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***Best Model Test Only***#\n",
        "#--------------------------#\n",
        "\n",
        "model.load_weights(file_path)\n",
        "\n",
        "############Pridecting results show######################\n",
        "\n",
        "target_names = ['class 0_1', 'class 2']\n",
        "\n",
        "print(\"Test-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(test_features, test_target))\n",
        "test_prediction_points = model.predict(test_features)  #Probability of each class\n",
        "test_prediction =(test_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for test set:\\n\\n\", classification_report(test_target, test_prediction, target_names=target_names))\n",
        "test_confusion_matrix = confusion_matrix(test_target, test_prediction)\n",
        "test_disp = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix, display_labels=target_names)\n",
        "test_disp.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n*********************************************\\nTrain-set evaluate result:\\n\")\n",
        "\n",
        "print(model.evaluate(train_features, train_target))\n",
        "train_prediction_points = model.predict(train_features)  #Probability of each class\n",
        "train_prediction =(train_prediction_points > 0.5).astype(numpy.int32)\n",
        "\n",
        "print(\"Report for train set:\\n\\n\", classification_report(train_target, train_prediction, target_names=target_names))\n",
        "train_confusion_matrix = confusion_matrix(train_target, train_prediction)\n",
        "train_disp = ConfusionMatrixDisplay(confusion_matrix=train_confusion_matrix, display_labels=target_names)\n",
        "train_disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-naGpkmkfml9",
        "outputId": "843597e9-c2e1-4aa2-8967-38e34ae9e870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-set evaluate result:\n",
            "\n",
            "131/131 [==============================] - 0s 2ms/step - loss: 3.6182 - accuracy: 0.7648\n",
            "[3.6182305812835693, 0.764790415763855]\n",
            "131/131 [==============================] - 0s 1ms/step\n",
            "Report for test set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_1       0.86      0.84      0.85      3369\n",
            "     class 2       0.40      0.45      0.42       806\n",
            "\n",
            "    accuracy                           0.76      4175\n",
            "   macro avg       0.63      0.64      0.64      4175\n",
            "weighted avg       0.77      0.76      0.77      4175\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*********************************************\n",
            "Train-set evaluate result:\n",
            "\n",
            "842/842 [==============================] - 1s 2ms/step - loss: 0.1157 - accuracy: 0.9501\n",
            "[0.11574887484312057, 0.9500742554664612]\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "Report for train set:\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   class 0_1       0.95      0.95      0.95     13470\n",
            "     class 2       0.95      0.95      0.95     13470\n",
            "\n",
            "    accuracy                           0.95     26940\n",
            "   macro avg       0.95      0.95      0.95     26940\n",
            "weighted avg       0.95      0.95      0.95     26940\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAGxCAYAAABhi7IUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABP50lEQVR4nO3deVwU9f8H8Ndy7HIul3Jsgq23pKlhGWqlSeJXU0nNNEot1EzUxDwrETXTKDUs07TyKM2j0jwKNU0tIUUUD0I8QlFxwZ8oK8q5O78/iKkNMNZZhHFfz8djHo925jOfeQ8PaN++P5/5jEIQBAFEREREVsqmtgMgIiIiqk1MhoiIiMiqMRkiIiIiq8ZkiIiIiKwakyEiIiKyakyGiIiIyKoxGSIiIiKrxmSIiIiIrJpdbQdAlTMajcjKyoKrqysUCkVth0NERGYQBAE3b96ERqOBjU3N1R0KCwtRXFxskb6USiUcHBws0pfcMBmqo7KysuDv71/bYRARkQQXL15EgwYNaqTvwsJCaBu6QJdjsEh/vr6+yMjIsMqEiMlQHeXq6goAuHDkQahdOJpJ96fnmrWu7RCIakQpSvAbfhT/X14TiouLocsxICO5IdSu0r4n9DeN0AZdQHFxMZMhqjvKh8bULjaSf8mJ6io7hX1th0BUM/566+e9mOagduX3hFRMhoiIiGTMIBhhkPjKdYNgtEwwMsVkiIiISMaMEGCEtGxI6vlyx7oaERERWTVWhoiIiGTMCCOkDnJJ70HemAwRERHJmEEQYBCkDXNJPV/uOExGREREVo2VISIiIhnjBGrpmAwRERHJmBECDEyGJOEwGREREVk1VoaIiIhkjMNk0jEZIiIikjE+TSYdkyEiIiIZM/61Se3DmnHOEBEREVk1VoaIiIhkzGCBp8mkni93TIaIiIhkzCDAAm+tt0wscsVhMiIiIrJqrAwRERHJGCdQS8fKEBERkYwZoYBB4maEwqxr7t+/H71794ZGo4FCocDmzZvFYyUlJZgyZQpat24NZ2dnaDQaDBkyBFlZWSZ95ObmIjw8HGq1Gu7u7oiIiEB+fr5Jm+PHj+OJJ56Ag4MD/P39ERsbWyGWjRs3okWLFnBwcEDr1q3x448/mnUvAJMhIiIiMtOtW7fQpk0bLF68uMKx27dv48iRI5g+fTqOHDmC77//Hunp6ejTp49Ju/DwcKSmpmLXrl3Ytm0b9u/fj5EjR4rH9Xo9unfvjoYNGyI5ORkffPABYmJisGzZMrFNQkICBg8ejIiICBw9ehRhYWEICwvDyZMnzbofhSBY+UpLdZRer4ebmxuun24EtStzVro/hWra1nYIRDWiVCjBXvyAvLw8qNXqGrlG+ffE4VQfuEj8nsi/aUT7h7LvKl6FQoFNmzYhLCysyjZJSUl47LHHcOHCBQQEBCAtLQ2BgYFISkpC+/btAQDx8fHo2bMnLl26BI1GgyVLluDtt9+GTqeDUqkEAEydOhWbN2/GqVOnAAAvvPACbt26hW3btonXevzxx9G2bVssXbq02vfAb1kiIiIZkzpEVr7VpLy8PCgUCri7uwMAEhMT4e7uLiZCABASEgIbGxscPHhQbPPkk0+KiRAAhIaGIj09HdevXxfbhISEmFwrNDQUiYmJZsXHCdREREQEoKza9E8qlQoqlUpSn4WFhZgyZQoGDx4sVp10Oh28vb1N2tnZ2cHT0xM6nU5so9VqTdr4+PiIxzw8PKDT6cR9/2xT3kd1sTJEREQkY5asDPn7+8PNzU3c5s6dKym2kpISDBw4EIIgYMmSJZa43RrByhAREZGMGQUFjIK0Ya7y8y9evGgyZ0hKVag8Ebpw4QL27Nlj0q+vry9ycnJM2peWliI3Nxe+vr5im+zsbJM25Z//q0358epiZYiIiEjGLFkZUqvVJtvdJkPlidCZM2fw888/w8vLy+R4cHAwbty4geTkZHHfnj17YDQa0aFDB7HN/v37UVJSIrbZtWsXmjdvDg8PD7HN7t27TfretWsXgoODzYqXyRARERGZJT8/HykpKUhJSQEAZGRkICUlBZmZmSgpKcGAAQNw+PBhrFmzBgaDATqdDjqdDsXFxQCAli1bokePHhgxYgQOHTqEAwcOYMyYMRg0aBA0Gg0A4MUXX4RSqURERARSU1Oxfv16xMXFYcKECWIcb7zxBuLj4zF//nycOnUKMTExOHz4MMaMGWPW/fDR+jqKj9aTNeCj9XS/upeP1u856W+RR+ufbnWx2vHu3bsXXbt2rbB/6NChiImJqTDxudwvv/yCLl26AChbdHHMmDHYunUrbGxs0L9/fyxatAguLi5i++PHjyMyMhJJSUmoV68exo4diylTppj0uXHjRrzzzjs4f/48mjZtitjYWPTs2dOMu2cyVGcxGSJrwGSI7lf3MhnafSIAzhK/J27dNKJb68wajbcu47csERERWTU+TUZERCRjllg0saYXXazrmAwRERHJmEGwgUGQNtBjsPIJMxwmIyIiIqvGyhAREZGMGaGAUWJtwwjrLg0xGSIiIpIxzhmSjsNkREREZNVYGSIiIpIxy0yg5jAZERERyVTZnCGJL2q18mEyJkNEREQyZoQNDJxALQnnDBEREZFVY2WIiIhIxjhnSDomQ0RERDJmhA3XGZKIw2RERERk1VgZIiIikjGDoIBBkLjoosTz5Y7JEBERkYwZLPA0mYHDZERERETWi5UhIiIiGTMKNjBKfJrMyKfJiIiISK44TCYdh8mIiIjIqrEyREREJGNGSH8azGiZUGSLyRAREZGMWWbRReseKGIyREREJGOWeR2HdSdD1n33REREZPVYGSIiIpIxIxQwQuqcIa5ATURERDLFYTLprPvuiYiIyOqxMkRERCRjlll00bprI0yGiIiIZMwoKGCUus6Qlb+13rpTQSIiIrJ6rAwRERHJmNECw2RcdJGIiIhkyzJvrbfuZMi6756IiIisHitDREREMmaAAgaJiyZKPV/umAwRERHJGIfJpGMyREREJGMGSK/sGCwTimxZdypIREREVo+VISIiIhnjMJl0TIaIiIhkjC9qlc66756IiIisHitDREREMiZAAaPECdQCH60nIiIiueIwmXTWffdERERk9VgZIiIikjGjoIBRkDbMJfV8uWMyREREJGMGC7y1Xur5cmfdd09ERERWj5UhIiIiGeMwmXRMhoiIiGTMCBsYJQ70SD1f7pgMERERyZhBUMAgsbIj9Xy5s+5UkIiIiKwekyEiIiIZK58zJHUzx/79+9G7d29oNBooFAps3rzZ5LggCIiOjoafnx8cHR0REhKCM2fOmLTJzc1FeHg41Go13N3dERERgfz8fJM2x48fxxNPPAEHBwf4+/sjNja2QiwbN25EixYt4ODggNatW+PHH380614AJkNERESyJvz11nopm2DmCtS3bt1CmzZtsHjx4kqPx8bGYtGiRVi6dCkOHjwIZ2dnhIaGorCwUGwTHh6O1NRU7Nq1C9u2bcP+/fsxcuRI8bher0f37t3RsGFDJCcn44MPPkBMTAyWLVsmtklISMDgwYMRERGBo0ePIiwsDGFhYTh58qRZ96MQBEEw6wy6J/R6Pdzc3HD9dCOoXZmz0v0pVNO2tkMgqhGlQgn24gfk5eVBrVbXyDXKvydG7nseShd7SX0V55dg2VMb7ypehUKBTZs2ISwsDEBZVUij0eDNN9/ExIkTAQB5eXnw8fHBypUrMWjQIKSlpSEwMBBJSUlo3749ACA+Ph49e/bEpUuXoNFosGTJErz99tvQ6XRQKpUAgKlTp2Lz5s04deoUAOCFF17ArVu3sG3bNjGexx9/HG3btsXSpUurfQ/8liUiIpIxAxQW2SwlIyMDOp0OISEh4j43Nzd06NABiYmJAIDExES4u7uLiRAAhISEwMbGBgcPHhTbPPnkk2IiBAChoaFIT0/H9evXxTb/vE55m/LrVBefJiMiIpIxoyB9nSDjX2NEer3eZL9KpYJKpTKrL51OBwDw8fEx2e/j4yMe0+l08Pb2NjluZ2cHT09PkzZarbZCH+XHPDw8oNPp7nid6mIyRLJ14ndnbPzUG2dOOCE32x4zvshAx//lAQBKS4CV7/shaY8aVy4o4aw2ot0TNxHxVha8fEvFPi6dU2H5bA3+SHJGaYkC2pYFGDJZh7ad/p7E9+k7DyA1yRkX0h3g36QIS35ON4njqw998fUC3wrxqRwN2HLuRA3dPVEZL98SRLydhUe73oTK0Yis8yrMj/LHmeNOAIA3F2ai+wvXTc45/Isr3g5vJH6OWZmBxg8VwN2rFDfzbHH0V1d8MccPudnShl5Ifvz9/U0+z5gxAzExMbUTzD1UZ5Oh8+fPQ6vV4ujRo2jbtm1th0N1UOFtGzR6qAChg3MxK8L0Xw9FBTY4e8IJL47PRqPAAuTn2WJJ9AOYMawRPok/LbaLHqrFA9oivL/xLFQORmxaXh/RQ7RYmZgGT++/k6bQQbk4ddQJGX84VohjwOs56DXk/0z2TRnYGM3bFlj4jolMubiVYsEPZ3A8wQXvvNQIN67Z4oFGxcjPszVpl7THFfOj/v6SKyk2rSIcO+CCdYu8kZttj3p+JRgRnYXpy88jqk/Te3IfJE35JGipfQDAxYsXTeYMmVsVAgBf37J/HGZnZ8PPz0/cn52dLX6f+/r6Iicnx+S80tJS5Obmiuf7+voiOzvbpE355/9qU368ujhnqAqFhYWIjIyEl5cXXFxc0L9//wo/8DsZN24cgoKCoFKpmMzVkEefvolhU3To9Fc16J+c1UbMW38OT/W5Af8mRWgZdBuRcy7hzHEn5Fwq+9du3jVbXP7TAQPH5KBRYCEeaFSMV9++gqICW5w/5SD2Nfrdy+jzyv/BL6C40jgcnY3w9C4Vt+tX7ZB52hGhg6/VzI0T/WVgZA7+L0uJ+VEBSE9xQvZFFY7sc8WVC6ZfYCXFCly/ai9u+Xmm/w7etLw+Th1xRs5lJf447Iz1n3ijxSO3YWvH52vkwAiFRTYAUKvVJtvdJENarRa+vr7YvXu3uE+v1+PgwYMIDg4GAAQHB+PGjRtITk4W2+zZswdGoxEdOnQQ2+zfvx8lJSVim127dqF58+bw8PAQ2/zzOuVtyq9TXUyGqhAVFYWtW7di48aN2LdvH7KystCvXz+z+nj11Vfxwgsv1FCEZK5belsoFAKc3QwAALWnAQ0aF+LnjZ4ovG0DQymw/SsvuNcrQdOH776qE7/WCw0aFaJ1h1uWCp2oUo931+P0MUe8/dl5rD+eisU70/G/Fysm4Q8H52P98VR8/uspjJ17Ca4epZX0VsbVvRRP97uOPw47wVBq3asSy0X5CtRSN3Pk5+cjJSUFKSkpAMomTaekpCAzMxMKhQLjx4/Hu+++iy1btuDEiRMYMmQINBqN+MRZy5Yt0aNHD4wYMQKHDh3CgQMHMGbMGAwaNAgajQYA8OKLL0KpVCIiIgKpqalYv3494uLiMGHCBDGON954A/Hx8Zg/fz5OnTqFmJgYHD58GGPGjDHrfmo1GTIajYiNjUWTJk2gUqkQEBCAOXPmVNrWYDAgIiICWq0Wjo6OaN68OeLi4kza7N27F4899hicnZ3h7u6OTp064cKFCwCAY8eOoWvXrnB1dYVarUZQUBAOHz5c6bXy8vLwxRdfYMGCBXj66acRFBSEFStWICEhAb///nu17m3RokWIjIxEo0aN/rsx1bjiQgW+mKNBl7DrcHY1AgAUCmDe+nM4d9IRYU1b41ltG3y/zBtz1vwJV3fDXV9nzyYPhA7OtWT4RJXyCyjGs0OuIStDhbde1GLbqnp4ffZlhDz/9+/f4b2u+OCNAEwZ2AhfzPFD6+B8zPn6T9jYmFZ9It7Owg9nT+DbP1JRX1OCmFe0/74ckejw4cNo164d2rVrBwCYMGEC2rVrh+joaADA5MmTMXbsWIwcORKPPvoo8vPzER8fDweHv6vua9asQYsWLdCtWzf07NkTnTt3NllDyM3NDTt37kRGRgaCgoLw5ptvIjo62mQtoo4dO2Lt2rVYtmwZ2rRpg2+//RabN29Gq1atzLqfWp0zNG3aNCxfvhwLFy5E586dceXKFXHtgH8zGo1o0KABNm7cCC8vLyQkJGDkyJHw8/PDwIEDUVpairCwMIwYMQLffPMNiouLcejQISgUZdlueHg42rVrhyVLlsDW1hYpKSmwt698cmBycjJKSkpMHtdr0aIFAgICkJiYiMcff9ziP4uioiIUFRWJn/89o5/uXmkJMOe1BwEBGDvvkrhfEIBP3moA93qlmL/pLJQORsR/44UZw7RY9ONpePlU/a/nqhz4yQ0F+bZ4ZiCTIap5ChvgzHFHrJhXNi/j3EknPNiiEL1evoafN3oCAPb94CG2P3/KERl/OGDV76fwcMd8pPzmKh7buMQb8d94wadBMcIn6DApLhPRQ7SABR+5ppphyTlD1dWlSxfcaZlChUKBWbNmYdasWVW28fT0xNq1a+94nYcffhi//vrrHds8//zzeP755+8c8H+otWTo5s2biIuLwyeffIKhQ4cCABo3bozOnTtX2t7e3h4zZ84UP2u1WiQmJmLDhg0YOHAg9Ho98vLy8Oyzz6Jx48YAyspw5TIzMzFp0iS0aNECANC0adUTA8sXeHJ3dzfZfzeP61XX3LlzTe6PLKM8Ecq+rETshrNiVQgAUn5zwaGf1fg27YS4v+nDl3Bkf0v8vMETL4zNqarbKsV/44UOIXnwqG9+IkVkrtwcO1w47WCy7+IZFTr3vFHlObpMFW5cs4XmwWKk/Pb3fn2uHfS5drj8pwqZZ1RYk5yGlkG3kZbsXEPRk6UYYf7rNCrrw5rV2jBZWloaioqK0K1bt2qfs3jxYgQFBaF+/fpwcXHBsmXLkJmZCaAswxw2bBhCQ0PRu3dvxMXF4cqVK+K5EyZMwPDhwxESEoJ58+bh3LlzFr8nKaZNm4a8vDxxu3jxYm2HJHvlidDlDBXmrT8Ltafp0FdRQdmvv82//gpsFIK45oY5dJlKHDvgwiEyumf+SHKGf+Mik30PNCpCzmVlFWcA9fyKofYwIDen6n8LK/76m7BXcgI1WYdaS4YcHSs+onwn69atw8SJExEREYGdO3ciJSUFr7zyCoqL/37CZ8WKFUhMTETHjh2xfv16NGvWTJzjExMTg9TUVPTq1Qt79uxBYGAgNm3aVOm1fH19UVxcjBs3bpjsv5vH9apLpVJVmMVPd1ZwywbnTjri3Mmy3yXdRSXOnXREziV7lJYAs0docfqYE6Z8cgFGgwK5OXbIzbETHytuGXQLLm4GfPBGAM6lOpStOTRLA91FJR7r9vcw5eWMsn5zr9qhuFAhXvPfjyfvWOcJT58SPPo0hzjp3vh+WX20eOQWBo3NhubBInR97jp6vpSLLSvqAQAcnAwYPj0LLR65BZ8GxWjb+SZiVpxHVoYSyXvLhsiat7uFPq/8Hxo9VADvB4rRptNNTPv0ArIylEhLdqrN26NqEizwJJlg5ZWhWhsma9q0KRwdHbF7924MHz78P9sfOHAAHTt2xOjRo8V9lVV3yid0TZs2DcHBwVi7dq04x6dZs2Zo1qwZoqKiMHjwYKxYsQLPPfdchT6CgoJgb2+P3bt3o3///gCA9PR0ZGZmmv24HtWc08ecMHlAE/HzZzEPAACeGZiLl97U4fedbgCA0c+0MDkv9tuzaNMxH25eBsxZew4r5/lhysAmMJQo0LB5IWJWZKDxQ3+/TPCjiQE4nugifh7dvTkAYNXBP+DrX5aMG43AzvWeeGZgLmxNl3ghqjGnjzlhVoQWr0y7gvCobOguKrE0WoNfNpXNEzIayxYSfeb563BWG3At2w5H9rliVawvSorL/i1cVGCDTv/Lw8tv6uDgZERujj0O/+KKOXE+Yhuq2+7mrfOV9WHNai0ZcnBwwJQpUzB58mQolUp06tQJV69eRWpqKiIiIiq0b9q0KVavXo0dO3ZAq9Xiq6++QlJSkrhUd0ZGBpYtW4Y+ffpAo9EgPT0dZ86cwZAhQ1BQUIBJkyZhwIAB0Gq1uHTpEpKSksRE59/c3NwQERGBCRMmwNPTE2q1GmPHjkVwcHC1J0+fPXsW+fn50Ol0KCgoEB8/DAwMNHnPCt29Nh3zsSMrpcrjdzpWrlmbArz3zZ93bPPBd2f/sx8bG2BN8h//2Y7I0g7+rMbBnyuvJBcX2uDtFxvf8fzzpxwxZeCd2xDd72r1abLp06fDzs4O0dHRyMrKgp+fH0aNGlVp29deew1Hjx7FCy+8AIVCgcGDB2P06NH46aefAABOTk44deoUVq1ahWvXrsHPzw+RkZF47bXXUFpaimvXrmHIkCHIzs5GvXr10K9fvztOWF64cCFsbGzQv39/FBUVITQ0FJ9++mm172348OHYt2+f+Ln88cOMjAw8+OCD1e6HiIjoTmrjabL7jUK407NxVGv0ej3c3Nxw/XQjqF2t+5eU7l+hmra1HQJRjSgVSrAXPyAvL6/G5oCWf0/03fkq7J2ljTiU3CrGD92/rNF46zJ+yxIREZFVYzJ0F0aNGgUXF5dKt6qG+YiIiGqCJd9NZq3q7Fvr67JZs2Zh4sSJlR6zxvIiERHVHj5NJh2Tobvg7e0Nb2/v2g6DiIiIyZAFcJiMiIiIrBorQ0RERDLGypB0TIaIiIhkjMmQdBwmIyIiIqvGyhAREZGMCYDkR+OtffVlJkNEREQyxmEy6ThMRkRERFaNlSEiIiIZY2VIOiZDREREMsZkSDoOkxEREZFVY2WIiIhIxlgZko7JEBERkYwJggKCxGRG6vlyx2SIiIhIxoxQSF5nSOr5csc5Q0RERGTVWBkiIiKSMc4Zko7JEBERkYxxzpB0HCYjIiIiq8bKEBERkYxxmEw6JkNEREQyxmEy6ThMRkRERFaNlSEiIiIZEywwTGbtlSEmQ0RERDImABAE6X1YMw6TERERkVVjZYiIiEjGjFBAwddxSMJkiIiISMb4NJl0TIaIiIhkzCgooOA6Q5JwzhARERFZNVaGiIiIZEwQLPA0mZU/TsZkiIiISMY4Z0g6DpMRERGRVWNliIiISMZYGZKOyRAREZGM8Wky6ThMRkRERFaNlSEiIiIZ49Nk0jEZIiIikrGyZEjqnCELBSNTHCYjIiIiq8bKEBERkYzxaTLpmAwRERHJmPDXJrUPa8ZkiIiISMZYGZKOc4aIiIio2gwGA6ZPnw6tVgtHR0c0btwYs2fPhvCPWdiCICA6Ohp+fn5wdHRESEgIzpw5Y9JPbm4uwsPDoVar4e7ujoiICOTn55u0OX78OJ544gk4ODjA398fsbGxNXJPTIaIiIjkTLDQVk3vv/8+lixZgk8++QRpaWl4//33ERsbi48//lhsExsbi0WLFmHp0qU4ePAgnJ2dERoaisLCQrFNeHg4UlNTsWvXLmzbtg379+/HyJEjxeN6vR7du3dHw4YNkZycjA8++AAxMTFYtmzZ3fyU7ojDZERERHJmgWEymHF+QkIC+vbti169egEAHnzwQXzzzTc4dOhQWVeCgI8++gjvvPMO+vbtCwBYvXo1fHx8sHnzZgwaNAhpaWmIj49HUlIS2rdvDwD4+OOP0bNnT3z44YfQaDRYs2YNiouL8eWXX0KpVOKhhx5CSkoKFixYYJI0WQIrQ0RERASgrBrzz62oqKhCm44dO2L37t04ffo0AODYsWP47bff8L///Q8AkJGRAZ1Oh5CQEPEcNzc3dOjQAYmJiQCAxMREuLu7i4kQAISEhMDGxgYHDx4U2zz55JNQKpVim9DQUKSnp+P69esWvW9WhoiIiGTMkitQ+/v7m+yfMWMGYmJiTPZNnToVer0eLVq0gK2tLQwGA+bMmYPw8HAAgE6nAwD4+PiYnOfj4yMe0+l08Pb2NjluZ2cHT09PkzZarbZCH+XHPDw87vJuK2IyREREJGOWfJrs4sWLUKvV4n6VSlWh7YYNG7BmzRqsXbtWHLoaP348NBoNhg4dKimO2sJkiIiIiAAAarXaJBmqzKRJkzB16lQMGjQIANC6dWtcuHABc+fOxdChQ+Hr6wsAyM7Ohp+fn3hednY22rZtCwDw9fVFTk6OSb+lpaXIzc0Vz/f19UV2drZJm/LP5W0shXOGiIiI5ExQWGarptu3b8PGxjR9sLW1hdFoBABotVr4+vpi9+7d4nG9Xo+DBw8iODgYABAcHIwbN24gOTlZbLNnzx4YjUZ06NBBbLN//36UlJSIbXbt2oXmzZtbdIgMYDJEREQka+VzhqRu1dW7d2/MmTMH27dvx/nz57Fp0yYsWLAAzz33HABAoVBg/PjxePfdd7FlyxacOHECQ4YMgUajQVhYGACgZcuW6NGjB0aMGIFDhw7hwIEDGDNmDAYNGgSNRgMAePHFF6FUKhEREYHU1FSsX78ecXFxmDBhgqV/hBwmIyIiour7+OOPMX36dIwePRo5OTnQaDR47bXXEB0dLbaZPHkybt26hZEjR+LGjRvo3Lkz4uPj4eDgILZZs2YNxowZg27dusHGxgb9+/fHokWLxONubm7YuXMnIiMjERQUhHr16iE6Otrij9UDgEIQpM5Bp5qg1+vh5uaG66cbQe3KAh7dn0I1bWs7BKIaUSqUYC9+QF5e3n/Owblb5d8TDZdPh42Tw3+fcAfG24W4MGJ2jcZbl1WrMrRly5Zqd9inT5+7DoaIiIjMw3eTSVetZKh8jO+/KBQKGAwGKfEQERGRuTjGI0m1kqHyGeJERERE9xtJE6gLCwtNJkMRERHRvcVhMunMnplrMBgwe/ZsPPDAA3BxccGff/4JAJg+fTq++OILiwdIREREd3CP31p/PzI7GZozZw5WrlyJ2NhYk5entWrVCp9//rlFgyMiIiKqaWYnQ6tXr8ayZcsQHh4OW1tbcX+bNm1w6tQpiwZHRERE/0Vhoc16mT1n6PLly2jSpEmF/Uaj0WTJbCIiIroHLDHMxWEy8wQGBuLXX3+tsP/bb79Fu3btLBIUERER0b1idmUoOjoaQ4cOxeXLl2E0GvH9998jPT0dq1evxrZt22oiRiIiIqoKK0OSmV0Z6tu3L7Zu3Yqff/4Zzs7OiI6ORlpaGrZu3YpnnnmmJmIkIiKiqtzjt9bfj+5qnaEnnngCu3btsnQsRERERPfcXS+6ePjwYaSlpQEom0cUFBRksaCIiIioegShbJPahzUzOxm6dOkSBg8ejAMHDsDd3R0AcOPGDXTs2BHr1q1DgwYNLB0jERERVYVzhiQze87Q8OHDUVJSgrS0NOTm5iI3NxdpaWkwGo0YPnx4TcRIREREVeGcIcnMrgzt27cPCQkJaN68ubivefPm+Pjjj/HEE09YNDgiIiKimmZ2MuTv71/p4ooGgwEajcYiQREREVH1KISyTWof1szsYbIPPvgAY8eOxeHDh8V9hw8fxhtvvIEPP/zQosERERHRf+CLWiWrVmXIw8MDCsXf44m3bt1Chw4dYGdXdnppaSns7Ozw6quvIiwsrEYCJSIiIqoJ1UqGPvrooxoOg4iIiO6KJSZAcwL1fxs6dGhNx0FERER3g4/WS3bXiy4CQGFhIYqLi032qdVqSQERERER3UtmT6C+desWxowZA29vbzg7O8PDw8NkIyIionuIE6glMzsZmjx5Mvbs2YMlS5ZApVLh888/x8yZM6HRaLB69eqaiJGIiIiqwmRIMrOHybZu3YrVq1ejS5cueOWVV/DEE0+gSZMmaNiwIdasWYPw8PCaiJOIiIioRphdGcrNzUWjRo0AlM0Pys3NBQB07twZ+/fvt2x0REREdGd8HYdkZidDjRo1QkZGBgCgRYsW2LBhA4CyilH5i1uJiIjo3ihfgVrqZs3MToZeeeUVHDt2DAAwdepULF68GA4ODoiKisKkSZMsHiARERHdAecMSWb2nKGoqCjxv0NCQnDq1CkkJyejSZMmePjhhy0aHBEREVFNk7TOEAA0bNgQDRs2tEQsRERERPdctZKhRYsWVbvDcePG3XUwREREZB4FLPDWeotEIl/VSoYWLlxYrc4UCgWTISIiIpKVaiVD5U+P0b33XLPWsFPY13YYRDViR1ZKbYdAVCP0N43waHaPLsYXtUomec4QERER1SK+qFUysx+tJyIiIrqfsDJEREQkZ6wMScZkiIiISMYssYI0V6AmIiIismJ3lQz9+uuveOmllxAcHIzLly8DAL766iv89ttvFg2OiIiI/gNfxyGZ2cnQd999h9DQUDg6OuLo0aMoKioCAOTl5eG9996zeIBERER0B0yGJDM7GXr33XexdOlSLF++HPb2f69/06lTJxw5csSiwREREdGd8a310pmdDKWnp+PJJ5+ssN/NzQ03btywRExERERE94zZyZCvry/Onj1bYf9vv/2GRo0aWSQoIiIiqqbyFailblbM7GRoxIgReOONN3Dw4EEoFApkZWVhzZo1mDhxIl5//fWaiJGIiIiqwjlDkpm9ztDUqVNhNBrRrVs33L59G08++SRUKhUmTpyIsWPH1kSMRERERDXG7GRIoVDg7bffxqRJk3D27Fnk5+cjMDAQLi4uNREfERER3QEXXZTurlegViqVCAwMtGQsREREZC6+jkMys5Ohrl27QqGoeqLVnj17JAVEREREdC+ZnQy1bdvW5HNJSQlSUlJw8uRJDB061FJxERERUXVYYp0gVobMs3Dhwkr3x8TEID8/X3JAREREZAYOk0lmsRe1vvTSS/jyyy8t1R0RERHVUZcvX8ZLL70ELy8vODo6onXr1jh8+LB4XBAEREdHw8/PD46OjggJCcGZM2dM+sjNzUV4eDjUajXc3d0RERFRoahy/PhxPPHEE3BwcIC/vz9iY2Nr5H4slgwlJibCwcHBUt0RERFRddzjdYauX7+OTp06wd7eHj/99BP++OMPzJ8/Hx4eHmKb2NhYLFq0CEuXLsXBgwfh7OyM0NBQFBYWim3Cw8ORmpqKXbt2Ydu2bdi/fz9GjhwpHtfr9ejevTsaNmyI5ORkfPDBB4iJicGyZcvu5qd0R2YPk/Xr18/ksyAIuHLlCg4fPozp06dbLDAiIiL6b/f60fr3338f/v7+WLFihbhPq9WK/y0IAj766CO888476Nu3LwBg9erV8PHxwebNmzFo0CCkpaUhPj4eSUlJaN++PQDg448/Rs+ePfHhhx9Co9FgzZo1KC4uxpdffgmlUomHHnoIKSkpWLBggUnSZAlmV4bc3NxMNk9PT3Tp0gU//vgjZsyYYdHgiIiI6N7R6/UmW1FRUYU2W7ZsQfv27fH888/D29sb7dq1w/Lly8XjGRkZ0Ol0CAkJEfe5ubmhQ4cOSExMBFA2muTu7i4mQgAQEhICGxsbHDx4UGzz5JNPQqlUim1CQ0ORnp6O69evW/S+zaoMGQwGvPLKK2jdurVJOYyIiIjkz9/f3+TzjBkzEBMTY7Lvzz//xJIlSzBhwgS89dZbSEpKwrhx46BUKjF06FDodDoAgI+Pj8l5Pj4+4jGdTgdvb2+T43Z2dvD09DRp88+K0z/71Ol0Fs1DzEqGbG1t0b17d6SlpTEZIiIiqgss+DTZxYsXoVarxd0qlapCU6PRiPbt2+O9994DALRr1w4nT57E0qVLZbvEjtnDZK1atcKff/5ZE7EQERGRmcrnDEndAECtVptslSVDfn5+Fd5A0bJlS2RmZgIAfH19AQDZ2dkmbbKzs8Vjvr6+yMnJMTleWlqK3NxckzaV9fHPa1iK2cnQu+++i4kTJ2Lbtm24cuVKhfFFIiIiun916tQJ6enpJvtOnz6Nhg0bAiibTO3r64vdu3eLx/V6PQ4ePIjg4GAAQHBwMG7cuIHk5GSxzZ49e2A0GtGhQwexzf79+1FSUiK22bVrF5o3b27x0alqJ0OzZs3CrVu30LNnTxw7dgx9+vRBgwYN4OHhAQ8PD7i7u3PojIiIqDbco8fqASAqKgq///473nvvPZw9exZr167FsmXLEBkZCaDshe7jx4/Hu+++iy1btuDEiRMYMmQINBoNwsLCAJRVknr06IERI0bg0KFDOHDgAMaMGYNBgwZBo9EAAF588UUolUpEREQgNTUV69evR1xcHCZMmCDpR1WZas8ZmjlzJkaNGoVffvnF4kEQERHRXbrHK1A/+uij2LRpE6ZNm4ZZs2ZBq9Xio48+Qnh4uNhm8uTJuHXrFkaOHIkbN26gc+fOiI+PN1mPcM2aNRgzZgy6desGGxsb9O/fH4sWLRKPu7m5YefOnYiMjERQUBDq1auH6Ohoiz9WDwAKQRCq9SOwsbGpdPY31Qy9Xg83Nzd0QV/YKexrOxyiGrEjK6W2QyCqEfqbRng0+xN5eXkmE5Iteo2/vieaTHkPtippix4bigpx9v23ajTeusysp8nu9LZ6IiIiuvfu9aKL9yOzkqFmzZr9Z0KUm5srKSAiIiIyA1/UKplZydDMmTPh5uZWU7EQERER3XNmJUODBg3inCEiIqI6hMNk0lU7GeJ8ISIiojqIw2SSVTsZquZDZ0RERHQvMRmSrNrJkNForMk4iIiIiGqFWXOGiIiIqG7hnCHpmAwRERHJGYfJJDP7Ra1ERERE9xNWhoiIiOSMlSHJmAwRERHJGOcMScdhMiIiIrJqrAwRERHJGYfJJGMyREREJGMcJpOOw2RERERk1VgZIiIikjMOk0nGZIiIiEjOmAxJxmSIiIhIxhR/bVL7sGacM0RERERWjZUhIiIiOeMwmWRMhoiIiGSMj9ZLx2EyIiIismqsDBEREckZh8kkYzJEREQkd1aezEjFYTIiIiKyaqwMERERyRgnUEvHZIiIiEjOOGdIMg6TERERkVVjZYiIiEjGOEwmHZMhIiIiOeMwmWRMhoiIiGSMlSHpOGeIiIiIrBorQ0RERHLGYTLJmAwRERHJGZMhyThMRkRERFaNlSEiIiIZ4wRq6ZgMERERyRmHySTjMBkRERFZNVaGiIiIZEwhCFAI0ko7Us+XOyZDREREcsZhMsk4TEZERERWjZUhIiIiGePTZNIxGSIiIpIzDpNJxmSIiIhIxlgZko5zhoiIiMiqsTJEREQkZxwmk4zJEBERkYxxmEw6DpMRERGRVWMyREREJGeChba7NG/ePCgUCowfP17cV1hYiMjISHh5ecHFxQX9+/dHdna2yXmZmZno1asXnJyc4O3tjUmTJqG0tNSkzd69e/HII49ApVKhSZMmWLly5d0HegdMhoiIiGSufKjsbre7lZSUhM8++wwPP/ywyf6oqChs3boVGzduxL59+5CVlYV+/fqJxw0GA3r16oXi4mIkJCRg1apVWLlyJaKjo8U2GRkZ6NWrF7p27YqUlBSMHz8ew4cPx44dO+4+4CowGSIiIiKz5efnIzw8HMuXL4eHh4e4Py8vD1988QUWLFiAp59+GkFBQVixYgUSEhLw+++/AwB27tyJP/74A19//TXatm2L//3vf5g9ezYWL16M4uJiAMDSpUuh1Woxf/58tGzZEmPGjMGAAQOwcOFCi98LkyEiIiI5EwTLbGaKjIxEr169EBISYrI/OTkZJSUlJvtbtGiBgIAAJCYmAgASExPRunVr+Pj4iG1CQ0Oh1+uRmpoqtvl336GhoWIflsSnyYiIiGTMkk+T6fV6k/0qlQoqlapC+3Xr1uHIkSNISkqqcEyn00GpVMLd3d1kv4+PD3Q6ndjmn4lQ+fHyY3dqo9frUVBQAEdHx+rf4H9gZYiIiIgAAP7+/nBzcxO3uXPnVmhz8eJFvPHGG1izZg0cHBxqIUrLY2WIiIhIziy46OLFixehVqvF3ZVVhZKTk5GTk4NHHnlE3GcwGLB//3588skn2LFjB4qLi3Hjxg2T6lB2djZ8fX0BAL6+vjh06JBJv+VPm/2zzb+fQMvOzoZarbZoVQhgZYiIiEjWFEbLbACgVqtNtsqSoW7duuHEiRNISUkRt/bt2yM8PFz8b3t7e+zevVs8Jz09HZmZmQgODgYABAcH48SJE8jJyRHb7Nq1C2q1GoGBgWKbf/ZR3qa8D0tiZYjua16+JYh4OwuPdr0JlaMRWedVmB/ljzPHnQAA7vVKEPH2FQQ9dRPObgac/N0Fi995AFkZZf8DcHUvxcsTdXjkqXx4a4qRl2uHhHg3rIr1xe2btrV5a3SfO/G7MzZ+6o0zJ5yQm22PGV9koOP/8gAApSXAyvf9kLRHjSsXlHBWG9HuiZuIeCsLXr5l67QcS3DB5AFNKu170Y/paN62AF996IuvF/hWOK5yNGDLuRPi5++X18f2VV7IyVJC7VGKJ569gVenXYHSwcqXLa4r7vHrOFxdXdGqVSuTfc7OzvDy8hL3R0REYMKECfD09IRarcbYsWMRHByMxx9/HADQvXt3BAYG4uWXX0ZsbCx0Oh3eeecdREZGignYqFGj8Mknn2Dy5Ml49dVXsWfPHmzYsAHbt2+XeLMV3ZfJ0Pnz56HVanH06FG0bdu2tsOhWuLiVooFP5zB8QQXvPNSI9y4ZosHGhUjP688iREw48vzMJQqEPOKFrfzbdBv5FXMW38OI55qjqICW3j6lMDLpxTLZ/kh87QDvBsUY9y8S/DyKcG7Ix+szduj+1zhbRs0eqgAoYNzMStCa3KsqMAGZ0844cXx2WgUWID8PFssiX4AM4Y1wifxpwEAge1v4ZuUkybnrYr1Q8pvLmjWpgAAMOD1HPQa8n8mbaYMbIzmbQvEz3u+d8eX7/lhwvxMBD56G5fPqfBhVAAUCuC1mKyauHW6DyxcuBA2Njbo378/ioqKEBoaik8//VQ8bmtri23btuH1119HcHAwnJ2dMXToUMyaNUtso9VqsX37dkRFRSEuLg4NGjTA559/jtDQUIvHe18mQ7UpNzcXM2bMwM6dO5GZmYn69esjLCwMs2fPhpubW22HZ1UGRubg/7KUmB8VIO7Lvvh3yfeBRsUIbH8bI7s0x4XTZZMAP57aAOuO/YGuz91A/FovXEh3xOwRD4rnXLmgwsr3/TD540zY2AowGhT37H7Iujz69E08+vTNSo85q42Yt/6cyb7IOZcwrmdz5Fyyh3eDEtgrBXh6/72ab2kJkLhDjb6v/h8Uf/3aOjob4ehsFNucS3VA5mlHjHv/krjvj8POeOjRW3i63w0AgK9/MbqEXUf6EScL3SlJVRfeTbZ3716Tzw4ODli8eDEWL15c5TkNGzbEjz/+eMd+u3TpgqNHj0oLrho4Z8jCsrKykJWVhQ8//BAnT57EypUrER8fj4iIiNoOzeo83l2P08cc8fZn57H+eCoW70zH/168Jh63V5Z9CRQX/Z3QCIICJcUKPPTorSr7dVYbcDvfhokQ1Sm39LZQKAQ4uxkqPZ640w03r9uh+wu5VfYRv9YLDRoVonWHv3//A9vfwpnjTjh1tCz5uXJBiaTdajzaTV9VN3Sv1dI6Q/cT2SZDRqMRsbGxaNKkCVQqFQICAjBnzpxK2xoMBkRERECr1cLR0RHNmzdHXFycSZu9e/fiscceg7OzM9zd3dGpUydcuHABAHDs2DF07doVrq6uUKvVCAoKwuHDhyu9VqtWrfDdd9+hd+/eaNy4MZ5++mnMmTMHW7durfDOFapZfgHFeHbINWRlqPDWi1psW1UPr8++jJDny74MLp51QPYle7w67Qpc3EphZ2/EwMgc1NeUwNOnpNI+1Z6leHF8Nn762ute3grRHRUXKvDFHA26hF2Hs6ux0jY7vvFCUJebqK+p/He7uFCBPZs8EDrYNFl6ut8NDJl4BW+GNUHPgDYYFhyIhzvmY/C4nEr7IZIj2Q6TTZs2DcuXL8fChQvRuXNnXLlyBadOnaq0rdFoRIMGDbBx40Z4eXkhISEBI0eOhJ+fHwYOHIjS0lKEhYVhxIgR+Oabb1BcXIxDhw5B8VctOTw8HO3atcOSJUtga2uLlJQU2NvbVzvWvLw8qNVq2NlV/eMuKipCUVGR+PnfC1+R+RQ2wJnjjlgxzw8AcO6kEx5sUYheL1/Dzxs9YShVYFbEg5iw4CK+S0uFoRQ4+qsrDu12FYcR/snJxYDZqzOQedoBX82vOOmUqDaUlgBzXnsQEICx8y5V2uZqlj2S97rirc/OV9nPgZ/cUJBvi2cGmiZDxxJcsO5jH4x57xJaPHIbWedVWDL9AaxZ6IPwqOwqeqN7qS4Mk8mdLJOhmzdvIi4uDp988gmGDh0KAGjcuDE6d+5caXt7e3vMnDlT/KzVapGYmIgNGzZg4MCB0Ov1yMvLw7PPPovGjRsDAFq2bCm2z8zMxKRJk9CiRQsAQNOmTasd6//93/9h9uzZGDly5B3bzZ071yRGki43x06cC1Tu4hkVOve8IX4+e8IJo59pDidXA+ztBeTl2iFu2xmcPm66hoWjswFz1v6Jgls2mBnxIAylHCKj2leeCGVfViJ2w9kqq0I713vC1aMUwd3zquwr/hsvdAjJg0d90wr2qlhfdOt/Hf8LL0uStC0LUXjbBnGT/DH4jWzYyHZ84T5yj58mux/J8tc4LS0NRUVF6NatW7XPWbx4MYKCglC/fn24uLhg2bJlyMzMBAB4enpi2LBhCA0NRe/evREXF4crV66I506YMAHDhw9HSEgI5s2bh3PnzlV1GRN6vR69evVCYGAgYmJi7th22rRpyMvLE7eLFy9W+96ocn8kOcO/cZHJvgcaFSHnsrJC29s3bZGXaweNtghN29xG4o6/J7s7uRjw3jd/oqRYgRnDtCgpkuWfDd1nyhOhyxkqzFt/FmrPyucKCUJZMhQy4Drsqiho6zKVOHbApcIQGVD25JrCxvSb0uavz1Y+zYTuI7L8v7q5K0+uW7cOEydOREREBHbu3ImUlBS88sor4ptxAWDFihVITExEx44dsX79ejRr1kx8u25MTAxSU1PRq1cv7NmzB4GBgdi0adMdr3nz5k306NEDrq6u2LRp038Oq6lUqgqLXZE03y+rjxaP3MKgsdnQPFiErs9dR8+XcrFlRT2xzRPP3sDDwfnwDShCcGge5q47h8R4NxzZ5wrg70TIwcmIhW/6w8nFAI/6JfCoXyJ+IRDVhIJbNjh30hHnTpb9/053UYlzJx2Rc8kepSXA7BFanD7mhCmfXIDRoEBujh1yc+xQUmxatUz5zQW6TBV6/OPhgX/bsc4Tnj4lePTpisPzjz+jx/bV9bB3szt0mUok73PBqg/80OGZPNhyqa06oXyYTOpmzWQ5TNa0aVM4Ojpi9+7dGD58+H+2P3DgADp27IjRo0eL+yqr7rRr1w7t2rXDtGnTEBwcjLVr14oLRDVr1gzNmjVDVFQUBg8ejBUrVuC5556r9Hp6vR6hoaFQqVTYsmXLffPuFrk5fcwJsyK0eGXaFYRHZUN3UYml0Rr8sslDbOPpU4LXYrLgXq8UuTl2+HmjB9Z+9PeLAZu0LkDLoNsAgJWJpnPShjzWEtmXKlaZiCzh9DEnk0UTP4t5AADwzMBcvPSmDr/vLKtejn6mhcl5sd+eRZuO+eLn+G+8ENg+HwFNTauk5YzGssrRMwNzK01uXhyvg0IhYGWsH67p7OHmWYrHn8nDsKk6qbdIlmKJp8GsvMwny2TIwcEBU6ZMweTJk6FUKtGpUydcvXoVqamplT7C3rRpU6xevRo7duyAVqvFV199haSkJGi1ZQuZZWRkYNmyZejTpw80Gg3S09Nx5swZDBkyBAUFBZg0aRIGDBgArVaLS5cuISkpCf379680Nr1ej+7du+P27dv4+uuvodfrxcnQ9evXhy3/KXVPHfxZjYM/V11l++GL+vjhi/pVHj+e6IJQTZuaCI3ojtp0zMeOrJQqj9/p2D9N+/TCHY/b2ABrkv+o8ritHfDSm9l46U1Olqb7lyyTIQCYPn067OzsEB0djaysLPj5+WHUqFGVtn3ttddw9OhRvPDCC1AoFBg8eDBGjx6Nn376CQDg5OSEU6dOYdWqVbh27Rr8/PwQGRmJ1157DaWlpbh27RqGDBmC7Oxs1KtXD/369atysvORI0dw8OBBAECTJqZL4WdkZODBBx+03A+BiIisHp8mk04hCFZeG6uj9Ho93Nzc0AV9Yaeo/mP8RHJS3eoGkdzobxrh0exPcWmVGrnGX98TwT1mwc5e2nSM0pJCJMZH12i8dZlsK0NERETEypAlyPJpMiIiIiJLYWWIiIhIzoxC2Sa1DyvGZIiIiEjOuAK1ZBwmIyIiIqvGyhAREZGMKWCBCdQWiUS+mAwRERHJGVeglozDZERERGTVWBkiIiKSMa4zJB2TISIiIjnj02SScZiMiIiIrBorQ0RERDKmEAQoJE6Alnq+3DEZIiIikjPjX5vUPqwYkyEiIiIZY2VIOs4ZIiIiIqvGyhAREZGc8WkyyZgMERERyRlXoJaMw2RERERk1VgZIiIikjGuQC0dkyEiIiI54zCZZBwmIyIiIqvGyhAREZGMKYxlm9Q+rBmTISIiIjnjMJlkHCYjIiIiq8bKEBERkZxx0UXJmAwRERHJGN9NJh2TISIiIjnjnCHJOGeIiIiIrBorQ0RERHImAJD6aLx1F4aYDBEREckZ5wxJx2EyIiIismqsDBEREcmZAAtMoLZIJLLFZIiIiEjO+DSZZBwmIyIiIqvGyhAREZGcGQEoLNCHFWMyREREJGN8mkw6JkNERERyxjlDknHOEBEREVk1VoaIiIjkjJUhyZgMERERyRmTIck4TEZERETVNnfuXDz66KNwdXWFt7c3wsLCkJ6ebtKmsLAQkZGR8PLygouLC/r374/s7GyTNpmZmejVqxecnJzg7e2NSZMmobS01KTN3r178cgjj0ClUqFJkyZYuXJljdwTkyEiIiI5M1poq6Z9+/YhMjISv//+O3bt2oWSkhJ0794dt27dEttERUVh69at2LhxI/bt24esrCz069dPPG4wGNCrVy8UFxcjISEBq1atwsqVKxEdHS22ycjIQK9evdC1a1ekpKRg/PjxGD58OHbs2HE3P6U7UgiCldfG6ii9Xg83Nzd0QV/YKexrOxyiGrEjK6W2QyCqEfqbRng0+xN5eXlQq9U1c42/vidCmk2Ana1KUl+lhiL8fHrBXcV79epVeHt7Y9++fXjyySeRl5eH+vXrY+3atRgwYAAA4NSpU2jZsiUSExPx+OOP46effsKzzz6LrKws+Pj4AACWLl2KKVOm4OrVq1AqlZgyZQq2b9+OkydPitcaNGgQbty4gfj4eEn3+2+sDBEREdFdy8vLAwB4enoCAJKTk1FSUoKQkBCxTYsWLRAQEIDExEQAQGJiIlq3bi0mQgAQGhoKvV6P1NRUsc0/+yhvU96HJXECNRERkZxZcAK1Xq832a1SqaBSVV11MhqNGD9+PDp16oRWrVoBAHQ6HZRKJdzd3U3a+vj4QKfTiW3+mQiVHy8/dqc2er0eBQUFcHR0NPMmq8bKEBERkZwZBctsAPz9/eHm5iZuc+fOveOlIyMjcfLkSaxbt+5e3GmNYWWIiIiIAAAXL140mTN0p6rQmDFjsG3bNuzfvx8NGjQQ9/v6+qK4uBg3btwwqQ5lZ2fD19dXbHPo0CGT/sqfNvtnm38/gZadnQ21Wm3RqhDAyhAREZG8lQ+TSd0AqNVqk62yZEgQBIwZMwabNm3Cnj17oNVqTY4HBQXB3t4eu3fvFvelp6cjMzMTwcHBAIDg4GCcOHECOTk5Yptdu3ZBrVYjMDBQbPPPPsrblPdhSawMERERyZoF5gyh+udHRkZi7dq1+OGHH+Dq6irO8XFzc4OjoyPc3NwQERGBCRMmwNPTE2q1GmPHjkVwcDAef/xxAED37t0RGBiIl19+GbGxsdDpdHjnnXcQGRkpJmCjRo3CJ598gsmTJ+PVV1/Fnj17sGHDBmzfvl3ivVbEZIiIiEjO7vEK1EuWLAEAdOnSxWT/ihUrMGzYMADAwoULYWNjg/79+6OoqAihoaH49NNPxba2trbYtm0bXn/9dQQHB8PZ2RlDhw7FrFmzxDZarRbbt29HVFQU4uLi0KBBA3z++ecIDQ29+/usAtcZqqO4zhBZA64zRPere7rOkHYs7GwkrjNkLMLPGR/XaLx1GStDREREcmYUYM4wV9V9WC8mQ0RERHImGMs2qX1YMT5NRkRERFaNlSEiIiI5u8cTqO9HTIaIiIjkjHOGJOMwGREREVk1VoaIiIjkjMNkkjEZIiIikjMBFkiGLBKJbHGYjIiIiKwaK0NERERyxmEyyZgMERERyZnRCEDioolG6150kckQERGRnLEyJBnnDBEREZFVY2WIiIhIzlgZkozJEBERkZxxBWrJOExGREREVo2VISIiIhkTBCMEQdrTYFLPlzsmQ0RERHImCNKHuax8zhCHyYiIiMiqsTJEREQkZ4IFJlBbeWWIyRAREZGcGY2AQuKcHyufM8RhMiIiIrJqrAwRERHJGYfJJGMyREREJGOC0QhB4jAZH60nIiIi+WJlSDLOGSIiIiKrxsoQERGRnBkFQMHKkBRMhoiIiORMEABIfbTeupMhDpMRERGRVWNliIiISMYEowBB4jCZYOWVISZDREREciYYIX2YzLofrecwGREREVk1VoaIiIhkjMNk0jEZIiIikjMOk0nGZKiOKs/SS1EieWFRorpKf9O6/wdM9y99ftnv9r2ouFjie6IUJZYJRqaYDNVRN2/eBAD8hh9rORKimuPRrLYjIKpZN2/ehJubW430rVQq4evri990lvme8PX1hVKptEhfcqMQrH2gsI4yGo3IysqCq6srFApFbYdz39Pr9fD398fFixehVqtrOxwii+Pv+L0lCAJu3rwJjUYDG5uae1apsLAQxcXFFulLqVTCwcHBIn3JDStDdZSNjQ0aNGhQ22FYHbVazS8Kuq/xd/zeqamK0D85ODhYbQJjSXy0noiIiKwakyEiIiKyakyGiACoVCrMmDEDKpWqtkMhqhH8HSeqGidQExERkVVjZYiIiIisGpMhIiIismpMhkg2zp8/D4VCgZSUlNoOhahW8G+AqGYwGSKqpsLCQkRGRsLLywsuLi7o378/srOzq33+uHHjEBQUBJVKhbZt29ZcoEQ1IDc3F2PHjkXz5s3h6OiIgIAAjBs3Dnl5ebUdGpFkTIaIqikqKgpbt27Fxo0bsW/fPmRlZaFfv35m9fHqq6/ihRdeqKEIiWpOVlYWsrKy8OGHH+LkyZNYuXIl4uPjERERUduhEUnGZIjqFKPRiNjYWDRp0gQqlQoBAQGYM2dOpW0NBgMiIiKg1Wrh6OiI5s2bIy4uzqTN3r178dhjj8HZ2Rnu7u7o1KkTLly4AAA4duwYunbtCldXV6jVagQFBeHw4cOVXisvLw9ffPEFFixYgKeffhpBQUFYsWIFEhIS8Pvvv1fr3hYtWoTIyEg0atTIjJ8IWZu6+jfQqlUrfPfdd+jduzcaN26Mp59+GnPmzMHWrVtRWlpq2R8C0T3G13FQnTJt2jQsX74cCxcuROfOnXHlyhWcOnWq0rZGoxENGjTAxo0b4eXlhYSEBIwcORJ+fn4YOHAgSktLERYWhhEjRuCbb75BcXExDh06JL7rLTw8HO3atcOSJUtga2uLlJQU2NvbV3qt5ORklJSUICQkRNzXokULBAQEIDExEY8//rjlfxhklerq30Bl8vLyoFarYWfHrxKSOYGojtDr9YJKpRKWL19e6fGMjAwBgHD06NEq+4iMjBT69+8vCIIgXLt2TQAg7N27t9K2rq6uwsqVK6sV25o1awSlUllh/6OPPipMnjy5Wn2UmzFjhtCmTRuzziHrUJf/Bv7t6tWrQkBAgPDWW2/d1flEdQmHyajOSEtLQ1FREbp161btcxYvXoygoCDUr18fLi4uWLZsGTIzMwEAnp6eGDZsGEJDQ9G7d2/ExcXhypUr4rkTJkzA8OHDERISgnnz5uHcuXMWvycic8jlb0Cv16NXr14IDAxETEyMWfdIVBcxGaI6w9HR0az269atw8SJExEREYGdO3ciJSUFr7zyCoqLi8U2K1asQGJiIjp27Ij169ejWbNm4hyfmJgYpKamolevXtizZw8CAwOxadOmSq/l6+uL4uJi3Lhxw2R/dnY2fH19zbtRoirU5b+Bcjdv3kSPHj3g6uqKTZs2mTWsRlRXMRmiOqNp06ZwdHTE7t27q9X+wIED6NixI0aPHo127dqhSZMmlf7Ltl27dpg2bRoSEhLQqlUrrF27VjzWrFkzREVFYefOnejXrx9WrFhR6bWCgoJgb29vElt6ejoyMzMRHBxs5p0SVa4u/w0AZRWh7t27Q6lUYsuWLXBwcDD/JonqIM56ozrDwcEBU6ZMweTJk6FUKtGpUydcvXoVqamplT6+27RpU6xevRo7duyAVqvFV199haSkJGi1WgBARkYGli1bhj59+kCj0SA9PR1nzpzBkCFDUFBQgEmTJmHAgAHQarW4dOkSkpKS0L9//0pjc3NzQ0REBCZMmABPT0+o1WqMHTsWwcHB1Z48ffbsWeTn50On06GgoEBcOC8wMBBKpfLufmh0X6nLfwPlidDt27fx9ddfQ6/XQ6/XAwDq168PW1vbmvvBENW02p60RPRPBoNBePfdd4WGDRsK9vb2QkBAgPDee+8JglBx8mhhYaEwbNgwwc3NTXB3dxdef/11YerUqeLkZJ1OJ4SFhQl+fn6CUqkUGjZsKERHRwsGg0EoKioSBg0aJPj7+wtKpVLQaDTCmDFjhIKCgipjKygoEEaPHi14eHgITk5OwnPPPSdcuXKl2vf21FNPCQAqbBkZGXf746L7UF39G/jll18q/f3l7zDdD/jWeiIiIrJqnDNEREREVo3JEJEFjBo1Ci4uLpVuo0aNqu3wiIjoDjhMRmQBOTk54mTSf1Or1fD29r7HERERUXUxGSIiIiKrxmEyIiIismpMhoiIiMiqMRkiIiIiq8ZkiIiIiKwakyEiqtKwYcMQFhYmfu7SpQvGjx9/z+PYu3cvFApFhRfl/pNCocDmzZur3WdMTAzatm0rKa7z589DoVCIr1YhInliMkQkM8OGDYNCoYBCoYBSqUSTJk0wa9YslJaW1vi1v//+e8yePbtabauTwBAR1QV8USuRDPXo0QMrVqxAUVERfvzxR0RGRsLe3h7Tpk2r0La4uNhiL4L19PS0SD9ERHUJK0NEMqRSqeDr64uGDRvi9ddfR0hICLZs2QLg76GtOXPmQKPRoHnz5gCAixcvYuDAgXB3d4enpyf69u2L8+fPi30aDAZMmDAB7u7u8PLywuTJk/HvZcj+PUxWVFSEKVOmwN/fHyqVCk2aNMEXX3yB8+fPo2vXrgAADw8PKBQKDBs2DABgNBoxd+5caLVaODo6ok2bNvj2229NrvPjjz+iWbNmcHR0RNeuXU3irK4pU6agWbNmcHJyQqNGjTB9+nSUlJRUaPfZZ5/B398fTk5OGDhwIPLy8kyOf/7552jZsiUcHBzQokULfPrpp2bHQkR1G5MhovuAo6MjiouLxc+7d+9Geno6du3ahW3btqGkpAShoaFwdXXFr7/+igMHDsDFxQU9evQQz5s/fz5WrlyJL7/8Er/99htyc3OxadOmO153yJAh+Oabb7Bo0SKkpaXhs88+g4uLC/z9/fHdd98BANLT03HlyhXExcUBAObOnYvVq1dj6dKlSE1NRVRUFF566SXs27cPQFnS1q9fP/Tu3RspKSkYPnw4pk6davbPxNXVFStXrsQff/yBuLg4LF++HAsXLjRpc/bsWWzYsAFbt25FfHw8jh49itGjR4vH16xZg+joaMyZMwdpaWl47733MH36dKxatcrseIioDpP+4nsiupeGDh0q9O3bVxAEQTAajcKuXbsElUolTJw4UTzu4+MjFBUVied89dVXQvPmzQWj0SjuKyoqEhwdHYUdO3YIgiAIfn5+QmxsrHi8pKREaNCggXgtQRCEp556SnjjjTcEQRCE9PR0AYCwa9euSuP85ZdfBADC9evXxX2FhYWCk5OTkJCQYNI2IiJCGDx4sCAIgjBt2jQhMDDQ5PiUKVMq9PVvAIRNmzZVefyDDz4QgoKCxM8zZswQbG1thUuXLon7fvrpJ8HGxka4cuWKIAiC0LhxY2Ht2rUm/cyePVsIDg4WBEEQMjIyBADC0aNHq7wuEdV9nDNEJEPbtm2Di4sLSkpKYDQa8eKLLyImJkY83rp1a5N5QseOHcPZs2fh6upq0k9hYSHOnTuHvLw8XLlyBR06dBCP2dnZoX379hWGysqlpKTA1tYWTz31VLXjPnv2LG7fvo1nnnnGZH9xcTHatWsHAEhLSzOJAwCCg4OrfY1y69evx6JFi3Du3Dnk5+ejtLQUarXapE1AQAAeeOABk+sYjUakp6fD1dUV586dQ0REBEaMGCG2KS0thZubm9nxEFHdxWSISIa6du2KJUuWQKlUQqPRwM7O9E/Z2dnZ5HN+fj6CgoKwZs2aCn3Vr1//rmJwdHQ0+5z8/HwAwPbt202SEKBsHpSlJCYmIjw8HDNnzkRoaCjc3Nywbt06zJ8/3+xYly9fXiE5s7W1tVisRFT7mAwRyZCzszOaNGlS7faPPPII1q9fD29v7wrVkXJ+fn44ePAgnnzySQBlFZDk5GQ88sgjlbZv3bo1jEYj9u3bh5CQkArHyytTBoNB3BcYGAiVSoXMzMwqK0otW7YUJ4OX+/333//7Jv8hISEBDRs2xNtvvy3uu3DhQoV2mZmZyMrKgkajEa9jY2OD5s2bw8fHBxqNBn/++SfCw8PNuj4RyQsnUBNZgfDwcNSrVw99+/bFr7/+ioyMDOzduxfjxo3DpUuXAABvvPEG5s2bh82bN+PUqVMYPXr0HdcIevDBBzF06FC8+uqr2Lx5s9jnhg0bAAANGzaEQqHAtm3bcPXqVeTn58PV1RUTJ05EVFQUVq1ahXPnzuHIkSP4+OOPxUnJo0aNwpkzZzBp0iSkp6dj7dq1WLlypVn327RpU2RmZmLdunU4d+4cFi1aVOlkcAcHBwwdOhTHjh3Dr7/+inHjxmHgwIHw9fUFAMycORNz587FokWLcPr0aZw4cQIrVqzAggULzIqHiOo2JkNEVsDJyQn79+9HQEAA+vXrh5YtWyIiIgKFhYVipejNN9/Eyy+/jKFDhyI4OBiurq547rnn7tjvkiVLMGDAAIwePRotWrTAiBEjcOvWLQDAAw88gJkzZ2Lq1Knw8fHBmDFjAACzZ8/G9OnTMXfuXLRs2RI9evTA9u3bodVqAZTN4/nuu++wefNmtGnTBkuXLsV7771n1v326dMHUVFRGDNmDNq2bYuEhARMnz69QrsmTZqgX79+6NmzJ7p3746HH37Y5NH54cOH4/PPP8eKFSvQunVrPPXUU1i5cqUYKxHdHxRCVbMjiYiIiKwAK0NERERk1ZgMERERkVVjMkRERERWjckQERERWTUmQ0RERGTVmAwRERGRVWMyRERERFaNyRARERFZNSZDREREZNWYDBEREZFVYzJEREREVo3JEBEREVm1/wfyRKcs32fgAQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/new_df/best_model_by_class2\")"
      ],
      "metadata": {
        "id": "EN4s0hM1fmuW",
        "outputId": "fb5d5448-5e33-4016-cdaa-433a55ac6ef0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}